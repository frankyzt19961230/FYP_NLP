9781118619988 Linear Systems         lege relege labora et invenies Linear Systems               Henri Bourlès                           First published 2010 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons Inc. Adapted and updated from Systèmes linéaires published 2006 in France by Hermes Science/Lavoisier  © LAVOISIER 2006  Apart from any fair dealing for the purposes of research or private study or criticism or review as permitted under the Copyright Designs and Patents Act 1988 this publication may only be reproduced stored or transmitted in any form or by any means with the prior permission in writing of the publishers or in the case of reprographic reproduction in accordance with the terms and licenses issued by the  CLA. Enquiries concerning reproduction outside these terms should be sent to the publishers at the undermentioned address:  ISTE Ltd  John Wiley & Sons Inc.  27-37 St George’s Road  111 River Street London SW19 4EU Hoboken NJ 07030 UK  USA  www.iste.co.uk  www.wiley.com © ISTE Ltd 2010  The rights of Henri Bourlès to be identified as the author of this work have been asserted by him in accordance with the Copyright Designs and Patents Act 1988.  Library of Congress Cataloging-in-Publication Data  Bourlès Henri.   [Systèmes linéaires. English]   Linear systems / Henri Bourlès.        p. cm.   Includes bibliographical references and index.   ISBN 978-1-84821-162-9  1.  Linear systems.  I. Title.    QA402.B62713 2010   003'.74--dc22 2010016973  British Library Cataloguing-in-Publication Data A CIP record for this book is available from the British Library  ISBN 978-1-84821-162-9 Printed and bound in Great Britain by CPI Antony Rowe Chippenham and Eastbourne.   ContentsPreface.........................................xiiiChapter1.PhysicalModels............................11.1.Electricsystem................................11.1.1.Meshrule................................11.1.2.Nodalrule................................21.2.Mechanicalsystem..............................31.2.1.Fundamentalprincipleofdynamics.................31.2.2.Lagrangianformalism.........................111.3.Electromechanicalsystem..........................131.4.Thermalhydraulicsystem..........................141.4.1.Balanceinvolume...........................141.4.2.Exitrate:Torricelli’sformula.....................151.4.3.Energybalance.............................151.5.Exercises...................................16Chapter2.SystemsTheory(I)...........................192.1.Introductoryexample.............................192.2.Generalrepresentationandproperties...................202.2.1.Variables................................202.2.2.Equations................................212.2.3.Time-invariantsystems........................212.2.4.Linearsystems.............................212.2.5.Lineartime-invariantsystems.....................222.2.6.Equilibriumpoint...........................232.2.7.Linearizationaroundanequilibriumpoint.............242.3.Controlsystems................................252.3.1.Inputs..................................252.3.2.Outputs.................................272.3.3.Latentvariables.............................28vviLinearSystems2.3.4.Classiﬁcationofsystems.......................302.3.5.Rosenbrockrepresentation......................312.3.6.State-spacerepresentation.......................332.3.7.Polesandorderofasystem......................332.3.8.Freeresponseandbehavior......................332.4.Transfermatrix................................362.4.1.Laplacetransforms...........................362.4.2.Transfermatrix:deﬁnition.......................372.4.3.Examples................................382.4.4.Transmissionpolesandzeros.....................392.4.5.*MacMillanpolesandzeros.....................412.4.6.Minimalsystems............................442.4.7.Transmissionpolesandzerosatinﬁnity...............462.5.Responsesofacontrolsystem.......................482.5.1.Input–outputoperator.........................482.5.2.Impulseandstepresponses......................512.5.3.Properbiproperandstrictlypropersystems.............512.5.4.Frequencyresponse..........................552.6.Diagramsandtheiralgebra.........................562.6.1.Diagramofacontrolsystem.....................562.6.2.Generalalgebraofdiagrams.....................572.6.3.Speciﬁcityoflinearsystems......................602.7.Exercises...................................61Chapter3.Open-LoopSystems..........................633.1.Stabilityandstaticgain...........................633.1.1.Stability.................................633.1.2.Staticgain................................643.2.First-ordersystems..............................653.2.1.Transferfunction............................653.2.2.Timedomainresponses........................653.2.3.Frequencyresponse..........................673.2.4.Bodeplot................................673.2.5.Caseofanunstableﬁrst-ordersystem................693.3.Second-ordersystems............................703.3.1.Transferfunction............................703.3.2.Timedomainresponses........................713.3.3.Bodeplot................................753.4.Systemsofanyorder.............................783.4.1.Stability.................................783.4.2.Decompositionofthetransferfunction...............793.4.3.AsymptoticBodeplot.........................803.4.4.Amplitude/phaserelation.......................823.5.Time-delaysystems.............................85Contentsvii3.5.1.Leftformtime-delaysystems.....................853.5.2.Transferfunction............................863.5.3.Bodeplot................................863.5.4.Example:ﬁrst-ordertime-delaysystem...............863.5.5.Approximationsofatime-delaysystem...............873.6.Exercises...................................91Chapter4.Closed-LoopSystems.........................954.1.Closed-loopstability.............................954.1.1.Standardfeedbacksystem.......................954.1.2.Closed-loopequations.........................954.1.3.Stabilityofaclosed-loopsystem...................974.1.4.Nyquistcriterion............................994.1.5.Smallgaintheorem...........................1034.2.Robustnessandperformance........................1044.2.1.Generalities...............................1044.2.2.Robustnessmargins..........................1054.2.3.UseoftheNicholschart........................1104.2.4.Robustnessagainstneglecteddynamics...............1114.2.5.Performance..............................1134.2.6.Sensitivitytomeasurementnoise...................1144.2.7.LoopshapingofL(s).........................1154.2.8.Degradationofrobustness/performancetrade-off..........1164.2.9.*ExtensiontotheMIMOcase....................1194.3.Exercises...................................127Chapter5.CompensationandPIDController.................1295.1.Onedegreeoffreedomcontroller......................1295.1.1.Closed-loopsystem..........................1295.1.2.Closed-loopequationsandstaticerror................1295.2.Leadcompensator..............................1315.2.1.Characteristicsofaleadcompensator................1315.2.2.Principlesofaleadcompensator...................1325.2.3.PDcontroller..............................1345.3.PIcontroller..................................1355.3.1.Principle.................................1355.3.2.Example.................................1365.4.PIDcontroller.................................1385.4.1.Integralactionandleadcompensator.................1385.4.2.ClassicformofaPIDcontroller...................1395.5.Exercises...................................142Chapter6.RSTController.............................1436.1.StructureandimplementationofanRSTcontroller............143viiiLinearSystems6.2.Closedloop..................................1456.2.1.Closed-loopequations.........................1456.2.2.Poleplacementandstability......................1466.3.Usualcase...................................1466.3.1.Disturbancerejection.........................1466.3.2.Absenceofstaticerror.........................1476.3.3.Measurementnoiseﬁltering......................1476.3.4.Problemresolution...........................1486.3.5.Choiceofthepoles...........................1506.3.6.Examples................................1566.4.*Generalcase.................................1626.4.1.Disturbancerejection.........................1626.4.2.Referencetracking...........................1626.4.3.Internalmodelprinciple........................1636.4.4.Filteringofmeasurementnoise....................1646.4.5.Problemresolution...........................1646.4.6.Choiceofpoles.............................1666.4.7.Examples................................1676.5.Exercises...................................171Chapter7.SystemsTheory(II)..........................1757.1.Structureofalinearsystem.........................1767.1.1.*Thenotionofalinearsystem....................1767.1.2.State-spacerepresentation.......................1767.1.3.Controllability.............................1787.1.4.Observability..............................1827.1.5.Canonicalstructureofasystem....................1867.2.Zerosofasystem...............................1897.2.1.Invariantzerosandtransmissionzeros................1897.2.2.Input-decouplingzeros........................1907.2.3.Output-decouplingzeros........................1917.2.4.Input–ouputdecouplingzeros.....................1927.2.5.Hiddenmodes.............................1927.2.6.Relationshipsbetweenpolesandzeros................1947.3.Stabilitystabilizabilityanddetectability..................1957.4.Realization..................................1977.4.1.Introduction...............................1977.4.2.SISOsystems..............................1977.4.3.*MIMOsystems............................2027.5.Flatness....................................2097.5.1.*Flatnessofnonlinearsystems....................2107.5.2.Flatnessoflinearsystems.......................2107.6.Exercises...................................211ContentsixChapter8.StateFeedback.............................2178.1.Elementarystatefeedback..........................2178.1.1.Generalprinciple............................2178.1.2.Poleplacementbystatefeedback...................2198.1.3.ChoiceofthepoleplacementintheSISOcase...........2238.1.4.*ChoiceofthepoleplacementintheMIMOcase.........2248.2.Statefeedbackwithintegralaction.....................2378.2.1.Insufﬁciencyofstatefeedback....................2378.2.2.Feedbackcontrolinthepresenceofdisturbances..........2378.2.3.Resolutionofthestaticproblem...................2388.2.4.Resolutionofthedynamicproblem.................2398.3.*Internalmodelprinciple..........................2438.3.1.Problemsetting.............................2438.3.2.Solution.................................2458.4.Exercises...................................248Chapter9.Observers................................2519.1.Full-orderobservers.............................2519.1.1.Generalprinciple............................2519.1.2.Statefeedback/observersynthesis..................2539.1.3.Statefeedback/observersynthesisandRSTcontroller.......2559.1.4.LTRmethod...............................2579.2.Statefeedback/observersynthesiswithintegralaction..........2649.2.1.Problemsetting.............................2649.2.2.Algebraicsolution...........................2659.2.3.ExtensionoftheLTRmethod.....................2679.3.*Generaltheoryofobservers........................2759.3.1.Reduced-orderobserver........................2759.3.2.Generalformalism...........................2779.4.Exercises...................................279Chapter10.Discrete-TimeControl........................28110.1.Introduction.................................28110.2.Discrete-timesignals............................28110.2.1.Discretizationofasignal.......................28110.2.2.z-transform..............................28210.2.3.Sampledsignal............................28210.2.4.Poissonsummationformula.....................28310.2.5.Samplingtheorem..........................28410.2.6.Hold..................................28810.3.Discrete-timesystems...........................28910.3.1.Generaldescription..........................28910.3.2.Sampledsystem............................29010.3.3.Discretizedsystem..........................291xLinearSystems10.3.4.State-spacerepresentationofadiscrete-timesystem.......29310.3.5.Calculationofthestateofadiscretizedsystem..........29510.4.Structuralpropertiesofdiscrete-timesystems..............29610.4.1.Polesandzeros............................29610.4.2.Controllability.............................29710.4.3.Observability.............................30110.4.4.Rosenbrockrepresentation......................30410.4.5.Stability................................30410.5.Pseudocontinuoussystems.........................30610.5.1.Bilineartransform..........................30610.5.2.Pseudocontinuousrepresentations..................30810.5.3.*Intrinsicdeﬁnitionofapseudocontinuoussystem........31010.5.4.Structuralpropertiesofpseudocontinuoussystems........31310.6.Synthesisofdiscrete-timecontrol.....................31310.6.1.Directapproaches...........................31310.6.2.Discretizationbyapproximation..................31310.6.3.Passagethroughapseudocontinuoussystem............31410.7.Exercises...................................319Chapter11.Identiﬁcation.............................32111.1.Randomsignals...............................32111.1.1.Momentsoforder1and2......................32211.1.2.Correlationandcross-correlation..................32211.1.3.Pseudo-randomsignals........................32511.1.4.Filteringandfactorization......................32611.1.5.Ergodicrandomsignals.......................32911.2.Open-loopidentiﬁcation..........................33111.2.1.Notation................................33111.2.2.Leastsquaresmethod.........................33111.2.3.Modelsandprediction........................34011.2.4.OutputErrormethodandARMAXmethod............34211.2.5.Consistencyoftheestimatorandresidues.............34411.2.6.Filteringofdata............................34711.3.Closed-loopidentiﬁcation.........................35611.3.1.Directandindirectapproach.....................35611.3.2.Consistencyofestimatorinthedirectapproach..........36011.3.3.Athirdpath..............................36311.4.Exercises...................................365Chapter12.Appendix1:Analysis........................36912.1.Topology...................................36912.1.1.Topologicalspaces..........................36912.1.2.Topologicalvectorspaces......................37112.1.3.Continuouslinearoperators.....................374Contentsxi12.2.Sequencesfunctionsanddistributions..................37512.2.1.Sequences...............................37512.2.2.Functions...............................37712.2.3.Distributions..............................38012.3.FourierLaplaceandztransforms.....................38712.3.1.Fouriertransformsofdistributions.................38712.3.2.Fourierseries.............................38912.3.3.Fouriertransformsofsequences...................39312.3.4.Laplacetransform...........................39412.3.5.z-transform..............................40112.4.Functionsofonecomplexvariable....................40412.4.1.Holomorphicfunctions........................40412.4.2.Functionsofamatrix.........................40612.4.3.Integrationinthecomplexplane..................40812.4.4.Applicationstoinversetransforms.................41212.4.5.Argumentprinciple..........................41612.5.Differentialequations............................41712.5.1.Generalities..............................41712.5.2.Lineardifferentialequations:constantcoefﬁcients........42112.6.Functionsofseveralvariables;optimization...............42812.6.1.FunctionsofclassC1.........................42812.6.2.FunctionsofclassC2.........................42912.6.3.Taylor’sformula...........................42912.6.4.Convexitycoercivityellipticity...................43012.6.5.Optimizationalgorithms.......................43212.7.Probabilisticnotions............................43812.7.1.Probabilityspace...........................43812.7.2.Randomvariable...........................43912.7.3.Conditionalexpectation.......................443Chapter13.Appendix2:Algebra.........................44713.1.Commutativeringsandﬁelds.......................44713.1.1.Generalities..............................44713.1.2.Divisibility...............................45013.1.3.Principalidealdomains.......................45313.1.4.Matricesovercommutativerings..................45513.1.5.Bézoutequation............................46013.2.Matricesoverprincipalidealdomains..................46313.2.1.Invertiblematricesoverprincipalidealdomains..........46313.2.2.Hermiteform.............................46313.2.3.Smithform..............................46413.2.4.Elementarydivisors..........................46813.2.5.Smithzeros..............................46913.2.6.Divisibilityofmatrices........................470xiiLinearSystems13.2.7.Coprimefactorizations........................47113.2.8.Bézoutmatrixequations.......................47213.3.Homomorphismsofvectorspaces.....................47313.3.1.Vectorspaces.............................47313.3.2.Homomorphismsandmatrices...................47513.3.3.Endomorphismsofvectorspaces..................47913.3.4.*Jordanform.............................48513.4.*Thelanguageofmodules.........................49113.4.1.Generalnotions............................49113.4.2.Modulesoverprincipalidealdomains...............49613.4.3.Structureofendomorphisms.....................50113.5.Orthogonalityandsymmetry........................50413.5.1.Orthonormalbasis..........................50513.5.2.Orthogonality.............................50513.5.3.Adjointendomorphism........................50613.5.4.Unitaryendomorphism........................50713.5.5.Normalendomorphism........................50713.5.6.Self-adjointendomorphism.....................50813.5.7.Singularvalues............................51013.6.Fractionsandspecialrings.........................51513.6.1.Rationalfunctions..........................51513.6.2.Algebra(cid:1)H∞.............................51613.6.3.*AlgebraH∞.............................51813.6.4.*Classiﬁcationofrings........................51913.6.5.*Changeofrings...........................519Chapter14.SolutionsofExercises........................52114.1.ExercisesofChapter1...........................52114.2.ExercisesofChapter2...........................52214.3.ExercisesofChapter3...........................52414.4.ExercisesofChapter4...........................52614.5.ExercisesofChapter5...........................52814.6.ExercisesofChapter6...........................52914.7.ExercisesofChapter7...........................53114.8.ExercisesofChapter8...........................53714.9.ExercisesofChapter9...........................54014.10.ExercisesofChapter10..........................54314.11.ExercisesofChapter11..........................549Bibliography.....................................553Index..........................................561PrefaceThenotionofsystemThenotionofsystemisthebasicconceptofcontroltheory.Whatisasystem?Itisquitedifﬁculttoaddressthisquestioninallitsaspects.Wecansaysomewhatlooselythatitisanentityconsistingofinteractingparts;anentitythatisitselfmostoftenalsointeractingwithothersystems.Thesolarsystemacomputersystemetc.areexamplesofasystem.Thecontrolsystemanalystisinterestedinsystemswhichareatleastinpartdesignedandconstructedbymaninordertobeutilized–acarengineortheentireautomobile;analternatorinapowerstationorthe“powersystem”initsentirety(consistingofproductioncenterslinesofenergytransportandconsumptioncenters);anairplanesuchastheA380Airbuswherecontrolsystemsplayaveryimportantrole;afactoryproductionlineetc.Wecanactuponthesesystemsandthesereacttoactionsexertedonthem.ObjectivesofsystemstheorySeveralobjectivesexistincontrolsystemstheory.ModelingTheabove-mentionedsystemsarepartofthematerialworldandarethusgovernedbythelawsofphysics.Byputtingthesysteminanequationbasedontheselawsweobtainamathematicalmodelwhichwillgreatlyfacilitateitsunderstanding.Thereforemodelingisoneoftheimportantactivitiesofthecontrolsystemsanalyst.Themodelingprocessdoesnotalwaysstartwiththelawsofphysics;itcansimplybebasedonamoreorlessempiricalandqualitativeobservationofthebehaviorofthesystem.Withintheframeworkofthisbookwewillhoweverlimitourselvestocaseswhereamathematicaldescriptionofthesystembehaviorispossible.Ingeneralthemodelobtainedconsistsofasetofdifferentialequations(sometimesofpartialdifferentialequations)orofdifferenceequations.xiiixivLinearSystemsIdentiﬁcationThemodelingprocessasunderstoodinvolvesdeterminingthestructureoftheequationswhichgovernthebehaviorofthesystemandalsoinﬁxingapriorithevaluesofcertainsystemparameters:forexamplethelengthsmassesresistancevaluescapacitancevaluesetc.Butitisoftenimpossibletocometoanaprioricompleteandpreciseunderstandingofalltheparametersofthismodel.Inordertoreﬁneandcompletethisunderstandingitbecomesnecessarytoproceedtoanidentiﬁcationofthesystem:fromthereactionsofthelattertoknownandgivenstimulationswecanundercertainconditionsidentifytheyetunknownparameters.Identiﬁcationisoneofthemajoraspectsincontroltheory.AnalysisOncethesystemismodeledandidentiﬁeditbecomespossibletoanalyzeitsbehavior.Thisanalysiscanbeverycomplex.AsanexampletheanalysisoftheEuropeanandNorthAmericanelectricsystemsisdifﬁcultandrequirespowerfulcomputingresourcesandenormousdatabasesbecausewedealwithverylargesystems.Andunderstandingthesesystemswellisessentialforsecurityreasons:pointssuchasthepossibilityof“black-out”risk?Ingeneraltheanalysisofasystemmakesitpossibletodetermineitsessentialproperties.ControlThussystemstheoryisa“theoretical”scienceinwhichoneofitsobjectivesisknowingsystemsthroughmodelingidentiﬁcationandanalysis.Butitisalso(wemaybetemptedtosayessentially)a“practical”scienceascienceof“action”.Wetrytounderstandsystemsinordertobeabletocontrolthemandtoregulatetheminthebestpossibleway.Thelastmajorchapterofsystemstheoryisthatofcontrol.Open-loopandclosed-loopsystemsAfundamentaldifferenceexistsbetween“open-loop”and“closed-loop”systems.Controllinganopen-loopsystemisdoingitblindlywithouttakingintoaccounttheresultsoftheactiontaken.Weknowtheexpression:“Icouldwalkdownthispathblind-folded”.BecauseIknowatwhichmomentIneedtoturnleftandthenright.IknowwhenIneedtoacceleratewhentoslowdown...AnditistruethatifaswithLaplace’sgeniuswehadaperfectknowledgeoftheworldwecouldcontrolinopen-loopeverysystembelongingtoit.Howeverourknowledgeofthingsisincomplete.Evenwitharouteweknowbyheartunpredictableeventsmayoccur:achildunexpectedlycrossingthestreetaspellofrainmakingtheroadmoreslipperythanusualetc.Itisthusnecessarytonoteatalltimesandtoadjustactionstakingintoaccounttherealitythatappearsfrommomenttomoment.Controltheoryistheart(orscience)ofmakingthisunceasingadjustmentwhichwecallalooporafeedbackoraservo-mechanism.Itisacommonconceptwhichneverthelessposesnumerousproblems.PrefacexvOneofthemajordifﬁcultiesencounteredwithfeedbacksystemsistheirpossibleinstability.Therearecertainlyunstableopen-loopsystemsbuttheyarerelativelyrare:ahelicopteranalternatorconnectedwithalongpowerlinecertaintypesofﬁghterjetsetc.Ontheotherhandnothingismorecommonplacethanmakingasystemunstablewithapoor-performingfeedbackloop.Letusthinkofachildstrivingtotakeashower:whenthewateriscoldhewantstoheatitupturnsthehotwatertapontooquicklyandgetsboilinghotwater;thenover-doingitthesamewaywiththecoldwatertaphemakesthewatericecoldandsoon.Thenotionoffeedbackcontrolisthusverypowerfulbutcannotbeappliedwithoutproperknowledge.PresentationThisbookisdividedintochaptersandsections;forexamplesection2.1istheﬁrstsectionofChapter2andsection2.1.3isthethirdsubsectionofsection2.1.Itcontainsthebasicsthatanon-specializedengineermustknowincontrolengineering.IhadtheopportunitytoteachthiscourseatConservatoireNationaldesArtsetMétiers(Paris)severalengineeringinstitutesGrandesEcolesEcoleNormaleSupérieuredeCachanandforthemostdifﬁcultpartsatParisXIUniversity(Master2level).ThecoursecontainedinthisbookisprogressivemakingitaccessibletoanyreaderwithanL2levelinscienceandincludesmanyexamples.Neverthelesstomakeitcoherentpassages(sectionsorgroupsofsentences)hadtobeincludedintheﬁrstchaptersthatcalluponsomewhatmoredifﬁcultnotionswhichmayneedasecondorthirdreading.Thesepassagesareprecededbyasterisksforsectionsandsituatedbetweentwoasterisksforgroupsofwordsorsentences.Thepurposeofthisbookistostudysystemmodelingidentiﬁcationanalysisandcontrol.Amongthesefourthemesmodelingisperhapsthetrickiestproblem:toknowhowtomodelelectricalmechanicalthermalhydraulicorothersystems–astheyarecomplex–apersonhastobeanelectricalmechanicalthermalhydraulicengineer.Itisphysicsingeneralallofphysicswhichisusefulformodeling;andmodelingwhichisthesubjectmatterofChapter1isnotexclusivetothecontrolengineer.Someexampleswillserveasbasicremindersofhydraulicsandthermodynamics.Remindersrelativetoelectricityaresomewhatlesssuccinct.WithrespecttomechanicsIthoughtthatitwouldbeusefultogointomoredetails.Howevertheonlyobjectiveofthepresentationistoenablethereadertounderstandexamplesandresolveexercises.Obviouslyitcannotreplaceatreatiseonmechanics.FromChapters2to11thecontrolengineerﬁndshimself/herselfinhis/herownprivatedomain.Thesechaptersdealwithlinearsystemsanalysiscontrolandidentiﬁcation.IhaveputtogethersomemathematicalelementswhichIdeemedessentialintheappendicesincludedinChapters12and13.ThiswillsparethereaderxviLinearSystemsfromconstantlyreferringtothebibliography:elementsofanalysisﬁrstandthenofalgebra.Someofthem(Smithformofapolynomialmatrix(cid:1)H∞algebramoduletheoryetc.)areprobablynewtomostreaders.ButIpreferrednottodiscusstheminthemainbodyofthetexttosavethelattertotheextentpossibleforwhattrulycomeswithinthescopeofcontroltheory.LikewisetheLaplacetransformandthez-transformwhicharemathematicaltoolsthatclassicallyappearhighintextbooksoncontroltheoryareincludedintheseappendices.Theseareorganizedasapresentationof“mathematicsforsystemstheory”withproofswhentheyareconstructiveorsimplyusefulforthereader’sunderstanding.Thereadercanthuschoosetorefertotheappendicesifneededortoreadthementirelyasactualchapters.Ihavedecidednottopresentthetheoryofmeasureandintegration.ConcerningthelatterIreferthereadertothesecondvolumeof[35]orto[103].Iachievedthiswiththehelpofsomemathematicalgymnasticshereandthere.About80exercisesaregiventothereaderstotestandreﬁnetheirunderstandingofthesubject.Solutionstomostoftheexercisesareprovided(sometimesinbrief)inChapter14.MATLABand/orSCILABﬁlestorunexamplesandsolveexercisescanbefoundatwww.iste.co.uk/bourles/LS.zip.CourseoutlineBeginner’scourseThebeginneratL3(i.e.third-yearundergraduate)levelcanstartwiththeeasiestpartsofChapter1.Thepartthatmightbedifﬁcultforthebeginnerissection1.2whichisdevotedtothemodelingofamechanicalsystem.Iadvisethereadertoapproachthispartinapragmaticwayusingtheexamplesandexerciseswhichshowwhatresultsareessentialandhowtomakegooduseofthem.InChapter2readersshouldleaveasideeverythingthatrelatestomulti-inputmulti-outputsystems.Itisbetteriftheyfocuson“left-forms”“right-forms”andonmethodswhichmakeitpossibletoobtainsystemtransferfunctions.Theywilleventuallydevelopparticularinterestintreatedexamplesandexercises.ThereadershouldstudyChapters3to6indepth(leavingasideofcoursethestarredpassages“intendedforasecondreading”).Frommypointofviewthereader’scoursethereforeendswiththeRSTcontroller(“usualcase”section).AdvancedcourseThereaderatM1(i.e.ﬁrst-yeargraduate)levelshouldbegintogodeeperintowhathe/sheonlyskimmedoverwhenhe/shewasabeginneri.e.section1.2ofChapter1andChapter2.AfterwardsthereaderwillresolutelymoveontoChapter7whichincludescomplementarymaterialonsystemstheoryandwillthenbereadytostudyPrefacexviiChapters8to11.Iadvisethereadertosystematicallyskipthepassagesrelatedtomoduletheory.AdvancedgraduatelevelThereaderatM2(i.e.second-yeargraduate)levelstillhasmanythingstodiscoverbeforelaunchingintospecializedcourses.Theseareallthepassagesthathe/sheskippedduringpreviousreadingsinparticularthosepresentingthetheoryofsystemsinthelanguageofmodules.Itmayalsobeintheinterestofthereaderstosystematicallyre-readtheelementsofmathematicsinAppendices1and2(Chapters12and13).Coherentcourseand“butterﬂy”courseThisbookshouldbereadinamoresystematicwaytounderstandthecontents.Anotherapproachtothisbookisalsopossibleforreaderswhowishtobesystematicwhilenotnecessarilybelongingtotheabovecategories:thesereadersshouldreadthechaptersofthebookwithoutskippinganypassageintheorderIhavepresentedthem(withtheexceptionoftheappendices–Chapters12and13–whichtheyshouldreadﬁrst).Ihaveoftenstartedreadingasciencebookinthatspiritandonlyalackoftimehasmademetakeanoppositeturnwhichisto“ﬂitabout”tryingtogoasfastaspossible(astrategybesideswhichdoesnotalwayspayoff).Idonothaveanywaytoproposetothe“butterﬂy”readersbutIhaveincludedadetailedindexattheendofthebookforthem.ChoicesandnecessitiesSincethisbookisaboutlineartime-invariantsystemsIhavedeemeditessentialtopresentthisconceptcorrectly.Suchasystemcannolongerbeproperlydeﬁnedbyitstransfermatrix(toopoorarepresentation)orbyKalman’sapproachasoneofitsstaterealizationsorbyRosenbrock’sapproachasa“systemmatrix”(toocontingentarepresentation).Wonham’s“geometricapproach”whichwasproposedinthe1980sintheendonlyreformulatesKalman’sapproachinalittlemoremodernmathematicallanguageanddoesnotseemtobeagoodanswer.Followingtheworksofexpertsindifferentialalgebraitappearedthatwhatcontrolengineerscall“system”and“linearsystem”shouldbedeﬁnedasanextensionofadifferentialﬁeldandamoduledeﬁnedoveraringofdifferentialoperatorsrespectively.Intheearly1990sthispropertywasrevealedbyM.Fliesstothecommunityofcontrolengineering(seethesynthesispresentedin[47]).SuchalanguagemayhavediscouragedabeginnerbutitisthisconceptionthatIhaveattemptedtomakethereadergraduallysensitivetoinChapters2and7.Inrealitymoduletheoryatthelevelusedhere(ﬁnitelygeneratedmodulesoverprincipalidealdomains)isverysimpleforareadersomewhataccustomedtoabstraction.ItdoesclarifymanipulationsthatcanbedonewithpolynomialmatricesaswellastheJordancanonicalform.ForthisreasonIhavemadeitthemainthemeof“passagesintendedforathirdreading”.ThereaderwishingtogofurtherintosystemsxviiiLinearSystemstheorycanreferto[47]onnonlinearaspectsand[22]onlineartime-varyingsystems([15]isapreliminaryversionof[22]).Moduletheoryishelpfulinpresenting(inthecaseoflinearsystems)thenotionofﬂatness(duetoFliessandhiscollaborators[48])whichhasbecomeessentialtoefﬁcientlyresolvetheproblemofmotionplanning(seesection7.5).Iwantedtoshowthatcontrolengineeringisnotmagicbutscience.OnthatpremiseIhaveinsistedinChapter4onthelimitationsduetocertaincharacteristicsofthesystemtobecontrolled(unstablepolesandzerosspeciﬁcally).Formoredetailsonthispointsee[51].Thecontrolengineerispermanentlyconfrontedwiththeproblemofuncertaintiesinmodeling.Thusthisbookisconstantlypreoccupiedwiththerobustnessofcontrollaws.Howeverthisisnotabookonrobustcontrol.Thetheoryofrobustnesshasbecomeextremelysophisticatedanditsrecentdevelopmentisnotcoveredhere.Besidesexcellentbooksonthisissue([107]and[122]amongothers)doexist.Ihaveemphasizedinthisbookoncontrol(whetherinpolynomialorstateformalism)thatsystemstabilizationisexceptionallytheroleofthefeedbackloop;thepointisoftentocompeltheoutputofsuchasystemtofollowareferencesignalinspiteofvariousdisturbancesthatitmaybesubjectedto.Thisiswhy–inordertodesignanRSTcontrolleraswellasacontrolbystatefeedbackpossiblywithanobserver–itisessentialtoﬁrstmakethenecessarymodelaugmentations(followingwhatWonhamcalledthe“internalmodelprinciple”).Withoutthisprerequisite“modernmethods”onlygeneratenonsense.Italsoappearedtomethatitwasimportanttoshowthatinthemulti-inputmulti-outputcaseacontrollawwithexecrablequalitiescancorrespondtoagoodpoleplacementbecausethelatterdoesnotdeterminetheformer.Theworstmethod(eventhoughitisquiteelegantfromastrictlytheoreticalpointofview)isprobablytheonethatconsistsofrevertingtoacyclicstatematrixthroughaﬁrstloopandthenproceedingasinthesingle-inputsingle-outputcase.Ihaveabstainedfrompresentingthelinear-quadratic(LQ)optimalcontrolandits“dual”versiontheKalmanﬁlter.FortheconceptionofcontrolbystatefeedbackandobserverIhaveindeedpreferredtoproposeessentiallyalgebraicmethodsforseveralreasons.Oncethesemethodsarewellassimilateditbecomeseasytoimplementanefﬁcient“LQGcontrol”.1ThetheoryofLQGcontrol(whichencompassesLQcontrolandKalmanﬁltertheories)isnowclassicandatabasicleveliswellcoveredintreatisessuchas[1]and[2].Ontheotherhandtheminimizationofacriterionisonlyaconvenientintermediatestepforthecontrolengineertoobtainacontrollawwiththedesiredproperties[78].Anotherelementthatkeptmefromincludingoptimalcontrol1.“LQG”standsfor“LinearQuadraticGaussian”.Prefacexixisthatitwouldhavemadethisbooklonger(especiallytocorrectlypresentcontinuous-timestochasticoptimalcontrolwithinthebackgroundIto’sdifferentialcalculus[68][118]).Ifoundthevolumeofthisbooktobelargeenough.Neverthelessexceptforafewsubtletiesthemethodsproposedinthemulti-inputmulti-outputcaseinChapters8and9onlydifferfromLQcontrolandKalmanﬁlteringintheirpresentationstyle(theminimizationofacriterionisnotpresentedasagoal–seeRemark245insection8.1.4).Iencouragethereadertocontinuethestudyofthisbookalongwiththatof“LQGcontrolwithfrequency-shapedweights”[2]whichisagoodcomplement.OnlyChapter10dealswithdiscrete-timesystemsandcontrol.Allpreviouschapters(includingtheoneonRSTcontroller)arethuspresentedinthecontextofcontinuous-timewhichisabitunusual.DespitetheideasspreadbysomepeopleIhaveexperiencedthatcontinuous-timeformalismoffersmuchmoreﬂexibilityforthedesignofcontrollawsespeciallywithrespecttothechoiceofpolesandthequestionof“roll-off”whichareessentialtorobustness.Oncethesynthesisofcontinuous-timecontrollawsismastereditisverysimpletoswitchtothesynthesisofdiscrete-timecontrollaws–withoutanyapproximation(thispointisofcoursediscussedindetailinChapter10).Chapter11discussesparametricidentiﬁcationofdiscrete-timesystemsbyminimizationofthel2normofthepredictionerror.Thepresentationisrelativelyclassicasfarasopen-loopidentiﬁcationisconcerned;asforclosed-loopidentiﬁcationitessentiallyincludesLjungandForsell’scontribution[82](asacomplementthereadercanconsult[75]andChapter10of[110]).Manyothertopicswouldhavebeenworthpresenting:forexampleanti-windupmethodsinthepresenceofsaturations(whichisclearlyexpoundedin[4])orgainscheduling(seesurveypapers[104]and[80])nottomentionnonlinearcontrol(forageneralpresentationofthisthemesee[108]and[62]andbeginwiththeﬁrstreferencebecauseitissimple;robustnonlinearcontrolisdiscussedin[49];seealso[48]whichdealswithﬂatnessfromboththeoreticalandpracticalviewpointsinadditiontosection7.5ofthisbook).Wecanalsocitethefollowingsubjects:adaptivecontrol(whichisthesubjectofalotofliteraturebut[57][3]and[108]canberecommendedasagoodinitiationonthistheme);fuzzyorneuralcontrol(thereadercanﬁndagenuinelyscientiﬁcpresentationofthelasttwotypesofcontrolin[40]);thecontrolofsystemsgovernedbydelayeddifferentialequationsorpartialdifferentialequations(onthissubjectsee[55]and[33]inthelinearcontextaswellas[58]and[59]wherenonlinearproblemsarediscussed).Asalreadymentionedtheextensionofmanymethodspresentedinthisbooktolineartime-varyingsystemscanbefoundin[22].NoteontheEnglisheditionThisEnglisheditionhasgivenmetheopportunitytocorrectmanyerrorswhichdespiteproof-readinghadbeenleftintheoriginalFrencheditionandalsoincludexxLinearSystemsadditionalinformationwhichmightbeuseful.FurthermoreithasgivenmetheopportunitytocompleteChapter7withasectiononﬂatnessandtoentirelyreviseChapter11onidentiﬁcation:themathematicalbasesincludedthereinaregiveninAppendix1andasectiononclosed-loopidentiﬁcationhasalsobeenadded.Finallythiseditioncontainsadditionalexercisesandoneofthempresentsthebasicsofwhatneedstobeknownaboutthe“deltatransform”.AcknowledgmentsIwouldﬁrstliketothankmy“elders”especiallyE.IrvingI.D.LandauandP.deLarminatwhohavesharedtheirexperiencewithmeduringinformalbutalwayspassionatediscussions.IalsothankM.Fliesswhohasopenedmyeyestothealgebraictheoryofsystems–whichIhopethisbookhasbeneﬁtedfrom–andP.Chantreforourexchangesonidentiﬁcation.IthankmysonNicolaswhohashelpedmeﬁnalizeﬁgures.IalsothankmytranslatorG.K.Kwanforhiseffortsandhiskindness.LastbutnottheleastIthankmywifeCorinnewhosepatiencehasoftenbeenseverelytestedduringthisworkwhichhasdemandedalotoftimeandenergyfromme.Chapter1PhysicalModels1.1.ElectricsystemElectriccircuitscanbemodeledbyapplyingKirchhoff’stwolaws:Kirchhoff’sCurrentLawandKirchhoff’sVoltageLaw.Theyarealsoknownasthenodalruleandthemeshrule.Followingarethetwoexamplesinwhichthesetwolawshavebeenapplied.1.1.1.MeshruleConsiderthe“RLC”circuitshowninFigure1.1.Thiscircuitconstitutesamesh.ThemeshrulestatesthatbetweenanytwopointsonameshpointsAandBinthiscircuitforexamplethepotentialdifferenceisindependentofthepaththatistaken(whichinphysicsisafundamentalpropertyofapotentialinthiscasetheelectricpotential).ThereforewehaveVA−VB=V=VR+VL+VCwhereVRVLandVCarethevoltagesacrosstheresistanceRtheinductanceLandthecapacitanceCrespectively.WehaveVR=RiVL=LdidtVC=1C(cid:1)t−∞i(τ)dτ;thereforeweobtaintheintegro-differentialequationV=Ri+Ldidt+1C(cid:1)t−∞i(τ)dτ.Differentiatingthisequationwegetthefollowinglineardifferentialequation:LCd2idt2+RCdidt+i−CdVdt=0.(1.1)1Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.2LinearSystemsBARLCViVRVLVCFigure1.1.RLCcircuit1.1.2.NodalruleConsidertheelectriccircuitgiveninFigure1.2whichisa“picircuit”usedtorepresentatransmissionlineinthedomainofelectricnetworks.AccordingtothemeshruledescribedabovewehaveVA=1C(cid:2)t−∞i1(τ)dτVB=1C(cid:2)t−∞i2(τ)dτVA−VB=Ri+Ldidt.Thenodalruleisbasedontheconservationofcharge.Accordingtothisruleatanynodeofacircuitwhichcanbeanypointwithinthecircuitthesumofcurrententeringanodeisequaltothesumofcurrentleavingthatnode.Hencewehavethefollowingtwoequations:iA=i+i1andi=iB+i2.Eliminatingthevariableiwecannowre-arrangetheequationsasfollows:⎧⎪⎪⎨⎪⎪⎩CdVAdt−i1=0CdVBdt−i2=0VA−VB−R(iA−i1)−Lddt(iA−i1)=0iA−i1−(iB+i2)=0.(1.2)VBVAiAiiBi1i2RLCCABFigure1.2.PicircuitPhysicalModels31.2.MechanicalsystemTherearetwomainapproachestomodelamechanicalsystem.Theﬁrstoneusesthefundamentalprincipleofdynamicsandisbasedontheanalysisofforcesandmomentsappliedtothevariouselementsofthesystem(particularlyinternalforcesi.e.forcesappliedbysubsystemsononeanother).ThesecondapproachusestheLagrangianformalismandisbasedontheenergyofthesystem(kineticandpotentialenergy).Theanalysisofinternalforcesisthenunnecessary.Thesetwoapproachesaresuccinctlyexpoundedandappliedtotwoexamples.1.2.1.FundamentalprincipleofdynamicsTorsorAtorsorisaﬁeldofantisymmetricvectorsinathree-dimensionalafﬁnespace.Let−→M•beaﬁeldofsuchvectors;thisisafunctionA(cid:3)→−−→MAand−−→MAiscalledthemomentofthetorsor−→M•atpointA.Thereexistsavector−→VcalledthecharacteristicvectorsuchthatforanypointsAandBwehavetherelation−−→MA=−−→MB+−−→AB∧−→V.Thetorsoristhusentirelydeterminedby−→Vandthemoment−−→MAforanypointAunderconsideration.Weareagreedonthenotation(cid:7)−→V(cid:8)Aforthepair(cid:9)−→V−−→MA(cid:10)and(cid:7)−→V(cid:8)forthetorsor−→M•.Thetwovectors−→Vand−−→MAarecalledtheelementsofreductionoftorsor(cid:7)−→V(cid:8)atpointA.Thecomomentof(cid:7)−→V(cid:8)A=(cid:9)−→V−−→MA(cid:10)and(cid:7)−→V(cid:2)(cid:8)A=(cid:9)−→V(cid:2)−−→M(cid:2)A(cid:10)isthescalar−→V.−−→M(cid:2)A+−→V(cid:2).−−→MA.ThiscomomentisinvariantinthesensethatitisindependentofthepointAconsidered(ascaneasilybeveriﬁedbythereader).Thereforethecomomentofthetorsors(cid:7)−→V(cid:8)and(cid:7)−→V(cid:2)(cid:8)iswelldeﬁnedandisdenotedby(cid:7)−→V(cid:8).(cid:7)−→V(cid:2)(cid:8).KinematictorsorAnexampleofatorsoristheﬁeldofvelocitiesofarigidbodyScalledthekinetictorsorofS;thistorsorisdeﬁnedrelativetoanorthonormalframeofreferenceR(wealsosaythetorsorisdeﬁnedinthisframeofreference).ThisistosaythatthetranslationalvelocitiesandangularvelocitiesaremeasuredrelativetotheoriginandalongtheaxesofR.Itscharacteristicvectoristhevectorof“instantaneousrotation”−→ω(thistorsoristhusdenotedby{−→ω}).IndeedaseasilyshownthevelocitiesattwopointsAandBonthisrigidbodyarerelatedby−→vA=−→vB+−−→AB∧−→ω.(1.3)NowletusconsideraframeofreferenceR(cid:2)attachedtotherigidbodyS.ThisconsistsofanoriginAandthreebasisvectors−→eii=123.Put−→ei=−−→AAiwhere4LinearSystemsAi’sarepointsonS.Wehaved−→eidt=−→vAi−−→vA=−−→AiA∧−→ωaccordingto(1.3)andthusd−→eidt=−→ω∧−→ei.Let−→Ybeanyvector;itscoordinateswithinR(cid:2)arethoseYi’ssuchthat−→Y=(cid:11)3i=1Yi−→ei.Differentiatingthisexpressionwegetd−→Ydt=3(cid:12)i=1dYidt−→ei+3(cid:12)i=1Yid−→eidt=3(cid:12)i=1dYidt−→ei+−→ω∧−→Y.Thisquantitydenotingitbyd−→Ydt/Risthederivativeof−→YinthereferenceframeR.Thevector(cid:11)3i=1dYidt−→eiisthederivativeof−→YinthismovingreferenceframeR(cid:2)andisdenotedbyd−→Ydt/R(cid:1).Wehavethusobtainedtheformulaestablishingtherelationbetweenthedifferentiationinamovingreferenceframeandthedifferentiationinaﬁxedreferenceframeasfollows:d−→Ydt/R=d−→Ydt/R(cid:1)+−→ω∧−→Y.(1.4)KinetictorsorKineticmomentThekineticmomentofamaterialsystemS(i.e.acollectionofmaterialpointsandrigidbodies–amaterialpointcanbeconsideredapunctualrigidbody)withrespecttoanypointAinreferenceframeRis−→σA(cid:1)(cid:1)S−−→AM∧−→vMdmwheredmisthedensityofmassatpointM;thisdensityisassumedtobeconstantovertime.1ElementsofreductionTheﬁeldofvectors−→σ•isatorsoraswillbeshownbelowanditiscalledthekinetictorsor.IndeedifBisanyotherpointwehave−→σA=(cid:2)S−−→(AB+−−→BM)∧−→vMdm=−−→AB∧(cid:2)S−→vMdm+(cid:2)S−−→BM∧−→vMdm.NowletObetheoriginofR;wethenget−→vM=d−−→OMdtthus(cid:1)S−→vMdm=ddt(cid:1)S−−→OMdm.BydeﬁnitionofthecenterofmassGofSwehave(cid:2)S−−→OMdm=m−−→OG1.Thesymbol(cid:1)means“equalsbydeﬁnition”.PhysicalModels5wheremisthetotalmassofS.Therefore−→σA=−→σB+−−→AB∧−→p(1.5)where−→pisthemomentumofSdeﬁnedas−→p(cid:1)m−→vG.Accordingto(1.5)−→σ•isactuallyatorsor{−→p}withcharacteristicvector−→p.CaseofarigidbodySupposeSisarigidbodyandAisanypointonthisrigidbody;accordingto(1.3)weget−→vM=−→vA+−−→MA∧−→ω=−→vA+−→ω∧−−→AMthus−→σA=(cid:2)S−−→AM∧(cid:9)−→vA+−→ω∧−−→AM(cid:10)dm=(cid:2)S−−→AMdm∧−→vA+(cid:2)S−−→AM∧−→ω∧−−→AMdm.ThelinearmappingIA:−→ω(cid:3)→(cid:1)S−−→AM∧−→ω∧−−→AMdmiscalledtheinertiatensorofSwithrespecttoAandhencewehave(cid:1)S−−→AM∧−→ω∧−−→AMdm=IA−→ω.(1.6)WecanexpressthistensorintermsofitscoordinatesinanorthonormalsystemofreferenceR(cid:2).Let(xyz)bethecomponentsof−−→AMinR(cid:2).Developingexpression(1.6)thelinearmappingIAisidentiﬁed2withtheinertiamatrixofSwithrespecttoAinR(cid:2)givenbyIA=⎡⎣(cid:1)S(cid:15)y2+z2(cid:16)dm−(cid:1)Sxydm−(cid:1)Sxzdm−(cid:1)Sxydm(cid:1)S(cid:15)x2+z2(cid:16)dm−(cid:1)Syzdm−(cid:1)Sxzdm−(cid:1)Syzdm(cid:1)S(cid:15)x2+y2(cid:16)dm⎤⎦.Wethusobtain−→σA=m−−→AG∧−→vA+IA−→ω.(1.7)ThisexpressioncanbesimpliﬁedwhenA=GorwhenAisaﬁxedpointonS(ifSrotatesaroundthispoint);then(1.7)reducesto−→σA=IA−→ω.(1.8)2.Oncethechoiceofbasesismadealinearmapping(alsocalledahomomorphism)isrepresentedbyamatrix(seesection13.3.2).Abusingthelanguagewecanidentifythislinearmappingwiththismatrix.Andthisiswhatwedohere.“Abuseoflanguage”(inthesensewidelyusedinmathematics)and“abuseofnotation”areconsideredtobesynonymousinthisbook.6LinearSystemsInertiamatrixMatrixIAisonlyconstantwhenthereferenceframeR(cid:2)isrigidlylinkedtorigidbodySanditisthereforeourinteresttolookatthingsinthecontextofthiscase.Ofcoursethisreferenceframeisinmotionandthederivativeof(1.8)isobtainedbyapplying(1.4).OntheotherhandthematrixIAissymmetricrealandthuscanbediagonalizedinanorthonormalreferenceframe(seesection13.5.6)andwhoseaxesarebydeﬁnitiontheprincipalaxesofinertiaofS.ThediagonalelementsobtainedinsuchamatrixarecalledtheprincipalmomentsofinertiaofS(withrespecttoAandrelativetotheprincipalaxesinquestion).TorqueInthecasewhere−→p=0thekineticmomentisindependentofthepointbeingconsideredaccordingto(1.5):foranypointsAandB−→σA=−→σB.Suchakineticmomentiscalledatorquewhichweshalldenoteby−→C.KineticenergyThekineticenergyTofamaterialsystemSisequaltothecomoment{−→ω}.{−→p}.IncaseofarigidbodyweobtainT=12(cid:15)mv2G+−→ω.IG−→ω(cid:16).(1.9)ForcetorsorNowconsiderasetofexternalforces−→f1...−→fnbeingappliedtopointsA1...AnofamaterialsystemS.Theresultantforceisgivenby−→f=(cid:11)nk=1−→fkwhiletheresultantmomentoftheseforceswithrespecttoapointOarbitraryatthistimeis−−→MO=(cid:11)nk=1−−→OAk∧−→fk.IfAisanotherarbitrarypointweimmediatelyobtaintheequality−−→MA=−−→MO+−→AO∧−→f(1.10)whichshowsthattheﬁeld−−→M•deﬁnesatorsorwithcharacteristicvector−→f;thistorsor(cid:7)−→f(cid:8)iscalledtheforcetorsor(ormorepreciselythetorsorofexternalforces).Fundamentalprincipleofdynamics(Newton’slaw)ExpressioninaGalileanreferenceframeLetObeaﬁxedpointinaGalileanreferenceframe(alsoknownasinertialreferenceframe)RandSbeamaterialsystem.ThefundamentalprincipleofPhysicalModels7dynamicsorNewton’slawiswrittenas(cid:7)−→f(cid:8)O=ddt{−→p}O.(1.11)Thisequalitybetweentorsorsshowsthatwehavethefollowingtworelations−→f=d−→pdt(1.12)−−→MO=d−→σOdt.(1.13)Itisreasonabletoemphasizethefactthat(1.11)isvalidinthereferenceframeR(orinanyotherGalileanreferenceframe).Inanon-GalileanframeofreferenceitisnecessarytotakeintoaccountthetorsorofinertialforcesaswellasCoriolisforces[76]–notionsthatcanbederivedfrom(1.4).ExpressioninamovingframeofreferenceﬁrmlyﬁxedinthecenterofmassNeverthelessletusconsiderthecenterofmassGofthematerialsystemS;from(1.5)weget−−→dσGdt=−−→dσOdt+ddt(cid:9)−−→GO∧−→p(cid:10)=−−→MO−−→vG∧−→p+−−→GO∧d−→pdt=−−→MO+−→faccordingto(1.12).Therefore−−→dσGdt=−−→MGwhichshowsthat(cid:7)−→f(cid:8)G=ddt{−→p}G.ExpressioninanarbitrarymovingframeofreferenceLetAbeanyarbitrarypoint.Wehaveaccordingto(1.8)−→σO=−→σA+−→OA∧−→pandthusd−→σOdt=d−→σAdt+−→vA∧−→p+−→OA∧d−→pdt.From(1.10)and(1.12)wealsohave−−→MA=−−→MO+−→AO∧d−→pdt.(1.14)8LinearSystemsFrom(1.13)weobtain−−→MA=d−→σAdt+−→vA∧−→pwhichisageneralizationof(1.13).Movingframeofreference:caseofarigidbodyLetAbeapointonrigidbodyS.Thekineticmoment−→σAisgivenbyexpression(1.7)therefored−→σAdt=m−→AG∧d−→vAdt+m−→vG∧−→vA+IAd−→ωdt+−→ω∧IA−→ω.Accordingto(1.14)weobtainthefollowing:−−→MA=IAd−→ωdt+−→AG∧md−→vAdt+−→ω∧IA−→ω.(1.15)Thisexpressioncanbesimpliﬁedinthefollowingcases:i)AisﬁxedandSrotatesaroundAorA=G(thesecondtermontheright-handsidedropsout);ii)rotationisaroundoneoftheprincipalaxesofinertia(thethirdtermdropsout).CaseofarigidbodyrotatingaroundanaxisConsiderthecasewhereSisarigidbodyrotatingaroundoneoftheprincipalaxesofinertiadenotedby−→Ozforexample.Let−→CbetheresultanttorqueexertedonS(directedalong−→Oz)andwrite−→C=C−→kwhere−→kistheunitvectorof−→Oz.TherotationvectorofSis−→ω=ω−→k.WehaveIO−→ω=J−→ω=Jω−→kwhereJistheprincipalmomentofinertiaofΣwithrespecttotheaxis−→Ozi.e.J=(cid:1)Σ(cid:15)x2+y2(cid:16)dm.(1.16)Equation(1.15)reducestoJdωdt=C.(1.17)PrincipleofactionandreactionInthecaseofamaterialsystemSthatconsistsofNinteractingrigidbodiesSitheglobalequation(1.11)isnotsufﬁcienttodeterminethemotionofeachoftherigidbodies.WethereforeareledtodecomposethesystemintoeachofitselementsSiandtotakeintoaccounttheforcesandmomentstheserigidbodiesexertononeanother.PhysicalModels9z2l+z1kzm1m2fFigure1.3.TrainAccordingtotheprincipleofactionandreactionif(cid:7)−−→fi→j(cid:8)and(cid:7)−−→fj→i(cid:8)representthetorsoroftheforcesexertedonSjbySiandonSibySjrespectivelywehave(cid:7)−−→fi→j(cid:8)+(cid:7)−−→fj→i(cid:8)=0.(1.18)ExampleofafrictionlesstrainConsideratraincomposedofalocomotiveandawagonsuchasdepictedinFigure1.3.Theequationsofthissystemareformulatedwiththehelpof(1.12)and(1.18).Thecouplingbetweenthelocomotiveofmassm1andthewagonofmassm2isrepresentedbyaspringwithconstantk(accordingtoHooke’slaw).Thepositionsofcenterofmassofthelocomotiveandthatofthecenterofmassofthewagonaredenotedbyz2andl+z1wherelisthedistancebetweenthetwocentersofmasswhenthetrainisatrest.Themotionalforceputintoactionbythelocomotiveisdenotedbyf.Thefrictionalforcesareneglectedalongwiththemassofthespring.Theresultantforceexertedonthelocomotiveisthereforef−frwherefristheforceexertedbythespringonthelocomotive.Asaresultwehavem1d2z1dt2=f−frandfr=k(z1−z2)withz1−z2beingthelengtheningofthespring.Thespringtransmitstheintegralforcefraccordingto(1.12).Thewagonisonlysubjecttothisforcefrandwethushavem2d2z2dt2=fr.Eliminatingthevariablefrweobtaintheequations(cid:19)m1d2z1dt2+k(z1−z2)−f=0m2d2z2dt2+k(z2−z1)=0.(1.19)ExampleoftheinvertedpendulumConsidertheinvertedpenduluminFigure1.4.Itconsistsofacarriagewithwhicharodoflengthlterminatedbyamassmisarticulated.Thefrictionalforcesaswell10LinearSystemsy1yMmAfr-frz1fmgFigure1.4.Invertedpendulumasthemassoftherodareconsiderednegligible;inadditionmassmisconsideredpunctual.Theinvertedpendulumisacteduponbyahorizontalforcefthatisexertedontothecarriage.Firstletusexaminetheforcesthatareexertedonthecarriage.Sincethecarriagemoveshorizontallyonlythehorizontalcomponentsoftheforceshavetobeconsidered.Thehorizontalresultantforceonthecarriageisf−frwherefristhehorizontalcomponentoftheforceexertedbytherigidbody(rod+massm)ontothecarriage.Accordingto(1.12)wethushaveMd2ydt2=f−fr.Nowletusexaminetherigidbody(rod+mass)whosecenterofmassGhascoordinates(y1z1).Theonlyhorizontalforceexertedonthisrigidbodyisfrhencemd2y1dt2=fr.Wehavethegeometricalrelations(cid:20)y1=y+lsinθz1=lcosθ.(1.20)Bywayoftheﬁrstoftheseequationsandbyeliminatingfrweobtain(M+m)d2ydt2−mlsinθ(cid:21)dθdt(cid:22)2+mlcosθd2θdt2=f.Wecanwriteequation(1.15)atthepointA.Theterm−→ω∧IA−→ωiszerosincetherotationiseffectedaroundoneoftheprincipalaxesofinertia.Therefore(1.15)iswrittenasIAd−→ωdt=−−→MA−−→AG∧md−→vAdtandthesecondtermontheright-handsidecanbeinterpretedasthemomentwithrespecttoAoftheinertialforce−md−→vAdtPhysicalModels11appliedatG.Theothermomenttobeaccountedforisthatofthegravitationalforce.WethusobtainJd2θdt2=mglsinθ−mld2ydt2cosθwhereJisthemomentofinertiaofmwithrespecttoAi.e.ml2.Finallyweobtain(cid:19)(M+m)d2ydt2−mlsinθ(cid:15)dθdt(cid:16)2+mlcosθd2θdt2−f=0ld2θdt2−gsinθ+d2ydt2cosθ=0.(1.21)1.2.2.LagrangianformalismForcertaintypesofmechanicalsystemsitmaybemoreconvenienttousetheLagrangianformalisminsteadofthefundamentalprincipleofdynamicsfortheformulationoftheequations.Asmentionedaboveoneoftheadvantagesisthattheinternalforcesofthesystemdonothavetobeconsidered.HolonomicsystemAmechanicalsystemSissaidtobeholonomicifthepositionofitsdifferentpartscanbecharacterizedbynindependentvariablesq1...qncalledthegeneralizedcoordinatesofthesystem.WethensaythatSisaholonomicsystemwithndegreesoffreedom.Forexamplethetrainaboveisaholonomicsystemwithgeneralizedcoordinatesq1=y1andq2=y2.Theinvertedpendulumisalsoaholonomicsystemwithgeneralizedcoordinatesq1=yandq2=θ.Thevectorspaceofdimensionninwhichthesecoordinatesaredeﬁnediscalledtheconﬁgurationspace.HolonomicrelationsThevariablesq1...qnareonlyindependentifallexistingrelationsbetweenthesubsystemshavebeentakenintoaccountandallredundantcoordinateshavebeeneliminated.*ForexamplesupposethatwhenallrelationsareremovedthesystemSonlydependsonNcoordinatesq1...qN.LetusalsoassumethattheonlyrelationsbywhichSisconstrainedareprelationsoftheformgk(q1...qN)=0k=1...p(1.22)wheregkarescalarfunctions.Relationsoftheform(1.22)involvingonlythepositionsqiarecalledtheholonomicrelations.Theserelationsareofapuregeometricalnature.Letn=N−pandsupposethattheJacobian∂(g1...gp)∂(qn+1...qN)isnon-zero.ThenaccordingtotheImplicitMappingTheoremwecanexpress(locally)qn+1...qNasafunctionofq1...qnwhichareindependentvariables.InthiscaseSisaholonomicsystemandq1...qnareitsgeneralizedcoordinates.*12LinearSystemsNon-holonomicrelationsThesituationbecomesmorecomplicatedinthecasewheretherelationsinvolvethevelocities.q1....qNaccordingtohj(cid:15)q1...qN.q1....qN(cid:16)=0.Theserelationsofkinematic(notgeometric)naturearecallednon-holonomic.ThenthesystemSitselfisalsocallednon-holonomic.Thissituationwillnotbeconsideredinthesequel.Kineticandpotentialenergies;externalforcesandtorquesLetSbeaholonomicsystemwithndegreesoffreedom.Itskineticenergyisthesumofthekineticenergiesofitssubsystems;itdependsonthetimederivativesofthegeneralizedcoordinatesandveryoftendependsonthesecoordinatesthemselves.ForeachpartoftherigidbodySitisgivenby(1.9).ThekineticenergyofSisdenotedbyT(cid:15)q1...qn.q1....qn(cid:16).InthesamemannerthepotentialenergyofSisthesumofthepotentialenergiesofitssubsystems.Thisenergyonlydependsonthegeneralizedcoordinates;itisdenotedbyU(q1...qn).MoreoverScanalsobesubjectedtoexternalforcesandtorquesi.e.whicharenotderivedfromthepotentialU(whetherwehavechosennottotakethemintoaccountinUorwhethertheycannotbederivedfromanypotential).Sotheforcefhastobetreatedasanexternalforceintheexampleofthetrainandthatoftheinvertedpendulum.GeneralizedforcesConsidertheexternalforcesandtorquesexertingonaholonomicsystemS.WhileoneofthegeneralizedcoordinatesqivarieswithaninﬁnitesimalquantityδqiandwhiletheothergeneralizedcoordinatesremainconstantaninﬁnitesimalworkδWi=Qiδqiisdoneasaresultoftheseexternalforcesandtorques.ThenQiissaidtobethe(external)generalizedforceintheithdirectionoftheconﬁgurationspace–orinshortthegeneralizedexternalforcealongqi.Intheexampleofthetrainfisthegeneralizedexternalforcealongy1whereasintheexampleoftheinvertedpendulumfisthegeneralizedexternalforcealongy.LagrangianequationsTheLagrangianofthesystemisL=T−U(1.23)andtheLagrangianequationsareddt(cid:9)∂L∂˙qi(cid:10)−∂L∂qi=Qii=1...n(1.24)where˙qi(cid:1)dqidt.PhysicalModels13ExampleofthetrainWehaveT=12(cid:15)m1˙y21+m2˙y22(cid:16).ThepotentialenergyisthatofwhatisstoredinsidethespringandisgivenbyU=12k(y1−y2)2.Wecanderive(1.19)from(1.24).ExampleoftheinvertedpendulumThekineticenergyofthecarriageisT1=12M˙y2;andthatoftheassembly(rod+mass)isT2=12m(cid:15)˙y21+˙z21(cid:16).Bywayoftheholonomicrelations(1.20)weobtainT=T1+T2=12(cid:23)M˙y2+m(cid:9)˙y2+2l˙y˙θcosθ+l2˙θ2(cid:10)(cid:24).ThepotentialenergyisthatoftheweightwhichisgivenbyU=mglcosθ.Applying(1.24)weimmediatelyobtain(1.21).1.3.ElectromechanicalsystemAlotofsystemsconsistofanelectricalandamechanicalpart.Thisisthecaseforexampleofanalternatorconnectedtoanelectricnetwork[70].Thisisalsothecaseofasynchronousmotor[112]orthatofanasynchronousmotor[24].Themodelingofthesesystemsrequirestoomuchcircuittheoryandtechniquestobecoveredinthiswork.WewillcontentourselveswithconsideringonlythecaseofaDCmotor[112]eventhoughthereseemstobeatrendwheretheDCmotorisbeingreplacedbythemotorswejustmentioned.Thewindingsoftherotorareconnectedtothearmaturebyringsandbrush.Thestatorgeneratesaﬁxedﬁeld(eitherusingpermanentmagnetsorusingwindingsinwhichaDCcurrentisﬂowing)whereintheturnsoftherotorarerotatedwithangularspeedω.WithavoltageVappliedtothearmatureacurrentipassesthroughtheseturns.ThechargeischaracterizedbyitsmomentofinertiaJ(seeFigure1.5).Armature VRLeiJConstant fieldFigure1.5.DCmotor14LinearSystemsAccordingtothemeshruleV=Ri+Ldidt+ewhereeisthebackelectromotiveforce.AccordingtoFaraday’slawe=dϕdtwhereϕisthemagneticﬂuxinthewindingsoftherotor.Thisﬂuxisconstantwithrespecttothestatorandvariesinproportiontoωandwithrespecttotherotor.Wecanthenlete=Kω;thecoefﬁcientK(cid:5)=0iscalledthemotorconstant.ThepowerexchangedbetweenthestatorandtherotorisP=ei(inelectricalterms)orP=Cmω(inmechanicalterms).ThereforeCmω=KωifromwhichwededucethatCm=Ki.Finallywegetaccordingto(1.17)Jdωdt=Cm−CrwhereCristheresistivetorqueduetofriction.ThisresistivetorqueisoftheformCr=λωwhereλisthefrictionalcoefﬁcient.Inadditiontheangularpositionθoftherotorsatisﬁestheequationdθdt=ω.Wecannowregrouptheequationsinthefollowingform:⎧⎨⎩Ri+Ldidt+Kω−V=0Jdωdt+λω−Ki=0dθdt−ω=0.(1.25)1.4.ThermalhydraulicsystemThermalorhydraulicsystemsarelargelyformulatedusingrelationsofbalance:conservationofmassconservationofenergy.“Mattercanneitherbecreatednordestroyedonlytransformed”.ThisformulaofLavoisierisundoubtedlythemostuniversalprincipleofscience.ConsiderforexampletheheatedtankinFigure1.6.ThistankconsistsofareservoirwithsectionS.ItcontainswaterwhichisheatedbyaresistanceR.Qeistherateofdischargeofwater(ataconstanttemperatureTe)enteringthereservoir.Thetankisemptyingthroughanopeningofcross-sectionσlocatedatthebottomofthereservoir;rateofwaterdischargingfromthetankisdenotedbyQs.ThetemperatureofthewaterinsidethetankisassumedtobeuniformanddenotedbyTs.TheheatingpowerisP.1.4.1.BalanceinvolumeForaninﬁnitesimalintervaloftimedtletdVbetheincreaseinvolumeofwaterinsidethereservoir.WehavedV=SdhwheredhistheincreaseinheightPhysicalModels15RTeQehPTsQsSFigure1.6.Heatedtankofwater.ThevolumeofwaterenteringthereservoirduringthistimeintervalisQedt.AndthevolumeexitingisQsdt.Wehavetherefore(undertheassumptionofincompressibility)Sdhdt=Qe−Qs.1.4.2.Exitrate:Torricelli’sformulaThisformulagivesvelocityvsofwaterexitingthereservoirunderheightofwaterh:vs=√2ghwheregisthegravitationalacceleration.SinceQs=σvsweobtainQs=σ√2gh.1.4.3.EnergybalanceLetdWbetheamountofelectricenergyfurnishedduringtheinﬁnitesimaltimeintervaldt.LetalsodWebethecaloriﬁcenergy(i.e.heatenergy)furnishedbytheliquidenteringthetankanddWibetheincreaseincaloriﬁcenergyintheinteriorofthetankthatproducesvariationofthetemperatureTs.Accordingtotheﬁrstprincipleofthermodynamicsorprincipleofconservationofenergy:dW+dWe=dWi.OntheotherhanddW=Pdt.DenotingbyµthemassofthevolumeofliquidandbycthespeciﬁcheatwehavedWe=Qeµc(Te−Ts)dtanddWi=ShµcdTs.16LinearSystemsVCLR1R2iFigure1.7.ElectriccircuitRe-arrangingtheequationsweobtain(cid:20)Sdhdt+σ√2gh−Qe=0ShdTsdt+Qe(Ts−Te)−Pµc=0.(1.26)1.5.ExercisesEXERCISE1.–ConsidertheelectriccircuitinFigure1.7.DeterminethedifferentialequationrelatingcurrentitovoltageV.EXERCISE2.–ConsiderthedoubleinvertedpendulumasshowninFigure1.8.Determinetheequationsofitsmotion.EXERCISE3.–[72]ConsiderthemixergiveninFigure1.9.Thismixerconsistsofareservoirthatreceivestwoliquidsofconstantconcentrationsc1andc2andwhosedischargeratesQ1andQ2areregulatedbymeansofvalves.Theliquidisstirredwithinthereservoirforconcentrationcstoremainuniform.Themixtureexitsthroughanopeningofcross-sectionσlocatedatthebottomofthetank.Determinetheequationsofthissystem.yMmf12ml1l2Figure1.8.DoubleinvertedpendulumPhysicalModels17c1Q1hcsQsSc2Q2Figure1.9.MixerEXERCISE4.–WeconsideranothermixerbutthistimethehotwaterattemperatureT1entersthetankwithrateQ1andthecoldwaterwithtemperatureT2enterswithrateQ2.(Itisabathtub.)ThetemperaturesT1andT2areﬁxed.Waterﬂowstothebottomofthetubwherewehaveomittedtoplugtheopeninghavingcross-sectionσ;temperatureofwaterinsidethetubisTs.Determinethesystemequations.WhataretheconnectionsbetweenthisexerciseandExercise3?Chapter2SystemsTheory(I)Theobjectiveofthischapteristospecifythenotionofsystemandtointroduceafewbasicconcepts.Chapter7providesthecomplementarymaterialthatisnecessaryforagoodunderstandingofthenotionsandmethodspresentedinChapters3to6.Inthispresentworkwearemainlyinterestedin“lineartime-invariant”systems(seesection2.2.5).Theothertypesofsystemswillonlybebrieﬂymentioned.2.1.IntroductoryexampleConsidertheRLCcircuitinFigure1.1(section1.1.1).Itisgovernedbyequation(1.1).TosimplifythecalculationsletusassumeR=0.Equation(1.1)canbewrittenasd2idt2+ω20i−kdVdt=0(2.1)withω20=1LCk=1L.Representationbyasecond-orderscalardifferentialequationThesystemvariablesthatareusedinrepresentation(2.1)arew1=iandw2=V.Equation(2.1)takestheform(cid:25)∂2+ω20−k∂(cid:26)(cid:27)w1w2(cid:28)=0(2.2)where∂isthedifferentialoperatorddt.19Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.20LinearSystemsThisrepresentationisasecond-orderscalardifferentialequationwithtwovariablesw1andw2.Thesevariablesarephysicalquantitiesandaswewillseeinthesequeltheyareofparticularinteresttous.Representationbyasystemofﬁrst-orderdifferentialequationsTheaboverepresentationisnottheonlypossibleone.Indeed(2.2)canbewrittenas∂(∂w1−kw2)+ω20w1=0.Introducingthevariablew3=∂w1−kw2weobtaintherepresentation(cid:27)∂−k−1ω200∂(cid:28)⎡⎣w1w2w3⎤⎦=0.(2.3)Thisrepresentationisasystemofﬁrst-orderdifferentialequations(composedoftwoscalardifferentialequations).Itisequivalentto(2.2).Thistimethreevariablesareinvolved:w1w2andw3.Variablew3hasnophysicalmeaning:itissimplyanintermediatevariable.Thisiswhatiscalledalatentvariable[96].Thereforesystemvariablesandequationsarenotunique.2.2.GeneralrepresentationandpropertiesThesystemsstudiedinChapter1havebeenmodeledusingvariablesandequations.Thesearedifferential(oralgebraic-differential)equationsinvolvingtime.Fromamathematicalpointofviewtimeisarealvariablewhichwecallcontinuoustime.Later(inChapter10)wewillstudycomputer-controlledsystems.Acomputeronly“knows”“discretetime”analogoustothesetofintegers.Forthemomentweareonlyconcernedwiththecontextofcontinuoustime.2.2.1.VariablesInageneralfashionwemakeuseofvariablesw1...wk.CaseoftheRLCcircuitinFigure1.1(section1.1.1):w1=iw2=V.CaseofthetraininFigure1.3(section1.2.1):w1=y1w2=y2w3=f.CaseoftheinvertedpenduluminFigure1.4(section1.2.1):w1=yw2=θw3=f.CaseoftheheatedtankinFigure1.6(section1.4):w1=hw2=Tsw3=P.Etc.Wewritew=[w1...wk]T.SystemsTheory(I)212.2.2.EquationsThealgebraicanddifferentialequationsrelatingthevariablescanbegatheredintheformF(w˙w...w(α))=0.(2.4)TheabovefunctionFisveryoftenavector-valuedfunctionhavingqcomponentswhenqisthenumberofscalarequations.Equation(2.4)isthegeneralrepresentationofanonlinearsystem(inthesenseof:possiblynonlinear).(Hereweexclude“inﬁnite-dimensionalsystems”althoughsuchsystemscanbeencounteredinpractice:forexampletime-delaysystems.)Wedenotethesystemdeﬁnedby(2.4)asΣ.2.2.3.Time-invariantsystemsThefunctionFinvolvesanumberofcoefﬁcients(incaseoftheRLCcircuitthesecoefﬁcientsareRLandC;incaseoftheinvertedpendulumthesecoefﬁcientsareMmlgetc.).Ifallthesecoefﬁcientsareconstant(i.e.realnumbers)Σissaidtobetime-invariantanditissaidtobetime-varyingintheoppositecasethatiswhenthecoefﬁcientsdependontime(*e.g.whentheyareelementsofadifferentialﬁeld*).2.2.4.LinearsystemsIfthefunctionFislinearΣissaidtobelinear.Equation(2.4)canthenbewrittenintheformα(cid:12)i=0Eiw(i)=0(2.5)wherethematricesEiareofsizeq×k(q=numberofequationsandk=numberofvariables).REMARK5.–Quiteoftenasystemissaidtobelinearwhenitsatisﬁestheprincipleofsuperposition.Letusclarifythis.(i)SupposeΣistime-invariantandtoclarifyideasthatthefunctionFof(2.4)isrationalwithrealcoefﬁcients.ThenΣislinearifandonlyifforanyvariableswAandwBsolutionsof(2.4)andforanyscalarsλAλB∈RλAwA+λBwBisagainasolutionof(2.4).ThissigniﬁesthatthesetofsolutionsisanR-vectorspace.(ii)*Considernowthecaseofatime-varyingsystemfbeingarationalfunctionwithcoefﬁcientsinadifferentialﬁeldK.ThenΣislinearifandonlyifthesetofsolutionsof(2.4)isak-vectorspacewherekisthesubﬁeldofconstantsofK(seesection13.1.1).ThissetisnotaK-vectorspace[22].*22LinearSystems2.2.5.Lineartime-invariantsystemsLetE(∂)bethepolynomialmatrix(seesection13.1.4Example490)deﬁnedbyE(∂)=α(cid:12)i=0Ei∂i.IfΣisassumedtobelinearandtime-invarianttheentriesofE(∂)arefromtheringR=R[∂]ofpolynomialsin∂withrealcoefﬁcients;E(∂)isofsizeq×k.Equation(2.5)canbeputintheequivalentformE(∂)w=0.(2.6)Theqequationsof(2.6)aresaidtobelinearlyindependentiftherankoverRofE(∂)(denotedbyr=rkRE(∂):seesection13.1.4)isequaltoq.Thisisofcourseonlypossibleifkthenumberofvariablesissuchthatk≥r.Itfollowsthatthetwoelectriccircuitsofsection1.1aswellasthetrainandtheDCmotorofsections1.2and1.3arealllinearsystems.Inrealityhoweverasystemisneverlinearintherigoroussense.Takeanextremelysimpleexample:accordingtoOhm’slawthepotentialdifferenceVacrossaresistanceRisrelatedtothecurrentitraversingitbythelinearrelationV=Ri.ButwhenthevoltageVandthusthecurrentintensityitakesontooimportantvalues(evenwithoutgettingtothepointofburningofftheresistance!)therelationbetweenVandibecomesnonlinearandneedstobewrittenintheformV=ρ(i).Thisfunctionρisthenonlinearcharacteristicoftheresistance.Butunderthecircumstancesof“normal”functioningwecanwritewithgoodprecisionthatf(i)=Ri.Thereforelinearsystemsareonlypurelytheoreticalviewsandresultsofsimpliﬁcations.Butthematerialworldisimmenselycomplex;wecanonlyclaimtounderstandandmasteritatleastpartiallyifwemakeadequatesimpliﬁcations.Thisconstitutesparexcellencetheknow-howofengineersandscientists.Itistruethatlinearsystemsdonotexistintheabsolutebutasnotionstheyplayaveryimportantroleincontroltheoryaswellasinothersciences.DEFINITION6.–Foralineartime-invariantsystemΣwecallthem=k−rtherankofthesystemwherer=rkRE(∂).DEFINITION7.–Alineartime-invariantsystemΣissaidtobedeterminedifitsrankiszero.Itissaidtobeunderdeterminedifitsrankispositive.REMARK8.–*(i)Equation(2.6)isthatofthemoduleM=[w]R(seesection13.4.1Theorem540)sothatwecanassociatetheﬁnitelypresentedR-moduleMwiththelinearsystemΣ.FromamoreabstractpointofviewwecanidentifyΣandMinsuchSystemsTheory(I)23awaythatalinearsystemisdeﬁned(fromamathematicalpointofview)asbeingaﬁnitelypresentedR-module([42][22]).(ii)TherankofΣaccordingtoDeﬁnition6coincideswiththerankofthemoduleMaccordingtoDeﬁnition552ofsection13.4.2.*AmongtheexamplesofChapter1therearenonlinearsystemsandtheyaremodeledassuch:theinvertedpendulumandtheheatedtankofsections1.2and1.4.Bylinearizingasystemaroundanequilibriumpointwecomebacktothelinearcase.Thislinearizationisaﬁrst-orderTaylorexpansion.Therepresentationobtainedisofcourseonlyvalidfortinyvariationsaroundthechosenequilibriumpoint.Wearenowgoingtospecifythesenotions.2.2.6.EquilibriumpointConsiderthetime-invariantsystemΣdeﬁnedby(2.4).DEFINITION9.–Thepointw∗=[w∗1...w∗k]TisanequilibriumpointofΣifF(w∗0...0)=0.Inotherwordswelocatetheequilibriumpoint(s)ofasystembyzeroingthederivativesofallordersofallvariables.CaseoftheinvertedpendulumByzeroingthederivativesin(1.21)(section1.2.1)weobtain(cid:20)f∗=0sinθ∗=0.Thissystemhasinﬁniteequilibriumpoints:y∗isarbitraryθ∗=0orπ(mod2π)f∗=0.Thephysicalinterpretationisclear:thependulumcanbeatequilibriuminanyregionoftheyaxis.Forittobeatequilibriumtheforceappliedontothecarriagemustbezero(otherwiseamovementofaccelerationwillbegiventothelatter).Finallythereareonlytwopossiblepositionsofequilibriumforthependulumitself:therodbeingverticalwiththemassontoporatbottom;wewillseelaterthatintheﬁrstsituationtheequilibriumpointisunstable(whichisthecaseofthependulumsaidtobeinverted)andthatitisstableinthesecondcase.CaseoftheheatedtankProceedingsimilarlywith(1.26)(section1.4.3)weobtain(cid:20)σ√2gh∗−Q∗e=0Q∗e(T∗s−Te)−1µcP∗=0.Theﬁrstoftheseequationsexpressestheequilibriuminvolume:therateofﬂowintothetankisequaltotherateofﬂowoutofit.Thesecondequationexpressestheequilibriuminenergy.24LinearSystems2.2.7.LinearizationaroundanequilibriumpointConsiderthesystemΣdeﬁnedbyequation(2.4);assumethatitistime-invariantandadmitsanequilibriumpointw∗.Thissystemislinearizableatthepointw∗ifFisdifferentiableinaneighborhoodof(w∗0...0).Letw=w∗+∆wwhere∆wisa“smallincrement”inthesensethat∆wanditsderivativesuptoorderαare“smallincrements”.1Wehaveneglectingsecond-ordertermsF(w.w...w(α))(cid:9)F(w∗0...0)+α(cid:12)i=0∂F∂w(i)(w∗0...0)∆w(i)=α(cid:12)i=0∂F∂w(i)(w∗0...0)∆w(i)andthereforeα(cid:12)i=0∂F∂w(i)(w∗0...0)∆w(i)(cid:9)0.DEFINITION10.–ThelinearapproximationofthesystemΣaroundtheequilibriumpointw∗isthelinearsystemΣlwithequation(cid:11)αi=0∂F∂w(i)(w∗0...0)∆w(i)=0.TheequationofΣlisoftheform(2.5)withEi=∂F∂w(i)(w∗0...0)thevariablewbeingreplacedbytheincrement∆w=w−w∗.Caseoftheinvertedpendulum(section1.2.1)Linearizing(1.21)inthevicinityof(y∗=0θ∗=0f∗=0)weobtaintheequationsofthelinearizedinvertedpendulum:(cid:19)(M+m)d2ydt2+mld2θdt2−f=0ld2θdt2−gθ+d2ydt2=0.(2.7)Caseoftheheatedtank(section1.4.3)Thelinearizationof(1.26)leadsto(cid:20)Sd∆hdt+σ√2gh∗∆h−∆Qe=0Sh∗d∆Tsdt+Q∗e∆Ts+(T∗s−Te)∆Qe−1µc∆P=0.1.*Fromthepointofviewoffunctionalanalysiswecanforexampleendowthecodomainofthefunctionwwiththenorm(cid:1)w(cid:1)=(cid:11)αi=0(cid:29)(cid:29)(cid:29)w(i)(cid:29)(cid:29)(cid:29)∞where(cid:1).(cid:1)∞denotesthenorminthespaceL∞.*SystemsTheory(I)25Setτ=Sσ(cid:30)2h∗gandV∗=Sh∗.Usingtheequilibriumrelationsweobtain(cid:19)d∆hdt+1τ∆h−1S∆Qe=0d∆Tsdt+2τ∆Ts−(T∗s−Te)(cid:9)−∆QeV∗+2τ∆PP∗(cid:10)=0.Tosimplifyandimproveconditionequationsitisnowinourinteresttonormalizevariablesortobemorepreciseto“reduce”them.Setw1=∆hh∗w2=∆TsT∗s−Tew3=τ∆QeV∗andw4=∆PP∗(onconditionofcoursethatnoneofthedenominatorsofthesefractionsiszero);thesevariableswi=i=1...4whichareunitlessarecalledthereducedvariables.Wethenobtainthereducedequations:(cid:20)dw1dt+1τw1−1τw3=0dw2dt+2τw2−1τ(−w3+2w4)=0.(2.8)Itequallymakessensetochoosetheunitoftimeproperlyinsuchawaythatτtakesanumericalvaluecloseto1.2.3.Controlsystems2.3.1.InputsGeneralpropertiesofinputvariablesAsalreadymentionedintheprefaceofthisbookthesystemsweareinterestedinarethesystemsausercanactupon.Thissigniﬁesthatbymeansofactuatorswecanimposetheevolutionofcertainvariableswhichwecallthecontrolvariables.TheenvironmentcanalsoactupontheconsideredsystemΣ(theenvironmentisitselfasystem).Thisimposesupontheevolutionofothervariables:whicharethedisturbances(asopposedtothecontrolsthedisturbancesarevariablesoverwhichtheuserhasnomastery).ThecontrolvariablesandthedisturbancesconstitutetheinputvariablesofΣ.Letu1...umbethosevariables;weputu=[u1...um]TandwesuccinctlycallittheinputofΣ.Wewillmakethefollowingthreehypotheses:i)Oncetheinputisﬁxedtheevolutionoftheremainingvariablesξ1...ξk−mofΣdependsonlyontheirinitialconditions.ii)Theinputuisan“independent”variableinthesensethattheredoesnotexistanon-zerofunction2fandanorderofdifferentiationβsuchthatf(cid:15)u˙u...u(β)(cid:16)=0.2.Toberigorousitisofcoursenecessarytomakesomehypothesesabouttheregularityoffunctionf.*Intheframeworkofdifferentialalgebraforexample[47]itis(andsoarealsothecomponentsoffunctionFofequation(2.4))arationalfunctionofseveralvariableswithcoefﬁcientsinRorinadifferentialﬁeld.*26LinearSystemsItisimportanttounderstandthishypothesiswell.ConsiderforexampleasystemΣwhoseonlyinputisadisturbanceu.ForthemodelingofΣweassumethatuisindependent.Nowthisdisturbancecanbeaconstantandthen˙u=0.ThislastequationisamodeloftheenvironmentofΣandisthereforenotpartoftheequationsofthissystem.Let˘ΣbethesystemthatconsistsofΣanditsenvironmentwhoseequationsarethoseofΣtogetherwiththeequation˙u=0.Thevariableu(whichis“dependent”)isnotaninputof˘Σ.iii)WhentheinitialconditionsofsystemΣareﬁxedtheevolutionofitsvariablesdependsonlyontheinputu.Lineartime-invariantcaseTheabovecanonlyfurtherbespeciﬁed(exceptifnotionsoutsideofthescopeofthistextarecalledupon)inthelineartime-invariantcase.Considersystem(2.6)wherethematrixE(∂)isofsizer×kandofrankr.ItfollowsthatthereexistsasubmatrixD(∂)ofE(∂)ofsizer×randnon-singulari.e.thedeterminantofD(∂)isnon-zeroandisanelementofR=R[∂](seesection13.1.4).ItisclearthatD(∂)iscomposedofcolumnsofE(∂).Afterrenumberingthesecolumnsifnecessarywecanassumethatthesearetheﬁrstrcolumnscorrespondingtotheﬁrstrvariablesξ1=w1...ξr=wr.Letwr+1=u1...wk=um.InadditionletE(∂)=(cid:25)D(∂)−N(∂)(cid:26).(2.9)Thensettingξ=[ξ1...ξr]Tandu=[u1...um]T(2.6)iswrittenasD(∂)ξ=N(∂)u.(2.10)Let¯ξbethevariableξwhenu=0.WethushaveD(∂)¯ξ=0.(2.11)Thesystemdeﬁnedby(2.11)isdetermined(seeDeﬁnition7section2.2.5).Asshowninsection2.3.8theevolutionof¯ξisdeterminedbytheinitialconditionsthereforehypothesisi)issatisﬁed.Ontheotherhandequation(2.10)doesnotinvolveanyrelationbetweenthecomponentsofuanditssuccessivederivatives;thisvariableuistherefore“independent”.Asaresulthypothesisii)issatisﬁed.Finallywewillseefromsection2.5.1thathypothesisiii)isalsosatisﬁed.SystemsTheory(I)27Thevariableuconstructedasmentionedabovecanthereforebetheinputofthesystem.ThisisnottheonlypossibilitysincethereareseveralwaystoextractfromE(∂)asquaresubmatrixD(∂)ofrankr.Butwhatevermethodwetakewewillalwaysﬁndanindependentvariableuhavingmcomponents.Wethereforearriveatthefollowingresult:THEOREM11.–(i)Theinputofasystemhasanumberofcomponentsequaltoitsrank.(ii)Itisalwayspossibletoselectaﬁnitesequence(u1...um)fromthesystemvariablesthatsatisﬁeshypothesesi)ii)iii)givenabove(insuchawaythatu=[u1...um]Tcanbethesysteminput).REMARK12.–*ConsiderthemoduleM=[w]Rdeﬁnedby(2.6)(seeRemark8section2.2.5).Themodule[u]RisasubmoduleofM(sincetheinputvariablesbelongtoM).(i)Theindependenceoftheinputmeansthat[u]RisafreeR-moduleofrankm.(ii)WehaveM=[ξ]R+[u]Randaccordingto(2.10)equation(2.11)isthatofthequotientmoduleM/[u]R(where¯ξisthecolumnmatrix(cid:15)¯ξi(cid:16)1≤i≤rsuchthat¯ξiisthecanonicalimageofξiinM/[u]R).Thefactthatthesystem(2.11)isdeterminedmeansthatthemoduleM/[u]Ristorsion(seesection13.4.2Corollary555(ii)).*ExampleLettherebeasystemofequations(cid:20)˙w1=w2−w3w2+w3=0.Inviewoftheﬁrstequationwemayperhapstrytochooseinputvariables:u1=w2andu2=w3.Butthisisnotpossiblefromthesecondequationwhereweneedu1+u2=0.Theorem11conﬁrmsthisimpossibility.WehaveE(∂)=(cid:27)∂−11011(cid:28).Thismatrixisofrankr=2overR[∂];thenumberofvariablesisk=3;thustherankofthesystemism=k−r=1.Thereisthereforeonlyoneindependentinput.2.3.2.OutputsTheuserwishestokeepwatchovertheevolutionofthesystemandthereforeofacertainnumberofitsvariables(oroffunctionsofsuchvariablesaswellastheirderivativestheirsecond-orderderivativesetc.).Thesequantitiesaremeasuredbymeansofsensorsandfromthenonconstitutethemeasurementsofthesystem.Inmanycasestheuserwouldliketomorepreciselyregulatecertainvariablestosomereferencevaluesﬁxedbytheformer.Forthattobepossiblethesevariables28LinearSystemsOutputs Inputs Latent variables Figure2.1.Controlsystemneedtobemeasured(unlesssomecomplexproceduresofestimationaredeveloped).Thecontrolledvariablesareingeneralpartofthemeasurements.Theoutputvariablesconsistofthosemeasuredvariablesandpossiblyothermanifestationsofthesystemits(non-measured)actionsonothersystemsforexample.2.3.3.LatentvariablesThesystemvariablesoftendonotreducetoinputandoutputvariables.Thesupplementaryvariablesarecalledthelatentvariables[117].Thesevariablesarenot“intrinsic”aswewillshowbelowusingtheexampleoftheDCmotor.Bydistinguishingitsinputandoutputvariablesfromalltheothervariablesweendowasystemwithasupplementarystructure.Thisiswhatisexpressedbythefollowingdeﬁnition:DEFINITION13.–Acontrolsystem3isatriple(Σuy)whereΣisasystemu=[u1...um]Tistheinputandy=[y1...yp]Tistheoutput.AcontrolsystemcanberepresentedasinFigure2.1.ExamplesCaseofRLCcircuitInthecaseoftheRLCcircuitgiveninFigure1.1(section1.1.1)wecanimpressavoltageVbymeansofapotentiometer.Afterthisequation(1.1)canbewrittenasLCd2idt2+RCdidt+i=CdVdt.(2.12)3.Inwhatfollowswewillcalla“controlsystem”simplya“system”wherethereisnoambiguity.Thiswillallowustoreturntotheterminologycommonlyusedintheliterature.SystemsTheory(I)29Theright-handmemberofthisequationbeingﬁxedthecurrentiisgovernedbyasecond-orderlineardifferentialequationanditsevolutiononlydependsoni(0)anddidt(0)(t=0takenastheinitialinstant);inotherwordstheinitialconditionofthesystematinitialinstant0is(cid:31)i(0)didt(0) .Wecanmonitortheevolutionofiusinganammeter.Letu=Vandy=i:uisthecontrolandyisthemeasurementofthecontrolsystemasdeﬁned.Wecanalsodothereverseﬁxingthevalueofiusingacurrentgenerator;andhencenowthevoltageVdependsonlyonV(0).WeobtainanothercontrolsystemwithinputiandoutputV.Wewillseelaterthatthisapproachcreatessomeinconvenienceeventhoughitistheoreticallypossible.Inbothcasestherearenolatentvariables.CaseofthetrainVerynaturallythecontrolvariableofthetrain(section1.2.1)istheforcef.Theconductorofthetrainusingthisvariablecandecidehowtopositionthewagonhencecontrollingz2.Ifallhemonitorsisz2thisvariableisthemeasurementandz1isthelatentvariable.Ifitusestheinformationofthepositionofthewagonaswellasthatofthelocomotive(whichmakesthetaskeasier)themeasurementbecomesy=[z1z2]Tandtherewillnolongerbeanylatentvariable.Letusverifythatthecontrolcanbechosenastheforcef:puttingξ1=z1ξ2=z2andu=fequations(1.19)canbewrittenintheform(2.10)withD(∂)=(cid:27)m1∂2+k−k−km2∂2+k(cid:28)N(∂)=(cid:27)10(cid:28).ThereforedetD(∂)=m1m2∂2(cid:21)∂2+km1+m2m1m2(cid:22)(cid:5)=0(2.13)andtherankconditionissatisﬁed.RecallthatwritingdetD(∂)(cid:5)=0meansthatdetD(∂)isnotthezeropolynomial.LaterwewillhavetoconsidervaluesofthecomplexvariablesforwhichdetD(s)iszero:thatisacompletelydifferentquestion:wethencalculatetherootsofdetD(s)andthatonlymakessenseifdetD(s)isnotthezeropolynomial.CaseoftheDCmotorThissystem(seesection1.3)hasr=3linearlyindependentequationsandk=4variables(includingamongthemthevelocityω)hencethenumberofinputsis30LinearSystemsm=k−r=1.LetusshowthatwecanchoosethevoltageVtobethecontrolvariable.Equations(1.25)canbewrittenintheform⎡⎣R+L∂K0−KJ∂+λ00−1∂⎤⎦⎡⎣iωθ⎤⎦=⎡⎣100⎤⎦V.(2.14)ThedeterminantofthematrixD(∂)ontheleftofthisequationis:detD(∂)=∂(cid:25)(J∂+λ)(R+L∂)+K2(cid:26)(cid:5)=0.ItisthereforequitepossibletochoosevoltageVtobethecontrolvariable.Thereexisttwomodesofcontrol:“positioncontrol”wherethecontrolledvariableisθand“velocitycontrol”wherethecontrolledvariableisω.Intheﬁrstcaseapositionsensorisalwaysavailabletousrarelyavelocitysensorandingeneralthecurrentiisnotmeasured.Wecanthereforeconsidery=θastheoutputiandωarethenlatentvariables.Butnotethatwecanrewrite(1.25)byreplacingintheﬁrsttwoequationsωbydθdtandbysuppressingthelastequation.Inthiscasethereareonlyr=2linearlyindependentequationsandk=3variables.Therankm=k−rremainsunchanged:itisanintrinsicquantitythatisitisindependentoftherepresentationchosen(*seeRemark8(ii)section2.2.5*).Howeverthelatentvariablesdependontherepresentationchosenasseeninthisexample.CaseoftheheatedtankEquations(2.8)(section2.2.5)canbewrittenintheform(cid:27)∂+1τ0−1τ00∂+2τ1τ−2τ(cid:28)⎡⎢⎢⎣w1w2w3w4⎤⎥⎥⎦=0.Wecantakey1=w1andy2=w2asoutputvariablesandu1=w3andu2=wasinputvariables.Physicallythisislogicalaswell:thetwomeansofactionareactuallytheinputdischargeandtheheatingi.e.w3andw4upto“reductionfactors”;andthetwovariablestoberegulatedaretheheightofthewaterinsidethetankandthetemperatureofwaterofthesamei.e.w1andw2afterreduction.Theaboveequationscanthenbewrittenas(cid:27)∂+1τ00∂+2τ(cid:28)y=(cid:27)1τ0−1τ2τ(cid:28)u.(2.15)2.3.4.ClassiﬁcationofsystemsWerefertothesystemshavingoneinputandoneoutputvariableasSingle-InputSingle-Output(SISO)systemsandthosewithmorethanoneinput-andSystemsTheory(I)31output-variableasMulti-InputMulti-Output(MIMO)systems.Thereareofcoursesystemsthatareintermediateinnature(SIMOandMISOsystemsbuttheseexpressionsarelessoftenusedintheliterature).2.3.5.RosenbrockrepresentationGeneralformulationTheoutputvariablesy1...ypasalreadyseenarelinearcombinationsofthevariablesw1...wkoftheirderivativesoftheirsecond-orderderivativesetc.Thusycanbewrittenintheformy=H(∂)wwhereH(∂)isapolynomialmatrixofsizep×k.BydecomposingH(∂)inthesamewayasin(2.9)(section2.3.1)whichisH(∂)=(cid:25)Q(∂)W(∂)(cid:26)whereQ(∂)andW(∂)arepolynomialmatricesofsizep×randp×mrespectivelyweﬁnallyobtain(cid:20)D(∂)ξ=N(∂)uy=Q(∂)ξ+W(∂)u(2.16)whereD(∂)isanon-singularmatrixofsizer×roverR=R[∂];(2.16)isaverygeneralrepresentationofacontrolsystem[100].DEFINITION14.–Thesetofequations(2.16)iscalledaRosenbrockrepresentationandisnotedas{DNQW}.Thevectorξofsucharepresentationiscalledthepartialstate(orthepseudo-state).REMARK15.–*ConsiderthesystemΣasanR-moduledenotedbyM(seeRemark8section2.2.5).Theﬁrstequationof(2.16)hasalreadybeeninterpretedinRemark12(section2.3.1).Thesecondonesimplymeansthatthevariablesyi(1≤i≤p)belongtoM.See[19]formoredetails.*LeftformThepartialstategenerallyconsistsoflatentvariablesexceptinparticularwhenthesecondequationof(2.16)isy=ξ(withQ(∂)=IpW(∂)=0);inthiscaseindeedtheRosenbrockrepresentationreducestooneequationwhoselatentvariablesareeliminated:thisequationcalledaleftformiswrittenasD(∂)y=N(∂)u.(2.17)Therepresentation(2.15)oftheheatedtankisofthisform.32LinearSystemsItisalsothecaseinrepresentation(2.12)oftheRLCcircuitwithu=Vy=iand(cid:20)D(∂)=∂2+RL∂+1LCN(∂)=1L∂afterexpression(2.12)hasbeendividedbyLCforD(∂)tobeamonicpolynomial.RightformEquationsoftheRLCcircuitinFigure1.1canalsobeobtainedbyusingthechargeqofthecapacitorasalatentvariable.Wethenobtain(cid:20)V=(cid:15)L∂2+R∂+1C(cid:16)q.i=∂q.Inordertogetamonicpolynomialofseconddegreeintheﬁrstequationputξ=Lq;weobtainwithu=Vandy=i(cid:20)u=(cid:15)∂2+RL∂+1LC(cid:16)ξ.y=1L∂ξ.(2.18)ThisisaRosenbrockrepresentationsuchas(2.16)whereN(∂)=1andW(∂)=0:(cid:20)u=D(∂)ξy=Q(∂)ξ.(2.19)Arepresentationofthisformiscalledingeneralarightform.InthepresentcasewehaveD(∂)=∂2+RL∂+1LCandQ(∂)=1L∂.Leftform↔rightformdualityFortheleftform(2.17)andtherightform(2.19)torepresentthesamecontrolsystemitisnecessarytohavethefollowingcorrespondences(assumingthatD(∂)isamonicpolynomial):(cid:20)D(∂)←→D(∂)N(∂)←→Q(∂)(seee.g.theRLCcircuitcase).Thisiswhatwecalltheleftform↔rightformduality(foralineartime-invariantSISOsystem).Soonwewillseefromsection7.1thataleftformanditsdual(whichsatisﬁesthedualityrelationabove)maynotalwaysberepresentationsofthesamecontrolsystem.Thesameappliesforarightformanditsdual.SystemsTheory(I)332.3.6.State-spacerepresentationAstate-spacerepresentationisaparticulartypeofRosenbrockrepresentationwhereD(∂)isoftheform∂In−AA∈Rn×nN(∂)=B∈Rn×mandQ(∂)=C∈Rn×p.Sucharepresentationisoftheform(cid:20)˙x=Ax+Buy=Cx+W(∂)u.(2.20)Thistypeofrepresentation(calledastate-pacesystem)willbestudiedindetailfromChapter7onwards.REMARK16.–ThecodomainofthestatexisRni.e.thestate-spaceisofdimensionn.Abusingthelanguagewecanalsosaythatthestate-spacerepresentationconsideredoritsstatevectorisofdimensionn.2.3.7.PolesandorderofasystemConsideracontrolsystemΣdescribedbytheRosenbrockrepresentation(2.16).DEFINITION17.–(i)ThepolesofΣaretherootsofdetD(s)(takingintoaccountmultiplicities).(ii)TheorderofΣisthedegreeofthepolynomialdetD(s).Theorderofasystemisthereforeequaltothenumberofitspoles.Thefollowingresultisobvious:THEOREM18.–SupposeΣisgivenbythestaterepresentation(2.20).ThenthepolesofΣaretheeigenvaluesofAanditsorderisthedimensionofitsstatevector.REMARK19.–*ConsiderthesystemΣandtheassociatedR-moduleM(seeRemarks812and15).ThenthepolesofΣaretheSmithzerosofthetorsionmoduleM/[u]R(seesection13.4.2).*2.3.8.FreeresponseandbehaviorLetΣbealineartime-invariantsystemofordern.DEFINITION20.–WecallfreebehaviorofΣthesetofitsvariableswhenthesysteminputismaintainedatzero(foranyinitialconditions).Thefreeresponseofthissystemisitsoutputinthesamesituation.34LinearSystemsSupposeagainthatΣisdescribedbytheRosenbrockrepresentation(2.16).Let¯ΣbethesystemobtainedfromΣbyforcingtheinpututo0.Itspartialstate¯ξisasolutionofD(∂)¯ξ=0.(2.21)CaseofascalarpartialstateOneofthefundamentalresultsintheclassictheoryoflineardifferentialequationswithconstantcoefﬁcientsisthefollowing[36](seealsosection12.5.2Theorem449):THEOREM21.–Letp1...pqbethedistinctrootsofD(s)(alsocalledthedistinctpolesofΣorof¯Σ)withorderofmultiplicityρ1...ρqrespectively.Thenanysolutionofequation(2.21)isoftheform¯ξ(t)=q(cid:11)j=1ρj(cid:11)k=1αjktk−1epjtwhereαjk∈C.Thesetofthesesolutions(whichisthefreebehaviorofthesystem)isaC-vectorspaceofdimensionn;abasisofthisspaceconsistsofthenfunctionsjk:t(cid:3)→tk−1epjt1≤k≤ρj1≤j≤q.Examples–Considerﬁrstthedoublyintegratingsystem¨y=u.(2.22)Thisequationisaleftform(2.17)(section2.3.5)withD(∂)=∂2andN(∂)=1.Thepolynomialdet(D(s))=D(s)=s2hasdoubleroots=0.Thesetofsystempolesistherefore{00}.Thisisasecond-ordersystem.–Considernowthesystem¨y=˙u.(2.23)ItisagainaleftformwithD(∂)=∂2andN(∂)=∂.Thesetofsystempolesislikepreviously{00}andthissystemisofthesecondorder.ItpresentsaparticularitywhichwillbestudiedinChapter7.*GeneralcaseGeneralizationofTheorem21Considernowthegeneralcasewherethepartialstatehasr>1components.ThereexistmatricesP(∂)andR(∂)invertibleoverR=R[∂]suchthatP(∂)D(∂)R(∂)=S(∂)SystemsTheory(I)35whereS(∂)=diag(α1(∂)...αr(∂))istheSmithformofD(∂)(seesection13.2.3).Let˜ξ=R−1(∂)ξ(whichislicitsinceR−1(∂)isapolynomialmatrixin∂).Thenthematrixdifferentialequation(2.21)isequivalenttorscalardifferentialequations⎧⎪⎨⎪⎩α1(∂)˜ξ1=0...αr(∂)˜ξr=0.ForeachoftheseequationswecanapplyTheorem21.Wecanthereforefactorizetheinvariantfactorsαk(∂)1≤k≤rintoprimesandthusmakeapparenttheelementarydivisorsofD(∂).ThesedivisorsdeterminetheSmithzerosofthematrixD(s)aswellastheirstructuralindices(seesection13.2.5).DEFINITION22.–ApoleofΣisaSmithzeroofD(s)*(ormoreintrinsicallyofM/[u]R:seeRemark19section2.3.7)*.ThestructuralindicestheorderandthedegreeofsuchapolearethestructuralindicestheorderandthedegreeofthisSmithzero.Oneobtainsthefollowingtheorem([12]IV.2.9):THEOREM23.–Letp1...pqbethedistinctpolesofΣandletσ(pj)bethesetofstructuralindicesofthepolepj(1≤j≤q).Anysolutionof(2.21)isoftheform¯ξ(t)=q(cid:11)j=1(cid:11)k∈σ(pj)ajktk−1epjt(2.24)whereajk∈Cr.Thesetofthesesolutions(thatisthefreebehaviorofthesystem)isaC-vectorspaceofdimensionn.ExampleLetustakeasimplecasewherethematrixD(∂)isalreadyinSmithform:D(∂)=⎡⎣∂−1000(∂−1)3(∂−2)00(∂−1)3(∂−2)2⎤⎦.Theorderofthesystemisn=degdetD(∂)=10.36LinearSystemsOntheotherhand(2.21)canbewrittenintheformofthreescalarequations:⎧⎨⎩(∂−1)¯ξ1=0(∂−1)3(∂−2)¯ξ2=0(∂−1)3(∂−2)2¯ξ3=0.Wethusobtainthefollowingsolutions(inthespaceE(R)consistingofallindeﬁnitelydifferentiablefunctionsR→C)⎧⎨⎩¯ξ1(t)=c1et¯ξ2(t)=c2et+c3tet+c4t2et+c5e2t¯ξ3(t)=c6et+c7tet+c8t2et+c9e2t+c10te2twherethe10constantsc1...c10arearbitrary.Weobtaintheform(2.24)inthefollowingmanner:¯ξ(t)=⎡⎣c1c2c6⎤⎦et+⎡⎣0c3c7⎤⎦tet+⎡⎣0c4c8⎤⎦t2et+⎡⎣0c5c9⎤⎦e2t+⎡⎣00c10⎤⎦te2t.AsexpectedthesolutionsformaC-vectorspaceofdimension10.2.4.Transfermatrix2.4.1.LaplacetransformsTheLaplacetransformisdetailedinsection12.3.4.Asfarasweareconcernedheretheessentialpointsareasfollows:Letu:R→Rmbeafunctionwithpositivesupporti.e.whichiszerofort<0.ItsLaplacetransformisafunctionofthecomplexvariableˆufromCtoCmdeﬁnedforRe(s)>γ(whereγistheabscissaofconvergence)byˆu(s)=(cid:2)+∞0−u(t)e−stdt(2.25)(thedeﬁnitioninsection12.3.4isextendedheretothecaseofvector-valuedfunctions).Ifˆu(s)isarationalfunctiontheabscissaofconvergenceisgivenbyγ=maxp∈PRepwherePisthesetofpolesofˆu(s).SystemsTheory(I)37TheLaplacetransformationL:u(cid:3)→ˆuislinear.Ittransformstheconvolutionproductintoanordinaryproduct.Ifuandallitsderivativesarezeroatt=0then(cid:15)Lu(n)(cid:16)(s)=snˆu(s)irrespectiveofwhattheordernofthederivativeis.InotherwordswithzeroinitialconditionsLaplacetransformationtransformsdifferentialoperator∂intothemultiplicationbythe“Laplacevariable”swhichisacomplexvariable.2.4.2.Transfermatrix:deﬁnitionTHEOREM24.–Let(Σuy)beacontrolsystem.ThereexistsauniquematrixofrationalfunctionsG(s)suchthatforzeroinitialconditionsˆy(s)=G(s)ˆu(s).(2.26)PROOF.WecanassumethatthesystemisdescribedbyaRosenbrockrepresentationbecauseasalreadysaidthisisthemostgeneraltypeofrepresentationofalinearcontrolsystem.ByapplyingtheLaplacetransformwithzeroinitialconditionsto(2.16)weobtain(cid:20)D(s)ˆξ(s)=N(s)ˆu(s)ˆy(s)=Q(s)ˆξ(s)+W(s)ˆu(s).ThematrixD(s)isasquarenon-singularthusinvertibleovertheﬁeldR(s)andwehaveˆξ(s)=D−1(s)N(s)ˆu(s);from(2.26)weobtainG(s)=Q(s)D−1(s)N(s)+W(s).(2.27)ThisprovestheexistenceofG(s).ItsuniquenessisalsoensuredsincetheequalityG1ˆu=G2ˆuforeveryˆu(wheretheinputvariablesareindependent)impliesG1=G2.DEFINITION25.–ThematrixG(s)∈R(s)p×m(whereR(s)denotestheﬁeldofrationalfunctionswithrealcoefﬁcients)iscalledthetransfermatrixofthecontrolsystem.Inthecasem=p=1(SISO)G(s)iscalledthetransferfunctionofthecontrolsystem.FromanalgebraicpointofviewthemajordifferencebetweenworkingwithtemporalsignalsorwiththeirLaplacetransformsisthatinthecaseoftemporalsignalstheoperatorsbelongtotheringR[∂]ofpolynomialsin∂;theseelementsarenotinvertibleingeneral.InthecaseofLaplacetransformsonthecontrarytheoperatorsarerationalfunctionsi.e.belongingtotheﬁeldR(s)andsoare38LinearSystemsinvertiblewhenevertheyarenon-zero.ThissimpliﬁcationbroughtalongbytheLaplacetransformationwithzeroinitialconditionsisofcoursepaidforbyalossofinformationwhichwillbestudiedinChapter7.Notethataleftformandarightformwhicharedualhavethesametransfermatrix.2.4.3.ExamplesLetusre-examinesomeoftheexamplesstudiedinChapter1.RLCcircuitConsiderequation(2.12)(section2.3.3).–Firstsupposetheinputchosenisu=Vandtheoutputisy=i.ThetransferfunctionisthenG(s)=CsLCs2+RCs+1.(2.28)–Ifonthecontrarywechooseu=iasinputandy=VasoutputthetransferfunctionistheinverseofthepreviousonewhichisG(s)=LCs2+RCs+1Cs.(2.29)DCmotorEquation(2.14)(section2.3.3)isoftheformD(∂)ξ=N(∂)u.Supposetheoutputisy=θ(positioncontrol).Wethenhavetherelationy=Q(∂)ξ+W(∂)uwithQ(∂)=(cid:25)001(cid:26)andW(∂)=0.SincewehaveaRosenbrockrepresentationwecanmakeuseoftherelation(2.27)tocalculatethetransferfunctionG(s).Inthepresentcaseanyhowitismoreeffectivetoreturntoequations(1.25)(section1.3)andthenpassintotheLaplacedomain.Wehave⎧⎨⎩(R+Ls)ˆı(s)+Kˆω(s)=ˆV(s)(Js+λ)ˆω(s)=Kˆı(s)sˆθ(s)=ˆω(s).Fromtheﬁrsttwoequationsweget(cid:25)(R+Ls)(Js+λ)+K2(cid:26)ˆω(s)=KˆV(s)SystemsTheory(I)39fromwhichweﬁnallyobtainˆθ(s)=Ks[(R+Ls)(Js+λ)+K2]ˆV(s).ThetransferfunctionofthecontrolsystemisthereforeG(s)=Ks[(R+Ls)(Js+λ)+K2].(2.30)HeatedtankThisisanMIMOsystem;itthereforehasatransfermatrix.Accordingto(2.15)(section2.3.3)itisG(s)=#11+τs0−121+τ2s11+τ2s$.(2.31)2.4.4.TransmissionpolesandzerosSISOcaseConsideraSISOsystemΣwithrationaltransferfunctionG(s).WewillseelaterthatthebehaviorofΣislargelycharacterizedbythepolesandzerosofG(s)(deﬁnedinsection13.6.1).DEFINITION26.–ThepolesandzerosofG(s)arecalledthetransmissionpolesandzerosofΣ.ExamplesCaseofRLCcircuitConsiderthecaseoftheRLCcircuitwithtransferfunction(2.28).ThistransferfunctioncanbeputintheformG(s)=kω0ss2+2ςω0s+ω20(2.32)withk=(cid:30)CLω0=1√LCς=R2(cid:30)CL.Ifς≥1thesystemhastworealtransmissionpolesformingtheset(cid:7)−ω0(cid:9)ς−%ς2−1(cid:10)−ω0(cid:9)ς+%ς2−1(cid:10)(cid:8).If0≤ς<1thenthesystemhastwocomplexconjugatetransmissionpoles:(cid:7)−ω0(cid:9)ς−i%1−ς2(cid:10)−ω0(cid:9)ς+i%1−ς2(cid:10)(cid:8).Moreoverthesetoftransmissionzerosofthesystemis{0}.40LinearSystemsCaseoftheDCmotorThetransferfunction(2.30)canbeputintheformG(s)=ks(s2+2ςω0s+ω20).(2.33)Thesystemthereforehasthreetransmissionpoles:{0p1p2}wherep1andp2aretherootsofthepolynomials2+2ςω0s+ω20andwhicharecalculatedabove.Thissystemhasnotransmissionzero.MIMOcaseDeﬁnition26remainsvalidintheMIMOcase.Weareledtodeﬁnewhatwemeanbyapoleandazeroofatransfermatrix.NaivedeﬁnitionLetusﬁrstlookatanaivedeﬁnitionofthesenotionsanditslimitations.Inasecondstepwewillfollowamorerigorousapproachbutamorecomplicatedonefromamathematicalpointofview.TransmissionpolesApoleofatransferfunctionG(s)isacomplexnumberpsuchthatlims→p|G(s)|=+∞.Letusexaminethecaseofatransfermatrix.ElementsofsuchamatrixG(s)aredenotedbyGij(s).ApoleofG(s)isacomplexnumberpsuchthatatleastoneoftheelementsGij(s)satisﬁeslims→p|Gij(s)|=+∞.Considerforexampletheheatedtankwhosetransfermatrixisgivenby(2.31).Fromtheabovedeﬁnitionweget−1τand−2τasthetransmissionpolesofthissystem.Thedifﬁcultyhere(resultingfromthelackofprecisionofthedeﬁnition)isthatweknownothingaboutthemultiplicity(ororder)ofthepole−2τ.IndeedtwoelementsofG(s)haveanabsolutevaluethattendsto+∞whens→−2τ.IsthesetofpolesofG(s)(repeatingapoleanumberoftimesequaltoitsmultiplicity)(cid:31)−1τ−2τ or(cid:31)−1τ−2τ−2τ ?Wewillanswerthisquestioninsection2.4.5.TransmissionzerosandblockingzerosAzeroofatransferfunctionG(s)isacomplexnumberzsuchthatG(z)=0.InthecaseofMIMOsystemsastraightforwardextensionofthisdeﬁnitionallowsustoobtainthenotionof“blockingzero”:SystemsTheory(I)41DEFINITION27.–AblockingzeroofsystemΣ(orofitstransfermatrixG(s))isacomplexnumberzsuchthatG(z)=0.Wecanneverthelessenvisageasecondtypeofextensiontotheclassicnotionofzeroofarationalfunctionproceedinginthefollowingmanner:TherankofthetransferfunctionG(s)overtheﬁeldR(s)ofrationalfunctions(thisrankisdenotedbyrkR(s)G(s)andissometimescalledthe“normalrank”ofG(s))isequalto1.SoazeroofG(s)isacomplexnumberzsuchthatrkCG(z)<rkR(s)G(s).Wecannowenvisagetoextendthisdeﬁnitiontothecaseofatransfermatrix.ForexampleifweconsiderthetransfermatrixG(s)oftheheatedtankrkR(s)G(s)=2andrkCG(z)=2forallvaluesofzatwhichG(z)isdeﬁnedi.e.foranyzthatisnotapoleofG(s).“Therefore”thissystemhasnotransmissionzero.Butthereareproblematiccases.ConsiderforexampleG(s)=(cid:27)1s00s(cid:28).Thesecondcolumnbecomeszerowhens=0“therefore”0isazeroofG(s)i.e.atransmissionzeroofthesystem.Butontheotherhands=0isapoleofG(s)whichrendersG(0)undeﬁned.Inviewofsuchanincoherenceonemustuseamorepreciseapproachleadingustodeﬁnethe“MacMillanpolesandzeros”.Deﬁnition26extendstotheMIMOcaseinthefollowingmanner:DEFINITION28.–ThetransmissionpolesandtransmissionzerosofasystemaretheMacMillanpolesandMacMillanzerosofitstransfermatrix.2.4.5.*MacMillanpolesandzerosSmith–MacMillanformofatransfermatrixLetG(s)beatransfermatrixtheelementsGij(s)ofwhicharerationalfunctions.Letd(s)betheleastcommondenominatoroftheserationalfunctions(i.ethelcmofitsdenominators).ThenwehaveG(s)=N(s)d(s)whereN(s)isapolynomialmatrix.WeknowthereexistpolynomialmatricesU(s)andV(s)invertibleoverR=R[∂]suchthatN(s)=U−1(s)S(s)V(s)42LinearSystemswhereS(s)=diag(α1(s)...αr(s)0...0)istheSmithformofN(s)(seesection13.2.3).Thepolynomialsα1(s)...αr(s)aretheinvariantfactorsofN(s);theyarenon-zeroandsuchthatα1(s)|...|αr(s).AsaresultwehaveG(s)=U−1(s)diag(cid:21)α1(s)d(s)...αr(s)d(s)0...0(cid:22)V(s).Letαi(s)d(s)=εi(s)ψi(s)(1≤i≤r)wheretherationalfunctionontheright-handsideisirreducible.WeobtainG(s)=U−1(s)Σ(s)V(s)(2.34)Σ(s)=diag(cid:21)ε1(s)ψ1(s)...εr(s)ψr(s)0...0(cid:22).(2.35)Itiseasytoshowthatε1(s)|...|εr(s)ψr(s)|...|ψ1(s).(2.36)OntheotherhandtheuniquenessoftheSmithformS(s)ofN(s)impliestheuniquenessofΣ(s)satisfying(2.34)(2.35)withthedivisibilityconditions(2.36).DEFINITION29.–ThematrixΣ(s)iscalledtheSmith–MacMillanformofG(s).DeﬁnitionoftheMacMillanpolesandzerosLet(s)=diag(ε1(s)...εr(s)0...0)Ψ(s)=diag(ψr(s)...ψ1(s)1...1)sothatΣ(s)=(s)Ψ−1(s)=Ψ−1(s)(s)wherethepolynomialmatrices(s)andΨ(s)areleft-andright-coprime(seesection13.2.6).SystemsTheory(I)43DEFINITION30.–(i)TheMacMillanpoles(resp.theMacMillanzeros)ofG(s)aretheSmithzerosofΨ(s)(resp.of(s)).(ii)ThestructuralindicestheordersandthedegreesoftheseMacMillanpoles(oroftheseMacMillanzeros)arethestructuralindicestheordersandthedegreesrespectivelyoftheseSmithzeros.(iii)IfG(s)isapropertransfermatrix(seesection13.6.1)itsMacMillandegreeisthesumofthedegreesofthepolynomialsψi(s)(1≤i≤r).REMARK31.–(i)Accordingto(2.34)and(2.35)G(s)=D−1l(s)Nl(s)whereDl(s)=Ψ(s)U(s)andNl(s)=(s)V(s).Wehavetheequality(cid:25)Dl(s)Nl(s)(cid:26)=(cid:25)Ψ(s)(s)(cid:26)diag{U(s)V(s)}.ThematricesU(s)andV(s)areinvertibleoverR=R[∂]soisalsodiag{U(s)V(s)}thereforethematrices{Dl(s)Nl(s)}areleft-coprime.Therefore(Dl(s)Nl(s))islike(Ψ(s)(s))aleft-coprimefactorizationofG(s)overR.AccordingtoTheorem509(section13.2.7)theMacMillanpoles(resp.theMacMillanzeros)ofG(s)aretheSmithzerosofDl(s)(resp.Nl(s)).(ii)WecanequallywriteG(s)=Nr(s)D−1r(s)whereDr(s)=V−1(s)Ψ(s)andNr(s)=U−1(s)(s).Since(cid:27)DrNr(cid:28)=(cid:27)V−100U−1(cid:28)(cid:27)Ψ(cid:28)(Nr(s)Dr(s))isaright-coprimefactorizationofG(s)overR.TheMacMillanpoles(resp.zeros)ofG(s)aretheSmithzerosofDr(s)(resp.Nr(s)).DEFINITION32.–Acontrolsystemwhichhasatransmissionpole(resp.zero)ats=0iscalledanintegratorsystem(resp.aderivatorsystem).ExampleoftheheatedtankThetransfermatrix(2.31)oftheheatedtankcanbewrittenasG(s)=a(cid:27)1s+a0−1s+2a1s+2a(cid:28)witha=1τ.ItisclearthatthetransmissionpolesandzerosofG(s)areidenticaltothoseof˜G(s)=G(s)a;itisthereforethislastmatrixthatwewillnowbeinterestedin.Wehave˜G(s)=1(s+a)(s+2a)(cid:27)s+2a0−(s+a)s+a(cid:28)=1(s+a)(s+2a)N(s).TheSmithformofN(s)isS(s)=(cid:27)100(s+2a)(s+a)(cid:28).44LinearSystemsAsaresulttheSmith–MacMillanformof˜G(s)isΣ(s)=(cid:27)1(s+2a)(s+a)001(cid:28).Theheatedtankthereforehastwotransmissionpoles{−a−2a}.Thesearesimplepoles(meaningthattheirorderandtheirdegreeareequalto1).Ontheotherhandthissystemhasnotransmissionzero.2.4.6.MinimalsystemsRelationbetweenthepolesofasystemanditstransmissionpolesConsiderasystemdescribedbytheRosenbrockrepresentation(2.16).Accordingto(2.27)itstransfermatrixisG(s)=Q(s)D−1(s)N(s)+W(s)=1detD(s)[Q(s)adj(D(s))N(s)+det(D(s))W(s)](2.37)whereadj(D(s))designatestheclassicaladjointofD(s)(seesection13.1.4).NowtherootsofdetD(s)arethepolesofthesystembeingconsidered(seesection2.3.7Deﬁnition17).Itfollowsfrom(2.37)that{transmissionpoles}⊂{systempoles}.(2.38)Thisinclusionisanequalityexceptwhenthefraction(2.37)isnotirreduciblei.e.whendetD(s)and[Q(s)adj(D(s))N(s)+det(D(s))W(s)]havecommonfactors.Indeedoncethesecommonfactorsarecancelledthe(MacMillan)polesofG(s)becomenomorethanjustastrictsubsetoftherootsofdet(D(s)).Thiscasewhichinawayis“pathological”isstudiedinChapter7.SystemsTheory(I)45MinimalcontrolsystemDEFINITION33.–Acontrolsystemissaidtobeminimaliftheinclusion(2.38)isanequality.4Thereadercanverifythatallsystemshavingbeenenvisageduptillnowintheexamplesandexercisesareminimalwiththeexceptionofsystem(2.23)insection2.3.8.Indeedthisoneherehaspoles{00}whileithasauniquetransmissionpole{0}.TransmissionorderTheorderofasystemisdeﬁnedatsection2.3.7(seeDeﬁnition17(ii)whichwemayreformulatebysayingthattheorderofasystemisthenumberofitspolescountingmultiplicities).DEFINITION34.–Wecallthenumberoftransmissionpolesofasystemthetransmissionorderofthissystem(countingmultiplicities).Accordingto(2.38)(transmissionorder)≤(systemorder).(2.39)Forexamplethesystem(2.23)isoforder2butitstransmissionorderis1.Itisclearthatacontrolsystemisminimalifandonlyif(transmissionorder)=(systemorder).Whenacontrolsystemisminimalwecantalkaboutits“order”withoutambiguity.Asaresult:TheRLCcircuitwithtransferfunction(2.28)isoforder2.TheDCmotorwithtransferfunction(2.30)isoforder3.*Theheatedtankwithtransfermatrix(2.31)isoforder2.*4.Notethatin[42]and[96]theminimalitycorrespondstoadifferentsituation.Ourdeﬁnitioncorrespondstothenotionofminimal(orirreducible)realizationintroducedbyKalman[65].46LinearSystems2.4.7.TransmissionpolesandzerosatinﬁnitySISOcaseConsideratransferfunctionG(s)=b(s)a(s)whereb(s)anda(s)belongtoR[s]arebothnon-zeroandofdegreenandmrespectively;writeb(s)=b0sm+b1sm−1...+bma(s)=a0sn+a1sn−1...+anwherea0b0(cid:5)=0.TherelativedegreeoftherationalfunctionG(s)isthereforeδ(G)=n−m(seesection13.6.1).WecanembedtheringR[s]intheﬁeldR((σ))oftheLaurentserieswithindeterminateσ=1/s(seesection13.1.1).Indeedanyelementc(σ)(cid:5)=0ofR((σ))isoftheformc(σ)=(cid:12)i≥νciσicν(cid:5)=0;thiselementbelongstoR[s]ifandonlyifci=0fori>0.AsaresultwecanembedtheﬁeldR(s)inR((σ)).Weobtainc(σ)=cνσν(cid:21)1+cν+1cνσ+···(cid:22)=cνσνυ(σ)whereυ(σ)isaunitoftheringR[[σ]].ApplyingthistotherationalfunctionG(s)weobtainG(s)=b0a0σn−mυ(σ).Remembernowthatσ=0ifandonlyifs=∞.Asaresult:–ifn−m>0wesaythatG(s)hasazeroatinﬁnityofordern−m(orthatG(s)hasn−mzerosatinﬁnity);–ifm−n>0wesaythatG(s)hasapoleatinﬁnityoforderm−n(orthatG(s)hasm−npolesatinﬁnity).Thesepoles(resp.zeros)atinﬁnityarethetransmissionpoles(resp.zeros)atinﬁnityofthesystemwithtransferfunctionG(s).ThetransferfunctionG(s)hasmﬁnitezeros(inCforexample)whicharetherootsofb(s)andnﬁnitepoles–therootsofa(s).Therefore:–ifn−m>0G(s)hasmﬁnitezerosandn−mzerosatinﬁnitysaynzerosinthesetCeconsistingofthecomplexplanetowhichweaddedthe“pointinﬁnity”;ontheotherhandG(s)hasnpolesinCe;SystemsTheory(I)47–ifm−n>0G(s)hasnﬁnitepolesandm−npolesatinﬁnitysaympolesinCe;andG(s)hasmzerosinCe.Fromwhichwegetthefollowingresult:PROPOSITION35.–LetG(s)bearationalfunction.Ithasasmanypolesasithaszerosifwealsoaccountforthepointatinﬁnity.Ontheotherhandthefollowingresultisasimpleconsequenceofthedeﬁnitions(seesection13.6.1):PROPOSITION36.–Atransferfunctionisproperifandonlyifithasnopoleatinﬁnity.*MIMOcaseTherationalebehindsection2.4.4fordeﬁningﬁniteMacMillanpolesandzerosofatransfermatrixcanalsobeusedforthepolesandzerosatinﬁnity.LetG(s)∈R(s)p×mbeatransfermatrixandconsiderG(s)asanelementofR((σ))p×mwhereσ=1/s.WeknowthatR((σ))istheﬁeldoffractionsofS=R[[σ]]theringofformalseriesinσandthattheringSisaprincipalidealdomain(seesection13.1.1).Thusletσκ(κ≥0)betheleastcommondenominatoroftheelementsofG(s)whentheelementsareconsideredthewaywehavejustindicated.WeobtainG(s)=N∞(σ)σκwhereN∞(σ)∈Sp×m.ThematrixN∞(σ)admitsaSmithformS∞(σ)andthereexistmatricesU∞(σ)andV∞(σ)invertibleovertheringSsuchthatN∞(σ)=U−1∞(σ)S∞(σ)V∞(σ)whereS∞(σ)=diag(σµ1...σµr0...0)0≤µ1≤...≤µr.ThereforeG(s)=U−1∞(σ)Σ∞σV∞(σ)whereΣ∞(σ)=1σκS∞(σ)=diag(σν1...σνr0...0)withνi=µi−κandthusν1≤···≤νr.DEFINITION37.–(i)ThematrixΣ∞(σ)iscalledtheSmith–MacMillanformofG(s)atinﬁnity.(ii)Let(¯ςi)1≤i≤rand(¯πi)1≤i≤rbetheﬁnitesequenceofnaturalnumbersdeﬁnedby:¯ςi=max(0νi)and¯πi=max(0−νi).Amongthenaturalnumbers¯ςi(resp.¯πi)thosethatarenon-zero(ifany)arecalledthestructuralindicesofthezerosatinﬁnity(resp.thepolesatinﬁnity)ofthetransfermatrixG(s);theyarearrangedinincreasing(resp.decreasing)orderandaredenotedbyςi1≤i≤ρ(resp.48LinearSystemsπi1≤i≤).(iii)Ifρ≥1(resp.≥1)G(s)issaidtohaveρzeros(resp.poles)atinﬁnitytheithonewithorderςi(resp.πi).(iii)Theinteger(cid:11)1≤i≤πi(resp.(cid:11)1≤i≤ρςi)iscalledthedegreeofthepoles(resp.thezeros)ofG(s)atinﬁnity.REMARK38.–WecanwriteG(s)(asanelementofR((σ))p×m)intheformG(s)=(cid:12)i≥ν1ΘiσiwhereΘi∈Rp×m.ThereforeG(s)isproperifandonlyifν1≥0i.e.ifG(s)hasnopolesatinﬁnity;G(s)isstrictlyproperifandonlyifν1≥1andthenG(s)issaidtohaveablockingzeroatinﬁnityoforderν1.WecannowgeneralizeDeﬁnition30(iii):DEFINITION39.–TheMacMillandegreeofatransfermatrix(notnecessarilyproper)isthesumofthedegreesofallitspoles(includingitspolesatinﬁnity).REMARK40.–WenaturallywillaskthequestionofwhetherProposition36willgeneralizetotheMIMOcase.Theanswerisnegative:see([64]section6.5).Thenumberofpoles(ﬁniteandinﬁnite)ofatransfermatrixcanexceeditsnumberofzeros(ﬁniteandinﬁnite)byaquantitycalledthedefectofthetransfermatrixG(s).Thedefectofaninvertiblesquaretransfermatrixiszero.2.5.Responsesofacontrolsystem2.5.1.Input–outputoperator*GeneralcaseConsideratime-invariantcontrolsystemΣpossiblynonlineardeﬁnedbyanequationsuchas(2.4)(section2.2.2).IndistinguishingtheinputsfromtheothervariablesofthesystemwecansupposethatthesystemequationisoftheformF(cid:9)ξ...ξ(β)u...u(γ)(cid:10)=0.Accordingtosection2.2.6anequilibriumpointw∗=(ξ∗u∗)issuchthatF(ξ∗0...0u∗0...0)=0.Lett0beaninitialinstantandubeaninputsuchthat(cid:20)u(t0)=u∗u(i)(t0)=01≤i≤γ.(2.40)SystemsTheory(I)49AccordingtoConditioniii)ofsection2.3.1thereexistsoneandonlyonesolutionξsuchthat(cid:20)ξ(t0)=ξ∗ξ(j)(t0)=01≤j≤β.(2.41)Inparticularoncetheinitialconditions(2.40)and(2.41)areimposedattimet0theoutputyofΣdependsonlyonu.Thereforethereexistsauniquenonlinearoperator˜Σt0w∗suchthaty=(cid:9)˜Σt0w∗(cid:10)(u).(2.42)DEFINITION41.–Theoperator˜Σt0w∗iscalledtheinput–outputoperatorassociatedwithΣfortheinitialcondition(t0w∗).Thefunctionydeﬁnedby(2.42)iscalledtheresponseofΣfortheinputuandtheinitialcondition(t0w∗).*Caseofalinearortime-invariantsystemIfΣisalinearsystemthenw∗=0isanequilibriumpoint.Itisnotnecessarilytheonlyone–asthereadercanshowaspartofanexerciseusingtheexampleofthesystem(2.23)(section2.3.8)–buttheproblemcanalwayscomedowntothecasewherew∗=0bytranslatingtheoriginofthespacewherewis“living”.Theinput–outputoperatorassociatedwithΣfortheinitialcondition(t00)canthenbedenotedmoreconciselyby˜Σt0.LikewiseifΣislinearandtime-invarianttheproblemcomesdowntothecasewheret0=0byatranslationoftheoriginoftime.Theinput–outputoperatorassociatedwithΣfortheinitialcondition(0w∗)canthenbedenotedby˜Σw∗.LastifΣislinearandtime-invariantwedenoteby˜Σtheinput–outputoperatorassociatedwithΣfortheinitialcondition(00).Thisoperatorisexploredinwhatfollowsnext.Caseofalineartime-invariantSISOsystemConsideralineartime-invariantSISOsystemΣwithtransferfunctionG(s)assumedtobearationalfunction.Withzeroinitialconditionswehaverelation(2.26)betweentheLaplacetransformoftheinputandthatoftheoutput(seesection2.4.2).LetgbetheinverseLaplacetransformofG(s).Intimedomainwehavey(t)=(g∗u)(t)t≥0.(2.43)Consequentlytheinput–outputoperatorassociatedwithΣistheconvolutionoperator˜Σ:u(cid:3)→g∗u.50LinearSystemsCaseofalineartime-invariantMIMOsystemLetΣbealineartime-invariantMIMOsystemwithtransfermatrixG(s)=(Gij(s))ofsizep×m.SupposethateachelementGij(s)isarationalfunction.Atzeroinitialconditionswehaveagainrelation(2.26).LetgijbetheinverseLaplacetransformofGij(s)andletgbethematrixwithentriesoftheelementsgij.ByextensionwecancallGtheLaplacetransformofgandwecaneasilyverifythatG(s)=(cid:2)+∞0−g(t)e−stdtforRe(s)>γ(2.44)γ=maxp∈PRepwherePisthesetofpolesofG(s).Thisexpressionisidenticalto(2.25)(section2.4.1).Wehavethereforeg=L−1(G(s)).LetubeaninputassumedtobealocallyintegrablefunctionwithvaluesinRm.Theinitialconditionsareassumedtobezeroandtheinputisassumedtobepositivelysupported:u(t)=0fort<0.Theconvolutionproductg∗ucanthenbedeﬁned:itisafunctionywithvaluesinRptheithcomponentyiofwhichisgivenbyyi=m(cid:12)j=1gij∗uj(1≤i≤p).Withthisdeﬁnitionoftheconvolutionproducttheexpression(2.26)inLaplacedomaincorrespondstoexpression(2.43)intimedomainwhichisthusextendedtotheMIMOcase.Wehavethusobtainedthefollowinggeneralresult:THEOREM42.–Theinput–outputoperatorassociatedwiththecontrolsystemΣistheconvolutionoperator˜Σ:u(cid:3)→g∗u.AbusingthelanguagewecansaythatG(s)isthetransfermatrixoftheconvolutionoperator˜Σ(aswellasofsystemΣ).Controlsystemandassociatedinput–outputoperatorTheknowledgeofthetransfermatrixG(s)isequivalenttothatoftheconvolutionoperator˜Σ.ButitdoesnotdeterminethecontrolsystemΣunlessitisminimal.Ifthatisnotthecaseindeedtheinclusion(2.38)isnotanequalitysothereexistpolesofthesystemΣwhicharenotrepresentedinthetransfermatrix.WewillseeinChapter7thattheminimalityofalineartime-invariantsystemisanecessaryandsufﬁcientconditionforsuchsystemtobeentirelycharacterizedbyitstransfermatrix(orbyitsassociatedconvolutionoperator).SystemsTheory(I)512.5.2.ImpulseandstepresponsesImpulseresponseInmathematicsgiscalledthekerneloftheconvolutionoperator˜Σ.FromthepointofviewofsystemstheoryletusﬁrstconsiderthecaseofaSISOsystem:gistheoutputofthesystemΣwhentheinputuisthe“Diracimpulse”(whichinmathematicallanguageistheDiracdistribution)δandtheinitialconditionsarezero.ThatiswhywecallgtheimpulseresponseofΣ.ThereforewecanidentifyasystembysubjectingittoaDiracimpulse:weonlyhavetomeasurethecorrespondingresponsewhichistheimpulseresponseanditremainstocalculateitsLaplacetransformtogetthetransferfunctionG(s).StepresponseThemethodjustdescribedishowevernotverypracticalbecausetheDiracdistributionisnotaninputwhichisphysicallyrealizable.Itispreferabletoreplaceitbytheunitstep1(t)deﬁnedby1(t)=(cid:20)0t<01t≥0.(2.45)Thecorrespondingresponse(withzeroinitialconditions)istheunitstepresponse(orthestepresponse).TheLaplacetransformofthestepresponseisG(s)s.Thederivativeofthestepresponse(inthesenseofdistributions)istheimpulseresponse(forthederivativeoftheunitstepistheDiracdistribution):seeequation(12.23)section12.2.3.ExtensiontoMIMOcaseIntheMIMOcaserecallthatgisamatrix(ofsizep×m).Itisagaincalledtheimpulseresponseofthesystem(orsometimesthematrixofimpulseresponses).Theentrygijistheithcomponentoftheoutputwhenthejthcomponentoftheinputisδandtheothercomponentsarezero.TheextensionofthenotionofstepresponsetotheMIMOcase(sometimescalledinthiscasethematrixofstepresponses)isdoneinasimilarmanner.2.5.3.ProperbiproperandstrictlypropersystemsPropersystemsTHEOREM43.–LetΣbealineartime-invariantcontrolsystem.Thefollowingconditionsareequivalent:(i)itsstepresponseisapiecewisecontinuousfunction(havingnodiscontinuityexceptattheorigin);(ii)itstransfermatrixG(s)isproper.52LinearSystemsPROOF.Theproofofthistheoremmakesuseoftheresultsofsection13.6.1;itisdetailedintheSISOcaseanditsextensiontotheMIMOcaseislefttothereader.(1)IfG(s)isproperwehaveG(s)=q0+H(s)whereq0=lim|s|→+∞G(s)andwhereH(s)isastrictlyproperrationalfunction.TheinverseLaplacetransformofH(s)isalocallyintegrablefunctionh.ThereforetheimpulseresponseofΣisg=q0δ+handitsresponsetoalocallyintegrableinputuisy(t)=q0u(t)+(cid:2)t0h(t−τ)u(τ)dτwhichisagainalocallyintegrablefunction.Ifuistheunitstepweobtainy(t)=q01(t)+(cid:2)t0h(τ)dτ(2.46)whichisacontinuousfunctionexceptatt=0ifq0(cid:5)=0.(2)ConverselysupposeG(s)isimproper.ThenthereexistsapolynomialQ(s)withpositivedegreesuchthatG(s)=Q(s)+H(s)whereH(s)justasbeforeadmitsaninverseLaplacetransformhwhichisalocallyintegrablefunction.PutQ(s)=(cid:11)nk=0qkskwheren=d◦(Q)≥1.Thestepresponseisy(t)=n(cid:12)k=1qkδ(k−1)(t)+q01(t)+(cid:2)t0h(τ)dτ(2.47)whichisasingulardistributionsinceqn(cid:5)=0.DEFINITION44.–ThecontrolsystemΣissaidtobeproperifitsatisﬁesoneoftheequivalentconditionsinTheorem43.REMARK45.–AswasshownintheproofofTheorem43alineartime-invariantcontrolsystemisproperifandonlyifitsresponsetoalocallyintegrablefunctionisagainlocallyintegrable.Thecontrolengineermustalwayschoosetheinputsandoutputsofasysteminsuchawaythattheresultingcontrolsystembeproper.Indeedtheunitstepisaninputcommonlyusedandifthesystemisnotproper(wesaythatitisimproper)theresultingoutputisa“singulardistribution”fromamathematicalpointofview(seetheSystemsTheory(I)53proofofTheorem43).Suchasignaldoesnotexistinrealityandinmoreconcretetermswhatactuallyhappensisthedestructionofthesystem(orthesaturationofitscomponents).Animpropersystemisthereforeabadlydesignedsystem.Suchasystemisconsiderednotphysicallyrealizableingeneral.Thispointmaybecontested;butitistruethatanimpropersystemisnotpracticallyrealizable.ExamplesConsideragaintheRLCcircuitinsection1.1.1.Withinputu=Vandoutputy=iitstransferfunctionis(2.28)andthusitisapropercontrolsystem.Ontheotherhandwithinputu=iandoutputy=Vitstransferfunctionis(2.29)andthistimeitisanimpropercontrolsystem.Withtheconventionsofsection2.3.3theDCmotorinsection1.3andtheheatedtankinsection1.4arepropersincetheyhaveastransferfunctions(2.30)and(2.31)respectively.StrictlyproperSystemTheproofofthefollowingtheoremissimilartothatofTheorem43:THEOREM46.–LetΣbealineartime-invariantcontrolsystem.Thefollowingconditionsareequivalent:(i)itsstepresponseisacontinuousfunction;(ii)itstransfermatrixG(s)isstrictlyproper.DEFINITION47.–ThecontrolsystemΣissaidtobestrictlyproperifitsatisﬁesoneoftheconditionsinTheorem46.THEOREM48.–Thesysteminstate-spaceform(2.20)(section2.3.6)isproper(resp.strictlyproper)ifandonlyifW(∂)∈Rp×m(resp.W(∂)=0).PROOF.NoticethatthetransfermatrixofthissystemisG(s)=C(sIn−A)−1B+W(s).(2.48)Onecanprovethefollowingproperty:theoutputofapropersystemisatleast“asregularas”itsinput;forexampleitsresponseforaninputhavingacertainnumberofdiscontinuitiescanhavethesamediscontinuitiesbutnotmore.Whiletheoutputofastrictlypropersystemis“moreregular”thanitsinputtheoutputofsuchasystemiscontinuousevenifitsinputisadiscontinuousfunction.Astrictlypropersystemthereforehasa“regularizing”effect.TheRLCcircuithavinginputu=Vandoutputy=iisastrictlypropercontrolsystemsoisthecaseofDCmotorwithtransferfunction(2.30)andtheheatedtankwithtransfermatrix(2.31).54LinearSystemsWeinsistedaboveonthefactthattheinputsandoutputsofasystemhavetobechoseninsuchamannerthattheresultingcontrolsystembeproper.Generallythissystemisstrictlyproperforthissystemasitdoesnotinstantaneouslyreacttoasolicitationbecauseofits“inertia”.ToclarifyideasconsideranSISOsystematrestatinitialtimet=0(zeroinitialconditions).Taketheunitstepasinputtheresultingoutputisthereforethestepresponse(2.47).ForasystemwithacertaininertialiketheDCmotor(whereitsoutputisy=θory=ω)forexamplethisstepresponsecannotgofromthevaluey=0(atinstantt=0−)toavalue≥αα>0att=0+:theangularspeedofthemotorcannot“jump”fromzerotoaquantity≥αwhenweapplyavoltagestepastheinputandthisisequallytrueafortiorifortheangularposition.Physicalcontrolsystemsareingeneraldesignedinsuchawaythattheyarestrictlyproper.REMARK49.–WehaveseenfromTheorem11thatitispossibletochooseaﬁnitesequence(u1...um)amongthevariablesofasystemsuchthatu=[u1...um]Tcanbeaninputofthissystem.Anotherquestioniswhethertheinputucanbechoseninsuchawaythattheresultingcontrolsystembeproper.Thisquestionisstudiedin[22].Obviouslythepropernessofacontrolsystemdependsnotonlyonthechoiceofitsinputsbutalsoonthechoiceofitsoutputs.Thedetailedanalysismadeinthecitedreferencewhereanalgorithmisalsoproposedtorealizeinasystematicmannerthechoiceofushowedtheimportanceofthenon-controllablepolesatinﬁnityalsocalledtheinput-decouplingzerosatinﬁnity(anotionthatisbeyondthescopeofthiswork:see[16]or[22]).Consideraproper(orevenstrictlyproper)controlsystem;ifthatsystemhasuncontrollablepolesatinﬁnityitcanstillgenerate–inresponsetoastepinputforexample–“impulsivemotions”consistingoflinearcombinationsoftheDiracdistributionanditsderivativeswhichwillleadtothesystemdestruction.BipropersystemSomepurelyelectricalorelectronicsystemsareproperbutnotstrictlyproperthesimplestexamplebeinganelectriccircuitconsistingofaresistanceRandavoltagegenerator.Theinputu=V(voltageacrosstheresistance)isrelatedtotheoutputy=i(electriccurrentintensitythroughtheresistance)bytherelationV=Ri.ThesystemtransferfunctionisthereforeG(s)=1R:whichisproperbutnotstrictlyproper.ItsinverseG−1(s)=RisdeﬁnedandisproperthusG(s)isbiproper(seesection13.6.1).Forthisreasonthecorrespondingcontrolsystemisalsoqualiﬁedasbiproper.Itisquiterarehavingtocontrolaphysicalsystemofthisnature;ontheotherhanditisquitecommonforacontrollertobebiproper.Ingeneral:DEFINITION50.–Alineartime-invariantsystemΣisbiproperifitstransfermatrixhasthisproperty.SystemsTheory(I)552.5.4.FrequencyresponseDeﬁnitionWeconsideronlytheSISOcaseheretheextensiontotheMIMOcaseistrivial(seeinsection2.5.2howthenotionofimpulseresponseisextendedtotheMIMOcase).Letu(t)=Acos(ω0t)t∈(−∞+∞)asinusoidalsignalbetheinputofsystemΣ.Tocalculatetheoutputofthesystemitisconvenienttowriteu(t)=Reuc(t)whereuc(t)=Aeiω0t.Letusexaminewhythisisso.TheresponseyofΣtotheinputuisequaltotheconvolutionproductg∗uwheregdenotestheimpulseresponseofΣ.Likewisethe(ﬁctitious)responseycofΣtothe(ﬁctitious)inputucisequaltotheconvolutionproductg∗uc.WehavethereforeRe(g∗uc)=Reg∗Reucandsincegisrealy=Reyc.1◦FirstofallconsiderthecasewhereG(s)hasnopoleswithpositiverealpart.Theimpulseresponsegisthereforeatempereddistributionanditresultsaccordingtosection12.3.1in:F(g∗uc)=FgFuc.Now(Fuc)(ω)=A(cid:2)+∞−∞ei(ω−ω0)tdt=2πAδ(ω−ω0)(see(12.34)section12.3.1).ThereforeFgFuc=(Fg)(ω0)FucandbytakingtheinverseFouriertransformweobtainyc(t)=(Fg)(ω0)uc(t)andﬁnallyyc(t)=G(iω0)uc(t).(2.49)Bydeﬁnitionthefunction(ordistribution)ω(cid:3)→G(iω)iscalledthefrequencyresponseofthesystemΣ.ThisistheFouriertransformoftheimpulseresponse.Itisnoweasytocalculatetheoutputy(t)byusingthepolardecompositionofG(iω0):G(iω0)=|G(iω0)|eiargG(iω0).Weget:yc(t)=A|G(iω0)|e[iω0t+argG(iω0)]fromwhichy(t)=A|G(iω0)|cos[ω0t+argG(iω0)].(2.50)Expression(2.49)usingcomplexsignalsissimplerthan(2.50)whichusesrealsignals.Thatiswhyitisinterestingtoworkwithcomplexnumbers.REMARK51.–ThefrequencyresponseFgisalsodeﬁnedinthecasewhereG(s)haspolesontheimaginaryaxisbutitisasingulardistribution(notafunction).56LinearSystemsNeverthelesstheoutput(2.50)isonlydeﬁnedifs=iω0isnotapoleofG(s).Thisoutputisthusdeﬁnedforallﬁnitevaluesofω0ifandonlyifG(s)hasnopolesontheimaginaryaxisandthusonlyhaspolesthathaveanegativerealpart(i.e.allpolesbelongtothelefthalf-plane5).Moreoverthesetoftheseresponsesisboundedwhenω0varies(andinparticulartendsto+∞)ifandonlyifG(s)isaproperrationalfunctionthenbelongingto(cid:1)H∞:seesection13.6.2.2◦TheabovedeﬁnitioncanbeextendedtoanytransferfunctionG(s)havingnopolesontheimaginaryaxis:itsfrequencyresponseisthefunctionω(cid:3)→G(iω).OntheotherhandthisfrequencyresponsecannolongerbedeﬁnedastheFouriertransformoftheimpulseresponsesincethisFouriertransformdoesnotexist.Withthisgeneralizeddeﬁnitionthefrequencyresponseisstillthequantitywhichmultipliesasinusoidalinputeiωttoproducetheoutputsignalprovidedthatthesystemisstabilizedbyfeedback.ExperimentaldeterminationofthefrequencyresponseofasystemThefrequencyresponseofasystemcanbedeterminedexperimentallyifthetransferfunctionG(s)isproperandhasonlypoleswithnegativerealpart(thisdoubleconditionisessentialasshowninRemark51;butasitturnsoutfromn◦2abovetheproceduremaybeextendedtothecasewhereG(s)alsohaspoleswithpositiverealpartprovidedthatthesystemisstabilizedbyfeedback.Itisappropriatetooperateinthefollowingmanner:–Forarepresentativesetoffrequenciesωputasinusoidalsignaluω(t)=Acos(ωt)atthesysteminput.Waitforasufﬁcientlylongtimeforthesteadystatetobereached(intheoryaswehaveseenabovewehavetowaitaninﬁnitelylongtime).–Foreachofthefrequenciesωabovemeasuretheoutputyω(t).Accordingto(2.50)theratiobetweentheamplitudeofyωandthatofuωisequalto|G(iω)|andthephasedifferencebetweenyωanduωisgivenbyargG(iω).–WethusobtainpointbypointthefrequencyresponseG(iω).2.6.Diagramsandtheiralgebra2.6.1.DiagramofacontrolsystemLetΣbeasystemwithinputuandoutputy.AcommonusageincontroltheoryistorepresentΣbyaboxandsomearrowsasshowninFigure2.2.Wecallthisthesystemdiagram.5.Thisexpressionreferstothecomplexplane.Inwhatfollowswecallthe“lefthalf-plane”theopensetC−={s∈C:Re(s)<0}.Itsclosure¯C−={s∈C:Re(s)≤0}iscalled“closedlefthalf-plane”.SimilarlywecalltheopensetC+={s∈C:Re(s)>0}the“righthalf-plane”anditsclosure¯C+={s∈C:Re(s)≥0}the“closedrighthalf-plane”.SystemsTheory(I)57uyFigure2.2.SystemdiagramSucharepresentationcanbeusedinverygeneralcasesandinparticularevenforasystemΣthatisnonlinearand/ortime-varying.Neverthelessthisrepresentationsuggeststhatweareonlyinterestedintheinput–outputoperator˜ΣassociatedwithΣ.Inthelineartime-invariantcaseitdoesnotresultinalossofinformationifandonlyifΣisminimal(seesection2.5.1).Thisremainstrueinthenonlinearand/ortime-varyingcase[18].Inthelineartime-invariantcasewecan(seeFigure2.2)replaceΣbyitstransfermatrixG(s).2.6.2.GeneralalgebraofdiagramsTherecanbeextremelydiverseconnectionsamongsystems[46].Weconsiderbelowconnectionsinparallelinseriesandwithunitfeedback.Theﬁrsttwoconnectionscorrespondtoanadditionandtoamultiplicationrespectively;thethirdisspeciﬁctosystemstheoryandisafundamentaloperationofthisscience.SystemsinparallelThesystemsΣ1andΣ2inFigure2.3aresaidtobe“inparallel”.LetΣbetheresultingsystemwithinputuandoutputy.Lety1andy2betheoutputsofΣ1andΣ2respectively.Wehavey1=˜Σ1uy2=˜Σ2uy=y1+y2asaresulty=˜Σu˜Σ=˜Σ1+˜Σ2.(2.51)Figure2.3.Systemsinparallel58LinearSystemsyzu12Figure2.4.SystemsinseriesNotethatΣisnotnecessarilyminimalevenifΣ1andΣ2areso[46].Expression(2.51)isvalideveninthenonlinearand/ortime-varyingcase.IfΣ1andΣ2arelineartime-invariantwithtransfermatricesG1(s)andG2(s)respectivelythenΣisagainlineartime-invariantanditstransfermatrixG(s)isgivenbyG(s)=G1(s)+G2(s).(2.52)SystemsinseriesThesystemsΣ1andΣ2inFigure2.4aresaidtobe“inseries”.ConsidertheresultingsystemΣwithinputuandoutputy.TheoutputzofΣ1isequaltotheinputofΣ2.Wehavez=˜Σ1uy=˜Σ2zthereforey=˜Σuwith˜Σ=˜Σ2˜Σ1.(2.53)Thecompositionoftheoperatorsisdenotedbyamultiplication.ThesystemΣisnotnecessarilyminimalevenifΣ1andΣ2areso[46].Expression(2.53)isvalideveninthenonlinearand/ortime-varyingcase.IfΣ1andΣ2arelineartime-invariantwithtransfermatricesG1(s)andG2(s)respectivelythenΣisagainlineartime-invariantanditstransfermatrixG(s)issuchthatG(s)=G2(s)G1(s).(2.54)Itisimportanttobeawarethattheproduct(2.54)isnotcommutativeintheMIMOcase.Regardingthecomposition(2.53)itisnotcommutativeinthenonlinearcaseevenifΣ1andΣ2areSISO.SystemsTheory(I)59v-+yuoFigure2.5.ElementaryfeedbackElementaryfeedbackConsiderthe“elementaryfeedback”inFigure2.5.ThesystemΣ0withinputyandoutputviscalledthe“open-loopsystem”.The“closed-loopsystem”withinputuandoutputyisdenotedbyΣ.Wehavethefollowingrelations:v=˜Σ0yy=u−vthus(cid:9)Ip+˜Σ0(cid:10)y=u(2.55)wherepisthenumberofcomponentsofy.Wecanexpressyasafunctionofuonlyiftheoperator(cid:9)Ip+˜Σ0(cid:10)isinvertible.6Thenthefeedbacksystemissaidtobe“well-deﬁned”.InthecasewhereΣ0islineartime-invariantwithtransfermatrixG0(s)thefeedbacksystemiswell-deﬁnedifandonlyifIp+G0(s)isinvertibleinthealgebraR(s)p×p.Wewillreturntothisinsection4.1.2.Assumingthatthefeedbacksystemiswell-deﬁnedweobtainfrom(2.55)y=˜Σuwhere˜Σ=(cid:9)Ip+˜Σ0(cid:10)−1.(2.56)Expression(2.56)isvalideveninthecasewhereΣ0isnonlinearand/ortime-varying.Ifitislineartime-invariantsoitisalsoΣanditstransfermatrixG(s)canbeexpressedasafunctionofthetransfermatrixG0(s)ofΣ0asfollows:G(s)=[Ip+G0(s)]−1.6.Wewillnotexplainthemeaningofthisinvertibilityinthegeneralcase(see[116]forfurtherreadingonthissubject).60LinearSystemsFigure2.6.DiagramexampleApplicationexampleConsiderFigure2.6.Thethreerulesdevelopedabovecanbeappliedtodeterminetheinput–outputoperatorrelatingutoy.ThesystemΣdeﬁnedbythisdiagramisconstitutedbyputtingfoursystemsinseries:–thesystemΣ5;–theloopwithinputvandoutputewhichisanelementaryfeedbacksuchasinFigure2.5with˜Σ0=˜Σ4˜Σ1(payattentiontotheorder!);–thesystemΣ1;–thesystemΣ3.Wehavethereforewiththeassumptionthattheelementaryfeedbackiswell-deﬁnedy=˜Σuwith˜Σ=˜Σ3˜Σ1(cid:9)I+˜Σ4˜Σ1(cid:10)−1˜Σ5.(2.57)2.6.3.Speciﬁcityoflinearsystems“Multiplication”ofoperatorsdeﬁnedinsection2.6.2isalwaysright-distributivewithrespecttoaddition;thismeans(asshownimmediately)(cid:9)˜Σ1+˜Σ2(cid:10)˜Σ3=˜Σ1˜Σ3+˜Σ2˜Σ3.Ontheotherhandmultiplicationisleft-distributivewithrespecttoadditiononlyinthecaseoflinearsystems(time-invariantornot).Morepreciselywehavetherelation˜Σ3(cid:9)˜Σ1+˜Σ2(cid:10)=˜Σ3˜Σ1+˜Σ3˜Σ2ifΣ3isalinearsystem.SystemsTheory(I)61TakeagaintheexampleinFigure2.6withtosimplify˜Σ3=Iand˜Σ5=I.Wehaveaccordingto(2.57)nomatterofwhatnaturearethesystemsΣ1andΣ4˜Σ=˜Σ1(cid:9)I+˜Σ4˜Σ1(cid:10)−1.SupposenowthatΣ1isalinearsystem.Wecanwritez=˜Σ1(cid:9)v−˜Σ4z(cid:10)fromwhichwehave(cid:9)I+˜Σ1˜Σ4(cid:10)z=˜Σ1vandﬁnally(assumingthattheloopiswell-deﬁned)z=(cid:9)I+˜Σ1˜Σ4(cid:10)−1˜Σ1.WehavethusobtainedthefollowingresultwhichissimilartoLemma520(section13.3.3):THEOREM52.–IfΣ1isalinearsystemwehavetheequality˜Σ1(cid:9)I+˜Σ4˜Σ1(cid:10)−1=(cid:9)I+˜Σ1˜Σ4(cid:10)−1˜Σ1.Thealgebraofdiagramsisthereforericherinthecontextoflinearsystemsthaninthatofnonlinearsystems.Itispossibletoderiverulesthatallowthesimpliﬁcationoflineardiagramswithsmallcalculationsinparticularthe“Masonrule”(seee.g.[77]).2.7.ExercisesEXERCISE53.–DeterminethetransferfunctionofthetraininFigure1.3withinputtheforcefandoutputthepositiony=z2.Whatarethetransmissionpolesandzerosofthissystem?EXERCISE54.–SamequestionsasaboveforthecaseoftheDCmotorwheretheoutputchosenisy=ω(speedcontrol).EXERCISE55.–*LetG(s)havethefollowingexpression:(a)#s+1(s−1)21(s+1)(s−1)0(s+1)2(s−1)3$;(b)#1(s+1)21(s+1)(s+2)1(s+1)(s+2)s+3(s+2)2$;(c)#1(s+1)21(s+1)(s+2)1(s+1)(s+2)s+1(s+2)2$.62LinearSystemsInthesethreecasesdeterminetheMacMillanpolesandzerosofthetransfermatrix.Whatisthetransmissionorderofasystemthathassuchatransfermatrix?EXERCISE56.–DeterminetheequilibriumpointsofthedoubleinvertedpendulumofExercise2(section1.5)andlinearizethissystemaroundthepointsy∗=0θ∗1=θ∗2=0.EXERCISE57.–SameproblembutforthemixerofExercise3.EXERCISE58.–InthecaseoftheinvertedpenduluminFigure1.4linearizedasabovewhatisthenumberofinputvariables?Whichonecanbereasonablychosenasthecontrolvariable?Thetwomeasuresbeingzandθarethereanylatentvariables?Willarepresentationotherthan(2.7)havelatentvariables?EXERCISE59.–WeconsiderthemixerlinearizedinExercise57.Whatisthenumberofinputvariables?Whichonesaretobechosen?Howmanyvariableswillneedtoberegulated?Andwhatarethey?EXERCISE60.–Manynonlinearcontrolsystemsaredescribedbyanonlinearstaterepresentationofthefollowingform:˙x=f(xu)y=g(xu)wherefandgareofclassC1.(i)Let(x∗u∗y∗)beanequilibriumpoint.Whataretherelationsthatthesequantitiesmustsatisfy?(ii)Writedownthestateequationsofthelinearizedsystemaboutthisequilibriumpoint.Chapter3Open-LoopSystemsInthischapterweonlyconsiderlineartime-invariantSISOsystemsunlessotherwisestated.3.1.Stabilityandstaticgain3.1.1.StabilityDEFINITION61.–ThelinearsystemΣ(assumedtobeminimal)issaidtobestableifitstransferfunctionG(s)(assumedtoberational)isproperandifallitspolesbelongtothelefthalf-plane.1ThisdeﬁnitionisalsovalidinthecaseofanMIMOsystem;thenG(s)isthetransfermatrixoftheminimalsystemΣandthepolesofthissystemcoincidewiththeMacMillanpolesofG(s)(seesection2.4.4).Wehavealreadyseenthisdoublecondition(seeRemark51insection2.5.4).Asaresultstabilityistheconditionthatallowsforan“open-loop”experimentaldeterminationofthefrequencyresponseofasystem.Thesetoftransferfunctionsthatsatisﬁestheabovedoubleconditioniswrittenas(cid:1)H∞(and(cid:1)Hp×m∞intheMIMOcasewithminputsandpoutputs)(seesection13.6.2).Wecallthisset:thesetofstabletransferfunctions.1.Seesection2.5.4footnote5.63Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.64LinearSystemsAsaresultofTheorem589(section13.6.2)asystemisstableifinwaysthatareequivalent:–itsoutputissolongasitsinput–itsoutputhasﬁniteenergyaslongasitsinputhasﬁniteenergy(withinadditionacontinuityconditionthatisspeciﬁedinthepreviouslycitedtheorem).3.1.2.StaticgainCaseofastablesystemLetΣbeastablelinearsystemandfbeitsstepresponse.Itisgivenasin(2.46)whereh∈L1.Ithasalimitasttendsto+∞givenbylimt→+∞f(t)=g0+(cid:2)+∞0h(τ)dτ.Wecanapplytheﬁnalvaluetheorem(seesection12.3.4):theLaplacetransformoffisˆf(s)=ˆg(s)ssothatlimt→+∞f(t)=lims∈Rs→0+sˆf(s)=lims∈Rs→0+ˆg(s)=ˆg(0).(3.1)Thisquantityiscalledthestaticgainofthesystem(orofitstransferfunction).Accordingto(3.1)thestaticgainhasseveralinterpretationswhichofcourseareequivalent:(i)Inthetimedomainitistheﬁnalvalueofthestepresponse.(ii)Inthefrequencydomainitisˆg(0)=Fg(0):frequencyresponseofasystematfrequencyzero.(iii)Wecanalsosaythatitisthegainmultiplyingaconstantsignaluproducinganoutputy(alsoconstant).CaseofanunstablesystemLetΣbeanunstablesystemhavingnotransmissionpoleats=0sothatthequantityˆg(0)isdeﬁned.Wecanalsocallthisquantitythestaticgainofthesystem.Interpretation(i)isnolongervalid(theﬁnalvalueofthestepresponseisnolongerdeﬁned);in(ii)theequalityˆg(0)=Fg(0)isnolongertruesincetheFouriertransformFgisnotdeﬁned;andﬁnally(iii)remainsvalidifthesystemisstabilizedbyfeedback.Open-LoopSystems653.2.First-ordersystems3.2.1.TransferfunctionThetransferfunctionofaﬁrst-ordersystemassumedtobestrictlyproperisoftheformG(s)=bs−a.(3.2)Asaresultthissystemhasauniquepoles=a;andthereforeitisstableifandonlyifa<0.Letusconsidersuchacase(thecaseofanunstablesystemwillbetreatedatsection3.2.5).Letk=−baandτ=−1a.ThenthesystemtransferfunctioniswrittenasG(s)=k1+τs.(3.3)Thequantityτ>0whichrepresentsaperiodoftimeiscalledthetimeconstantofthesystem.Itisimmediatelyclearthatkisthestaticgain.3.2.2.TimedomainresponsesImpulseresponseTheimpulseresponsegofthissystemistheinverseLaplacetransformofG(s)i.e.g(t)=beat1(t)=kτe−tτ1(t).Inparticularg(0+)=kτ.ThegraphofthisfunctionisrepresentedinFigure3.1(solidline)wherek=1(unitstaticgain)andτ=10s.StepresponseThestepresponseisthefunctionhdeﬁnedbyh(t)=(cid:2)t0g(τ)dτ=ba(cid:15)eat−1(cid:16)1(t)=k(cid:9)e−tτ−1(cid:10)1(t).(3.4)ThegraphofthisfunctionisrepresentedinFigure3.2(solidline)fork=1(unitstaticgain)andτ=10s.Notethatthestepresponseiscontinuousattheorigin(whichisduetothefactthatthetransferfunctionisproper)contrarytotheimpulseresponse(thederivativeofthestepresponse).WeseeinFigures3.1and3.2howtoconstructtheslopeattheorigin(dashedlines)fromthetimeconstantτfortheimpulseresponseandthestepresponserespectively.66LinearSystems010203040506000.010.020.030.040.050.060.070.080.090.1Impulse ResponseTime (sec)AmplitudeFigure3.1.Impulseresponseofaﬁrst-ordersystem010203040506000.10.20.30.40.50.60.70.80.91Step ResponseTime (sec)AmplitudeFigure3.2.Stepresponseofaﬁrst-ordersystemOpen-LoopSystems673.2.3.FrequencyresponseThefrequencyresponseisthe(complex-valued)functiondeﬁnedbyFg(ω)=G(iω)=k1+iτω.Thereareseveralwaystorepresentthisfrequencyresponse:–Amplitudeandphaseasafunctionofthefrequency:thisistheBodeplotrepresentation.–CurvesthattracetheevolutionofthecomplexnumberFg(ω)=G(iω)inthecomplexplaneasafunctionoftheparameterω:thisistheNyquistplotrepresentation.–Curvesthattracetheevolutionofthecoordinates(|Fg(ω)|argFg(ω))asafunctionoftheparameterω:thisistheBlackplot.NyquistandBlackplotsareonlyinterestingforthestudyofclosed-loopsystemswhichisdonelater.AtthismomentwewillstudythefrequencyresponseusingBodeplotsonly.3.2.4.BodeplotMagnitudeWehaveassumingk>0|G(iω)|=k√1+τ2ω2.(3.5)ThisnumberexpressedindecibelsiscalledthemagnitudeofG(iω).Wehavebydeﬁnition2|G(iω)|dB=20log|G(iω)|.(3.6)OntheotherhandwewillonlyconsiderpositivefrequencieswithoutgeneratinganylossofinformationsinceFg(−ω)=Fg(ω)andwewillplotthesefrequenciesonalogarithmicscale.2.Throughoutthistextwedenotebylogthedecimallogarithmwhereasthenaturallogarithmisdenotedbyln.68LinearSystemsFor0<τω(cid:16)1:|G(iω)|dB∼=20logk(3.7)(withanerrorthattendsto0whileω→0+).Forτω(cid:18)1:|G(iω)|dB=20logk−10|G(iω)|dB=20logk−20log(cid:15)1+τ2ω2(cid:16)=20logk−20log(τω)−10log(cid:21)1+1τ2ω2(cid:22)∼=20logk−20log(τω)(3.8)(withanerrorthattendsto0whileω→+∞).Thislastexpressionisalinethathasaslopeof−20dBperdecade(thefrequencychangesbyadecadewhenitismultipliedby10anoctavewhenmultipliedby2;arateofchangeof−20dBperdecadecorrespondstoapproximatelyarateofchangeof−6dBperoctave).AsaresulttheBodemagnitudeplothastwoasymptotes:–thehorizontalasymptote(3.7)forlowfrequencies;–theobliqueasymptote(3.8)forhighfrequencies.Thesetwoasymptotesintersectatthe“cornerpoint”atabscissaω=1τandformwhatwecallthe“asymptoticBodemagnitudecurve”.Forω=1τ|G(iω)|dB=20logk−20log2(cid:9)20logk−3.Thusatthe“cornerfrequency”pointtheBodemagnitudecurveis3dBbelowitsasymptoticcurve.PhaseOntheotherhandwehaveargG(iω)=−arctan(τω).(3.9)ThisargumentexpressedindegreesiscalledthephaseofG(iω).Itisequallytracedasafunctionofωthislastvariableisrepresentedinalogarithmicscaleandwecallthisgraphthe“Bodephaseplot”.Open-LoopSystems69-20-18-16-14-12-10-8-6-4-20Magnitude (dB)10-2100101-90-60-300Phase (deg)Bode DiagramFrequency  (rad/sec)Figure3.3.Bodeplotofaﬁrst-ordersystemItisclearthatlimω→0argG(iω)=0◦limω→+∞argG(iω)=−90◦.TheBodephaseplothastwohorizontalasymptotesat0◦forlowfrequenciesandat−90◦forhighfrequencies.AtthecornerfrequencyasdeﬁnedbeforewehaveargG(cid:21)iτ(cid:22)=−arctan1=−45◦BodeplotThesetofthetwoBodecurvesthatofthemagnitudeandthephaseformthe“Bodeplot”onwhichweoftenaddthe“asymptoticdiagram”asisshowninFigure3.3(correspondingtok=1andτ=10);theBodeplotisathinrigidbodylinewhiletheasymptoticplotisathickerdashedline.3.2.5.Caseofanunstableﬁrst-ordersystemAﬁrst-orderunstablesystemhasatransferfunctionoftheform(3.2)witha≥0.70LinearSystemsIntegratorsystemFirstconsiderthecasewherea=0i.e.G(s)=bs.Assumingthatb>0(thecaseb=0istrivialandthecaseb<0isonlydifferentbyachangeofsign)wecanputb=1ττ>0.ThereforeG(s)=1τs.Consequently:(cid:127)|G(iω)|=1τωfromwhichwehaveforω>0:20log|G(iω)|=−20log(τω).TheBodemagnitudeplotisthereforeastraightlinewithaslopeof−20dBperdecade.Wehave|G(iω)|=1(whichis0dB)forω=1τ.(cid:127)argG(iω)=−90◦.SystemwithapositivepoleConsidernowthecasewherea>0.WecanwriteG(s)intheformG(s)=k1−τs.AsaresultG(iω)=k1−τiωandso|G(iω)|=k√1+τ2ω2whichisidenticalto(3.5):ifthepoleischangedtoitsoppositetheamplituderemainsunchanged.OntheotherhandargG(iω)=arctan(τω)anexpressionthatisofoppositesignto(3.9):ifthepoleischangedtoitsoppositethephaseischangedinthesameway.3.3.Second-ordersystems3.3.1.TransferfunctionConventionsThetransferfunctionofasecond-ordersystemassumedtobestrictlyproperisoftheformG(s)=b1s+b2s2+a1s+a2.Open-LoopSystems71Ifb1(cid:5)=0thissystemhasazero(oftransmission)z=−b2b1;ifb1=0thissystemhasnozero.Inwhatfollowsinthissectionwewillonlybeinterestedinasystemwithoutzero(b1=0);seesection3.4forastudyofthegeneralcase.PolesThediscriminantofthedenominatoris∆=a21−4a2.–If∆≥0G(s)hastworealpoles(indistinctfor∆=0)andisthustheproductoftwoﬁrst-ordertransferfunctions.Wearethereforeledtothesamestudyasbefore(seesection3.4formoredetails).–If∆<0wehavenecessarilya2>0andwecanputa2=ω20ω0>0.Toclarifythecalculationswecanfurtherputa1=2ςω0b2=kω20.WiththeseconventionsthesystemtransferfunctioncanbewrittenasG(s)=kω20s2+2ςω0s+ω20.Wehavek=G(0)thuskisthestaticgain.Thereduceddiscriminantofthedenominatoris∆(cid:2)=ω20(cid:15)ς2−1(cid:16).Since∆(cid:2)<0wehave|ς|<1andthepolesare(cid:7)−ςω0−iω0%1−ς2−ςω0+iω0%1−ς2(cid:8).Thesystemisstableifandonlyifthesetwocomplexconjugatepoleshavenegativerealpartsi.e.ifς>0.ThesetwopolesthenlieinthecomplexplaneasshowninFigure3.4.Thequantityω0istheabsolutevalueofbothpolesconsideredi.e.thedistanceinthecomplexplanebetweenanyofthesepolesandtheorigin.Wecallthistheundampednaturalfrequency.TheangleΨinFigure3.4isrelatedtoςbytherelationς=sinΨ.Thisςtermiscalledthedampingcoefﬁcient.3.3.2.TimedomainresponsesThetwotypicaltimedomainresponses(impulseandstep)arenowlookedatinthecaseofunitystaticgain(k=1).72LinearSystemsFigure3.4.ComplexconjugatepolesImpulseresponseTheimpulseresponsegisgivenbyg(t)=L−1G(s)=L−1(cid:20)ω20s2+2ςω0s+ω20&=ω0√1−ς2e−ςω0tsin(cid:9)ω0%1−ς2t(cid:10)1(t).(3.10)Thisresponseoscillatesatthenaturalfrequencyωp=ω0√1−ς2.Theimpulseresponse(3.10)isplottedinFigure3.5forω0=10rad/sandfordifferentpositivevaluesofthedampingcoefﬁcientς:0.1(‘-’)0.3(‘--’)0.5(‘-.’)and0.7(‘:’).3Weobservethatthesmallerthevalueofthedampingcoefﬁcientthemoreoscillatorytheimpulseresponse.Forς=0theimpulseresponsebecomespurelysinusoidalwithfrequencyω0(fromwhichcomesthenameofthisoscillation).3.Thatisrigidbodydasheddash-dotteddottedlinesrespectively.Open-LoopSystems73Figure3.5.Impulseresponseofasecond-ordersystemStepresponseThestepresponseisthefunctionhdeﬁnedbyh(t)=(cid:2)t0g(τ)dτ=(cid:27)1−e−ςω0t(cid:21)cosωpt+ς√1−ς2sinωpt(cid:22)(cid:28)1(t).(3.11)ThisstepresponseisplottedinFigure3.6forω0=10rad/sandforς=0.10.30.5and0.7(withthesameconventions).Themaindifferencewiththeimpulseresponseistheﬁnalvalue:1(becausethestaticgainofG(s)is1)insteadof0(ofcoursethisﬁnalvalueisonlydeﬁnedforς>0i.e.whenthesystemisstable).Themaximumofthestepresponsehisobtainedattheﬁrsttimethatitsderivativegiszerothatist=πωp.Thismaximumisthereforeh(cid:21)πωp(cid:22)=1+e−ςπ√1−ς2(cid:1)M(ς).Thegraphofthefunctionς(cid:3)→M(ς)isrepresentedinFigure3.7(forς∈[01)).Weobservethattheovershootiszeroforς=1isoftheorderof5%forς=0.7andoftheorderof10%forς=0.6;ittendsto100%whenςtendsto0.74LinearSystemsFigure3.6.Stepresponseofasecond-ordersystem00.20.40.60.8111.11.21.31.41.51.61.71.81.92Figure3.7.OvershootasafunctionofthedampingcoefﬁcientOpen-LoopSystems753.3.3.BodeplotThefrequencyresponseofthesystemisgivenbyG(iω)=ω20ω20−ω2+2iςω0ω=11−υ2+2iςυwhereυ(cid:1)ωω0isthenormalizedfrequency.StudyoftheamplitudeThesquareoftheamplitudeofG(iω)is|G(iω)|2=1(1−υ2)2+4ς2υ2.Thisquantityisindependentofthesignofthedampingcoefﬁcientς;withoutlossofgeneralityletς>0(caseofastablesystem).ThisvaluebecomesamaximumwhenitsdenominatorDisminimum.Ontheotherhandthisisafunctionofonly=υ2.WehavedDd=2(cid:15)υ2−1(cid:16)+4ς2=2(cid:25)υ2−(cid:15)1−2ς2(cid:16)(cid:26).(3.12)Thereforei)Forς>√22:Thederivative(3.12)willnotbezeroandsothefunctionω(cid:3)→|G(iω)|doesnothaveamaximumon(0+∞).Wesaythatthesystemdoesnothavearesonance.ii)For0<ς<√22:Thederivative(3.12)becomeszeroforυ=√1−2ς2i.e.forω=ωrwithωr(cid:1)ω0√1−2ς2.Thisfrequencyωristhatatwhich|G(iω)|attainsitsmaximumon(0+∞):itistheresonancefrequency.Wecaneasilyverifythat|G(iωr)|=12ς√1−ς2.Thisquantityistheratiobetweenthemaximumvalueof|G(iω)|andthestaticgain(whichis1inthiscase).Thisvalueiscalledtheresonancefactorandisdenotedbyλ.ItissometimesexpressedindecibelsanddenotedbyQ:Q=20logλ.Notethatforthesmallvaluesofςwehaveωr(cid:9)ω0andλ(cid:9)12ς.76LinearSystemsAsaresultwhenthedampingcoefﬁcientςtendsto0theresonancefactortendsto+∞.Inadditiontheresonancefrequencyωraswellasthenaturalfrequencyωptendtowardstheundampednaturalfrequencyω0.Letu(t)=Acos(ωt)asinusoidalsignalofamplitudeAbetheinputofthesystem.Asshowninsection2.5.4theoutputisasinusoidalsignalofamplitudeA|G(iω)|.Thisamplitudeismaximumwhenω=ωrandthismaximumamplitudetendsto+∞whenςtendsto0.Thisshowsthatthesystembecomesunstablewhenitsdampingcoefﬁcienttendsto0sincewecanﬁndaboundedinputtowhichcorrespondsanunboundedoutput(whichcontradictsPropertyv)ofTheorem589giveninsection13.6.1.iii)Forς=√22:Thisisthevaluethatmakesthetransitionbetweentheabsenceorpresenceofresonance.Thisnumber(cid:9)√22(cid:9)0.7(cid:10)iscalledthecriticalvalueofthedampingcoefﬁcient(orofthedamping).Recallthataccordingto(3.12)dDd(0)=−2(cid:15)1−2ς2(cid:16)ω20.Thisquantityiszeroifandonlyifthesystemiscriticallydamped.Itisobviousthatitisalsotrueford|G|dω(0).Thecriticaldampingthereforehappenswhenthegraphoftheamplitude|G|asafunctionofthefrequencyωhasaahorizontaltangentatω=0(theamplitudeandthefrequencyarebothplottedinalinear–andnotlogarithmic–scaleinthiscase).StudyofthephaseWehaveargG(iω)(cid:1)ϕ(υ)=(cid:20)−arctan2ς1−υ2ifυ<1⇔ω<ω0−arctan2ς1−υ2−πifυ>1⇔ω>ω0.Weseethatifwechangeςto−ςϕ(υ)ischangedto−ϕ(υ).CaseofastablesystemAssumethatς>0.Wehaveinparticular–Forυ→0ϕ(υ)→0.–Forυ→+∞ϕ(υ)→−π.–Forυ→1ϕ(υ)→−π2andthisfunctionϕ(υ)iscontinuousatυ=1.Open-LoopSystems77Ontheotherhanddϕdυ=−11+(cid:9)2ς1−υ2(cid:10)2(2ς)2υ(1−υ2)2=−4ςυ(1−υ2)2+4ς2thereforedϕdυ(1)=−1ς.Thevariationofthephaseinthevicinityofυ=1isthereforeallthemorerapidasthedampingcoefﬁcientisweakened.CaseofanunstablesystemAsalreadymentionedasignchangeinthedampingcoefﬁcientimpliesasignchangeinphase.Theabovestudyremainsvalidapartfromadifferenceinsign.BodeplotinlogarithmicscaleAsfortheﬁrst-ordersystemalreadystudiedwecannowplottheamplitudeandthephaseasafunctionoffrequency(toprovideamoreintrinsiccharactertothesecurveswehavechosentousethenormalizedfrequencyontheabscissa);theamplitudeisexpressedindecibelsthephaseisexpressedindegreesandalogarithmicscaleisusedforthefrequency.ThisrepresentationmakesapparenttheBodeasymptoticcurves:–Forυ→0|G(iω)|→1(i.e.0dB).–Forυ→+∞|G(iω)|∼1υ2andtherefore20log|G(iω)|=−40logυ+(termsthattendto0whenυ→+∞).Theasymptoticdiagramofthemagnitudethusconsistsoftwolinesegments:–Ahorizontallinesegmentat0dB(ormoregenerallythevalueofthestaticgainexpressedindecibels).–Alinesegmentofslope−40dBperdecade.Thesetwolinesegmentsintersectattheabscissapointυ=1(i.e.ω=ω0).Theasymptoticdiagramofthephasefollowsimmediatelyfromthestudyofthephasemadeabove.WeﬁnallyobtaintheBodeplotsinFigure3.8correspondingtodampingratios0.10.30.5and0.7withthesameconventionsasabove.Theasymptoticdiagramisalsoplotted.78LinearSystems20Bode DiagramFrequency (rad/sec)0–20–40–60–800–45–90–135–18010–210–1100101102Phase (deg)Magnitude (dB)Figure3.8.Bodeplotofasecond-ordersystem3.4.Systemsofanyorder3.4.1.StabilityConsiderthesystemofordern≥1representedbytheleftform(2.17).ItstransferfunctionG(s)=N(s)/D(s)isproperifandonlyif(i):d◦(N)≤d◦(D)(seesection13.6.1).Thissystemisminimalifandonlyif(ii):thepolynomialsN(s)andD(s)arecoprime(accordingtoDeﬁnition33ofsection2.4.6).SupposeConditions(i)and(ii)aresatisﬁed.ThereforeaccordingtoDeﬁnition61(section3.1.1)thesystemconsideredisstableifandonlyifCondition(iii)givenbelowissatisﬁed:(iii)AllrootsofthepolynomialD(s)belongtolefthalf-plane.DEFINITION62.–ApolynomialD(s)∈R[s]issaidtobeHurwitzifCondition(iii)aboveissatisﬁed.WriteD(s)=n(cid:12)j=0ajsn−ja0(cid:5)=0.(3.13)PROPOSITION63.–ForapolynomialD(s)tobeHurwitzitisnecessarythatallthecoefﬁcientsaj(0≤j≤n)beofthesamesign.Open-LoopSystems79PROOF.ThepolynomialD(s)canbefactorizedintothefollowingformD(s)=a0’k(s+αk)’l(s+βl+iγl)(s+βl−iγl)=a0’k(s+αk)’l(cid:15)s2+γ2l+2βl(cid:16)whereαk>0βl>0ifD(s)isHurwitz.Indevelopingtheaboveexpressionweobtain(3.13)withallthecoefﬁcientsaj(0≤j≤n)ofthesamesignasa0.REMARK64.–(i)TheconditioninProposition63isnecessarybutnotsufﬁcientforthepolynomialD(s)tobeHurwitzexceptifn≤2.InthegeneralcaseanecessaryandsufﬁcientconditionisprovidedbytheRouthcriterionwhichwewillnotelaborate(seee.g.[26]).(ii)ItisequallypossiblethankstoanextensionoftheRouthcriteriontodeterminethenumberofrootsofD(s)thatbelongtotherighthalf-plane[6].(iii)AtlastsupposethecoefﬁcientsajofD(s)areuncertaininthesensethateachofthemsatisﬁestheboundsaj≤aj≤ajandonlythelowerandupperboundsajandajareknown.LetDbethesetofpolynomialsD(s)whosecoefﬁcientsaj(0≤j≤n)satisfytheaboveconditions.AcriterionowedtoKharitonovprovidesanecessaryandsufﬁcientconditiondependingonlyontheboundsajandaj(0≤j≤n)forallpolynomialsbelongingtoDtobeHurwitz(seee.g.[7]).Whenaj=aj(0≤j≤n)theKharitonovcriterionreducestotheRouthcriterion.3.4.2.DecompositionofthetransferfunctionThetransferfunctionG(s)ofasystemofordern≥1isoftheformG(s)=(kNk(s)(kDk(s)where1Nk(s)and1Dk(s)aretransferfunctionsoftheﬁrstorderorofthesecondorderwithtwocomplexconjugatepoles.Thestudyofthese“elementarytransferfunctions”hasbeendoneabove.Wehavelog|G(iω)|=(cid:12)klog))))1Dk(iω)))))−(cid:12)klog))))1Nk(iω)))))(3.14)argG(iω)=(cid:12)karg(cid:21)1Dk(iω)(cid:22)−(cid:12)karg(cid:21)1Nk(iω)(cid:22).ThereforeweeasilyobtainthecurvesofG(s)(amplitudeandphase)fromtheBodeplotsoftheelementarytransferfunctions1Nk(s)and1Dk(s).80LinearSystemsREMARK65.–From(3.14)itiseasytoshowthattheBodemagnitudeasymptoticdiagramhasaslopeof−20δ(G)dB/decadeinthehighfrequencieswhereδ(G)istherelativedegreeofthetransferfunctionG(s)(seesection13.6.1).3.4.3.AsymptoticBodeplotConstructionofasymptoticplotTheasymptoticBodeplotofG(s)isparticularlysimpletoconstruct.Themethodisasfollows:i)Determinetheabscissaeofthe“cornerfrequencies”oftheasymptoticdiagramwhicharetheabsolutevaluesofthepolesandzeros.Placethemontheaxisoftheabscissae.ii)Foreachofthecornerfrequenciesapplythefollowingruletothevariationofslopeofthemagnitudeandthevariationofphasearulewhichfollowsfrompreviousresults:StablepoleUnstablepoleStablezeroUnstablezero∆(slope)−20dB/dec.−20dB/dec.20dB/dec.20dB/dec.∆(phase)−90◦90◦90◦−90◦TableoftheruleofcornerfrequenciesWecallastable(resp.unstable)poleapolewhichhasnegative(resp.non-negative)realpartandsimilarlyforazero.Example1LetG(s)=100(s−1)s(s+10).–Thecornerfrequenciesare:ω=0(correspondingtothepoles=0)ω=1(correspondingtotheunstablezeros=1)andω=10(correspondingtothestablepoles=−10).–ConstructtheasymptoticBodeplotatlowfrequencies:forω→0G(iω)∼−10iω.Amplitude:slope−20dB/decadepassingthroughthepoint(cid:15)ω=1|G|dB=20(cid:16).Phase:90◦(modulo360◦).–Therestofthediagramcanbeobtainedbyapplyingthe“cornerfrequency”rulefromtheabovetable.TheasymptoticplotobtainedisrepresentedinFigure3.9(dashedlines)alongwiththe“true”Bodeplot(solidlines).Thephasevariesfrom90◦to−90◦.Open-LoopSystems81-1001020304050 Magnitude (dB)10-1100101102-90-4504590Phase (deg)Frequency (rad/sec)Figure3.9.Bodeplot–Example1Example2LetG(s)=s(s+10)(s2−s+100)(s+1).–Thecornerpointsareω=0(correspondingtothezeros=0)ω=1(correspondingtothestablepoles=−1)andω=10(correspondingtothestablezeros=−10aswellastotwounstablecomplexconjugatepoleswithabsolutevalue10).–Constructtheasymptoticplotinthelowfrequencies:forω→0G(iω)∼iω10.Amplitude:slopeof20dB/decadepassingthroughthepoint(cid:15)ω=1|G|dB=−20(cid:16).Phase:90◦.–Thecornerfrequencyatω=1posesnoproblem.–Atthecornerpointω=10wemustaccountfor:-theeffectofthezerowhichinducesavariationofslopeofmagnitudeof+20dB/decadeandavariationofphaseof+90◦;-theeffectofthetwocomplexconjugatepoleswhichinducesavariationofslopeofmagnitudeof−40dB/decadeandavariationofphaseof+180◦;-i.e.intotal:avariationofslopeofmagnitudeof−20dB/decadeandavariationofphaseof+270◦.TheasymptoticplotobtainedisdepictedinFigure3.10togetherwiththe“true”Bodeplot(withthesameconventionsasforFigure3.9).Notethatthetrueplotseparatessigniﬁcantlyfromtheasymptoticplotatthevicinityofresonance.82LinearSystems-50-40-30-20-10010Magnitude (dB)Frequency (rad/sec)10-1101102100-360-270-180-90Phase (deg)Figure3.10.Bodeplot–Example23.4.4.Amplitude/phaserelationAmplitude/phaserelationintheasymptoticBodeplotThetableofthe“cornerfrequencyrule”showsthatthecompleteasymptoticBodeplotofasystemisentirelydeterminedbyitsasymptoticdiagramofmagnitudeifweassumethatthissystemonlyhasstablepolesandzeros.Suchasystemisstable(apropertythatfollowsfromthestabilityofitspoles);wearenowgoingtoidentifywhichpropertyfollowsfromthestabilityofitszeros.MinimumphasesystemsTheoppositeofitsphasethatis−argG(iω)iscalledthephaseshiftofthesystem(orofitstransferfunction).LetzbeacomplexnumberwithnegativerealpartandletG(s)andG∗(s)betwostabletransferfunctionssuchthatG∗(s)=s+zs−zG(s).Wehave))))iω+ziω−z))))=1andaccordingtothe“tableofthecornerfrequencyrule”thephaseofthetransferfunctions+zs−zdecreasesfrom0◦to−180◦whenthefrequencygoesfrom0to+∞.Open-LoopSystems83AsaresulttheamplitudeA(ω)ofG∗(s)isequaltothatofG(s)atallfrequencieswhereasthephaseshiftofG∗(s)ishigherthanthatofG(s).Withthesamerationaleweimmediatelyestablishthefollowingresult:THEOREM66.–Giventheamplitudeω(cid:3)→A(ω)ofthefrequencyresponseofastabletransferfunctionthereexistsaninﬁnitenumberofstabletransferfunctionshavingafrequencyresponsewiththesameamplitude.AmongthesetransferfunctionsthereexistsauniqueoneG(s)whosephaseshiftisminimum:allzerosofthishaveanegativerealpart.AlltheothersareoftheformG∗(s)=G(s)’ks+zks−zkwheretheproductisﬁniteandthezkarecomplexnumbersinthelefthalf-plane(providedthat¯zkappearsintheproductifzkdoesfortherationalfunctionG∗(s)tohaverealcoefﬁcients).DEFINITION67.–Astablesystemissaidtohaveminimumphaseifallitstransmissionzerosbelongtothelefthalf-plane(inotherwordsifitstransferfunctionhasastableinverse)andnon-minimumphaseotherwise.REMARK68.–Byextensionaminimalcontrolsystempossiblyunstableand/orMIMOissaidtohaveminimumphaseifallitstransmissionzeroslieinthelefthalf-plane.Bode’amplitude/phaserelationsForaminimumphasesystemwehaveseenabovethattheasymptoticdiagramofthephaseiscompletelydeterminedbytheasymptoticdiagramofthemagnitude.WecangoalittlefurtherandshowthatthephaseargG(iω)itselfisentirelydeterminedbythemagnitudeA(ω)=ln|G(iω)|.4Indeedwehavethefollowingresult:foranyfrequencyωc≥0argG(iωc)−argG(0)=2π(cid:2)+∞0A(ω)−A(0)ω2−ω2cdω=1π(cid:2)+∞−∞dAdν(cid:27)lncoth|ν|2(cid:28)dν(3.15)whereν=ln(cid:9)ωωc(cid:10).(Foracompleteproofsee[9]Chapter14or[39]section7.2).4.Forconvenienceweconsiderln|G(iω)|thenaturallogarithmof|G(iω)|ratherthan20log|G(iω)|theexpressionofmagnitudeindecibels.84LinearSystems-2.5-2-1.5-1-0.500.511.522.500.511.522.533.544.55Figure3.11.DensityfunctionpsiThisrelationoftencalledBode’samplitude/phaserelationisalsocalled(morejustly)theBayard–Boderelation.Putψ(ν)=lncoth|ν|2inawaythataccordingto(3.15)argG(iωc)−argG(0)=1π(cid:2)+∞−∞dAdνψ(ν)dν.(3.16)Thegraphofthe“densityfunction”ψisplottedinFigure3.11.Notethatψisapositiveevenfunctionandthatψ(ν)tendsto+∞asνtendsto0.Inadditiononecanshowthat(cid:2)+∞−∞ψ(ν)dν=π22.(3.17)ApproximationofphasefrommagnitudeSupposethattheslopeofthemagnitude(i.e.anelementofZis−20ndB/decadeinthevicinityofthefrequencyωcwherenisarationalinteger).Assumealsothatthisneighborhoodisanintervalofkdecadescenteredaroundωc(frequenciesarerepresentedinalogarithmicscale).Gettingbacktovariableνthismeansthattheslopeofmagnitudeis−20ndB/decadeforν∈(cid:25)−k2ln10k2ln10(cid:26).Open-LoopSystems85–Fork→+∞wehaveexactlyaccordingto(3.16)and(3.17)argG(iωc)=argG(0)−nπ2.(3.18)Thisisalsothevaluewegetbyusingthe“tableofcornerfrequencyrule”oftheBodeasymptoticdiagram.–Forﬁnitekthevalue(3.18)isobviouslyonlyanapproximationofthephasetheprecisionofwhichdependsnotonlyonkbutalsoontheslopeofmagnitudeoutsidetheinterval(cid:25)−k2log10k2log10(cid:26)ofvaluesofν.Togiveanideaoftheerrorthatcanbemadeifweuse(3.18)tocalculatethephaseletI(k)=2π2(cid:2)k2ln10−k2ln10ψ(ν)dν.WehaveI(05)=0.31I(1)=0.74I(2)=0.92I(4)=0.99.(3.19)Thisjustmeansthatwhenk=1forexampletheerrorincurredonthephasecannotexceed25%butthispercentagecanconstituteanorderofmagnitudeandmostoftenalowerbound.Thereforeiftheslopeofmagnitudeisapproximately−20dB/decadeoveronedecadecenteredaroundωctheestimatedphaseisbetween−112◦and−68◦atfrequencyωc.3.5.Time-delaysystems3.5.1.Leftformtime-delaysystemsThegeneraltheoryoftime-delaysystemsisbeyondthescopeofthisbook(seee.g.[55]and[58]).Wewilllimitourselvesheretogivingelementswhichareamongthesimplestbutstillveryusefulones.Consideralineartime-invariantSISOsystemdeﬁnedbytheleftformD(∂)y(t)=N(∂)u(t).Nowsupposethattheinputuisdelayedbecauseforexampleofatimelapsebetweenthemomenttheorderoftheactionistransmittedandwhentheactuatorstartstoact(thisdelaycanbethetimeofinformationpropagationorthetimeoftransportinthemedium).TheaboveequationbecomesD(∂)y(t)=N(∂)u(t−τ)(3.20)whereτ>0denotesthetimelapseinquestion.Belowwewillonlystudytime-delaysystemsgovernedbyanequationofthisform.86LinearSystems3.5.2.TransferfunctionWeknowthatthedelayedinputu(τ):t(cid:3)→u(t−τ)isequaltotheconvolutionproductδ(τ)∗uwhereδ(τ)isthedelayedDiracdistributionofthetimeτ(section12.2.3).ThereforeaccordingtotheexchangetheoremandthefactthatL(cid:15)δ(τ)(cid:16)(s)=e−τswehaveL(cid:15)u(τ)(cid:16)(s)=e−τsˆu(s)whereˆu(s)istheLaplacetransformofu.Thereforethetransferfunctionofsystem(3.20)isG(s)=e−τsH(s)(3.21)whereH(s)=N(s)D(s).3.5.3.BodeplotMagnitudeWehaveaccordingto(3.21)G(iω)=e−iτωH(iω).Asaresult|G(iω)|=|H(iω)|.(3.22)PhaseOntheotherhandargG(iω)=argH(iω)−τω.(3.23)3.5.4.Example:ﬁrst-ordertime-delaysystemThesimplestcaseisaﬁrst-ordertime-delaysystemwithtransferfunctionG(s)=ke−τs1+Ts.(3.24)InthefollowingweassumethatthestaticgainG(0)=kisequalto1.Open-LoopSystems870510152025303540455000.10.20.30.40.50.60.70.80.91tauTFigure3.12.Stepresponseofaﬁrstordertime-delaysystemStepresponseThestepresponseofthissystemisrepresentedinFigure3.12forT=10sandτ=5s.Weseethatfortheﬁrstﬁvesecondswhichisthedurationofthedelaytheoutputremainsatzero.BodeplotTheBodeplotofthissystemisshowninFigure3.13.Notethatthemagnitudeplotisidenticaltothatofthenon-delayedsystem(conformingto(3.22));ontheotherhandthephaseplothasaverynovelallureinrelationtowhatwehaveseensofar.Whenωtendsto+∞wehaveindeedaccordingto(3.23)argG(iω)=−π2−τω+(termstendingto0).InotherwordsargG(iω)decreaseslinearlywithrespecttoωandexponentiallywithrespecttologω(recallthatthefrequencyisinlogarithmicscale)fromwhichthereisaveryrapiddecrementofthephaseinhighfrequencies3.5.5.Approximationsofatime-delaysystemThetransferfunction(3.21)isnotrationalandthatiswhatisparticularwithtime-delaysystems.Itishoweverstillpossibletoapproachthiskindoftransferfunctionsinadifferentmannerbywayofrationalfunctionsleadingtothestudyofthese88LinearSystems10-210-1100101-50-40-30-20-100maginitude(dB)10-210-1100101-3000-2000-10000frequency : rad/sphase:degFigure3.13.Bodeplotofaﬁrst-ordertime-delaysystemtime-delaysystemsintheclassicframeworkofsystemsgovernedbyordinarydifferentialequations.A-approximationsLeten(s)(n≥1)betherationalfunctiondeﬁnedbyen(s)=(cid:9)1−(τs)/(2n)1+(τs)/(2n)(cid:10)n.(3.25)THEOREM69.–Wehaveforeveryn≥1andeveryω|en(iω)|=1andlimn→+∞en(s)=e−τs;thisconvergenceisuniformoneverycompactset|s|≤σstartingfromanyranknsuchthatn>τσ2.PROOF.Wecanwriteen(s)=en(ln(1−τs2n)−ln(1+τs2n))withn(cid:9)ln(cid:9)1−τs2n(cid:10)−ln(cid:9)1+τs2n(cid:10)(cid:10)=n(cid:27)−τsn+1nε(cid:21)1n(cid:22)(cid:28)=−τs+ε(cid:21)1n(cid:22)whereε(cid:15)1n(cid:16)isatermthattendsto0whenntendsto+∞.Asaresulten(s)=e−τse−ε(1n)andthereforeen(s)convergestoe−τsbecausee−ε(1n)tendsto1.ToOpen-LoopSystems89provetheuniformconvergencewemakeuseoftheﬁrst-orderTaylorexpansionwiththeLagrangeremainder.Letkbeapositiveinteger;thereadercanverifythatinsidethedisc|s|≤kτwehaveforn>k2theupperbound))))en(s)−e−τse−τs))))≤k22nwhichshowshowfasttheconvergenceoftherelativeerroris.Writingen(iω)=e−iϕn(ω)andsetting∆ϕn(ω)=ωτ−ϕn(ω)(whichisthephaseerrorwhiletheamplitudeerroriszero)weeasilyobtainfromtheupperboundstatedabove|∆ϕn(ω)|≤arcsin(cid:9)k22n(cid:10)for|ω|≤kτ.Theorem69showsthatatime-delaysystemcanbeviewedasan“inﬁniteordersystem”.Indeedthetransferfunctionen(s)hasnpolesequalto−2nτ.Asntendstoinﬁnitythenumberofthesepolesalsobecomesinﬁnite.Theytendto−∞whileremainingontherealaxis.Asfarasthefunctionofthecomplexvariables(cid:3)→e−τsisconcernedithasnopoleinthecomplexplane(itisanentirefunction).Inﬁnite-ordersystemsarethesubjectofextensiveresearchandliterature[85].ExampleConsideragainthesystemwithtransferfunction(3.24)withthesamevaluesaskτandTconsideredabove.Replacee−τsbyen(s).ThestepresponsesareshowninFigure3.14forn=1(-)2(--)and3(-.).Theseresponsesaretobecomparedwiththestepresponseoftheexactdelaysystem(Figure3.12).Weseethattheaccuracyoftheapproximationgetsmuchbetterwithn.Theapproximationalreadysatisﬁedwhenn=1isgoodwhenn=2andexcellentwhenn=3.PadéapproximationGeneralcaseThePadéapproximationsaremoreclassicthanthepreviousonesthatwehavecalled“A-approximations”.Letn(s)=(cid:11)nk=0hk(−τs)k(cid:11)nk=0hk(τs)k(3.26)wherethecoefﬁcientshkaredeﬁnedbythefollowingrecurrentformula:hk+1=n−k(2n−k)(k+1)hk0≤k≤n−1h(0)=1.90LinearSystemsStep Response1.210.80.60.4Amplitude0.20–0.251015202530Time (sec)354050450Figure3.14.StepresponsesofA-approximationsFigure3.15.StepresponsesofPadéapproximationsOpen-LoopSystems91OnecanshowthattheTaylorpowerseriesexpansionsofn(s)andofe−τsinscoincideuptotheorder2n.Inaddition|n(iω)|=1=))e−iτω)).(3.27)Therationalfunctionn(s)iscalledthePadéestimateofordernofe−τs.Accordingto(3.27)n(s)isaperfectapproximationofe−τsasfarastheamplitudeisconcerned;thequalityoftheapproximationofthephasedependsontheordern(andincreaseswithituptilltheoccurrenceofnumericalerrors).PadéestimationsoftheﬁrstandsecondorderWehave1(s)=1−τs21+τs22(s)=1−τs2+(τs)2121+τs2+(τs)212.ThereadercanshowthattheTaylorexpansionof1(s)andof2(s)coincidewiththatofe−τsuptotheorders2and4respectively.ExampleConsideronemoretimethesystemwithtransferfunction(3.24)usingthepreviousvaluesofkτandT.Replacee−τsbyn(s).ThestepresponsesareshowninFigure3.15forn=12and3withthesameconventionsasabove.A-approximations(3.25)andPadéapproximations(3.26)areofaqualitythatisquasi-equivalent.Theadvantageof(3.25)liesinthequantiﬁcationthathasbeendonefortheapproximationerror.3.6.ExercisesEXERCISE70.–WeconsidertheRLCcircuitwithtransferfunction(2.32).i)Determinethestaticgainofthissystem.ii)Determinethefrequencyωratwhich|G(iω)|ismaximum.iii)Determinethemaximumof|G(iω)|.92LinearSystems00.10.20.30.40.50.600.010.020.030.040.050.060.070.080.090.1Step ResponseTime (sec)AmplitudeFigure3.16.Stepresponse–Question(a)EXERCISE71.–Inthisexercise“identifyingasystem”issynonymouswith“determiningitstransferfunctionexperimentally”.Wecanidentifyasystembyagraphicalmethodeitherfromthetimedomainresponse(e.g.stepresponse)orfromthefrequencydomainresponse.(a)DeterminethesystemwhichhasastepresponseasshowninFigure3.16.(b)SamequestionforthestepresponseshowninFigure3.17.(c)Apointbypointlist(seesection2.5.4)allowsustoobtainthefrequencyresponseontheBodeplotinFigure3.18.Identifythecorrespondingsystem.EXERCISE72.–“Tohaveanegativestart”.WeherebystudyunderwhatconditionthestepresponseofastablesystemΣ“goesinthewrongdirection”.Thatistosayitheadsduringtheﬁrstmomentstowardadirectionthatisoppositetotheﬁnalvalue.Weusuallyrefertosuchasystemashavinga“negativestart”oranundershoot(and“havingapositivestart”intheoppositecase).Intuitiontellsusthatasystemwithnegativestartishardtocontrol(ifthereaderimaginesacarwhichwhenweturnthesteeringwheelclockwisegoesforabriefinstanttowardtheleftbeforegoingtowardtheright...).LetΣbeastablesystemwithtransferfunctionG(s)ofrelativedegreer>0(seesection13.6.1).(a)WritedowntheexpressionofG(s)asafunctionofitszerositspolesandagain.(b)Lety(i)(0+)=00≤i≤r−1andy(r)(0+)(cid:5)=0.Whatisthenecessaryandsufﬁcientconditioninvolvingtheratioρ=y(r)(0+)G(0)fortheOpen-LoopSystems93012345602468101214Step ResponseTime (sec)AmplitudeFigure3.17.Stepresponse–Question(b)-20-1001020Magnitude (dB)10-210-1100-180-135-90-450Phase (deg)Frequency (rad/sec)Figure3.18.Bodeplot–Question(c)94LinearSystemssystemtohaveanegativestart?(c)CalculateρasarationalfunctionofthepolesandzerosofG(s).(d)Showthatthedenominatorofthisrationalfunctionispositive.Canwesaythesamethingaboutitsnumerator?(e)ShowthatthesystemΣhasanegativestartifandonlyifthenumberofitspositiverealzerosisodd.(f)IsthereagreementbetweenthisresultandA-approximationsaswellasPadéapproximationsoftheabove-studiedtime-delaysystem?Chapter4Closed-LoopSystems4.1.Closed-loopstability4.1.1.StandardfeedbacksystemWehaveemphasizedintheprefacetothisbooktheimportanceofstabilityforclosed-loopsystems.WecallsuchasystemasthatdepictedinFigure4.1a“standardfeedbacksystem”.ThesystemtobecontrolledPwithtransfermatrixP(s)isfedbackthrougharegulatorKwithtransfermatrixK(s).SystemsPandKareassumedtobeminimal.1RecallthattheLaplacetransformationofasignalxisdenotedbyˆx.4.1.2.Closed-loopequationsLety=(cid:27)y1y2(cid:28)u=(cid:27)u1u2(cid:28)andv=(cid:27)v1−v2(cid:28).AlsoletM=(cid:27)P00K(cid:28)andJ=(cid:27)0−II0(cid:28)(whereIisanidentitymatrixofasuitablesize).Imposingzeroinitialconditionsweobtainˆy=Mˆuˆu=ˆv+Jˆy(4.1)1.Thishypothesisismissinginnumerousworksonstabilizationofinﬁnite-dimensionalsystems.Thedeﬁnitionofa“minimalsystem”isindeedinthiscasenotatrivialproblem.See[14]formoredetails.95Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.96LinearSystems+--+v1u1y1v2u2y2PKFigure4.1.Standardfeedbacksystemfromwhich[I−MJ]ˆy=Mˆv.NotethatI−MJ∈R(s)(p+m)×(p+m)wherep=dim(y)andm=dim(u)(seesection13.6.1).Whatisinquestionnowiswhetherˆycanbeexpressedinauniquemannerasafunctionofˆv;thenitwillalsobethesameforˆuaccordingto(4.1).Thedeﬁnitionbelowspeciﬁesthatgiveninsection2.6.2.DEFINITION73.–Theclosed-loopsysteminFigure4.1iswell-posedifthematrixofrationalfunctionsI−MJisinvertibleinthealgebraR(s)(p+m)×(p+m).RecallthatI−JM=(cid:27)IP−KI(cid:28)thusdet(I−JM)=det(I+PK).Thereforetheclosed-loopsystemiswell-posedifandonlyifdet(I+PK)isnotidenticallyzero.Fromthispointonwardswewillconsideronlyawell-posedclosed-loopsystem.2LetLo=PKLi=KPSo=(Ip+Lo)−1To=Ip−SoSi=(Im+Li)−1Ti=Im−Si.2.Adetailedanalysisofthosephenomenawhichariseinthecaseofanill-posedclosed-loopsystemisgivenin[46].Closed-LoopSystems97ThetransfermatricesLoSoandToarecalledtheopen-looptransfermatrixthesensitivityfunctionandthecomplementarysensitivityfunctionrespectivelyattheoutputofSystemP.ThetransfermatricesLiSiandTiarecalledtheopen-looptransfermatrixthesensitivityfunctionandthecomplementarysensitivityfunctionrespectivelyattheinputofSystemP.Wecaneasilyestablishtherelations(cid:27)ˆy1ˆy2(cid:28)=(cid:27)SoPToTi−SiK(cid:28)(cid:27)ˆv1ˆv2(cid:28)(4.2)(cid:27)ˆu1ˆu2(cid:28)=(cid:27)SiSiKSoP−So(cid:28)(cid:27)ˆv1ˆv2(cid:28)(4.3)whicharetheclosed-loopequations.4.1.3.Stabilityofaclosed-loopsystemDEFINITION74.–Theclosed-loopsystemPcinFigure4.1isstableif:(i)itiswell-posedand(ii)thesystemwithinputv=(cid:27)v1v2(cid:28)andoutputy=(cid:27)y1y2(cid:28)isstableinthesenseofDeﬁnition61(section3.1.1).3REMARK75.–Accordingto(4.1)Condition(ii)givenaboveisequivalenttothefollowing:(ii’)thesystemwithinputv=(cid:27)v1v2(cid:28)andoutputu=(cid:27)u1u2(cid:28)isstable(inthesenseofDeﬁnition61).InwhatfollowsinthisparagraphthesystemsPandKaresupposedtobeSISO.ThetransferfunctionsP(s)andK(s)arethusrationalfunctionswrittenintheformP(s)=B(s)A(s)K(s)=R(s)S(s)(4.4)whereA(s)B(s)R(s)S(s)arepolynomialswithrealcoefﬁcientsi.e.areelementsoftheringR[s](seesection13.1.1).Therationalfunctionsin(4.4)areassumedtobeirreducible.Theopen-looptransferfunctionL(s)=P(s)K(s)(4.5)isidenticalattheinputandtheoutputofP(Li=Lo=L).3.Accordingtosomeauthorswhatisdeﬁnedhereisthe“internalstability”ofafeedbacksystem.98LinearSystemsTHEOREM76.–(i)ThefeedbacksystemPciswell-posedifandonlyifAcl(s)=A(s)S(s)+B(s)R(s)(cid:5)=0inR[s].(ii)ThetransmissionpolesofPcaretherootsofAcl(s).(iii)IfthesystemsPandKareproperandatleastoneofthemisstrictlyproperthenthefeedbacksystemPciswell-posedandproper.(iv)SupposethefeedbacksystemPcisproper;thenitisstableifandonlyiftherootsofAcl(s)alllieinthelefthalf-plane.PROOF.*(i)Wehave1+PK=1+BARS=AS+BRAS.(ii)Accordingto(4.2)Pc=(cid:27)BSAS+BRBRAS+BRBRAS+BR−ARAS+BR(cid:28)=1AS+BR(cid:27)BSBRBR−AR(cid:28).Let(cid:27)α100α2(cid:28)betheSmithformofthematrix(cid:27)BSBRBR−AR(cid:28).Theﬁrstinvariantfactorα1isthegreatestcommondivisorofthepolynomialsBSBRandAR(seesection13.2.3Proposition502);itdividesBRandARandsoitdividesRsinceBandAarecoprime;itdividesBSandBRandsoitdividesBsinceSandRarecoprime.Thisinvariantfactorα1isthereforeprimewithAS+BR(sinceitdividesBRandisprimewithAS).MoreoverthereexistpolynomialsB1andR1suchthatB=B1α1andR=R1α1.Thesecondinvariantfactorα2isamultipleofα1andisoftheformβα1whereβ∈R[s].Theproductα1α2=βα21isequaltothedeterminant−(cid:15)ARBS+B2R2(cid:16)=−BR(AS+BR)=−α21B1R1(AS+BR).Asaresultβ=−B1R1(AS+BR)andα2=−α1B1R1(AS+BR).TheSmith–MacMillanformofPcistherefore1AS+BR(cid:27)α100α2(cid:28)=(cid:27)α1AS+BR00−B1R1α1(cid:28).Sinceα1isprimewithAS+BRtheMacMillanpolesofPc(i.e.thetransmissionpolesofPc)aretherootsofAcl(s).(iii):Theveriﬁcationislefttothereader.(iv)ThisisanobviousconsequenceofDeﬁnition61.*REMARK77.–ThetwosystemsPandKcanbeproperwhilePcisnotasshownbytheexamplewhereP(s)=−s+1sandK(s)=1.ItcanalsohappenthatPcisill-posedsuchasinthecaseP(s)=−1andK(s)=1.DEFINITION78.–Acl(s)=A(s)S(s)+B(s)R(s)isthecharacteristicpoly-nomialofthefeedbacksystemPc(thatisoftheclosed-loop).Closed-LoopSystems994.1.4.NyquistcriterionAssumethattheopen-looptransferfunctionL(s)isstrictlyproper(whichimpliesthatPciswell-posedandproperaccordingtoTheorem76).Letf(s)=Acl(s)(s+1)ν(4.6)whereν=d◦(Acl)sothat0(cid:5)=lim|s|→+∞f(s)<+∞.By(4.5)f(s)=A(s)S(s)(s+1)ν(1+L(s)).LetnP(resp.nK)bethenumberofpolesofP(s)(resp.K(s))(countingmultiplicities)locatedintheclosedrighthalf-plane¯C+={s∈C:Re(s)≥0}.CasewhereP(s)andK(s)havenopolesontheimaginaryaxisDEFINITION79.–Thepathω(cid:3)→L(iω)ω∈(−∞+∞)iscalledtheNyquistplotofL(s)(seesection12.4.3).SinceL(−iω)=¯L(iω)(where¯L(iω)isthecomplexconjugateofL(iω))theNyquistplotofL(s)isentirelydeterminedbythepath[0+∞)(cid:20)ω(cid:3)→L(iω).IftheNyquistplotpassesthroughthepoint−1(calledthecriticalpoint)theclosed-loopsystemisunstablebecauseithasanimaginarypole.IfnotletNbethenumberofturnstheNyquistplotL(s)goesaroundthepoint−1inthedirectsense(whichisanti-clockwise).Thefollowingtheoremisfundamental(andisgeneralizedtothecaseoflinearsystemswithcommensuratedelaysin[14];seealso[84][85]).THEOREM80.–(Nyquistcriterion).Theclosed-loopsystemisstableifandonlyiftheNyquistplotdoesnotpassthroughthepoint−1andnP+nK=N.PROOF.LetA>0andconsidertheclosedpathλformedbytheconcatenationofthefollowingpaths:(i)γ1:ω(cid:3)→iωω∈(−AA);(ii)γ2:θ(cid:3)→Ae−iθθ∈(cid:15)−π2π2(cid:16).(Thereaderisrequestedtodrawadiagram.)Theclosedpathλiscalledthe“Bromwichcontour”.LetAbelargeenoughforλtoencircleonetimeclockwiseallthepolesandzerosofP(s)andK(s)thatarelocatedin¯C+ifany.Letf1(s)=A(s)S(s)(s+1)νf2(s)=1+L(s).Accordingto(12.76)(seesection12.4.3)j(0;f◦λ)=j(0;f1◦λ)+j(0;f2◦λ).OntheotherhandaccordingtoCauchy’sargumentprinciple(Theorem447section12.4.5)j(0;f1◦λ)=nP+nKandj(0;f2◦λ)=−N.Asaresultj(0;f◦λ)=nP+nK−N.Accordingto(4.6)j(0;f◦λ)isthenumberofrootsofAcl(s)located100LinearSystemsin¯C+andsobyTheorem76anecessaryandsufﬁcientconditionforstabilityoftheclosed-loopsystemPcisj(0;f◦λ)=0i.e.nP+nK=N.SinceL(s)isstrictlyproperasA→+∞theimageofL◦γ2reducestotheorigin.EXAMPLE81.–LetP(s)=1−s(1+s)3.TheNyquistplotofthistransferfunctionisshowninFigure4.3.Therealaxisiscutat−1k0wherek0=2.01.SupposethesystemPisfedbackbyaproportionalregulatorK(s)=k>0.WehavenP=nK=0thereforeaccordingtotheNyquistcriteriontheclosed-loopsystemisstableifandonlyifk<k0.Werefertok0asthecriticalgain.EXAMPLE82.–LetP(s)=2s+2s2−s−2.TheNyquistplotofP(s)isshowninFigure4.4.ThepolesofP(s)are−1and2andsonP=1.IfasinExample81wefeedbackthesystemthroughagaink>0wehavenK=0.TheNyquistplotofP(s)cutstherealaxisatthepoint−1.Thenecessaryandsufﬁcientconditionforstabilityoftheclosed-loopsystemasprovidedbytheNyquistcriterionisN=1i.e.k>1.CasewhereP(s)andK(s)havepolesontheimaginaryaxisSupposenowthatA(s)orS(s)hasrootsontheimaginaryaxis.ThenononehandnPandnKdonothaveaclearmeaningandontheotherhandtheArgumentprincipleisnolongerapplicableiftheclosedpathλisdeﬁnedasintheproofofTheorem80.Thesolutionconsistsinreplacingtheimaginaryaxisbyan“indentedimaginaryaxis”asdeﬁnedbelow.Letpk=iωk(k=1...r)bethedistinctrootsofA(s)S(s)locatedontheimaginaryaxisarrangedinsuchawaythatωk+1>ωk.LetIk(ε)beasegmentontheimaginaryaxiscenteredatpkandoflength2εwhereε>0isarealnumbersufﬁcientlysmallinorderthatIk(ε)∩Ik+1(ε)=∅(k=1...r−1).ReplaceeachsegmentIk(ε)bythehalfcircleJk(ε)ofradiusεandcenteredatpkandgoingroundthatpointtotheleft(seeFigure4.2).TheindentedimaginaryaxisisthesetI(ε)obtainedafterreplacingallthesegmentsIk(ε)(k=1...r)bythesemi-circlesJk(ε).LetγεdenotethepathtraversedbythecomplexvariableswhenittravelsoverI(ε)withastrictlyincreasingimaginarypart(wecancallγεthe“directedindentedimaginaryaxis”).ThedeﬁnitionoftheBromwichcontourcanclearlybeextendedtothiscase.WearenowgoingtoreplaceDeﬁnition79bythefollowing:DEFINITION83.–TheNyquistplotofL(s)isL◦γε.LetnPandnKbetheintegersasdeﬁnedaboveandNbethenumberofturnstheNyquistplotofL(s)madearoundpoint−1anti-clockwise.Weobtainthefollowing:Closed-LoopSystems101xpkIk()Jk()Figure4.2.ConstructionoftheindentedimaginaryaxisTHEOREM84.–WithNasdeﬁnedabovethestatementofTheorem80remainsvalid.EXAMPLE85.–LetP(s)=1s(cid:15)s2+s+12(cid:16).Thesystemwithsuchatransferfunctionisfedbackbyaproportionalregulatorwithgaink>0.ThereforeA(s)=s(cid:15)s2+s+12(cid:16).TheproductA(s)S(s)hasarootontheimaginaryaxisattheorigin.Thedirectedindentedimaginaryaxisconsistsofthefollowingparts:(a)Thequartercircleparameterizedbyεe−iθwhereθincreasesfrom−πto−π2;(b)Thesetofalliωwhenωincreasesfromε>0to+∞;(c)Thesymmetricwithrespecttotherealaxisoftheconcatenatedpaths(a)and(b).Forε→0+P(cid:15)εe−iθ(cid:16)∼2εeiθ.Thereforefora“verysmall”εtheimageof(a)byP(s)isthequartercircleof(“verylarge”)radius2εcoveredwithanangleθincreasingfrom−πto−π2.ThecompleteNyquistplotisshowninFigure4.5(wherewehavetoimaginethesemi-circleatinﬁnity;itistraversedinthedirectsense).ThepolesofP(s)are012(−1+i)12(−1−i).AsaresultnP=1.WehavenK=0thereforeanecessaryandsufﬁcientconditionforstabilityoftheclosed-loopsystemisN=1.NowtheNyquistplotcutstherealaxisataround−2.Thusclosed-loopstabilityholdswhen0<k<12.*NyquistcriterionintheMIMOcaseIntheMIMOcaseitisnecessarytodistinguishtheopen-looptransfermatricesattheinputandoutputofthesystemPaswehaveseeninsection4.1.1.Anyhow102LinearSystems-1-0.8-0.6-0.4-0.200.20.40.60.81-1-0.8-0.6-0.4-0.200.20.40.60.81Nyquist DiagramReal AxisImaginary AxisFigure4.3.NyquistplotofP(s)–Example81-1.5-1-0.500.5-1-0.8-0.6-0.4-0.200.20.40.60.81Nyquist DiagramReal Axis Imaginary AxisFigure4.4.NyquistplotofP(s)–Example82Closed-LoopSystems103-7-6-5-4-3-2-10123-5-4-3-2-1012345Nyquist DiagramReal AxisImaginary AxisFigure4.5.NyquistplotofP(s)–Example85det(Ip+Lo(s))iszeroifandonlyifdet(Ip+Li(s))isalsozeroaccordingtoLemma520(section13.3.3).Thereforeifweputgo(s)=det(Ip+Lo(s))−1gi(s)=det(Im+Li(s))−1wehavego(s)(cid:5)=−1ifandonlyifgi(s)(cid:5)=−1.LetNbethenumberofturnseffectedbytheNyquistplotofgo(s)(alsocalledthe“MIMONyquistplot”)aroundpoint−1anti-clockwise.Thefollowingtheoremisprovedin([114]section7.4):THEOREM86.–WithNasdeﬁnedabovethestatementofTheorem80remainsvalid.4.1.5.SmallgaintheoremThesmallgaintheoremisoftenusedinrobustnesstheory.ConsiderthestandardfeedbacksysteminFigure4.1;supposeP(s)∈(cid:1)Hp×m∞andK(s)∈(cid:1)Hm×p∞.Inthelineartime-invariantcaseconsideredherethesmallgaintheoremisexpressedasfollows:THEOREM87.–If(cid:23)P(cid:23)∞(cid:23)K(cid:23)∞<1thenthefeedbacksystemisstable.PROOF.WehaveLi(s)∈(cid:1)Hm×m∞and(cid:23)Li(cid:23)∞≤(cid:23)P(cid:23)∞(cid:23)K(cid:23)∞<1.Lets∈¯C+andλ(s)beaneigenvalueofLi(s).AccordingtoProposition578(section13.5.7)104LinearSystemswehave|1+λ(s)|≥1−|λ(s)|and|λ(s)|≤(cid:23)Li(cid:23)∞.Therefore|1+λ(s)|>0andsothematrixIm+Li(s)isinvertibleinCm×m.Itfollowsthatallthetransfermatricesoftheexpression(4.2)havetheirentriesbelongingto(cid:1)H∞;thusaccordingtoDeﬁnition74theproofiscomplete.4.2.Robustnessandperformance4.2.1.GeneralitiesTherobustnessofafeedbacksystemisthecapacityofsuchasystemtopreservestabilityinthepresenceofmodelerrors(robustnessofstability)oreventomaintainacertainperformanceinthepresenceofsucherrors(robustnessofperformance).Robustnessofperformanceimpliesrobustnessofstabilitybutconversedoesnothold.Inmostcases“robustcontrol”issynonymouswith“prudentcontrol”.Thecontrolengineerdesignsthecontrolofasystembasedonamodel(moreorlessexplicit).Therealitybeinginﬁnitelycomplex;themoreprecisethemodelisthemorecomplicateditbecomes(ofthehigherorderwithnonlinearitiesetc.)andthelessusableitistosynthesizeacontrollaw.Recallwhatwasstatedinsection2.2.5:anapparentlyverysimplesystemsuchasavoltagegeneratorsupplyingacurrentthrougharesistance;iswithagoodapproximationrepresentedbytheelementaryequationV=Ri.Howeverthisequationdoesnottakeintoaccountthefactthatifweconsiderimportantvariationsofthevoltagetheresistancebecomesanonlinearelement;itisthereforemoreprecisetowriteV=ρ(i)whereρisthecharacteristicfunctionoftheresistanceandρislinearonlyinthevicinityof0.Moreoverifwestudythebehaviorofsuchasystemwehavetotakeintoaccountthepropagationvelocityoftheelectricﬁeld.Themodelobtainedisthengovernedbythewaveequation(whichisapartialdifferentialequation)andisofan“inﬁniteorder”;sinceitisnonlinearaccordingtowhathasbeensaidaboveandeventhoughitwassimpleatthebeginningithasbecomeverycomplicated.Ifsuchbigcomplication(whichbecomesinﬁniteinthelastanalysis)ismostoftenuselessitisbecauseinpracticeitisnotnecessarytorepresentrealityinthemostaccurateway.Foracontrolengineera“goodmodel”hasjustthenecessarycomplexitythatenableshimtodesignthecontrolofhis/hersysteminsuchawayastoobtaintheperformancewhichcorrespondstohis/herspeciﬁcationssheet.Themorestringenttheperformanceisrequiredthemorepreciseandthemorecomplexthemodelmustbe.Converselyiftheonlymodelavailableisveryimprecisetherewillbenoothersolutionbuttorunthesystemwithapoorperformanceandsowithcaution.Howevercautionisavirtuethatisnotsufﬁcientinitself:skillisalsonecessary.Robustnesstheoryisnotlimitedtorecommendingservo-controladjustmentﬂabbiness.Itevenhappensthattherobustnessofacontrollawrequirestheuseoflargegains;italldependsonthenatureofmodelerrorsoruncertainties.Closed-LoopSystems105iL0a1rReImFigure4.6.NyquistplotofL(s)Considerforexamplethesystemwithinputuandoutputygovernedbythedifferentialequation˙y=θy+u(4.7)whereweonlyknowthatthecoefﬁcientθisinsideaninterval[θminθmax]0≤θmin<θmax.Usingaproportionalcontrollawu=−kywecanonlybesuretostabilizethesystembyusingk>θmaxthuschoosingasufﬁcientlylargegain.Inrealityrobustnessisnotanabsolutenotion:itisalwaysrelativetoaclassofmodelingerrors.Inmostofthecasesencounteredinpracticetheseerrorsaresuchthatrobustnessagainsttheseerrorsissynonymouswithprudencebutexample(4.7)showsthatitisimportantnottogeneralizethisruleimproperly.4.2.2.RobustnessmarginsSupposethattheNyquistplotofL(s)=P(s)K(s)isasshowninFigure4.6.Thenominalfeedbacksystem(i.e.withnomodelingerror)isassumedtobestable.GainmarginLet−1/g1and−1/g2bethetwonumbersclosestto−1andsituatedoneithersideofthatpointandwheretheNyquistplotcutstherealaxis.ThegainmarginistheintervalMg=(g1g2)(0≤g1<1<g2≤+∞).AccordingtotheNyquist106LinearSystemscriterionthesigniﬁcanceofthismarginisasfollows:supposethatthetransferfunctionP(s)isreplaced(duetothemodelerror)bygP(s)wheregispositiverealnumberthefeedbacksystemremainsstableifandonlyifg∈Mg.Thevaluesg1andg2areoftenexpressedindecibels.Wecanestimatethatagainmarginiscorrectifitincludes[−3dB6dB].Onemaycallg1dBthegainreductionmarginandg2dBthegainaugmentationmargin.Butnotethatmanycontrolengineersrefertowhatwecallabovethegainaugmentationmarginasthegainmargin;thisusageisdiscouragedasitisasourceofambiguity.PhasemarginThephaselagmarginistheangleΦr∈[02π)asshowninFigure4.6.Ithasthefollowingsigniﬁcance:supposethatduethetomodelingerrorthetransferfunctionP(s)isreplacedbye−iφP(s)(0≤φ<2π).Thentheclosed-loopsystemremainsstableifandonlyifφ<Φr.ThephaseleadmarginistheangleΦa∈[02π)asshowninFigure4.6foranalogousreasons.Howeverinsteadofconsideringaphaselag−φweconsiderthistimeaphaseleadφ.ThephasemarginistheintervalMp=(−ΦaΦr).Theanglesareoftenexpressedindegrees.Onecanconsiderthataphasemarginiscorrectifitincludes[−30◦30◦].Manycontrolengineersrefertowhatwehavecalledabovethephaselagmarginasthephasemargin.DelaymarginThenotionsofgainmarginandphasemarginareveryclassicandimportant.Theyarehoweververyinsufﬁcientincharacterizingrobustness[20].Innumeroussituationsthemodelingerrorisanalogoustoadelayinserieswiththesystem.Thisdelaymaybeduetosampling(inthecaseofacomputer-controlledsystem);itmayalsobedueforvariousreasonstodelaysbetweenthesensorandtheregulatorandbetweenthisandtheactuator.Intheendwewillseethatthisdelaycanapproximatecertainkindofneglecteddynamics.Supposenowthatweintroduceaparasiticdelayτ>0intotheloopwithtransferfunctione−τs.Wecallunitygainfrequencyafrequencyωsuchthat|L(iω)|=1.Fors=iωwhereωisaunitygainfrequencythedelayτintroducesaphaseshift−ωτthusaphasedelaythathasalineardependencyonω.Thisphasedelayisthushigherthanthephaselagmarginiftheunitygainfrequencyconsideredistoolarge.Thisexplainswhythephaselagmarginisnotofmuchsigniﬁcanceinsuchasituationandleadsustothefollowingdeﬁnition[20]:thedelaymarginMristheleastupperboundoftheparasiticdelaysforwhichtheclosed-loopsystemremainsstable.LetusseenowhowwecancalculatethedelaymarginbasedontheNyquistplot.(i)If|L(iω)|<1forallωthedelaymarginisinﬁnite.(ii)SupposethatsuchisnottheClosed-LoopSystems107casethenthesetoffrequenciesωforwhich|L(iω)|=1non-empty;let{ω1...ωn}bethisset.4Foreveryk∈{1...n}letΦkbetheanglein[02π)havingavalueargL(iωk)−π(modulo2π).ThedelaymarginisdeﬁnedasMr=min(cid:7)Φkωkk=1...n(cid:8).(4.8)Inthecasewheretheunitygainfrequencyisuniqueandisdenotedbyω0weobtainMr=Φrω0.WewilltakegreatcareinexpressingtheanglesΦkandthefrequenciesωkincoherentunits.ForexampleifΦkisexpressedinradiansandωkinrad/sMrisobtainedinseconds.Wecannotintheabsoluteprovideavaluewithwhichthedelaymargincanbeconsideredsufﬁcient:suchavalue(whichisoneoftheinterestsinsuchanotion)dependsonthesystemtobecontrolledtheexpectedperformancethesamplingperiodetc.Oneoftheruleswecanformulateisthatinthecasewherecontrolisdiscretizedthedelaymarginhastobeatleastthesamplingperiod[74].ModulusmarginConsidertheNyquistplotasshowninFigure4.7.Inthecaseconsideredthegainaugmentationmarging2islarge(oftheorderof3i.e.closeto10dB)whereasthegainreductionmarginisg1=0(orindB−∞).Similarlythephaselagmarginislarge(oftheorderof60◦)whilethephaseleadmarginhasthesamevaluebysymmetry.HowevertheNyquistplotofL(s)passesveryclosetothecriticalpoint−1consequentlyasmallerrorinthemodelcandestabilizethefeedbacksystem.ThisshowsthatthephaseandgainmarginsneedtobecompletedbyanothercriterionwhichisthedistancebetweentheNyquistplotandpoint−1.Thisdistanceiscalledthemodulusmargin.Wecanestimatethatthemodulusmarginiscorrectifitisatleast0.5.Usingthemodulusmarginwecanobtainalowerboundforthegainandphasemargins.IndeedelementarygeometryshowsthatMg⊃(cid:21)11+Mm11−Mm(cid:22)(4.9)Mp⊃(cid:21)−2arcsinMm22arcsinMm2(cid:22).(4.10)4.Thissetisnecessarilyﬁnitewhentheopen-loopsystemisﬁnite-dimensionali.e.whenL(s)isarationalfunction.108LinearSystemsxr-1/g20-1ReImL(i)Figure4.7.ModulusmarginToclarifyideaswithMm=0.5theabovelowerboundsbecome:Mg⊃(−3.5dB6dB)Mp⊃(−29◦29◦).ThemodulusmargincaneasilybeexpressedusingthesensitivityfunctionSi=11+L.Firstnotethatthissensitivityfunctionbelongsto(cid:1)H∞duetothefactthatthenominalfeedbacksystemisstable(seesections3.1.1and4.1.3).WehaveMm=infω≥0|1+L(iω)|=1supω≥0)))11+L(iω))))thusaccordingtosection13.6.2Mm=1(cid:10)Si(cid:10)∞.(4.11)SinceL(s)isastrictlypropertransferfunctionwehavelimω→+∞|Si(iω)|=1.Asaresult(cid:23)Si(cid:23)∞canbeinterpretedastheresonancefactorofthesensitivityfunctionSi.Theexpression(4.11)showsthatthemodulusmarginistheinverseofthisresonancefactor.Closed-LoopSystems109ReIm0-1L(i)-Figure4.8.M-circleHallchartItiscommontotraceintheNyquistplanethe“λ-circles”(alsocalled“M-circles”)makingitpossibletogeometricallydeterminetheresonancefactorofthe“complementarysensitivityfunction”To=Ti=1−Si.WethenobtaintheHallchart.The“λ-circle”CλisthelocusofthepointsMsuchthatOMAM=λ(λ≥0)whereArepresentsthecriticalpoint−1.LetM=x+iy.ThereadercaneasilyverifythatthecoordinatesxandyofMinthecomplexplanearesolutionsofthealgebraicequation(cid:15)λ2−1(cid:16)x2+2λ2x+(cid:15)λ2−1(cid:16)y2+λ2=0andCλisthereforethecirclewithcenter−λ2λ2−1andradiusλ|λ2−1|.OnesuchcircleCλisshowninFigure4.8inthecaseλ>1(themostcommoninpractice).TheNyquistplotofL(s)isalsoplottedinthisﬁgure(atleastthepartoftheplotthatcorrespondstofrequenciesω≥0).Thenorm(cid:23)To(cid:23)∞isλbecausethisNyquistplotistangentialtoCλanddoesnotpenetrateintotheinteriorofthiscircle.Iflims→0|To(s)|=1(cid:23)To(cid:23)∞=λisthetheresonancefactorofTo(s).ComplementarymodulusmarginThecomplementarymodulusmarginMmcistheleastupperboundofthequantities1/λsuchthattheNyquistplotofL(s)doesnotpenetrateintothecircleCλ(withλ>1).ItthusfollowsthatMmc=1(cid:10)To(cid:10)∞.110LinearSystemsFigure4.9.BlackplotofL(s)4.2.3.UseoftheNicholschartTheBlackplotofatransferfunctionL(s)hasalreadybeendeﬁned(seesection3.2.3):itisthecurvethatrepresentstheevolutionofthecoordinates(|L(iω)|dBargL(iω))asafunctionoftheparameterω.ItisconvenienttorepresentthiscurveontheNicholschartwherethecurves|To|=constandargTo=const(whereTo=L1+L)areplotted.ConsiderforexamplethesystemPwithtransferfunctionP(s)=1−s/3s(1+s/2)(1+2s)fedbackbytheregulatorKwithtransferfunctionK(s)=0.35(thistransferfunctionbeingaconstanttheregulator–orthecontrolitgenerates–issaidtobe“proportional”).ThetransferfunctionL(s)=P(s)K(s)isrepresentedintheNicholschartinFigure4.9.Thephaselagmarginis225◦−180◦=45◦(obtainedatthefrequencyof0.3rad/t.u.wheret.u.isthetimeunitused).Thegainmarginisabout(−∞12dB).Thequantity(cid:23)T0(cid:23)∞(dB)isseenonthechart:itisslightlylessthan3dB(aprecisecalculationshowsthat(cid:23)T0(cid:23)∞(cid:9)1.31i.e.about2.39dB).TheNicholschartcanbeusedtodeterminethegainofanadequateproportionalregulator.Thegaink=0.35isthatofaproportionalregulatorwhichwhenfeedingbackthesystemPprovidesaphaselagmarginof45◦.Closed-LoopSystems1114.2.4.RobustnessagainstneglecteddynamicsNeglecteddynamicswithoutresonanceThetransferfunctionofthemodelPisagaindenotedbyP(s).Wemakethehypothesisthattheactualcontrolsystem˜P(assumedtobelineartime-invariant)hasthefollowingtransferfunction:˜P(s)=P(s)11+τ1s···11+τnswheretheτk>0(1≤k≤n)denotetheneglectedtimeconstants.Lets=iωandsuppose|τkω|(cid:16)1(1≤k≤n)thatistosay|τks|(cid:16)1.Itfollowsthat([17]section2.2.1)˜P(s)∼=P(s)(1−τs)∼=P(s)e−τsτ=n(cid:12)k=1τk.Inotherwordsthe“neglectedtimeconstants”haveaneffectanalogoustoaneglecteddelay.Thedelaymarginthereforecorrectlyrepresentstherobustnessofthefeedbacksystemagainstthiskindofneglecteddynamics.NeglecteddynamicsofanykindInamoregeneralcasethedelaymarginisinsufﬁcientasacriterionforrobustness.Considerthecaseofa“multiplicativemodelingerror”.Thetransferfunction˜P(s)oftheactualcontrolsystem˜PisexpressedasafunctionofthetransferfunctionP(s)ofthemodelPaccordingtoarelationoftheform˜P(s)=(1+E(s))P(s).ThetransferfunctionE(s)(ortheassociatedminimalsystemE)iscalleda“multiplicativemodelingerror”.Considerthefollowingconditions:(i)TheregulatorwithtransferfunctionK(s)stabilizesP(s);(ii)P(s)and˜P(s)havethesamenumberofpolesintheclosedrighthalf-plane.(iii)P(s)and˜P(s)havethesamepolesontheimaginaryaxis(takingintoaccountmultiplicities)–thesetofthesepolescanofcoursebeempty.(iv)ThereexistsatransferfunctionW1(s)∈(cid:1)H∞(seesection13.6)suchthat|E(iω)|<|W1(iω)|foranyfrequencyω≥0.LetEbethesetoftransferfunctionsE(s)satisfyingConditions(ii)–(iv)aboveassumingthatCondition(i)issatisﬁed.112LinearSystemsTHEOREM88.–Anecessaryandsufﬁcientconditionforthefeedbacksystem(consistingofthecontrolsystem˜PfedbackbytheregulatorK)tobestableforanymodelingerrorE(s)∈Eis(cid:23)W1To(cid:23)∞≤1.PROOF.(A)AccordingtoCondition(iii)theBromwichcontoursof˜L(s)=˜P(s)K(s)andthatofL(s)=P(s)K(s)areidentical.(B)LetNbethenumberofturnstheNyquistplotofL(s)goesaroundthepoint−1inthedirectsense.AccordingtoHypothesis(i)andtheNyquistcriterion(Theorem84)wehaveN=nP+nKwherenPandnKdenoterespectivelythenumberofpolesofP(s)andK(s)thatbelongtotheclosedrighthalf-plane.Accordingtothissametheoremandobservation(A)hereaboveanecessaryandsufﬁcientconditionforKtostabilize˜Pis˜N=n˜P+nKwhere˜NisthenumberofturnstheNyquistplotgoesaroundpoint−1inthedirectsenseandn˜Pdenotesthenumberofpolesof˜P(s)belongingtotheclosedrighthalf-plane.AccordingtoConditions(ii)and(iii)wehaven˜P=nPandthusthenecessaryandsufﬁcientconditionaboveisequivalentto˜N=N.(C)Theonlysupplementaryinformationwehaveon˜P(s)isCondition(iv).Itisequivalentto)))˜L(iω)−L(iω))))<|W1(iω)L(iω)|forallω≥0whichmeansthatforeveryfrequencyω˜L(iω)belongstotheopendiskcenteredatL(iω)andwithradius|W1(iω)L(iω)|.Thenecessaryandsufﬁcientconditiondeterminedat(B)isthereforesatisﬁedifandonlyiftheradius|W1(iω)L(iω)|doesnotexceedthedistance|1+L(iω)|fromthepointL(iω)tothepoint−1andthisforanyfrequencyω(seeFigure4.10).Thisconditioniswrittenas|W1(iω)L(iω)|≤|1+L(iω)|andisequivalentto|W1(iω)To(iω)|≤1.Thisinequalityissatisﬁedforeveryfrequencyωifandonlyif(cid:23)W1To(cid:23)∞≤1.Supposethatωisafrequencyatwhichtheuncertaintyormodelerrorislargethatis|W1(iω)|(cid:18)1.ThenecessaryandsufﬁcientconditioninTheorem88implies|To(iω)|≤1|W1(iω)|(cid:16)1.(4.12)WethushaveTo(iω)∼L(iω)andCondition(4.12)leadsto|L(iω)|(cid:25)1|W1(iω)|(cid:16)1(4.13)wherethesymbol(cid:25)signiﬁes“isdominatedby”([12]sectionV.1).Asaresultagoodamountofrobustnessagainstamultiplicativeerrorisobtainedusinga“smallloopgain”.Ingeneraltheneglecteddynamicsareessentiallylocatedinhighfrequencies.Andthusthecondition|L(iω)|(cid:16)1needstobesatisﬁedforlargevaluesofω.Closed-LoopSystems113-10ReImL(i)1+L(i)Disquesd'incertitudederayonIW1(i)L(iIUncertainty disksFigure4.10.Uncertaintydisks4.2.5.PerformanceSection4.2.1alreadysuggestedthattheanalysisofrobustnessisnotseparablefromthatofperformance.Oneoftheessentialtasksofthecontrolengineerisindeedtomanagethebestcompromisebetweenrobustnessandperformance.Aswewillseebelowthesetwopropertiesareingeneralantagonistic.SupposethatinFigure4.1thevariablebeingcontrolled(orregulated)5isz=u2andthat−v2=disadisturbance.Withv1=0weobtainfrom(4.3)theexpressionˆz(s)=So(s)ˆd(s).Intheabsenceofanyfeedbackwehaveˆz(s)=ˆd(s)underthesameconditionsandthetransferfunctionbetweenthedisturbancedandthevariabletoberegulatedzisthereforeequalto1.Asaresultthefeedbackisefﬁcient(intermsofperformance)atfrequencyωifandonlyif|So(iω)|<1.Thisisindeedtheconditionunderwhichthefeedbackwillattenuatethedisturbancethataffectsthecontrolledvariable(atfrequencyω).Wesaythatthereisarejectionofthisdisturbance(atthissamesamefrequencyω)ifSo(iω)=0.5.Aregulationproblemisaspecialcaseoftrackingproblemwherethecontrolledvariableistobemaintainedconstant.Herethisconstantiszero.114LinearSystemsNotethatbecauseL(s)isastrictlypropertransferfunctionwehavelimω→+∞|So(iω)|=1thereforethefeedbackwillnecessarilynolongerbeefﬁcientatveryhighfrequencies.Wecanquantifytheexpectedperformance(inaccordancewiththespeciﬁcations)bychoosingatransferfunctionW2(s)in(cid:1)H∞suchthattheperformanceisconsideredtobesatisfactoryif|W2(iω)So(iω)|≤1foreveryfrequencyω.Thisconditionisequivalentto(cid:23)W2So(cid:23)∞≤1.(4.14)AnadvantageofsuchaformulationisthatCondition(4.14)isinthesameformastheonestatedinTheorem88.Thisallowsustotransformtheproblemofmanagingthecompromiserobustness/performancetoaproblemofoptimizationinthe(cid:1)H∞space–whichisoutsidethescopeofthisbook(seee.g.[122]).Considerafrequencyωatwhichaheavyattenuationofthedisturbanceisrequired.Wehavetherefore|W2(iω)|(cid:18)1.Condition(4.14)leadsto|So(iω)|≤1|W2(iω)|(cid:16)1.(4.15)WethushaveL(iω)∼1/So(iω)and(4.15)yields|L(iω)|(cid:2)|W2(iω)|(cid:18)1.(4.16)ItisclearthatConditions(4.13)and(4.16)arecontradictoryiftheyarerequiredatthesamefrequencyω.Wethereforecannotobtainfromaclosed-loopsystemagoodperformanceatafrequencyatwhichalargemodelingerrorispresent.Ingeneralperformanceisrequiredatlowfrequencies.Andthatisthereasonwhythecondition|L(iω)|(cid:18)1mustbesatisﬁedatsmallvaluesofω.4.2.6.SensitivitytomeasurementnoiseNowsupposethat(seeFigure4.1)v2representsthemeasurementnoiseaddingtothecontrolledvariable(especiallybecauseofimperfectionsofthesensor).Itisveryimportantthatthecontrolu1ofthesystemshouldnotbetoosensitivetothisnoise.Ifnotindeedthecontrolwillbeveryagitatedanditwillresultinprematurewearontheactuators(especiallyifthesearemadeofmechanicalparts).Accordingto(4.3)thetransferfunctionbetweenv2andu1isSi(s)K(s)=So(s)K(s)(becauseatthismomentonlytheSISOcaseisconsidered).Theagitationduetothismeasurementnoiseisharmfulessentiallyathighfrequenciesforwhichwehavetherelation(4.13)andthus|So(iω)|∼1and|So(iω)K(iω)|∼|K(iω)|.Itisthereforeimportanttolimitthegainoftheregulatorinhighfrequencies.Closed-LoopSystems115Intheabsenceoffeedbackthetransferfunctionbetweenv2andthecontrolvariabley1isobviouslyzero.Aperverseeffectofthefeedbackcanmakey1agitatedbecauseofthenoisev2.Inclosed-loopthetransferfunctionbetweenv2andy1is(accordingto(4.2))To(s).Tolimittheperniciouseffectexaminedhereweareledtoimposeaconditionsuchas(4.12)whichimplies(4.13)ontheopen-looptransferfunction.4.2.7.LoopshapingofL(s)Conditions(4.13)and(4.16)uniquelyimposedontheopen-looptransferfunctionL(s)mustbesatisﬁed:theﬁrstoneinhighfrequenciesandthesecondoneinlowfrequencies.Ontheotherhanditisnecessarytohavesufﬁcientphasemargin.AssumingthatL(0)>0letusconsiderthemostfavorablecasewhereL(s)isstableandminimumphase(wewillelaboratelaterwhythiswillbethemostfavorablecase)andassumethatL(0)>0.Letω0betheunitygainfrequency(whichwewillassumetobeuniqueforsimplicity).Let−20ndB/decade(n∈N)betheslopeofthemagnitudeofL(iω)inaneighborhoodofω=ω0.TheasymptoticBodeplotshowsthatforn=2thephaseisabout−180◦whichcorrespondstoazerophasemargin.Ifn>2itisnothardtoseethattheNyquistplotofL(s)encirclespoint−1clockwiseandsotheclosed-loopsystemisunstable.Theonlytwopossiblevaluesfornare1and0.Inthecasen=1theBayard–Boderelation(seesection3.4.4(3.19))showsthatifthecorrespondingslopeismaintainedover1decadecenteredlogarithmicallyatω0thephaseofL(s)atthatfrequencycannotbesmallerthan−90(1+026)∼=−113◦whichguaranteesaphaselagmarginof180−113=67◦largelysufﬁcient.Ontheotherhandmaintainingthesameslopeoveronlyhalfadecadeguaranteesaninsufﬁcientphaselagmargin.HavinggottentheseresultswecandeducethetypicalshapeaBodeplotofL(s)hastohavei.e.theloopshaping6weneedtoachieveforthistransferfunction(seeFigure4.11).ThedifﬁcultproblemsarethosewhereperformanceandrobustnessrequirementsaresuchthatL(s)mustbethetransferfunctionofaveryselectiveﬁlterinthetwotransitionbands:theregiontotheimmediateleftofthelinesegmentofslope−20dB/decadeandtheregionlocatedtotheimmediaterightofthatsamesegment.Thislargeselectivitynecessitatesacontrollerofhighorder.Thevalueofthecutofffrequencyω0islimitedbythenecessarydelaymargin.6.Thisloopshapingdoestakeintoaccountthenecessitypreviouslypointedoutoflimiting|K(iω)|inhighfrequencies.116LinearSystemsFigure4.11.LoopshapingofL(s)4.2.8.Degradationofrobustness/performancetrade-offUnstablepolesIntuitivelyanunstablesystemishardertocontrolthanastablesystem.WecansupportthisintuitivejudgmentbyreasoningonthesensitivityfunctionSo(s).Theclosed-loopsystempossessesbothagoodmodulusmarginandagoodcapacityofattenuationofadisturbanceaddingtothecontrolledvariableundertheconditionthatthequantity(cid:23)So(cid:23)∞shouldnotbetoolargecomparedto1.InparticularamodulusmarginMm≥0.5isobtainedwhen(cid:23)So(cid:23)∞≤2andthefeedbackintermsofdisturbanceattenuationisefﬁcientatfrequencyωif|So(iω)|≤1(seesections4.2.2and4.2.5).ThefollowingresultwasobtainedbyBode[9]inthecaseofastableopen-loopandgeneralizedbyFreudenbergandLooze[50](seealso[51]section3.4)inthecaseofanarbitraryopen-loop.SupposethetransferfunctionL(s)hasthefollowingtwoproperties:(i)ThesetofpolesofL(s)withpositiverealpartis{pj:j∈J}whereJisaﬁnitesetofindices(possiblyempty);(ii)L(s)hasarelativedegreeδ(L)≥2.Closed-LoopSystems117Thenthefollowingrelation(calledtheBoderelationortheBode–Freudenberg–Loozerelation)holds:+∞(cid:2)−∞ln|So(iω)|dω=2π(cid:11)j∈JRepj.(4.17)Thequantityontheright-handsideof(4.17)isnon-negativeandiszeroifandonlyifL(s)onlyhasstablepolesorpolesatthelimitofinstability(meaningtheyareontheimaginaryaxis).Andwededucethefollowingresult:PROPOSITION89.–IfthereexistsanintervaloffrequenciesIbwithnon-emptyinterioronwhich|So(iω)|<1(i.e.ln|So(iω)|<0)theremustexistanotherintervaloffrequenciesImwithnon-emptyinterioronwhich|So(iω)|>1sothattheintegralontheleft-handsideof(4.17)isnon-negative(andpositiveifL(s)haspolesintherighthalf-plane).Inotherwordstheefﬁciencyofacontrolsystem(intermsofdisturbanceattenuation)atcertainfrequenciesisobtainedatthepriceofaharmfuleffectofsuchacontrolsystem(relativetothesamepoint)atotherfrequencies.Thisislikea“communicatingvessels”effect.WhenL(s)hasunstablepolestheright-handsideof(4.17)ispositivewhichonthewholemakesthesituationmoreunfavorable.UnstablezerosContrarytowhatwemightthink“unstablezeros”aremorepenalizingthan“unstablepoles”.Thisiswhatwewillshowbelow.Supposetheopen-looptransferfunctionL(s)hasthefollowingtwoproperties:(i)thesetofpolesofL(s)withpositiverealpartis{pj:j∈J}whereJisaﬁnitesetofindices(possiblyempty);(ii)L(s)hasatleastonezerozwithpositiverealpart.TheBlaschkeproduct[103]associatedwiththesetofpolesP={pjj∈J}istherationalfunctionBP(s)=’j∈Jpj−s¯pj+sifJisnon-empty=1ifJisempty.118LinearSystemsThistransferfunctionBP(s)belongsto(cid:1)H∞.AssumethatJisnon-empty.Sinceforanyindexj∈J)))pj−s¯pj+s)))≤1ifandonlyifRe(s)≥0wehave|BP(s)|≤1ifandonlyifRe(s)≥0.Letz=x+iybeazeroofL(s)withrealpartx>0andletθzbethefunctiondeﬁnedbyθz(ω)=arctanω−yx.Wecanprovethefollowingresult[50]:+∞(cid:2)−∞ln|So(iω)|dθz(ω)=−πln|BP(z)|.(4.18)Theright-handsideof(4.18)isnon-negative.Inadditiondθz(ω)=Wz(ω)dωwithWz(ω)=dθzdω(ω)=xx2+(y−ω)2>0whichleadstothefollowingconclusion:PROPOSITION90.–ThepropertystatedinProposition89stillholds.Proposition90isnotaredundantversionofProposition89becauseitisbasedondifferenthypotheses:whilehypotheses(i)and(i’)aresimilar(ii)and(ii’)areverydifferent.Ifwecomparetheleft-handsideof(4.17)tothatof(4.18)theintegralinthelastquantityincludesasanadditionalterm“weightingfunction”Wz(ω)whichbeingmaximumatω=y“focalizes”thisintegralonthefrequenciesintheneighborhoodofy.ThereadercancheckthefollowingusingTheorem432(section12.2.3):if(xn)isasequenceofpositivenumberstendingtozerothen(cid:15)1πdθzn/dω(cid:16)(wherezn=xn+iy)tendstoδy:t(cid:3)→δ(t−y)inD(cid:2);therefore1π+∞(cid:1)−∞ln|So(iω)|dθzn(ω)→ln|So(iy)|.Asaresultwhentheopen-looptransferfunctionofL(s)hasan“unstablezero”zitispenalizingforthesensitivityfunctionandthusforthemodulusmargin.Letusseewhatremedywecanﬁndinsuchasituation.LetL(cid:2)(s)=(cid:21)z+sz−s(cid:22)kL(s)wherekisthemultiplicityofthezeroz.ThetransferfunctionL(s)doesnothaveazeroats=zanymorebutats=−z(withamultiplicityequaltok).ProceedinginthismannerforeachunstablezeroofL(s)wearriveatanopen-looptransferfunctionClosed-LoopSystems119whichhasnounstablezeroanymore.TosimplifytherationalesupposeL(s)hasauniqueunstablezeroz.WehaveL(s)−L(cid:2)(s)=L(cid:2)(s)E(s)(4.19)withE(s)∼2kszass→0.Asaresultthe“multiplicativeerror”thatiscausedbyreplacingL(s)withL(cid:2)(s)issuchthat|E(s)|(cid:16)1ifandonlyif|s|(cid:16)|z|2k.Letω0bethelargestunitygainfrequencyofL(s)thatisω0=sup{ω:|L(iω)|=1}.Accordingtotheaboveunstablezerozbringsaninsigniﬁcantdeteriorationonthemodulusmarginontheconditionthatω0(cid:16)|z|2k.(4.20)Thereforeanunstablebut“rapid”zero(whichmeanstheabsolutevalue|z|islarge)isattheendonlyalittlepenalizing.Ifitis“slow”itisnecessaryto“re-shape”theclosed-loopsystemuntilcondition(4.20)issatisﬁedandthereforetogiveupobtaininggoodperformance.REMARK91.–TheaboveisonlyvalidwhenRe(z)>0.IndeedifL(s)hasazeroz=iy(ontheimaginaryaxis)−zisagainontheimaginaryaxisandthetransferfunctionL(cid:2)(s)asconstructedaboveisunstable.Tosimplifytheanalysisofthissituationsupposethatz=iyisasimplezero(i.e.withmultiplicity1)ofL(s).PutL(cid:2)(s)=z+α+sz−sL(s)α>0.(4.21)ThetransferfunctionL(cid:2)(s)nolongerhasazeroats=zbutats=−z−αtherealpartofwhichis−α<0.Wethenhaverelation(4.19)with|E(iω0)|(cid:16)1ifα(cid:16)|ω0|(cid:16)|y|2.(4.22)Condition(4.22)iscoherentwith(4.20)eventhoughitcorrespondstoaquitedifferentsituation.4.2.9.*ExtensiontotheMIMOcaseRobustnessmarginsToextendtheaboveresultstothecaseofMIMOsystemsitisnecessarytocarefullydistinguishSiandTiwhicharethesensitivityfunctionandthe120LinearSystemscomplementarysensitivityfunctionrespectivelyattheinputofsystemPfromSoandTowhichareattheoutputofP(seesection4.1.2).Thisleadsustodeﬁnetwomodulusmargins:thatattheinputofPdenotedbyMmiandthatattheoutputofPdenotedbyMmo.Thesequantities(calledtheinputmodulusmarginandtheoutputmodulusmargin)aredeﬁnedbythefollowingrelations:Mmi=1(cid:23)Si(cid:23)∞Mmo=1(cid:23)So(cid:23)∞(seesection13.6.2forthedeﬁnitionofthe“H∞norm”ofatransfermatrix).Wecansimilarlydeﬁnetwocomplementarymodulusmargins–attheinputofPdenotedbyMmciandattheoutputofPdenotedbyMmco–bytherelationsMmci=1(cid:23)Ti(cid:23)∞Mmco=1(cid:23)To(cid:23)∞.TheMIMONyquistplotcannotprovideageometricalinterpretationfortheserobustnessmargins(seeExercise105).WecanalsodeﬁneagainmarginMgiandaphasemarginMpiattheinputoftheMIMOsystemPandagainmarginMgoandaphasemarginMpoattheoutputofsamesystemproceedinginthefollowingmanner[13]:DEFINITION92.–(i)Theinput(resp.output)gainmarginisthelargestintervalMgi(resp.Mgo)containing1suchthatthefeedbacksystemremainsstablewhenthetransfermatrixP(s)isreplacedbyP(s)Λ(resp.ΛP(s))foreverynon-negativesymmetricrealmatrixΛwhoseeigenvaluesbelongtosuchaninterval.(ii)Theinput(resp.output)phasemarginisthelargestintervalMpi(resp.Mpo)containing0suchthatthefeedbacksystemremainsstablewhenthetransfermatrixP(s)isreplacedbyP(s)eiΦ(resp.eiΦP(s))foreverysymmetricrealmatrixΦwhoseeigenvaluesbelongtosuchaninterval.NotethatΛandΦrepresenttheuncertaintiestheﬁrstisonthe“gain”andthesecondisonthe“phase”.THEOREM93.–Relations(4.9)and(4.10)extendtotheMIMOcasethatisMg∗⊃(cid:21)11+Mm∗11−Mm∗(cid:22)(4.23)Mp∗⊃(cid:21)−2arcsinMm∗22arcsinMm∗2(cid:22)(4.24)with∗=ioro.Closed-LoopSystems121_KPMFigure4.12.UncertainfeedbacksystemPROOF.Theproofismadeforthegainandphasemarginsatsysteminput;thereasoningissimilarforthesamemarginsatsystemoutput.(i)LetΛ=(Im−M)−1.ThentheblockdiagramofthefeedbacksystemwithP(s)replacedbyP(s)ΛisrepresentedasshowninFigure4.12.ThematrixMisfedbacktothetransfermatrix(Im+KP)−1=Si.Accordingtothesmallgaintheorem(Theorem87section4.1.5)asufﬁcientconditionfortheclosed-looptoremainstableistherefore¯σ(M)(cid:23)Si(cid:23)∞<1andaccordingto(4.23)thisyields¯σ(M)<Mmi.(4.25)SinceM=Im−Λ−1condition(4.25)holdsifandonlyifforeveryeigenvalueλofΛwehaveλ∈(cid:9)11+Mmi11−Mmi(cid:10).(ii)LeteiΦ=(Im−M)−1.Thesufﬁcientconditionofstability(4.25)cannowbewrittenas7¯σ(cid:15)Im−e−iΦ(cid:16)<Mmi.(4.26)WehaveIm−e−iΦ=2ie−iΦ/2sinΦ/2([52]section9.12)andtherefore¯σ(cid:15)Im−e−iΦ(cid:16)=2¯σ(sinΦ/2).Condition(4.26)isthereforesatisﬁedifandonlyif|ϕ|<2arcsin(Mmi/2)foreveryeigenvalueϕofΦ.7.ThematrixM=Im−e−iΦhascomplexentriesneverthelessthesmallgaintheoremisstillvalidinthatcase.OntheotherhandeiΦisthe“phase”atagivenfrequencyofatransfermatrixbelongingto(cid:5)Hm×m∞.122LinearSystemsMIMOdelaymarginDEFINITION94.–Theinput(resp.output)delaymarginistheleastupperboundMRi(resp.MRo)oftherealnumbersrforwhichtheclosed-loopsystemremainsstablewhenthetransfermatrixP(s)isreplacedbyP(s)e−Rs(resp.e−RsP(s))foranysymmetricrealmatrixR≥0whoseristhelargesteigenvalue.THEOREM95.–AnupperboundofthedelaymarginMR∗is(cid:23)sT∗(s)(cid:23)−1∞with∗=ioro.PROOF.*TheproofisdoneforthedelaymarginatinputofP.Since˜P(s)=P(s)(Im+E(s))replacingP(s)by˜P(s)=P(s)e−RsintroducesamultiplicativeerrorE(s)=e−Rs−Imattheinput.TheratioE(s)/sisfedbacktosTi(s)inthesensewheretheclosed-loophasanequivalentdiagramasdepictedinFigure4.1withP(s)andK(s)replacedbyE(s)/sandsTi(s)respectively(seesections2.6.2and2.6.3).NowE(s)s=e−Rs/2(cid:9)e−Rs/2−eRs/2(cid:10)/sandthusfors=iω(cid:5)=0¯σ(cid:21)E(iω)iω(cid:22)≤2¯σ(cid:9)e−Riω/2(cid:10)¯σ(cid:21)sin(Rω/2)ω/2(cid:22).Weimmediatelygetsupω>0¯σ(cid:21)sin(Rω/2)ω/2(cid:22)=r.(4.27)Ontheotherhandsin(Rs/2)s/2doesnothaveapoleintheclosedrighthalf-planeandsoaccordingto(4.27)thistransfermatrixbelongstotheHardyspaceHm×m∞(seesection13.6.3).Accordingtothesmallgaintheorem(Theorem87section4.1.5)thestatementofwhichisstillvalidwhen(cid:1)H∞isreplacedbyH∞asufﬁcientconditionforstabilityoftheclosed-loopsystemisr(cid:23)sTi(s)(cid:23)∞<1.Thisprovesthetheorem.*RobuststabilitytheoremLetusgeneralizeTheorem88.Considertheso-calledstandarddiagraminFigure4.13.ThesystemPaiscalledtheaugmentedsystemand∆representsthemodeluncertainty.ThesystemsPa∆andKareassumedtobelineartime-invariantandminimal.ThereforetheycanbeidentiﬁedwiththeirrespectivetransfermatricesPa(s)∆(s)andK(s).Closed-LoopSystems123u1y1y2u2PaKFigure4.13.StandarddiagramDenotebyFl(PaK)(resp.Fu(Pa∆))thelineartime-invariantsystem–orthecorrespondingtransfermatrix–withinputu1andoutputy1when∆=0(resp.withinputu2andoutputy2whenK=0);FlandFuarecalledlinearfractionaltransformations(LFTs)andaseasilyseentheyaregivenbyFl(PaK)=Pa11+Pa12K(I−Pa22K)−1Pa21Fu(Pa∆)=Pa21∆(I−Pa11∆)−1Pa12+Pa22wherePa=(cid:27)Pa11Pa12Pa21Pa22(cid:28).LetB1(Pa)bethesetofallrationaltransfermatrices∆forwhichthetwofollowingconditionshold:(i)σ(∆(iω))<1∀ω≥0;(ii)Fu(Pa∆)andFu(Pa0)havethesamenumberofpolesintheclosedright-halfplaneandhavethesamepolesontheimaginaryaxis(ifany)takingintoaccountmultiplicities.Onecanprovethefollowing[89]:THEOREM96.–AssumethatKstabilizesFu(Pa0).ThenKstabilizesFu(Pa∆)forevery∆∈B1(Pa)ifandonlyif(cid:23)Fl(PaK)(cid:23)∞≤1.ToseehowTheorem96canbeusedconsidere.g.thecaseofamultiplicativemodelerror.Supposethetransfermatrix˜P(s)ofthesystemtobecontrolled˜PcanbeexpressedasafunctionofthetransfermatrixP(s)ofthemodelPfollowing˜P(s)=(Ip+E(s))P(s).Thediagramoftheclosed-loopsystemcanberepresentedasin124LinearSystems-y2u2PKy1u1W1+Figure4.14.Closed-loopwithmultiplicativeerrorFigure4.14whereσ(∆(iω))<1∀ω≥0.ThisdiagramcanthenbeputintheformgiveninFigure4.13with(cid:27)0P−W1I−P(cid:28).ThereforeFu(Pa∆)=−(I+W1∆)P=−(I+E)P=−˜PwhereE=W1∆andFl(PaK)=−PK(I+PK)−1W1.Considerthefollowingconditions:(i)ThecontrollerwithtransfermatrixK(s)stabilizesP(s);(ii)P(s)and˜P(s)havethesamenumberofpolesbelongingtotheclosedrighthalf-planetakingintoaccountmultiplicities;(iii)P(s)and˜P(s)havethesamepolesontheimaginaryaxistakingintoaccountmultiplicities;(iv)ThereexistsatransferfunctionW1(s)∈(cid:1)H∞suchthat¯σ(E(iω))<|W1(iω)|foranyfrequencyω≥0.LetEbethesetoftransfermatricesE(s)satisfyingconditions(ii)–(iv)assumingthatcondition(i)holds.Thefollowingwhichwasprovedin[114](section7.4)isnowanobviousconsequenceofTheorem96(comparewithTheorem88):COROLLARY97.–Anecessaryandsufﬁcientconditionfortheclosed-loopsystemtobestableforanymodelingerrorE(s)∈Eis(cid:23)W1To(cid:23)∞≤1.REMARK98.–(i)Theorem96isprovedusingtheMIMONyquistcriterion(Theorem86)notthesmallgaintheorem.Indeeditisnotassumedthat∆isatransfermatrixwithentriesin(cid:1)H∞(and∆canactuallyhavepolesintheclosedrighthalf-plane).(ii)InCorollary97themultiplicativemodelerrorEisassumedtobeattheoutputofP.IfthiserrorisattheinputofPthenecessaryandsufﬁcientconditionofclosed-loopstabilityischangedto(cid:23)W1Ti(cid:23)∞≤1.Regardingperformanceaninequalitysuchas(4.14)canalsocharacterizeasatisfactoryattenuationofadisturbanceattheoutputofP.ThecompromisebetweenClosed-LoopSystems125robustnessandperformanceremainsclearifthemultiplicativeuncertaintyislocatedattheoutputofPsinceSo+To=IpbutthisisnolongerthecasewhenthemultiplicativeuncertaintyisattheinputofPsincethenwemustcompareSoandTiandthesetwoquantitiesnolongerhaveasimplerelationship.ShapingofsingularvaluesofLo(s)Theloopshapingstudiedinsection4.2.7canalsobeextendedtotheMIMOcase.ThetransfermatricesLi(s)=K(s)P(s)andLo(s)=P(s)K(s)havetobedistinguished.IfweareinterestedinrobustnessagainstuncertaintieslocatedattheoutputofPaswellasintheattenuationofadisturbanceaffectingtheoutputofPthetransfermatrixLo(s)oftheopenloopatoutputofPhastobeconsidered.Supposewehave˜P(s)=(Ip+E(s))P(s)andthatthemultiplicativeerrorE(s)atoutputofPisonlylargeinhighfrequenciesinotherwordstheabovetransferfunctionW1(s)satisﬁes|W1(iω)|(cid:18)1onlyforlargevaluesofω.ThenthenecessaryandsufﬁcientconditioninCorollary97whichis(cid:23)W1To(cid:23)∞≤1onlyimplies¯σ(To(iω))(cid:16)1forlargevaluesofω.SinceTo=(Ip+Lo)−1Lowehave¯σ(To(iω))∼¯σ(Lo(iω))andthecondition¯σ(To(iω))|W1(iω)|≤1implies¯σ(Lo(iω))(cid:25)1|W1(iω)|(cid:16)1.(4.28)Thisreplacescondition(4.13).SupposealsothatheavyattenuationofthedisturbanceisrequiredbyspeciﬁcationssheetonlyatoutputofPforsmallvaluesofω.Thisrequirementtranslatesintoaninequalitysuchas(4.14)wherethetransferfunctionW2(s)onlysatisﬁes|W2(iω)|(cid:18)1forsmallvaluesofω.SinceSo=(Ip+Lo)−1wehaveaccordingtoProposition581(section13.5.7)¯σ(So(iω))=1σ(Ip+Lo(iω))andthisquantitycannotbelargerthan1|W2(iω)|fromwhichwegettheconditionσ(Lo(iω))(cid:2)|W2(iω)|(cid:18)1(4.29)whichreplaces(4.16).Conditions(4.29)(athighfrequencies)and(4.29)(atlowfrequencies)plustheconsiderationsonthemodulusmargincontributetoshapeofthesingularvaluesofLo(iω)asinFigure4.15.Robustness/performancecompromiseTherobustness/performancecompromiseseemsmoredifﬁculttoachieveinthecaseofMIMOsystems(ascomparedtoSISOones)whenthesingularvaluesofLo(iω)areverydifferentthatistosaywhenthematrixLo(iω)isill-conditioned(see126LinearSystemsdBlog|W2(i)||W1(i)|1(Li(i))(Li(i))Figure4.15.LoopshapingintheMIMOcasesection13.5.7).ThisisnonethelessinevitablewhenthesystembeingcontrolledPisitself“ill-conditioned”(whichistosaythatitstransfermatrixP(iω)hasnon-zerosingularvaluesofaverydifferentmagnitude)asituationwhichisveryunfavorable.ThemostimportantpointisthatLo(iω)thenmusthaveaconditionnumbercloseto1atfrequenciesωsuchthat¯σ(Lo(iω))(cid:9)1(wethenhave¯σ(Lo(iω))(cid:9)σ(Lo(iω))(cid:9)1):thisistheso-called“singularvaluebalancing”method(whichisvisiblynotusedinthecaseofFigure4.15).Wecannotgointomuchfurtherdetailsonthissubject(seee.g.[90]).Theresultspresentedinsection4.2.8relativetothedegradationoftherobustness/performancetrade-offduetothepresenceofunstablepolesandzeroscanbegeneralizedforthemostparttothecaseofMIMOsystems(see[51][27]).SystemconditioningInthepreviousdiscussionwehavepointedoutamajordifﬁcultypresentedby“ill-conditioned”MIMOsystems.Wearenowgoingtoexamineaseconddifﬁculty.TheshapingofsingularvaluesasshowninFigure4.15relatestothetransfermatrixLo(s).Assumingthatthisshapingiscorrectlydonecanwededucethattheclosed-looppresentsgoodrobustnessandperformanceattheinputofP?InotherwordsdoesthatalsoimplycorrectloopshapingwiththesingularvaluesofLi(s)?WehavethefollowingresultforthecasewherethetransfermatricesP(s)ofPandK(s)ofKaresquare(ofdimensionm×m)andinvertible(inthealgebraR(s)m×m)denotingbyκ(.)theconditionnumberofthematrixinparentheses(section13.5.7):Closed-LoopSystems127PROPOSITION99.–Wehave1κ(P)σ(Lo)≤σ(Li)≤¯σ(Li)≤κ(P)¯σ(Lo)(4.30)1κ(P)σ(Li)≤σ(Lo)≤¯σ(Lo)≤κ(P)¯σ(Li).(4.31)PROOF.WehaveLi=KP=P−1PKP=P−1LoPandtherefore¯σ(Li)≤¯σ(cid:15)P−1(cid:16)¯σ(P)¯σ(Lo)=κ(P)¯σ(Lo).Theotherinequalitiesareprovedbysimilarlinesofreasoning.AgoodapproachforrobustcontrolofMIMOsystemshastoleadtoacorrectshapingofboththesingularvaluesofLo(s)andthoseofLi(s).TheH∞synthesismethodbasedontherepresentationofP(s)bya“normalizedcoprimefactorization”duetoMacFarlaneandGlover[89]issupposedtoleadtosucharesult(howevertheirworkisoutsidethescopeofthisbook).FromProposition99onecanconcludethattheregulationofwell-conditionedMIMOsystemsposesnomajorproblem.Howeverproblemsassociatedwithill-conditionedMIMOsystemsseemalmostintractablewhateverthemethodusedtosynthesizethecontrollaw.4.3.ExercisesEXERCISE100.–ConsiderthetransferfunctionP(s)=1s(1+s)(1+025s)ofaminimalsystemP.(i)TracetheasymptoticBodediagramofP(s)andsketchfromtheretheshapeofitsBodeplot.(ii)TracetheshapeoftheNyquistdiagramofP(s).(iii)TracetheshapeofitsBlackplot.(iv)GivenbelowisatableofvaluesoftheBodeplotofP(s):ω(rad/s)04412Gain(dB)63-33-14Phase(◦)-120-149-180Determinetheproportionalcontrollerforwhichtheclosed-loopsystemhasaphasemarginof60◦.Whatarethenthevaluesofthedelaymarginandgainmargin?EXERCISE101.–LetP(s)=1s2bethetransferfunctionofaminimalsystemPfedbackbyaminimalcontrollerKwithtransferfunctionK(s)=ss+1.(i)AccordingtoTheorem76isthefeedbacksystemstable?(ii)Determinethetransfermatrixofrelation(4.2).Isthatconsistentwith(i)?(iii)TracetheNyquistplotofL(s).(iv)ByapplyingtheNyquistcriterion(Theorem84)studythestabilityoftheclosed-loop128LinearSystemssystem.(v)SomeauthorsstatetheNyquistcriterionwithanindentedimaginaryaxisthatisdifferentfromthatconstructedforTheorem84:thesemi-circlesJk(ε)goroundthepointspklocatedontheimaginaryaxistotherightinsteadoftotheleft(seeFigure4.2)nP(resp.nK)isthereforethenumberofpolesofP(s)(resp.K(s))(accountingformultiplicities)thatbelongtotheopenrighthalf-plane.Accordingtothisexampleisthecriterionobtainedcorrect?EXERCISE102.–LetP(s)=1s−1bethetransferfunctionofaminimalsystemPfedbackbytheminimalcontrollerKwithtransferfunctionK(s)=s−1s+1.(i)Answeragainquestions(i)–(iv)ofExercise101(usingTheorem80insteadofTheorem84).(ii)SomeauthorsreplacethestatementofTheorem80bythefollowing:“Theclosed-loopsystemisstableifandonlyifnL=NwherenListhenumberofpolesofL(s)(accountingformultiplicities)locatedintheclosedrighthalf-plane”.Isthisstatementcorrect?EXERCISE103.–ThehypothesesarethoseofTheorem80.IfP(s)andK(s)bothbelongto(cid:1)H∞showthataccordingtotheNyquistcriterionwehavethefollowingsufﬁcientconditionofstability:“Theclosed-loopsystemisstableifallpointsofintersectionbetweenthecurve{L(iω)ω≥0}andtherealaxisarelocatedtotherightofpoint−1.Isthisanecessarycondition?EXERCISE104.–AtransferfunctionP(s)∈(cid:1)H∞issaidtobepositivereal(PR)ifReP(iω)≥0foranyω≥0andisquasi-strictlypositivereal(QSPR)ifReP(iω)>0foranyω≥0[61].ThehypothesesarethoseofTheorem80.UsingtheNyquistcriterionshowthatifbothP(s)andK(s)arePRandatleastoneofthemisQSPRthentheclosed-loopsystemisstable.EXERCISE105.–*LetPγbeasystemdeﬁnedbythedifferentialequation˙y=A0y+BγuwithA0=(cid:27)−100−1(cid:28)andBγ=(cid:27)1−γ01(cid:28)γ≥0fedbackbythecontrollerK=I2.(i)TracetheMIMONyquistplotofLo(s)=P(s)K(s)=K(s)P(s)=Li(s).Whatisthedistancefromthisplottocriticalpoint−1?(ii)AssumethatthemodelingerrorcanbeeffectedbyreplacingtheabovematrixA0bythematrixAε=(cid:27)−10ε−1(cid:28)ε>0.Showthatforanyε>0theclosed-loopsystemisunstableforasufﬁcientlylargeγ.(iii)Showthatforγ→+∞Mmiγ=Mmoγtendstoward0whereMmiγ(resp.Mmoγ)istheinput(resp.output)modulusmarginofthesystemPγfedbackbythecontrollerK.(iv)Whatcanbeconcluded?Chapter5CompensationandPIDController5.1.Onedegreeoffreedomcontroller5.1.1.Closed-loopsystemWehavealreadyseeninthepreviouschapterthatinsomecasesaproportionalcontrollercouldbeusedtostabilizeasystembyfeedbackoreventoobtainagivenphasemargin(seeExample85section4.1.4andExercise100section4.3).Neverthelessitisclearthattheclassofproportionalcontrollersisfartoorestrictedtoallowforapropershapingoftheopen-looptransferfunction(seesection4.2.7).Thischapterisdedicatedtothestudyofclassiccontrollersthatcanimprovetherobustness/performancecompromisewhenacontrollerwithonedegreeoffreedom(1-DOF)isused.The1-DOFcontrollerKisshowninFigure5.1.InthisﬁgureuandydenoterespectivelythecontrolandtheoutputofPnisthemeasurementnoise(duetothepresenceofthesensor)duanddyareadditivedisturbancesactingonuandyrespectivelyandrisareferencesignal.Theobjectiveoftheregulationistorendertheerrore=y−rassmallaspossibleinsteadystateassumingthatthespectrumofthedisturbancesandthatofthereferencesignalisonlylocatedatlowfrequenciesandthusthesesignalscanbemodeledasconstants.Thespectrumofthemeasurementnoiseisassumedtobelocatedathighfrequencies.5.1.2.Closed-loopequationsandstaticerrorThesystemPisSISOthereforethereisnoneedtodistinguishthesensitivityfunctionsatinputandatoutputofP;bothofthemaredenotedbySo.WethushaveSo=11+LwhereL=PK=KPistheopen-looptransferfunction.ThecomplementarysensitivityfunctionisdenotedasTo=1−So.Itiseasytoshow129Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.130LinearSystemsKP+-y+udu+dy+0nFigure5.1.1-DOFcontrollerthattheclosed-loopsystemisgovernedbythefollowingrelations(assumingthattheinitialconditionsarezero):ˆe(s)=So(s)(cid:9)ˆdy(s)−ˆr(s)+P(s)ˆdu(s)(cid:10)−To(s)ˆn(s)(5.1)ˆu(s)=So(s)ˆdu(s)+K(s)So(s)(ˆr(s)−ˆn(s)).(5.2)AlltheanalysisdoneinChapter4remainsvalid.Westillneedtopreciselyestablishunderwhatconditionthestaticerroriszero(i.e.e(t)=0insteadystate);itisunderstoodthattheclosed-loopsystemisstable.Thisonlymakessenseiftheinﬂuenceofnoneisneglectedwhichmeansonehastoassumethatcondition(4.13)holdswithanappropriateweightingfunctionW1(s)(seesection4.2.6).Letusconsiderthefollowingtwocases:(i)du=0.ThenthestaticerroriszeroifandonlyifthestaticgainofSo(s)iszero(seesecton3.1.2)i.e.So(0)=0.(ii)du(cid:5)=0.ThenthestaticerroriszeroifandonlyifthestaticgainofbothSo(s)andSo(s)P(s)iszero.Tospecifytheabove-givenconditionsletusexpressthetransferfunctionsP(s)andK(s)intheformofirreduciblerationalfunctionsB(s)A(s)andR(s)S(s)respectivelywhereA(s)B(s)R(s)S(s)belongtoR[s].WehaveSo(s)=A(s)S(s)A(s)S(s)+B(s)R(s)So(s)P(s)=B(s)S(s)A(s)S(s)+B(s)R(s).Asaresult:(a)Insituation(i)thestaticerroriszeroifandonlyifA(0)=0orS(0)=0.SinceL(s)=B(s)R(s)A(s)S(s)thismeansthattheopen-looptransferfunctionhasazeroCompensationandPIDController131pole(i.e.theopen-loopisanintegratorsystem:seeDeﬁnition32).IfPisalreadyanintegratorsystemitisunnecessarythatcontrollerKbeonetoo.(b)Insituation(ii)thestaticerroriszeroifandonlyifthetwofollowingconditionshold:A(0)S(0)=0andB(0)S(0)=0.SupposetheﬁrstequalityholdsthenB(0)cannotbezeroforotherwisethecharacteristicpolynomialoftheclosed-loopAcl(s)=A(s)S(s)+B(s)R(s)wouldhavearootequalto0andtheclosed-loopsystemwouldbeunstable(seeTheorem76section4.1.3).Thereforeitisnecessary(andsufﬁcient)thatS(0)=0i.e.thatthecontrollerKbeintegrator.5.2.Leadcompensator5.2.1.CharacteristicsofaleadcompensatorAleadcompensatorisaminimalsystemwithtransferfunctionformHατ(s)=1+ατs1+τswhereτ>0isatimeconstantandαisarealnumbergreaterthan1.TheBodeplotofHατ(s)(withitsasymptoticdiagram)isrepresentedinFigure5.2forthecaseofτ=1andα=5.Themaximumvalueϕdofthephase(denotedasφintheﬁgure)isgivenbyϕd=arcsinα−1α+1(5.3)Figure5.2.Bodeplotofaleadcompensator132LinearSystemsanditisattainedattheangularfrequencyω0=1τ√α(5.4)whichisthelogarithmicmeanof1ατand1τ(thisfrequencyisdenotedasωinFigure5.2).Wecaneasilyverifythatγ=|Hατ(iω0)|=√α(5.5)(seeExercise106(i)).5.2.2.PrinciplesofaleadcompensatorNowwewilldetermineacompensatorthat:(i)producesamaximumphaseleadϕdatagivenfrequencyω0>0(with0<ϕd<90◦);(ii)hasunitygainatthatfrequency.Accordingtosection5.2.1theminimalsystemwithtransferfunctionH(s)=1γHατ(s)answersthequestionwithaccordingto(5.3)and(5.4)α=1+sinϕd1−sinϕdτ=1ω0√α(5.6)andﬁnallywegetH(s)=1√α1+√αω0s1+1ω0√αs.(5.7)Consideragaintheexampleinsection4.2.3.Supposewewishtoobtainaphaselagmarginof45◦nolongeratfrequency0.3rad/t.u.butat0.5rad/t.u.Itisthusnecessaryto“accelerate”thefeedbacksystemandonecanforthispurposemakeuseofaleadcompensator.TheBlackplotofP(s)=1−s/3s(1+s/2)(1+2s)isrepresentedinFigure5.3wherethetimeunitisinseconds.Tobringthepointat0.5rad/stoagainof0dBandaphaseof180◦+45◦=225◦twooperationsarenecessary:(i)toaddagainof−2.91dBwhichcorrespondstoaratioof0.72;(ii)toaddaphaseof225◦−202◦=23◦atafrequencyω0=0.5rad/s.Operation(ii)isdoneusingaleadcompensatorhavingcharacteristicsaccordingto(5.6)and(5.7)α=2.28andH(s)=0.661+3.0s1+1.324s.TheappropriatecontrollerthushasatransferfunctionK(s)=0.72H(s)=0.471+3.0s1+1.32s.TheBlackplotofthecompensatedsystem(i.e.L(s)=P(s)K(s))isrepresentedinFigure5.4.CompensationandPIDController133Figure5.3.BlackplotofP(s)Figure5.4.BlackplotofL(s)134LinearSystemsFigure5.5.StepresponsesInFigure5.5weseeacomparisonoftwostepresponsesofthefeedbacksystem:thatobtainedwiththeproportionalcontrollerdeterminedatsection4.2.3(withgaink=0.35)(--)andthatobtainedwiththecontrollerK(s)determinedhereabove(-).Weclearlyseetheaccelerationduetotheleadcompensator.5.2.3.PDcontrollerThetheoreticaltransferfunctionofaproportionalandderivative(PD)controllerisoftheformKPD(s)=k(1+Tds)(5.8)wherek>0isthe“gainoftheproportionalterm”andTd>0isthe“timeconstantofthederivativeterm”.Suchacontrollerhasanimpropertransferfunctionandthereforeisnotpracticallyrealizable(seesection2.5.3)anddoesnotabidebytheconstraintashighlightedinsection4.2.6.The“purederivative”ofthetransferfunctionTdsisthereforereplacedbya“ﬁlteredderivative”withtransferfunctionTds1+TdNswhereN>0isarealnumbercalledthe“ﬁlteringcoefﬁcientofthederivativeterm”.Thetransferfunction(5.8)isthusreplacedbyKPDF(s)=k(cid:21)1+Tds1+TdNs(cid:22)=λ1+ατs1+τswithλ=kτ=TdNandα=N+1.AsaresultKPDF(s)isthetransferfunctionofaleadcompensatorinserieswithaproportionalcontroller.CompensationandPIDController1355.3.PIcontroller5.3.1.PrincipleIntheexampleconsideredinsection5.2.2astepresponsewithoutstaticerrorisobtainedbecausethesystemPconsideredisintegrator.NonethelessaconstantdisturbanceduaddedattheinputofPwillintroduceastaticerrorwhichisalsoinevitableinanotherexamplewherePisnotanintegrator(seesection5.1.2).Thatiswhythemostwidelyusedcontrollershaveanintegralaction.1Amongthesecontrollersthosethathavethesimpleststructureareofthe“proportionalandintegral”type(acontrollerthatisonlyofintegraltypecannotensurethestabilityoftheclosed-loopforaclassofcontrolsystemsPthatissufﬁcientlylarge).Thetransferfunctionofaproportionalandintegral(PI)controllerisoftheformKPI(s)=k(cid:21)1+1TIs(cid:22)(5.9)wherek>0isthe“gainoftheproportionalterm”andTI>0isthe“timeconstantoftheintegralaction”.Wehaveforω>0argKPI(iω)=arg(1+iTIω)−π2=arctan(TIω)−π2.(5.10)WecanthereforedetermineaPIcontrollerhaving“high-frequencygain”γ>0andsuchthatforagivenω0>0|KPI(iω0)|=1−argKPI(iω0)=ϕI0<ϕI<90◦(5.11)whereϕIisthephaselagproducedbythePIcontrolleratfrequencyω0.From(5.10)thetwoaboveinequalitiesholdifandonlyifTI=1ω0tanϕI(5.12)γ=TIω0(cid:30)1+(TIω0)2(5.13)(seeExercise106(ii)).1.OnecasewheresuchanactionmustbeavoidedisthatwherePisaderivatori.e.P(0)=0(seeDeﬁnition32).136LinearSystemsWiththeabovecompensationweobtainaphaselagmarginequaltoMpontheconditionthatargP(iω0)−ϕI=−180◦+Mp.Accordingto(5.11)thisisrealizableifandonlyifMp−180◦<argP(iω0)<Mp−90◦.(5.14)5.3.2.ExampleConsidertheminimalsystemPwithtransferfunctionP(s)=21−0.1s(1+0.5s)(1+s).Thetimeunitisassumedtobeinseconds.Wewishtomakeuseofthissysteminawaythatthephaselagmarginis60◦.TheBlackplotofP(s)isrepresentedinFigure5.6.Startwithaproportionalcontrol.BywayofverticaltranslationwemovepointAPwithaphaseof−120◦(obtainedforanangularfrequencyof2rad/s)tounitygainbytheadditionofagainof3.79dBwhichcorrespondstoaratioof1.55.Theopen-looptransferfunctionthusbecomesLP(s)=3.11−0.1s(1+0.5s)(1+s).Figure5.6.BlackplotofP(s)CompensationandPIDController137Theunitygainangularfrequencyisthenω0P=2rad/s.Nowaddanintegralaction.SincethisproducesaphaselagitisnecessarytoﬁrstconsiderapointAPIintheBlackplotthathasaphaselargerthanAP;thispointisobtainedforanangularfrequencysmallerthan2rad/sω0PI=1rad/sforexample.Wewillbringthispointtounitygainandaphaseof−120◦bytwosuccessiveoperations:(i)addingagainof−2.06dBwhichcorrespondstoaratioof0.79;(ii)aphaselagof120◦−77.5◦=42.5◦.Operation(ii)canbeobtainedbyaPIcontrollerthathasintegralactiontimeconstantTI=1.09s(accordingto(5.12))andwhosehigh-frequencygainisγ=0.74(accordingto(5.1.2)).ThePIcontrollerfoundthushasatransferfunctionKPI(s)givenby(5.9)withk=0.79×0.74=0.58andTI=1.09s.TheBlackplotsofLP(s)(-)andLPI(s)=KPI(s)P(s)(--)areshowninFigure5.7.FinallythestepresponsesofthetwofeedbacksystemsareshowninFigure5.8(lineswiththesameconventions).Onecanobservethe“slowing-down”oftheresponsebecauseoftheadditionoftheintegralaction(butalsothesuppressionofthestaticerror).Figure5.7.Blackplotsofcompensatedsystems138LinearSystemsFigure5.8.Stepresponses5.4.PIDcontroller5.4.1.IntegralactionandleadcompensatorAsseenaboveanintegralactionmakesitpossibletoeliminatethestaticerroratthepriceofslowing-downoftheclosed-loopsystem(section5.1.2)whereasaleadcompensatoracceleratestheclosed-loopsystem(section5.2.2).Wethusunderstandthatbycombiningthesetwoeffectsonecaneliminatethestaticerrorwhilemaintainingasatisfactoryrapidityontheclosed-loopsystem.IfweuseasimplePIcontrollerthefrequencyω0atunitygainisrestrictedbycondition(5.14).Supposewewanttoobtainahigherunitygainfrequencyω0notsatisfyingthiscondition.Wecanproceedinthefollowingmanner:(i)realizeaphaseleadϕdofunitygainatangularfrequencyω0(0<ϕd<90◦).(ii)putinserieswiththeabovephaseleadaPIcontrollerhavingunitygainatfrequencyω0andphaselagϕIatthissamefrequency(0<ϕI<90◦)whereargP(iω0)+ϕd−ϕI=−180◦+Mp.(5.15)(iii)putinserieswiththetwoabovecompensatorsagain1|P(iω0)|(ifexpressedindBaddagainof−|P(iω0)|dB).Relation(5.15)isrealizablewiththeaboveconstraintsoverϕdandϕIifandonlyifMp−270◦<argP(iω0)<Mp−90◦(5.16)CompensationandPIDController139aconditionthatislessrestrictivethan(5.14).ThecontrollerﬁnallyobtainedisofthePIDtypeaswewillseenext.Notethatrelation(5.15)involvesonlythedifferenceϕd−ϕI;wecanthusarbitrarilyﬁxthesumS=ϕd+ϕIordetermineSinsuchawaysoastominimizeanappropriatecriterion(e.g.theovershootofthestepresponse).AreasonablechoiceisS=90◦.Letusillustratewhathasbeensaidhereabovethroughtheexampleinsection5.3.2.Supposewewouldliketheclosed-loopsystemtohaveaphasemarginMp=60◦butwithunitygainangularfrequencyω0=2rad/s.WehaveargP(iω0)=−120◦thuscondition(5.14)isnotsatisﬁedwhereas(5.16)is.Accordingto(5.15)wehaveϕd=ϕI.TakingS=90◦wethusobtainϕd=ϕI=45◦.Accordingto(5.12)TI=0.5sandγ=1√2from(5.13);thePIcontrollerhavingunitygainatangularfrequencyω0hasatransferfunctionKPI(s)=1√2(cid:15)1+2s(cid:16)=0.707(cid:15)1+2s(cid:16).From(5.6)α=3+2√2andtheappropriateleadcompensatorhasatransferfunctiongivenby(5.7)i.e.H(s)=0.4141+1.21s1+0.21s.Finallythelastcompensationtorealizeisagainof3.79dBcorrespondingtoaratioof1.55.ThetransferfunctionofthedesiredPIDcontrolleristhereforeKPID(s)=0.4531+1.21s1+0.21s(cid:21)1+2s(cid:22).TheBlackplotofthecompensatedsystemisshowninFigure5.9.TheBodeplotsofthecompensatedsystemsareshowninFigure5.10:withtheproportionalcontroller(-)thePIcontroller(--)andthePIDcontroller(-.).ThestepresponsesofthefeedbacksystemsareshowninFigure5.11andtheBodeplotsofthethreecontrollersobtainedareshowninFigure5.12(lineswiththesameconventions).5.4.2.ClassicformofaPIDcontrollerThemostclassicformofthetransferfunctionofaPIDcontrollerisK(s)=k*1+1TIs+Tds1+TdNs+(5.17)wherek>0isthe“gainoftheproportionalterm”TI>0isthe“timeconstantoftheintegralaction”Td≥0isthe“timeconstantofthederivativeterm”andwhereN>0isthe“ﬁlteringcoefﬁcientofthederivativeterm”.140LinearSystemsFigure5.9.BlackplotofthecompensatedsystemwithPIDWecanwrite(5.17)intheformK(s)=k(1+N)s2+k(cid:9)NTd+1TI(cid:10)s+kNTITds2+NTds.Figure5.10.BodeplotsofcompensatedsystemsCompensationandPIDController141Figure5.11.Stepresponses(withPPIPID)Thetransferfunctionofthecontrollerobtainedinsection5.4.1isoftheformK(cid:2)(s)=κ1+ατ1s1+τ1s(cid:21)1+1τ2s(cid:22).(5.18)Figure5.12.BodeplotsofPPIandPIDcontrollers142LinearSystemsAsaresultwehaveK(s)=K(cid:2)(s)ifandonlyifδ=1+(α−1)τ1τ2:N=αδ−1Td=Nτ1k=κδTI=δτ2(5.19)(seeExercise106(iii)).ThecontrollerobtainedbycascadingaleadcompensatorandaPIcontrolleristhusaclassicPIDcontrollerifandonlyifN>0i.e.τ1<τ2(5.20)arelationwhichissatisﬁediftheleadcompensatoractsuponfrequenciesthatarehigherthantheintegralactionwhichisoften(butnotalways)thecase.Reconsidertheexampleingiveninsection5.4.1:weobtainedacontrollerK(cid:2)(s)oftheform(5.18)withκ=0.453τ1=0.21α=5.83τ2=0.5.Condition(5.20)issatisﬁedandthereforethecontrolleriscalledaclassicPID.Therelations(5.19)yieldδ=3.02;N=0.92;Td=0.19s;k=1.37;TI=1.50s.5.5.ExercisesEXERCISE106.–(i)Proveexpressions(5.3)(5.4)and(5.5).(ii)Proveexpressions(5.12)and(5.13).(iii)Provetherelations(5.19).EXERCISE107.–LetPbethesystemconsideredinExercise100(section4.3).(i)SupposethereisaconstantdisturbancethataddstotheoutputofP(priortothecontrolledvariable).DoestheproportionalcontrollercalculatedinExercise100rejectthedisturbance?Inotherwordsistheregulationmadewithoutanystaticerror?(ii)Wewouldnowliketoacceleratethefeedbacksysteminamannertoobtainaunitygainangularfrequencyof1rad/sandalsoretainaphaselagmarginof60◦.Determineanappropriatecontroller.(iii)SupposenowthataconstantdisturbanceisaddedtotheinputofP.Istheanswertoquestion(i)stillexact?(iv)DetermineaPIDcontrollerforwhichthephasemarginis60◦withaunitygainangularfrequencyof1rad/s.Chapter6RSTController6.1.StructureandimplementationofanRSTcontrollerInthepreviouschapterwestudiedaclassofcontrollersofbasicstructure.AlthoughthesecontrollersarequitecommonintheindustryitisclearfromChapter4thattheydonotenableustoﬁndanaccuraterobustness/performancecompromise.Inordertomanagethiscompromiseinasatisfactorymanner–andeveninnumerouscasesjusttoobtainastableclosed-loopintheabsenceofmodelingerror–itbecomesnecessarytoconsideramuchlargerclassofcontrollers.Thisisthesubjectofthepresentchapter.ConsideraSISOsystemgovernedbythedifferentialequationA(∂)y=B(∂)u+d(6.1)where∂=ddtandwhereA(∂)∈R[∂]andB(∂)∈R[∂]arethepolynomialsA(∂)=∂n+a1∂n−1+···+anB(∂)=b1∂n−1+···+bn;disanunmeasureddisturbanceyisthevariabletoberegulatedanduisthecontrol.Wearealsogivenareferencesignalrandwecalle=y−r(6.2)thetrackingerror.Ourpurposeistodesignacontrollerthatmakesezeroinsteadystate–irrespectiveofthepresenceofthedisturbanced–andensuresagoodtransient143Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.144LinearSystemsresponse.Itisnecessarytomeasureyandwedenotethemeasuredvariablebyzsothatz=y+ν(6.3)whereνisthemeasurementnoise.AnRSTcontrollerislinearlyrelatedtotheinpututhemeasuredvariablezandthereferenceraswellastoaﬁnitenumberofderivativesofthesevariables.ItisthusgovernedbyadifferentialequationoftheformS(∂)u=−R(∂)z+T(∂)r(6.4)whereS(∂)R(∂)andT(∂)areelementsofR[∂].AnRSTcontrollerthushastwoinputs(zandr)andoneoutput(u).ItstransfermatrixisH(s)=(cid:23)−R(s)S(s)T(s)S(s)(cid:24).(6.5)Itisproperifandonlyifthefollowingconditionholds:max{d◦(R)d◦(T)}≤d◦(S).(6.6)ThestructureoftheRSTcontrollerresultingfromequation(6.4)isrepresentedinFigure6.1.ItcannotbeimplementedassuchbecausethedifferentialpolynomialsR(∂)andT(∂)generatederivativesofsignalszandr(exceptintheveryparticularcasewherethepolynomialsareofdegreezero).LetV(∂)beapolynomialsuchthatmax{d◦(R)d◦(T)}≤d◦(V)≤d◦(S)andallitsrootsbelongtothelefthalf-plane.InpassingintotheLaplacedomainwithzeroinitialconditionsequation(6.4)yieldsS(s)V(s)ˆu(s)=−R(s)V(s)ˆz(s)+T(s)V(s)ˆr(s).TS1Rr+-zuFigure6.1.InitialstructureoftheRSTcontrollerRSTController145sVsTsSsVsVsRsrˆ+-szˆsuˆFigure6.2.ImplementationoftheRSTcontrollerThisequationleadstotheimplementationdiagramgiveninFigure6.2wherethethreetransferfunctionsR(s)V(s)T(s)V(s)andV(s)S(s)areproper.Suchacontrollerissaidtobewiththreedegreesoffreedom(3-DOF)–withreferencetotheabovethreetransferfunctions.IfT=Rrelation(6.4)reducestoS(∂)u=−R(∂)(z−r)andthecontrolleris1-DOF(seesection5.1).OntheotherhandifpolynomialR(∂)hasallitsrootsinthelefthalf-planeandifd◦(T)≤d◦(R)wecanchooseV=RandthecontrollerinFigure6.2isreducedtotwodegreesoffreedom(2-DOF).6.2.Closedloop6.2.1.Closed-loopequationsAccordingto(6.1)(6.2)(6.3)and(6.4)weobtainAcl(∂)y=S(∂)d−B(∂)R(∂)ν+B(∂)T(∂)r(6.7)Acl(∂)e=S(∂)d+{B(∂)[T(∂)−R(∂)]−A(∂)S(∂)}r−B(∂)R(∂)ν(6.8)Acl(∂)u=−R(∂)d+A(∂)T(∂)r−A(∂)R(∂)ν(6.9)whereAcl(∂)=A(∂)S(∂)+B(∂)R(∂).(6.10)From(6.8)and(6.9)the“characteristicpolynomialofthecontrolledsystem”isAcl(∂)givenby(6.10)whichiscoherentwithTheorem76andDeﬁnition78(section4.1.3).146LinearSystems6.2.2.PoleplacementandstabilityPoleplacementconsistsofﬁxinginadvancepolynomialAcl(∂)orequivalentlytherootsofthispolynomiali.e.thepolesofthecontrolledsystem.WeareledtodeterminepolynomialsS(∂)andR(∂)suchthatequation(6.10)canbesatisﬁed.ThisequationisaBézoutequation(seesection13.1.5).ItadmitsasolutionifandonlyifAcl(∂)isamultipleofthegreatestcommondivisor(gcd)ofA(∂)andB(∂)(Theorem494).AccordingtoDeﬁnition74(section4.1.3)thecontrolledsystemisstableifandonlyiftherootsofAcl(∂)arealllocatedinthelefthalf-plane(takingintoaccountthefactthatd◦(A)<d◦(B)andcondition(6.6)).AsaresultitispossibletostabilizethesystemPdeﬁnedbytheleftformA(∂)y=B(∂)u(6.11)ifandonlyifgcd(A(∂)B(∂))hasallitsrootsinthelefthalf-plane;inadditionitispossibletoobtainarbitrarilychosenpolesforthecontrolledsystemifandonlyifgcd(A(∂)B(∂))=1.Weareledtothefollowingdeﬁnition:DEFINITION108.–System(6.11)iscontrollable(resp.stabilizable)ifgcd(A(∂)B(∂))=1(resp.therootsofgcd(A(∂)B(∂))alllieinthelefthalf-plane).Neverthelessobtainingacontrolledsystemwithcorrectlyplacedpolesresolvesonlypartoftheproblem.Itisnowappropriatetoconsiderthiswholeproblemandthensolveit.6.3.UsualcaseThecaseweconsiderhereisonewherethereferencesignalandthedisturbancehavetheirspectruminlowfrequenciesandthusaremodeledasconstantsandwherethemeasurementnoisehasitsspectruminhighfrequencies.Thisframeworkhasalreadybeenstudiedinsections4.2.54.2.6and5.1.1.Ontheotherhandsystem(6.11)isassumedtobecontrollable.6.3.1.DisturbancerejectionLetusstartbyprovidinganinterpretationofdisturbanced:supposethatthesystemtobecontrolledisaffectedbytwoconstantdisturbancesoneatinputduandtheotheratoutputdyasshowninFigure5.1.WritingP(s)=B(s)A(s)weobtainˆy(s)=ˆdy(s)+B(s)A(s)(cid:9)ˆuc(s)+ˆdu(s)(cid:10)RSTController147whereucisthecontrolsignalprovidedbythecontroller.Asaresultweobtain(6.1)withu=ucandd=A(∂)dy+B(∂)du.Wecanhavetwocaseswhered=0:(i)du=0andA(0)=0(integratorsystem:seeDeﬁnition32);(ii)dy=0andB(0)=0(derivatorsystem:seeDeﬁnition32).Inthesetwoveryspeciﬁccasesthereisnoneedfordisturbancerejectionwhichsimpliﬁesthestructureofthecontroller.Theyfallintothegeneralcontextofsection6.4butwewillnotbecoveringitintherestofthissection.Theonlyinformationatourdisposalaboutthedisturbancedisthatitsatisﬁestheequation∂d=0.Accordingto(6.8)itis“rejected”–inotherwordsthankstothecontrolmechanismitdoesnotaffecttheerroreinsteady-state–ifandonlyifS(∂)isamultipleof∂orequivalentlyifS(∂)hasazeroroot:S(0)=0.(6.12)Theaboveconstraintimposesthecontrollertohaveazeropole(fromexpression(6.5)ofthetransfermatrix)orinotherwordstobeanintegrator(Deﬁnition32).6.3.2.AbsenceofstaticerrorTheonlyinformationwehaveaboutreferencesignalristhatitsatisﬁestheequality∂r=0.Supposecondition(6.12)holds.Accordingto(6.8)thetermrdoesnotaffecttheerroreinsteady-stateifandonlyifB(0)[T(0)−R(0)]=0.IfB(0)=0from(6.9)and(6.10)s=0isarootofthecharacteristicpolynomialAcl(s)andthefeedbacksystemisnotstable;thispossibilityistoberuledout.Thestaticerroristhereforezero(whenν=0)ifandonlyifT(0)=R(0).(6.13)6.3.3.MeasurementnoiseﬁlteringAsmentionedinsection4.2.6itisnecessaryforthecontrollertohavelimitedgainathighfrequenciesinsuchawaythatthemeasurementnoisedoesnotintroduceanexcessiveagitationintothecontrolvariable.Accordingto(6.6)therelativedegreeδ(cid:15)RS(cid:16)isnecessarily≥0andwecanarriveatimposingaconditionsuchthatδ(cid:15)RS(cid:16)≥δ0(6.14)148LinearSystemswhereδ0≥0isanon-negativeintegerchoseninadvance.Ifδ0=0condition(6.14)imposesnothingmorethan(6.6).Withδ0≥1condition(6.14)imposesablockingzeroatinﬁnityoforderδ0tothetransferfunctionRSofthecontroller(seesection2.4.7Remark38).AccordingtoRemark65(section3.4.2)theBodediagramofthetransferfunctionRShasaslopethatisatmost−20δ0dB/decadeathighfrequencies.Itisthusthetransferfunctionofalow-passﬁlter(forδ0≥1);thelargertheδ0themoreselectivetheﬁlter.Theintegerδ0istheroll-offofthecontroller(alreadymentionedinthepreface).Notethatthetransferfunctionthatrelatesνtoeis−To(s)(accordingtosections4.2.6and(6.8))andδ(To)=δ(cid:15)BA(cid:16)+δ(cid:15)RS(cid:16)(seesection13.6.1).Sinceδ(cid:15)BA(cid:16)≥1condition(6.14)impliesδ(To)≥δ0+1.Theerroreisthuslesssensitivetothehigh-frequencymeasurementnoisethanthecontrolvariableu.6.3.4.ProblemresolutionStabilizabilityconditionPolynomialS(∂)isoftheform∂SI(∂)whereSI(∂)∈R[∂].PutAI(∂)=A(∂)∂i.e.AI(∂)=∂n+1+a1∂n+···+an∂.(6.15)From(6.10)wehaveAI(∂)SI(∂)+B(∂)R(∂)=Acl(∂).(6.16)Nowletusapplythetheoryinsection13.1.5witha(s)=AI(s)b(s)=B(s)x(s)=SI(s)y(s)=R(s)andc(s)=Acl(s).PROPOSITION109.–ThereexistpolynomialsSI(∂)andR(∂)suchthatthepolynomialAcl(∂)hasallitsrootsinthelefthalf-planeifandonlyifB(0)(cid:5)=0orequivalentlyifandonlyifthesystem(6.11)hasnozeroequalto0(i.e.isnotaderivatorsystem).PROOF.(i)IfB(0)(cid:5)=0polynomialsAI(∂)andB(∂)arecoprime.IndeedA(∂)andB(∂)hasnorootsincommonforthesystem(6.11)isassumedtobecontrollableandlikewise∂andB(∂)donothaveanyrootincommon.AsaresultaccordingtoTheorem494(section13.1.5)equation(6.16)admitsasolution(SI(s)R(∂))foranypolynomialAcl(∂).(ii)IfB(0)=0Acl(0)=0becauseSI(0)=0.RSTController149DegreesofthepolynomialsAssumethatB(0)(cid:5)=0fromnowon.ThepolynomialR(s)isoftheformy0sυ+y1sυ−1+···+yυ.Sincethepolynomialsa(s)andb(s)arecoprimethereexistsauniquesolution(xy)=(SIR)forwhichυ=α−1whereα=n+1thusυ=n.From(6.14)wehaved◦(S)=n+δ0thereforeξ(cid:1)d◦(SI)=n+δ0−1.Sinceβ=n−1andδ0≥0wehaveξ≥βandthereforethesimpliﬁcationindicatedattheendofsection13.1.5canbemade.ThepolynomialsSIandAclarechosentobemonic.Wehaved◦(AS)=2n+δ0andd◦(BR)≤2n−1thusd◦(Acl)=2n+δ0.PutS(∂)=∂n+δ0+σ1∂n+δ0−1+···+σn+δ0−1∂R(∂)=r0∂n+r1∂n−1+···+rnAcl(∂)=∂2n+δ0+c1∂2n+δ0−1+···+c2n+δ0.SylvestersystemTakingintoaccounttheabove-mentionedsimpliﬁcationtheSylvestersystem(13.13)(section13.1.5)canbewrittenas⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣10······00······0a11..................a1.........0......an......0b1...00......1......0.........a1bnb10.........0......b1...an......bn...0·········00···0bn⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣σ1σ2......σn+δ0−1r0r1...rn⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣c1−a1...cn−ancn+1............c2n+δ0⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦.(6.17)LetMbethematrixgivenaboveontheleft.Itsorderis2n+δ0.Thenumberofcolumnscontainingtermsai(resp.bi)isequalton+δ0−1(resp.n+1).Thenumberofzerosbelowthelastan(resp.abovetheﬁrstb1)isequalto1(resp.δ0).150LinearSystemsThematrixMisoftheform⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣10...0...0∗.................................100∗......∗Σ(ab)∗0.........0bn⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦where∗eachtimestandsforanon-speciﬁedsubmatrix.SinceΣ(ab)isinvertibleandbn(cid:5)=0thismatrixisinvertible.Thisconﬁrmsthatthesystem(6.17)admitsauniquesolution.6.3.5.ChoiceofthepolesIntroductionTheintegralactionofthecontrollerensuresalargegainofL(s)atlowfrequencies.Byimposingδ0>1weensureasmallgainofL(s)andofK(s)=R(s)S(s)athighfrequencies.Thestabilityoftheclosed-loopsystemisguaranteed(intheabsenceofmodelingerror)aslongastherootsofAcl(∂)arechoseninthelefthalf-plane.Anumberofspeciﬁcationspresentedinsection4.2arethereforesatisﬁed.Butsomeamongthesearenotyetconsideredinparticular:(i)Howtoobtainasufﬁcientmodulusmargin?(ii)Howtoobtainasufﬁcientdelaymargin?(iii)Howtoobtainasufﬁcientrapidity?Tomeettheserequirementstheclosed-looppolesmustbechoseninanappropriateway.Forthisanalysisweconsidertheclosed-loopintheequivalentformshowninFigure6.3.-sAI1sBsSsRIyFigure6.3.Closed-loop(equivalentdiagram)RSTController151sAI1BsKIu*Iuy_Figure6.4.PartialstatefeedbackPartialstatefeedbackAsaﬁrststepassumethatthepartialstateξoftheminimalsystemwithtransferfunctionB(s)AI(s)isavailabletoprovidethefeedbackneeded(seesection2.3.5).Thissystem(theinputofwhichisuI=∂u)isrepresentedbytherightform(cid:20)B(∂)ξ=yAI(∂)ξ=uIwhereAI(∂)isthepolynomialofdegreen+1deﬁnedby(6.15).ThecontrolvariableuIoftheabovesystemischosentobeoftheformuI=u∗I−K(∂)ξwhereu∗IisasignalprovidedbythecommandandK(∂)isapolynomialsuchthatthecharacteristicpolynomialoftheclosed-loopisamonicpolynomialAcofthesamedegreeasAIwhichisn+1(seeFigure6.4).ThepolynomialAcisthusoftheformAc(∂)=∂n+1+γ1∂n+···+γn+1.Theclosed-loopisgovernedbythedifferentialequationAI(∂)ξ=u∗I−K(∂)ξfromwhichwegetAc(∂)ξ=u∗IAc(∂)=AI(∂)+K(∂).(ItisessentialtonotethatthetransferfunctionB(s)isnotpartoftheloop.)AsaresultthepolynomialK(∂)isdeterminedbyK(∂)=Ac(∂)−AI(∂)(6.18)=(γ1−a1)∂n+···+(γn−an)∂+γn+1.152LinearSystemsTheopen-looptransferfunctioncorrespondingtoFigure6.4isgivenbyLe(s)=K(s)AI(s).ThereforethecorrespondingsensitivityfunctionisgivenbySe(s)=11+Le(s)=AI(s)Ac(s).(6.19)Letp1...pn+1betherootsofAI(s)andπ1...πn+1betherootsofAc(s).Therootsπk(1≤k≤n+1)areofcourseassumedtohavenegativerealpartssothatSe(s)∈(cid:1)H∞.Itisnothardtoprovethefollowinglemma:LEMMA110.–Asufﬁcientconditionfortheinequality)))iω−pkiω−πk)))≤1tobesatisﬁedforanyrootpkandatanyfrequencyωis:Imπk=ImpkandReπk≤−|Repk|.Lemma110providesasimplewayofchoosingtherootsofAc(s)asafunctionofthoseofAI(s)inordertoget(cid:23)Se(cid:23)∞=1andthusamodulusmarginequalto1whenthepartialstateisusedinthefeedbackloop.Indeedwehave|Se(iω)|=’1≤k≤n+1))))iω−pkiω−πk)))).Thefollowingrulethusfollowsforanyintegerk1≤k≤n+1:RULE111.–ChooseImπk=ImpkandReπk≤−|Repk|(withReπk<0).Thecriterionthatpresidesovertheaboveruleisuniquelythemodulusmargin.Otherconsiderationsofcourseneedtobetakenintoaccountinparticulartherapidityandthedelaymarginaswellaspossiblythedampingcoefﬁcientofeachpole.Ontheotherhandthisruleiscloselyrelatedtothe“linearquadraticcontrol”(see[20]PartIandTheorem247ofsection8.1.4).SupplementarypolesCaseofaminimumphasesystemInrealitythepartialstateisingeneralunavailableforthecontrol(exceptintheparticularcasewhere0(cid:5)=B(∂)∈R).ThatiswhyitisnecessarytocomebacktotheBézoutequation(6.16).WechooseapolynomialAcl(∂)oftheformAcl=AcBF(6.20)whereAc(∂)isamonicpolynomialofdegreen+1therootsofwhicharechosenaccordingtoRule111andF(∂)isapolynomialofdegreem=δ(cid:15)BA(cid:16)+δ0−1withrootsinthelefthalf-planeandsuchthattheproductB(∂)F(∂)isamonicRSTController153polynomial.ThatwayAcl(∂)isindeedamonicpolynomialofdegree2n+δ0alltherootsofwhichareinthelefthalf-planesincefromhypothesistherootsofB(∂)satisfythislastcondition.TheBézoutequation(6.16)canthusbewrittenasAISI=B(AcF−R).SinceAIandBarecoprimethereexistsapolynomialQsuchthatSI=BQ.ThispolynomialQisnecessarilyofdegreeδ(cid:15)BA(cid:16)+δ0−1=d◦(F);wegetR=AcF−AIQ.(6.21)Theopen-looptransferfunctionisthereforeL=BAIRSI=BAIAcF−AIQBQ=FQAcAI−1fromwhichwededucethesensitivityfunction:So=11+L=QFAIAc.Equation(6.21)isaBézoutequationwithunknown(RQ)forgivenAcAIandF.Previouslywehaveseenthatthisequationadmitsauniquesolutionsuchthatd◦(R)≤nandd◦(Q)=δ(cid:15)BA(cid:16)+δ0−1=m.Thissolution(RQ)dependslinearlyonFandisdenotedasΨ(F).ThelinearmappingΨiscontinuous(inthesensespeciﬁedinTheorem112).Letnow(Fk)beasequenceofpolynomialshavingthesamepropertiesasthepolynomialFaboveandsuchthatask→+∞allrootsofFktendto−∞whileremainingontherealaxis.Thereexistsasequenceofnon-zerorealnumbers(λk)suchthatλkFk→1(thesequenceconvergesinthesensespeciﬁedinTheorem112).Foranyintegerk(RkQk)=Ψ(Fk).Accordingto(6.21)wehaveλkRk+AIλkQk=AcλkFk.(6.22)SinceK+AI=AcwededucethatλkRk→KandλkQk→1.ThereforeQkFk→1andthesensitivityfunctionSok=QkFkAIAcconvergestothesensitivityfunctionSegivenby(6.19).Thefollowingtheoremdetailswhathasjustbeendiscussed:154LinearSystemsTHEOREM112.–Letthecharacteristicpolynomialoftheclosed-loopbedeﬁnedby(6.20)wheretherootsofAcarechosenaccordingtoRule111andassumethatFisreplacedbyFkwhere(Fk)isasequenceofpolynomialsasspeciﬁedabove.Then(cid:15)(cid:23)Sok−Se(cid:23)∞(cid:16)→0.PROOF.*1)LetX(resp.Y)bethesubspaceoftheR-vectorR[∂]consistingofallpolynomialswithdegree≤m(resp.≤n).ConsiderthelinearmappingΨ:X→X×Ydeﬁnedabove.ThespacesXYareﬁnite-dimensionalthereforeΨiscontinuous(seesection12.1.3).LetDbeaninﬁnitecompactsetincludedintheclosedrighthalf-plane.Thensups∈D|f(s)|=(cid:23)f(cid:23)(f∈Xresp.f∈Y)isanormonX(resp.Y)and(cid:23)(fg)(cid:23)=sup((cid:23)f(cid:23)(cid:23)g(cid:23))(f∈Xg∈Y)isanormonX×Y.Let−1τ1k...−1τmkbetherootsofFkwhereτik>0(τik∈R).WehaveλkFk(s)=’1≤j≤m(1+τjks)(6.23)andlimk→+∞τjk=0.Asaresult(λkFk)→1uniformlyonDi.e.((cid:23)λkFk−1(cid:23))→0.WehaveΨ(1)=(K1)andthus((cid:23)λkRk−K(cid:23))→0and((cid:23)λkQk−1(cid:23))→0becauseofthecontinuityofΨ.Thismeansthat(λkRk)→Kand(λkQk)→1uniformlyonD.WehaveSok−Se=(cid:9)QkFk−1(cid:10)Seandthussups∈D|Sok(s)−Se(s)|≤sups∈D))))Qk(s)Fk(s)−1))))(cid:23)Se(cid:23)∞andthisquantitytendsto0ask→+∞.2)Wehavefrom(6.22)λkRk(s)Ac(s)+AI(s)Ac(s)λkQk(s)=λkFk(s).Fork→+∞therootsofλkRkconvergetothoseofKaccordingto1)andd◦(λkRk)<d◦(Ac);thereforefor|s|→+∞(sremainingintheclosedrighthalf-plane)λkRk(s)Ac(s)→0uniformlywithrespecttokthusλkQk(s)−λkFk(s)→0uniformlywithrespecttokbecauseAI(s)Ac(s)→1.Nowlets=iωω→+∞;wehave|1+τjkiω|≥1foranyj∈{1...m}andanynaturalintegerktherefore|λkFk(iω)|≥1accordingto(6.23);asaresult)))Qk(iω)Fk(iω)−1)))→0uniformlywithrespecttok.Foranyε>0itfollowsthatthereexistsafrequencyΩ>0onlydependingonεsuchthat|Sok(iω)−Se(iω)|≤εforanyintegerkaslongasω≥Ω.Ontheotherhandaccordingto1)thereexistsanintegerk0suchthat|Sok(iω)−Se(iω)|≤εforanyk≥k0andanyω∈[0Ω](withD=[0Ω]);thisintegerk0onlydependsonε.Foranyk≥k0(cid:23)Sok−Se(cid:23)≤ε(seesection13.6.2)andthisprovesthetheorem.*RSTController155REMARK113.–(i)Thistheoremshowsthatonecanobtainamodulusmarginascloseto1asonewouldlikebychoosingapolynomialforFwhichisofdegreem=δ(cid:15)BA(cid:16)+δ0−1andtherootsofwhicharenegativerealwithsufﬁcientlylargeabsolutevalue.Accordingtorelations(4.9)(4.10)foramodulusmarginof1wehaveaguaranteedgainmargin(−6dB+∞)andaguaranteedphasemargin(−60◦60◦).(ii)Thistheoremiscloselyrelatedtothe“LTRmethod”developedin[38].Neverthelessinthecitedreferenceonlythesimpleconvergenceof(Sok)toSeisobtainedwhichdoesnotallowonetoconcludeanythingaboutthebehaviorofthemodulusmargin.Seesection9.1.4formoredetails.REMARK114.–LetSIn=BQn.ThetransferfunctionofthecontrollerobtainedaboveisRn(s)sSIn(s)=λnRn(s)sλnSIn(s)→K(s)sB(s).Thetransferfunctionobtainedbytakingthelimitisthusimproper(exceptifd◦(B)=n−1).Thusacompromiseneedstobefoundbetweenthemodulusmargin(whichwillbeallthelargerastherootsofFare“faster”i.e.willhavealargerabsolutevalue)andthesensitivityofthecontroltothemeasurementnoise.Caseofanon-minimumphasesystemTheprocedurejustdescribedisobviouslynotapplicabletothecaseofanon-minimumphasesystemsincetherootsofB(s)arethenunstable.(i)IfB(s)hasatleastonerootzwithpositiverealpartwehaveseeninsection4.2.8thatwecouldreplacethezerozbythestablezero−z.WewillthusproceedthiswayforeachzerozsuchthatRez>0.(ii)IfB(s)hasatleastonerootontheimaginaryaxis(orsoclosetotheimaginaryaxisthatitcanbeconsideredaslocatedonit)wecanproceedasindicatedinRemark91(section4.2.8)byreplacingthiszerozby−z−αwhereα>0issuchthatcondition(4.22)holds;thisprocedurecanbecarriedoutforeachnon-zerorootofB(s)locatedontheimaginaryaxis.WethusconstructfromB(s)apolynomialB∗(s)allrootsofwhichbelongtothelefthalf-plane.Asalreadymentionedinsection4.2.8themultiplicativeerrormadewhenreplacingB(s)byB∗(s)issmallifallunstablezerosofB(s)are“fast”withrespecttothemaximumunitygainfrequencyoftheclosed-loopsystem.Thisconsiderationmayleadtomaketheclosed-loopsystemslowthuswithapoorperformanceasindicatedinthissamesection.LastlythechoicemadeforthepolynomialAcl(∂)isfollowing:Acl=AcB∗F.(6.24)OfcoursetheBézoutequationtobesolvedisstill(6.16).156LinearSystemsChoiceofthepolynomialT(∂)ThepolynomialT(∂)canbechoseninsuchawaysothatsomeundesirabledynamicscanbecancelledinthetransferfunctionrelatingthereferencesignalrtotheoutputy.Thistransferfunctionisaccordingto(6.7)BTAcl.ThetwoonlyconstraintsthatT(∂)needstoobeyisthedegreecondition(6.6)whichisd◦(T)≤n+δ0andequality(6.13).Itisdifﬁculttomakemoreprecisegeneralrecommendationsasshowninthefollowingexamples.REMARK115.–Withd◦(T)=n+δ0weobtainδ(BT/Acl)=d◦(B)+n+δ0−(2n+δ0)=δ(B/A).ThusbychoosingTadivisorofAclthecontrolledsystembehaves(intheabsenceofanydisturbanceandmodelingerror)likeasystemofthesameorderassystemP.6.3.6.ExamplesPIDdesignedbypoleplacementConsidertheminimalsystemPwithtransferfunctionP(s)=21−0.1s(1+0.5s)(1+s)(seesections5.3.25.4.1and5.4.2).Thissystemisdeﬁnedbytheleftform(∂+1)(∂+2)y=−410(∂−10)u.(6.25)Thissystemisnon-minimumphasebutitszerocanbeconsidered“fast”.LetusapplytheabovetheoryusingAc(∂)=(∂+2)3andAcl(∂)=Ac(∂)(∂+10)(whenceδ0=0).WeobtainR(∂)=(∂+2)(∂+1.26)101.26(6.26)S(∂)=∂(∂+16.18).(6.27)WecanputthetransferfunctionR(s)S(s)intheclassicform(5.17)ofthetransferfunctionofaPIDcontrollerwith:k=1.53;Ti=1.23s;Td=0.26s;N=4.22.WiththetimeunitinsecondstheBlackplotoftheopen-looptransferfunctionL(s)=B(s)A(s)R(s)S(s)isshowninFigure6.5.BychoosingT(∂)=R(∂)thecontrollerhas1-DOFthatisitisaPIDinthemosttraditionalform.WecanseethebehaviorofthefeedbacksysteminFigure6.6withregardtothefollowingevents:(i)unitstepcommandattimet=0;(ii)unitstepdisturbanceofamplitude0.3addedtoyattimet=5s.*ThemeasurementnoiseisaGaussianwhitenoisewithstandarddeviation0.1(thecomputationstepis0.1s).1*Thetimeresponsesofthecontrolled1.Itisadiscrete-timewhitenoisethecomputationstepisconsideredasbeingthesamplingperiod(seesection11.1).RSTController157Figure6.5.Blackplot(open-loopwithPID)systemwiththePIDdesignedbypoleplacement(-)andofthatwiththePIDdesignedusingthegeometricmethodofsections5.4.1and5.4.2(--)aresimulatedover10softime.Thetwocontrolledsystemshavepracticallyidenticalbehavior.InbothcasesthenoiseﬁlteringispoorthisisoneofthemajorinconveniencesofthePID(duetothederivativeactionevenifthederivativeisﬁltered!).Figure6.6.TimedomainresponseswiththetwoPIDs158LinearSystemsAddingblockingzerosatinﬁnityConsideringthesamesystemasbeforei.e.deﬁnedbytheleftform(6.25)takeAc(∂)=(∂+2)3againandconsiderthefollowingtwocases:(i)Acl(∂)=Ac(∂)(∂+10)2(fromwhichδ0=1);(ii)Acl(∂)=Ac(∂)(∂+10)3(fromwhichδ0=2).InbothcaseswechooseT=R.Incase(i)weobtainR(∂)=(∂+2)(∂+1.23)1001.22(6.28)S(∂)=∂(∂+α)(∂+¯α)(6.29)withα=11.5+7.83i.Incase(ii)weobtainR(∂)=(∂+2)(∂+1.20)10001.20(6.30)S(∂)=∂(∂+β)(cid:15)∂+¯β(cid:16)(∂+18.46)(6.31)withβ=7.27+8.35i.CallK2theRSTcontrollerthepolynomialsofwhicharedeﬁnedby(6.26)and(6.27)K3theonethepolynomialsofwhicharedeﬁnedby(6.28)and(6.29)andK4theonethepolynomialsofwhicharedeﬁnedby(6.30)and(6.31)(K1denotesthePIDdeterminedinsections5.4.1and5.4.2).WithK2thephaselagmarginis65.3◦andthedelaymarginis0.5s(aswecanseeinFigure6.5).WithK3(resp.K4)thephaselagmarginis62.5◦(resp.60.4◦)andthedelaymarginis0.6s(resp.0.67s).Addingfastsupplementarypolestotheclosed-loopsystemthushasatendencytoslightlyreducethephaselagmarginandincreasethedelaymargin(andthereforedeceleratingtheclosed-loopsystem).Thesevariationshardlysigniﬁcantareduetothefactthatthesupplementarypolesareat−10whicharestillquitefarfrominﬁnity...ThecomparisonoftheBodeplotsoftheopen-looptransferfunctionsisshowninFigure6.7forthecontrollersK2(-)K3(--)andK4(-.).ForthetimeresponsesinFigure6.8theeventssimulatedarethesameasthepreviousones(seeFigure6.6wheretheconventionsforthelinesarethesameasabove).WeseethattheBodeplotsarealmostidenticaluptotheunitygainfrequency(whichisoftheorderof2rad/s)andbecomeverydifferentatfrequencieshigherthan10rad/s(correspondingtotheabsolutevalueofthe“fastpoles”added).Thegreatertheroll-offδ0thebetteristhemeasurementnoiseﬁltered(thisisparticularlyclearonthecontrolsignal).Otherthanthis(eventhoughthatisimportantevencrucial)thethreetimeresponsesareverysimilar.RSTController159Figure6.7.BodeplotsofthethreecontrollersFigure6.8.Timedomainresponseswiththethreecontrollers160LinearSystemsAdifﬁcultcaseLetasystemnowbedeﬁnedbytheleftform(cid:15)∂2+1(cid:16)y=(∂−5)u.Thissystemispurelyoscillatoryandnon-minimumphase.Thedifﬁcultyinthepresentcaseisthatifwedampthepoles±itoomuchthephasemarginwilldegrade.Aftersomeoftrialanderrorthefollowingchoiceturnsouttobecorrect:Ac(∂)=(∂+1)(∂+0.5+i)(∂+0.5−i)Acl(∂)=Ac(∂)(∂+5)(∂+10)wherethepole−10isaddedtogetδ0=1.ThepolynomialT(∂)ischoseninsuchawaysoastocancelbothpoorlydampedcomplexconjugatepolesoftheclosed-loopandtherapidpole.OneobtainsR(∂)=−12.5|γ|2(∂+γ)(∂+¯γ)S(∂)=∂(∂+λ)(cid:15)∂+¯λ(cid:16)T(∂)=µ(∂+0.5+i)(∂+0.5−i)(∂+10)withγ=0.15+0.69iλ=8.50+5.84iandwhereµissuchthatT(0)=R(0)=−12.5.TheBodeplotoftheopen-looptransferfunctionL(s)isshowninFigure6.9.Wenotethatthephaselagmarginisapproximately44◦whichisreasonable.Thesameconclusionisforthegainmarginwhichisintherange(−∞10dB).ThetimeresponsesinFigure6.10showthesystembehaviorfacedwiththefollowingevents:(i)unitstepcommandatt=0;(ii)stepdisturbanceofamplitude0.3addingtoyattimet=10s.Wecanseetheexcellentqualityofthestepresponsewithoutovershootandhavinga“negativestart”(seesection3.6Exercise72).Theresponsetothedisturbancehasverydifferentdynamicsbecausecontrarytothereferencesignalthisdisturbanceexcitestheoscillatorydynamicsoftheclosed-loop.OfcoursetheseoscillatorydynamicscanonlybecancelledbyT(s)intheabsenceofmodelingerror.Thisexampleillustratestheadvantageofa3-DOFcontrollerovera1-DOFcontrollerwhoseresponsetoastepcommandwouldbeverypoorinthepresentcaseRSTController161Figure6.9.Bodeplot(caseofthepureoscillatorysystem)Figure6.10.Timedomainresponse(systemoriginallyoscillatory)162LinearSystems6.4.*GeneralcaseSupposenowthatthedisturbancedandthereferencesignalraregovernedbylineardifferentialequationswithconstantcoefﬁcientsoftheformD1(∂)d=0D2(∂)r=0.The“usualcase”previouslyconsideredwasaparticularcaseofwhatisnowdiscussedwithD1(∂)=D2(∂)=∂.Herethereferencesignalcanbeforexamplearamp(inwhichcaseD2(∂)=∂2)andthedisturbancecanbesinusoidal(byputtingD1(∂)=∂2+ω20).Thedisturbancecanalsobethesuperpositionofarampandasinusoid(D1(∂)=∂2(cid:15)∂2+ω20(cid:16))etc.Themeasurementnoiseisagainassumedtohaveitsspectruminhighfrequenciesandsystem(6.11)isassumedtobecontrollable.6.4.1.DisturbancerejectionAccordingto(6.8)thedisturbancedisrejectedifandonlyifS∈(D1)(6.32)where(D1)denotestheidealinR[∂]generatedbyD1(seesection13.1.1).Thissigniﬁesthattheleftmemberoftherelation(6.32)isamultipleofthepolynomialD1.Thisconditiongeneralizes(6.12).6.4.2.ReferencetrackingInexpression(6.8)thetermdependentonrbecomeszeroifandonlyifB(T−R)−AS∈(D2).(6.33)Ifwewishtoobtainagoodrobustnessofperformancecondition(6.33)needstobesatisﬁedevenwhenthepolynomialsA(∂)andB(∂)areuncertain.Asaresult(6.33)leadstothedoubleconditionS∈(D2)(6.34)T−R∈(D2).(6.35)Condition(6.35)generalizes(6.13).RSTController1636.4.3.InternalmodelprincipleConditions(6.32)and(6.35)arebothsatisﬁedifandonlyifS∈(D1)∩(D2)=(D)whereD=lcm(D1D2)(seesection13.1.3).ThismeansthereexistsapolynomialSIsuchthatS=SID.(6.36)Condition(6.36)isthe“InternalModelPrinciple”[119].2Wemakethefollowinghypothesis:ASSUMPTION116.–Dhasallitsrootsontheimaginaryaxis.ThereasonforAssumption116isasfollows:(i)IfDhasrootsinthelefthalf-planeonecanwritethispolynomialintheformD=DsDiwhereDsisapolynomialwhichhasallitsrootsinthelefthalf-planeandwhereDiisapolynomialprimewithDs.Thuseverysignalw(disturbanceorreferencesignal)satisfyingthedifferentialequationD(∂)w=0canbedecomposedasw=ws+wiwhereDs(∂)ws=0andDi(∂)wi=0(seesection2.3.8Theorem449).Sincews(t)→0ast→+∞thereisnoneedtoaccountforthissignalwhichiszeroinsteadystate.Thiscaseisthustobeexcluded.(ii)IfDhasrootsintherighthalf-planethereexistsasignalw(disturbanceorreferencesignal)satisfyingthedifferentialequationD(∂)w=0andwhichdivergesexponentially.Acontrolsignalthuscanonlycancelouttheerroreasymptoticallyifititselfalsodivergesexponentially.Thisiswhythiscaseistobeexcludedaswell.Itisalsoimportanttonotethefollowing:foraboundedcontrolsignaltobeabletoasymptoticallycancelouttheerrorewhilethesystemisexcitedbyasignalw(disturbanceorreferencesignal)thissignalitselfmustbeboundedtoo.ThissignalischaracterizedbythefactthatitisasolutiontothedifferentialequationD(∂)w=0;itisthereforenecessarythattherootsofD(s)besimpleandlocatedontheimaginaryaxis(section12.5.2Theorem449).Neverthelessinsomecontrolproblemstherequirementscallforthetrackingofareferencerampsignalforaﬁnitedurationoftime.ThisiswhycontrarytosomeauthorswekeepthepossibilitythatD(s)mayhavemultiplerootsontheimaginaryaxisopen(thisdoesnothoweverchangeanythingtothetheory).2.InthequotedreferencetheInternalModelPrincipleisstatedintheframeworkofstate-spacesystems.Seesection8.3.164LinearSystems6.4.4.FilteringofmeasurementnoiseThepresenceofmeasurementnoisemakesitnecessarytoimposecondition(6.32);theargumentdevelopedinsection6.3.3remainsunchanged.6.4.5.ProblemresolutionStabilityconditionLetp=d◦(D)andp2=d◦(D2).WriteAI(∂)=A(∂)D(∂)andletAI(∂)=∂n+p+α1∂n+p−1+···+αn+p.(6.37)From(6.10)and(6.36)weareledtoresolvetheBézoutequation(6.16)withunknownsSIandRandalsotoapplythetheoryinsection13.1.5witha(s)=AI(s)b(s)=B(s)x(s)=SI(s)y(s)=R(s)andc(s)=Acl(s).ThefollowingisageneralizationofProposition109.PROPOSITION117.–ThereexistpolynomialsSI(∂)andR(∂)suchthatthepolynomialAcl(∂)hasallitsrootsinthelefthalf-planeifandonlyifthepolynomialsD(∂)andB(∂)arecoprime.PROOF.LetAf=gcd(AIB).TheBézoutequation(6.16)admitsasolution(SIR)ifandonlyifAclisoftheformAcl(∂)=Af(∂)Al(∂)(fromTheorem494)whereAl(∂)isanarbitrarypolynomial.Nowgcd(AB)=1;thusAf=gcd(DB)andaccordingtoAssumption116anecessaryandsufﬁcientconditionforAftohaveallitsrootsinthelefthalf-planeisAf=1.DegreesofthepolynomialsLetgcd(DB)=1fromnowon.Throughthesamerationaleasinsection6.3.4wearriveatthefollowingconclusions:d◦(R)≤n+p−1d◦(S)=n+p+δ0−1d◦(SI)=n+δ0−1d◦(Acl)=2n+p+δ0−1.RSTController165Wehaveξ=d◦(SI)=n+δ0−1β=n−1andδ0≥0thusξ≥βandthesimpliﬁcationindicatedattheendofsection13.1.5canbemade.ThepolynomialsSIandAclarethereforechosentobemonic.NowletSI(∂)=∂n+δ0−1+x1∂n+δ0−2+···+xn+δ0−1R(∂)=r0∂n+p−1+r1∂n+p−2+···+rn+p−1Acl(∂)=∂2n+p+δ0−1+c1∂2n+p+δ0−2+···+c2n+p+δ0−1.SylvestersystemWiththeabove-mentionedsimpliﬁcationtheSylvestersystem(13.13)canbewrittenas⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣10···0············0α1..................α2...1......0.........α1...0b10...αn+p...α2...1...b1......0.........α1bn......0......αn+p...α20bn...b1......0.........0......0······0αn+p0···0bn⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣x1.........xn+δ0−1r0......rn+p−1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣c1−α1............cn+p−αn+pcn+p+1...c2n+p+δ0−1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦166LinearSystemsTheabove-givenmatrix(left)isoforder2n+p+δ0−1;itconsistsofn+δ0−1columnscontainingcoefﬁcientsαiandn+pcolumnscontainingcoefﬁcientsbi.6.4.6.ChoiceofpolesChoiceofAcl(∂)Theapproachinsection6.3.5appliesinbroadoutlineforthecalculationofAcl(∂).Thispolynomialtakesontheform(6.24)whereAc(∂)=(1≤k≤n+p(∂−πk)hasitsrootsπkchosenaccordingtoRule111asafunctionoftherootspkofAI(∂)whereB∗(∂)isconstructedasindicatedinsection6.3.5andwhereF(∂)isapolynomialofdegreeδ(cid:15)BA(cid:16)+δ0−1andthepolesofwhichare“fast”.Followingtherationaleinsection6.3.5weobtainthefollowing:THEOREM118.–Ifsystem(6.11)isminimumphasethestatementofTheorem112remainsvalid.Ontheotherhandinthecasewhereasystemisinnon-minimumphasetheremarksinsection6.3.5remainvalidaswell.ChoiceofT(∂)TheconstraintsimposedonthepolynomialTare(6.35)and(6.6).Condition(6.35)meansthatthereexistsapolynomialQ(∂)∈R[∂]suchthatT−R=−QD2.(6.38)Asinsection6.3.5onewouldliketochoosepolynomialT(∂)insuchawaythatitwillcancelcertainundesirabledynamicsinthetransferfunctionBTAclrelatingthereferencertotheoutputy.SupposeAs(∂)isthefactorofAcl(∂)thatwewouldliketocancel.ThenThastobeoftheformT=TIAs(6.39)whereTI∈R[∂].Accordingto(6.38)and(6.39)thepolynomialsTIandQhavetosatisfytheBézoutequationAsTI+D2Q=R.(6.40)Thisequationadmitsmultiplesolutionssincegcd(AsD2)=1(fromAssumption116).Thecasep2=0istrivialsoletusassumethatp2≥1.Amongthesolutionsto(6.40)auniqueoneexistssuchthatd◦(TI)≤p2−1(section13.1.5TheoremRSTController167494).WecanbesurethatthereexistsapolynomialTsatisfying(6.38)(6.39)andtheconstraint(6.6)onlyifd◦(As)+p2−1≤n+p+δ0−1i.e.d◦(As)≤n+p−p2+δ0.(6.41)REMARK119.–Withd◦(As)=n+p−p2+δ0wehaved◦(T)=n+p+δ0−1.Asaresultδ(cid:21)BTAcl(cid:22)=d◦(B)+d◦(T)−(2n+p+δ0−1)=d◦(B)−n+(n+p+δ0−1)−(n+p+δ0−1)=δ(cid:21)BA(cid:22).ThisgeneralizesRemark115.NotethatTcanbechosentobeadivisorofAclonlyifTIisaunitthusofdegreezero;thishappenswhenp2=1.6.4.7.Examples1.ConstantreferencesinusoidaldisturbanceConsideragaintheminimalsystemPdeﬁnedbytheleftform(6.25)andsupposeweaddatitsoutputasinusoidaldisturbancedywithfrequency1rad/s.Theobjectivehereistotrackaconstantreferencesignal.Therelativedegreeδ0isimposedtobe1.TheoreticallyD1(∂)=∂2+1.Inawaytobroadenthe“resonancepeak”ofthetransferfunction1D(s)itispreferabletouseaverysmallbutnon-zerodampingcoefﬁcientς.Withς=0.5%weobtainD1(∂)=∂2+0.01∂+1=(∂+η)(∂+¯η)withη(cid:9)0.005+i.OntheotherhandD2(∂)=∂thusD(∂)=∂(cid:15)∂2+0.01∂+1(cid:16).ThetheoryisappliedwithAs(∂)=(∂+1+i)(∂+1−i)(∂+10)2Acl(∂)=As(∂)(∂+2)(∂+1)2.168LinearSystemsCondition(6.41)issatisﬁedandweobtainR(∂)=(∂+1)(∂+2)(∂+α)(∂+¯α)80|α|2S(∂)=D(∂)(∂+β)(cid:15)∂+¯β(cid:16)T(∂)=As(∂)R(0)As(0)withα=0.338+0.639iandβ=11.50+8.30i.TheBodeplotoftheopen-looptransferfunctionisshowninFigure6.11.Wenotethelargegaininlowfrequencies(inordertotrackthereferencewithoutstaticerror)andintheneighborhoodof1rad/s(fortherejectionofsinusoidaldisturbance);theroll-offaccentuatesfrom10rad/s(thisfrequencyistheabsolutevalueoftherootofthepolynomialF(∂)=∂+10usedforobtainingδ0=1).Thegainaugmentationmargin(about10dB)andthephaselagmargin(about45◦)arecorrectalongwiththedelaymargin(0.3s).ThesimulationofthecontrolledsystemshowninFigure6.12correspondstothefollowingevents:(i)stepcommandatt=0;(ii)sinusoidaldisturbancedyoffrequency1rad/sandamplitude0.5addingtoyfromt=10s;*(iii)allalongthesimulationaGaussianwhitenoisewithstandarddeviation0.03isaddedtothemeasurement(thecomputationstepisof0.1s).*Thestepresponseisexcellent;thedisturbanceisrejectedbutwithmoreoscillatorydynamics(becausethedynamicsFigure6.11.BodeplotofL(s)(Example1)RSTController169Figure6.12.Timedomainresponse(Example1)correspondingtotherootsofAs(∂)areexcitedbythisdisturbance).Wenotethatfromt=10sthecontrolbecomessinusoidal(ifweleaveasidetheeffectofthemeasurementnoise)counteractingthedisturbancedy.2.RampreferencesinusoidaldisturbanceConsideragaintheminimalsystemPdeﬁnedbytheleftform(6.25)andsupposethatasinusoidaldisturbancedywithfrequency1rad/sisaddedtotheoutputofP.Differentfromwhatwaspresentedpreviouslytheobjectivenowistotrackareferencesignalwhichisaramp.Therelativedegreeδ0isﬁxedtobe1.WeagainchooseD1(∂)=∂2+0.01∂+1butD2(∂)=∂2.ThetheoryisappliedusingAs(∂)=(∂+0.4+i)(∂+0.4−i)(∂+10)2Acl(∂)=As(∂)(∂+2)(∂+1)3.Condition(6.41)issatisﬁedandweobtainR(∂)=(∂+2)(∂+1)(∂+0.40)(∂+α)(∂+¯α)290.40×|α|2S(∂)=D(∂)(∂+β)(cid:15)∂+¯β(cid:16)T(∂)=µAs(∂)(∂+0.28)170LinearSystemsFigure6.13.BodeplotofL(s)(Example2)whereα=0.19+0.88iβ=11.40+8.05iandwhereµissuchthatT(0)=R(0)=58(wealsohaveT(cid:2)(0)=R(cid:2)(0)where(.)(cid:2)denotesthederivativeofthepolynomialinparentheses).TheBodeplotoftheopen-looptransferfunctionisshowninFigure6.13.Theslopeis−40dB/decadeinthelowfrequenciesandthegainincreasesagainintheneighborhoodof1rad/s(fortherejectionofthesinusoidaldisturbance).TheFigure6.14.Timedomainresponse(Example2)RSTController171gainaugmentationmargin(about10dB)andthephaselagmargin(about41◦)aresufﬁcientalongwiththedelaymargin(0.35s).ThesimulationofthecontrolledsystemshowninFigure6.14correspondstothefollowingevents:(i)referencerampsignalstartingatt=0untilitreachesthevalueof1andthenmaintainingthissignalatthisvalue;(ii)sinusoidaldisturbancedywithfrequency1rad/sandamplitude0.5comesonatt=35s.ThesignalyandthereferencerareshownintheupperpartofFigure6.14.Notethattherampisfollowedwithouttrackingerror.Thedynamicsofthedisturbancerejectionareverydifferentfromthoseofthereferencetracking.Itwasnecessarytoonlyweaklydamptheclosed-loopinordertoobtaincorrectphaseandgainmargins(fortheopen-loopispurelyoscillatorywithadoublepoleattheoriginandisnon-minimumphase).6.5.ExercisesEXERCISE120.–Detailtheproofofequalities(6.8)(6.9)and(6.10).EXERCISE121.–Extendthetheorypresentedinsection6.3.4tothecasewhereB(∂)=b0∂n−1+b1∂n+···+bntosimplifytherationaleﬁrstassumethatδ0>0whenb0(cid:5)=0andthenconsiderthecaseδ0=0andb0(cid:5)=0.EXERCISE122.–JustifythechoicemadeforpolynomialAcl(∂)inthefourexamplesgiveninsection6.3.6.EXERCISE123.–LetPbeaminimalsystemwithtransferfunctionG(s)=s+2s(s+1).1)DetermineanRSTcontrollerforthissystemthathasthefollowingproperties:(i)itrejectstheconstantdisturbancesduanddythatareaddedtotheinputandoutputofPrespectively;(ii)itensuresthetrackingofaconstantreferencesignalwithoutstaticerror;(iii)Acl(s)hasroots−1(tripleroot)−2and−10;(iv)theonlypoleofthetransferfunctionbetweenreferencerandoutputyis−1(doublepole).2)HowissuchachoiceofAcljustiﬁed?3)WhatistherelativedegreeofthetransferfunctionRS?EXERCISE124.–LetPbeaminimalsystemwithtransferfunctionP(s)=s+2(s+1)2forwhichwedesignanRSTcontrollerhavingProperties(i)and(ii)ofExercise123.Wechoose−1(triplepole)and−2tobethepolesoftheclosed-loop.(i)DetermineRandSandthenToftheformλ(∂+1)whereλissuchthatthestaticerroriszero.(ii)ExplicitlydeterminethesensitivityfunctionSo(s)andthenthemodulusmargin.(iii)Calculatethetransferfunctionbetweenthereferencesignalrandtheoutputyandthenexplicitlydeterminethestepresponse.Whatcanweconclude?172LinearSystemsFigure6.15.BodeplotofL(s)(Exercise126)EXERCISE125.–Thisexerciseisacontinuationofthepreviousone.Thistimewechooseaclosed-loophavingcomparedtothatofExercise124asupplementarypole−α.(i)Whatseemstobeareasonablevalueofα?(Defendyourviewpoint!).(ii)ChooseavalueofαthatseemspertinentthendeterminetheRSTcontrollersuchthatthestepresponseisthesameasinExercise124.(iii)WhataretheadvantagesandinconveniencesofthetwoRSTcontrollers?EXERCISE126.–Thisexerciseisacontinuationofthepreviousone.ChoosingfromExercise125α=5weobtaintheopen-looptransferfunctionL(s)theBodeplotofwhichisshowninFigure6.15.(i)Isthisresultcoherent?(Explain!)(ii)SupposenowthatPisasimpliﬁedmodelofasystemoffourthorderwhosetransferfunctionis˜P(s)=P(s)E(s)withE(s)=ω20s2+2ςω0s+ω20whereς=5.10−3andω0≥30rad/s.ShowtheshapeoftheBodeplotofE(s)andinparticularcalculatethemaximumof|E(iω)|andatwhichfrequencythemaximumisattained.(iii)SupposingwecontrolthecompletesystemusingtheRSTcontrollerdeterminedinExercise125(withα=5)istheclosed-loopsystemstable?WhatwillhappenifweusethecontrollerdeterminedinExercise124?(Defendyourargument.)RSTController173EXERCISE127.–Consideracontrollableandobservablesecond-ordersystemwithtransferfunctionG(s)=1/(cid:15)s2+s−1(cid:16).(i)DesignanRSTcontrollerforthissystemplacingallthepolesat−1suchthatthetransferofthecontrolledsystembetweenreferenceandoutputisoftheorderof2andsuchthatδ0=0.(ii)Samequestionwithδ0=1.(iii)Intheabovetwocasesisthechoiceofthepolespertinent?EXERCISE128.–Extendthetheorypresentedinsection6.4.5tothecasewhereB(∂)=b0∂n−1+b1∂n+···+bn(seeExercise121)Chapter7SystemsTheory(II)Thetheoryoflineartime-invariantsystemsturnedouttobeessentiallyalgebraicwhichistheapproachthischapterwillbefollowing.Givenitsgeneralnatureandsigniﬁcancesuchanapproachwillnotbepossibleunlessweusethe“languageofmodules”aspresentedinsection13.4.Fromthepointofviewofapplicationsitissufﬁcientinmostcasestoconsider“state-spacesystems”(section2.3.6).Fundamentallyhoweverthisframeworkistoonarrow.Takeanexample:oneofthemostimportantnotionsofsystemstheoryiscontrollability.Wehavealreadycomeacrossthisnotioninsection6.2.2(seeDeﬁnition108)concerningSISOsystemsrepresentedbyleftforms.Howevertheredoesnotexisttwotypesofcontrollabilityonefortheseleftformsandanotheroneforstate-spacesystems.Itisthereforenecessarytodeﬁnecontrollabilityforaclassofsystemsthatencompassesthesetwotypesofrepresentations.TherearealsosystemsthataredescribedbyaRosenbrockrepresentationwhichiscompletelygeneralasshowninsection2.3.5.ThereforeitappearsthatitissufﬁcienttoproperlydeﬁnethecontrollabilityofaRosenbrockrepresentation.Butisitaparticularrepresentationofthesystemwhichiscontrollableorisitthesystemitself?TheworksofWillemsandFliess([117][42])datingbacktothebeginningofthe1990shaveclearlydemonstratedthatthisisapropertyofthesystemandnotofoneofitsrepresentations.Controllabilityisthereforeaconcreteandobjectiveproperty.Henceforththisistheviewpointthatdeservestobeexpounded.Paradoxicallytoachievethisanoftenabstractlanguageisnecessary.175Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.176LinearSystems7.1.Structureofalinearsystem7.1.1.*ThenotionofalinearsystemLetusbeginbyrecallingwhathasbeenestablishedintheremarksofChapter2.Inwhatfollowstheterm“system”signiﬁes“lineartime-invariantsystem”unlessotherwisestated.(i)AsystemΣcanbeidentiﬁedwithaﬁnitelypresentedR-module1MwhereR=R[∂](Remark8section2.2.2).ThismoduleMisdeﬁnedbyanequationsuchthat(2.6)E(∂)w=0.Thevariablesw1...wkgenerateMandwewriteM=[w]Rwherew=[w1...wk]T.ThematrixE(∂)isamatrixofdeﬁnitionofM(orofthesystemΣ).WecanassumethatE(∂)isleft-regular(i.e.fullrowrank)becauseRisaprincipalidealdomain(seesections13.1.4and13.4.2)andthenE(∂)∈Rr×kwherer=rkE(∂).ThesystemΣ(identiﬁedwiththemoduleM)becomesacontrolsystemoncetheinputvariablesu1...umandtheoutputvariablesy1...yparechosen.Aﬁnitesequenceofvariablesu=[u1...um]TisapossibleinputforΣif(i)uisindependenti.e.[u]RisafreeR-moduleofrankmand(ii)themoduleM/[u]Ristorsion(Remark12section2.3.1).TheonlyaprioriconstraintontheoutputvariablesisthattheybelongtoM(Remark15section2.3.5).OnecanalwaysrepresentasysteminaRosenbrockformliketheonedeﬁnedbyrelation(2.1.1)ofsection2.3.5(Remark15section2.3.5).ThepolesofthecontrolsystemΣaretheSmithzerosofthetorsionmoduleM/[u]R(Remark19section2.3.7).7.1.2.State-spacerepresentationWeowetoFliess[42]thefollowingresult:THEOREM129.–Everycontrolsystemadmitsastate-spacerepresentation(oftheform(2.20)section2.3.6).PROOF.*1)SincethemoduleT=M/[u]Ristorsionitisaﬁnite-dimensionalR-vectorspace(accordingtoTheorem565ofsection13.4.3).Let¯η=(¯ηi)1≤i≤nbeabasisofthisvectorspace.Since∂¯ηiisanR-linearcombinationofthe¯ηjs1≤j≤n1.Orwhatcomestothesamething(becauseRisNoetherian)aﬁnitelygeneratedR-module.Wecannonethelessconsideralinearsystemdeﬁnedoveranon-Noetherianring(seesection13.2.3Remark500(ii));suchasystemcanbedeﬁnedas(orbeidentiﬁedwith)aﬁnitelypresentedmodule.SystemsTheory(II)177thereexistsamatrixA∈Rn×nsuchthat∂¯η=A¯η.Therealsoexistnelementsηi∈Msuchthat¯ηiisthecanonicalimageofηiinT(1≤i≤n).AsaresultthereexistmatricesBj∈Rn×m(0≤j≤ssﬁnite)suchthatBs(cid:5)=0and∂η=Aη+(cid:11)0≤j≤sBj∂ju.(7.1)Ifs≥1thenletη∗=η−Bs∂s−1u.Thisyields∂η∗=Aη∗+*(cid:11)0≤j≤s−2Bj∂j+B(cid:2)s−1+uwhereB(cid:2)s−1=ABs+Bs−1.Iteratingthisprocedureweobtaintheform∂x=Ax+Bu.(7.2)2)ItfollowsfromthisexpressionthatthemoduleTisdeﬁnedbytheequation∂¯x=A¯xwhere¯x=(¯xi)1≤i≤nand¯xiisthecanonicalimageofxiinT(1≤i≤n).ThereforeT=[¯x]Randasthequantities¯xiareR-linearlyindependent¯xisabasisofthevectorR-spaceT.Let¯yibethecanonicalimageofyi∈MinT.ThereexistsamatrixW(∂)∈Rp×msuchthaty=Cx+W(∂)u(7.3)andexpression(2.20)ofsection2.3.6isobtained.*REMARK130.–Asmentionedinsection2.5.3weareonlyinterestedinpropercontrolsystemsinpractice.FromTheorem48apropersystemΣadmitsastate-spacerepresentationoftheform(cid:20)∂x=Ax+Buy=Cx+Du(7.4)whereA∈Rn×nB∈Rn×mC∈Rp×nandD∈Rp×m;inadditionD=0ifandonlyifΣisstrictlyproper.Suchastate-spacerepresentationisdenotedby{ABCD}inwhatfollowsandas{ABC}ifD=0.AstothepossibilityofobtainingsucharepresentationsystematicallyseeRemark49(section2.5.3).DEFINITION131.–ThematricesABCandDabovearecalledrespectivelythestatematrixtheinputmatrixtheoutputmatrixandthedirecttermmatrix(ormoresuccinctlythedirectterm).Theﬁrstoftheequationsofequation(7.4)isthestateequationthesecondistheoutputequation.178LinearSystemsyMAfFigure7.1.CarriageREMARK132.–SupposewechangethebasisinRnaccordingtox=PηwherethematrixP∈Rn×nisinvertible.Inthenewbasistherepresentationis(cid:20)∂η=P−1APη+P−1Buy=CPη+Du.(7.5)EXAMPLE133.–ConsideracarriageofmassMrollingfrictionlessonahorizontalplaneandsubjectedtoaforcef(seeFigure7.1).AssumingthatthewheelshavenegligiblemassandmomentofinertiawehaveaccordingtoNewton’slaw(equation(1.12)section1.2.1)f=M˙vwherev=˙yisthevelocityofthecarriage(yisthepositionwithrespecttoaﬁxedpoint).Thecontrolsystemwithinputu=fandoutputyisthusdescribedbythestate-spacerepresentation˙x=(cid:27)0010(cid:28)x+(cid:27)1M0(cid:28)uy=(cid:25)01(cid:26)xwherex(cid:1)(cid:25)vy(cid:26)T.Inthiscaseallthestatecomponentshaveaphysicalsigniﬁcance.Thisisnotalwaysso.7.1.3.ControllabilityAsystemiscontrollableifandonlyifnoneofthevariablesofthissystemsatisﬁesanon-trivialhomogenousdifferentialequation;inotherwordsifandonlyifallitsvariablesarefree(seesections13.4.1and13.4.3).Indeedsupposethereexistsanon-invertiblepolynomiala(∂)andavariablev∈Msuchthata(∂)v=0.TheSystemsTheory(II)179evolutionofavariablev∈E(R)satisfyingthesamedifferentialequationonlydependsoninitialconditionsandhencenopossibleactioncancauseanyinﬂexionontheevolutionofthisvariable.Converselytheevolutionofafreevariable(notdeterminedbyinitialconditions)canbegovernedbyanappropriateaction.*Totranslatewhatwehavejustsaidintomathematicalwordswebeginbythefollowingremark:REMARK134.–LetMbethemoduleassociatedwithasystemΣ.AccordingtoCorollary555(section13.4.2)themoduleMisfreeifandonlyifitistorsion-free.WearenowledtothefollowingdeﬁnitionduetoFliess[42]:DEFINITION135.–AsystemΣiscontrollableiftheassociatedmoduleMisfree.*REMARK136.–Thereexistsa“behavioral”deﬁnitionofcontrollabilityduetoWillems[117](seealso[96]section5.2).Thestudyoftheequivalencebetweenthecontrollability“àlaWillems”andthatofDeﬁnition135wascarriedoutbyFliess[43].Accordingtothesetwodeﬁnitionsthecontrollabilityofasystemisaconceptneitherrelatedtothetypeofrepresentationchosenforthesystemnorrelatedtoaparticularchoiceofthecontrolvariables.THEOREM137.–*AcontrolsystemΣiscontrollableifandonlyifitcanberepresentedbya“rightform”(seesection2.3.5).*PROOF.*AcontrolsystemΣisrepresentablebytheequalities(2.19)ofsection2.3.5ifandonlyiftheassociatedmoduleisfree(withbasisξ).*THEOREM138.–LetΣbeasystemdeﬁnedbyaRosenbrockrepresentation{DNQW}(Deﬁnition14section2.3.5);ΣiscontrollableifandonlyifthematricesDandNareleft-coprime.PROOF.ThisisobviousaccordingtoTheorem505(section13.2.6)andRemark134.COROLLARY139.–“Popov–Belevitch–Hautus(PBH)testforcontrollability”.AsystemΣdeﬁnedbyaRosenbrockrepresentation{DNQW}iscontrollableifandonlyifrkC(cid:25)D(s)N(s)(cid:26)=rforeverys∈C(wherer(cid:1)rkRD(∂)).Inparticularastate-spacesystem{ABCD}iscontrollableifandonlyifrkC(cid:25)sIn−AB(cid:26)=nforeverys∈C.PROOF.Thematrices{DN}areleft-coprimeifandonlyiftheSmithformof(cid:25)DN(cid:26)is(cid:25)Ir0(cid:26)(accordingtoTheorem505ofsection13.2.6)i.e.whenthestatedconditionholds.180LinearSystemsREMARK140.–ThenecessaryandsufﬁcientconditionexpressedbyTheorem138generalizesDeﬁnition108ofsection6.2.2validonlyinthecaseofanSISOsystemdeﬁnedbyaleftform.THEOREM141.–“Kalmancriterionforcontrollability”[65].Astate-spacesystem{ABCD}iscontrollableifandonlyifrkΓ=nwherenisthedimensionofthestatevector(section2.3.6Remark16)andwhereΓisthe“controllabilitymatrix”deﬁnedbyΓ=(cid:25)BAB...An−1B(cid:26).(7.6)PROOF.*1)Aswehaveindicatedinsection2.3.6astate-spacerepresentation{ABC∗}isaRosenbrockrepresentation{DNQ∗}ofaparticulartypewhereD(∂)=∂In−AN(∂)=BandQ(∂)=C.AccordingtoTheorem138thesystemdeﬁnedbythisrepresentationiscontrollableifandonlyifthematrices∂In−AandBareleft-coprimethatistosayif(cid:25)∂In−AB(cid:26)isright-invertibleaccordingtoDeﬁnition506(section13.2.6).ThisamountstosayingthatifvT=(cid:25)v1...vn(cid:26)isarowofelementsofarightR-moduletheequalityvT(cid:25)∂In−AB(cid:26)=0impliesvT=0.Theﬁrstoftheseequalitiesisequivalentto(a)∂vT=vTAand(b)vTB=0.Multiplying(b)by∂ontherightweobtainvT∂B=0andhencevTAB=0accordingto(a).Thislastequalitymultipliedontherightby∂impliesthatvT∂AB=0andhencevTA2B=0accordingto(a)etc.AsaresulttheequalityvT(cid:25)∂In−AB(cid:26)=0impliesvTΓ=0.IfrkΓ=nthisimpliesvT=0.ThereforetheconditionrkΓ=nissufﬁcientforthesystemtobecontrollable.2)Letusshowbycontradictionthatthisisalsoanecessarycondition.LetrkΓ=ρ<n.LetP=(cid:25)P1P2(cid:26)∈Rn×nwherethesubmatrixP1consistsofρlinearlyindependentcolumnsofΓandwhereP2isasubmatrixchosensuchthatPwillbeinvertible.Suchamatrixexistsaccordingtothe“theoremoftheincompletebasis”(Corollary517section13.3.1).IndeedthecolumnsofP1areidentiﬁedwithlinearlyindependentvectorsxi(ρ+1≤i≤n)inthecanonicalbasisofRn.Wecandeterminethevectorsxi(ρ+1≤i≤n)insuchawaythat(xi)1≤i≤nisabasisofRn.Thevectorsxi(ρ+1≤i≤n)areidentiﬁedwithcolumnvectorsinthecanonicalbasisofRnandformthematrixP2whichisbeingsoughtafter.LetuandbbethehomomorphismsrepresentedbythematricesAandBrespectivelywhenthebaseschoseninRnandRmarethecanonicalbases.ThesubspaceofRnspannedbythevectorsxi(1≤i≤ρ)isE1=(cid:11)0≤i≤n−1uiimb(whereu0=In).AccordingtotheCayley–Hamiltontheorem(Theorem537section13.3.4)unisanR-linearcombinationoftheendomorphismsui0≤i≤n−1andasaresultE1isu-invariant.Asshowninsection13.3.2(seeequation(13.24))thematrix˜Arepresentinguinthebasis(xi)1≤i≤ni.e.˜A=P−1APisthereforeoftheform˜A=(cid:27)Ac∗0A¯c(cid:28)SystemsTheory(II)181wherethesquarematrices˜Acand˜A¯careoforderρandn−ρrespectively.Ontheotherhandimb⊂E1andthusthematrix˜BrepresentingbinthecanonicalbasisofRmandthebasis(xi)1≤i≤nofRni.e.˜B=P−1B(seeequation(7.5)section7.1.2)isoftheform˜B=(cid:27)Bc0(cid:28)whereBc∈Rρ×m.Puttingx=Pηwhereη=(cid:27)xcx¯c(cid:28)weobtain(cid:20)∂xc=Acxc+∗x¯c+Bcu∂x¯c=A¯cx¯c(7.7)where∗isanunspeciﬁedmatrix.Consequentlythesubmodule[η¯c]RofMgeneratedbythecomponentsofx¯cistorsionandnotreducedto0.ThemoduleMisthereforenotfreeandtheassociatedsystemisthereforenon-controllable.*REMARK142.–(i)SincethecontrollabilitymatrixΓdeﬁnedbyequation(7.6)dependsonlyonAandBfromnowonwewillcallitthe“controllabilitymatrixof(AB)”anddenoteitasΓ(AB).Wewillexpressthatitisofranknbysayingthat“(AB)iscontrollable”.(ii)AnelementarycalculationshowsthatforanymatrixP∈GLn(R)2Γ(cid:15)P−1APP−1B(cid:16)=P−1Γ(AB).(7.8)Asaresultthecontrollabilityof(AB)onlyinvolvesthehomomorphismsuandbdeﬁnedbythesematricesinthechosenbases[119].(iii)Originallycontrollabilityhasbeendeﬁnedforstate-spacesystemsbyKalman[65]inthefollowingmanner:“Astate-spacesystem{ABCD}iscontrollableifforanyinitialcondition(t0x0)andanypointx∗inthestatespacethereexistsaninstantt1>t0andacontroludeﬁnedon[t0t1]forwhichthestatexpassesbetweentheinstantt0andtheinstantt1fromthevaluex0tothevaluex∗”.Kalmanshowedinthecitedreferencethatanecessaryandsufﬁcientconditionforthesystem{ABCD}tobecontrollableinthesenseasspeciﬁedjustnowistheonegiveninTheorem141(inthecaseofdiscrete-timesystemsseeDeﬁnition315andTheorem317(section10.4.2)).REMARK143.–ItiseasytoshowusingTheorem141thatthecarriageofExample133isacontrollablesystem.Itisequallyeasytoshowthatfromanyinitialinstant2.GLn(R)isthegenerallineargroupofthesquarematricesofordernoverRi.e.themultiplicativegroupconsistingofallinvertiblematricesofordernwithrealentries:seesection13.1.4.182LinearSystemst0andanyinitialstatex0=(cid:25)v0y0(cid:26)T(x0=x(t0))onecanbringthecarriagetoanarbitrarilychosenstatex∗=(cid:25)v∗y∗(cid:26)Tatanytimet1>t0(x(t)=x∗)bymodulatingtheforcefontheinterval[t0t1]inanappropriatemannerwhichisKalman’sdeﬁnitionofcontrollability(Remark142(iii)).*UsingthenotationintheproofofTheorem141weobtainthefollowingresult:PROPOSITION144.–ThetorsionsubmoduleofMisT(M)=[x¯c]R.PROOF.1)Weknowthat[x¯c]R⊂T(M).2)ToprovetheconverseconsiderthequotientmoduleM/[x¯c]R=[¯xc¯u]Rwherethecomponentsof¯xcandof¯uarethecanonicalimagesofthoseofxcandofurespectively.Weobtain∂¯xc=Ac¯xc+Bc¯u.(7.9)WehaverkΓ(cid:9)˜A˜B(cid:10)=ρandΓ(cid:9)˜A˜B(cid:10)=(cid:27)BcAcBc...An−1cBc00...0(cid:28)andhenceρ=rk(cid:25)BcAcBc...An−1cBc(cid:26)=rkΓ(AcBc);thelastequalityisaconsequenceoftheCayley–Hamiltontheorem(seeabovetheproofofTheorem141).ThereforethemoduleM/[x¯c]RisfreeandthusT(M)⊂[x¯c]R*DEFINITION145.–Wecallthecontrollablequotientsystemthesystemwithequation(7.9)(*associatedwiththequotientmoduleM/T(M)*)3.Theaboveprovesthat(AcBc)iscontrollable.Furthermoreonecanrepresentthedecomposition(7.7)called“Kalmancontrollabilitydecomposition”bythediagraminFigure7.2:7.1.4.ObservabilityIntuitivelyacontrolsystemisobservableifwhenbothitsinputandoutputareidenticallyzeroallitsvariablesarenecessarilyzero(inothertermseveryevolutionofthesystemvariablescanbedetectedfromtheinputandoutput).3.*Sinceasystemisassociatedwithaﬁnitelypresentedmoduleaquotientsystemisassociatedwithaquotientofthismodule[22].*SystemsTheory(II)183yControllable  Non controllableu+Figure7.2.Kalmancontrollabilitydecomposition*Toexpresswhathasbeenjustsaidinmathematicaltermsthelanguageofmodulesisonceagainthebestsuitedone:thecontrolsystemconsideredisobservableifandonlyifM[yu]R=0(7.10)andwearethusledtothefollowingdeﬁnition[42]:DEFINITION146.–AcontrolsystemΣwithinputuandoutputyisobservableiftheassociatedmoduleMsatisﬁestheequalityM=[yu]R.*THEOREM147.–AcontrolsystemΣisobservableifandonlyifitcanberepresentedbya“leftform”(seesection2.3.5).PROOF.*AcontrolsystemΣisrepresentablebytheequality(2.17)ofsection2.3.5ifandonlyiftheassociatedmoduleisM=[yu]R.*THEOREM148.–LetΣbeacontrolsystemdeﬁnedbyaRosenbrockrepresentation{DNQW};ΣisobservableifandonlyifthematricesDandQareright-coprime.PROOF.*ThesystemΣisobservableifandonlyiftheequality(7.10)issatisﬁed.LetξbethepartialstateofΣand_ξbethecolumnmatrix;theentriesarethecanonicalimagesofthecomponentsofξinM/[yu]R.Weget(cid:27)D(∂)Q(∂)(cid:28)_ξ=0.(7.11)184LinearSystemsΣisobservableifandonlyifthisequalityimplies_ξ=0whichisthecaseifandonlyifthematrix(cid:27)D(∂)Q(∂)(cid:28)isleft-invertible.Thismeansthatthematrices{DQ}arerightcoprime(seesection13.2.6).*FollowingasimilarlineofreasoningasintheproofofCorollary139andofTheorem141weobtainthefollowingresults(withthesamenotation):COROLLARY149.–“Popov–Belevitch–Hautus(PBH)testforobservability”.AsystemΣdeﬁnedbyaRosenbrockrepresentation{DNQW}isobservableifandonlyifrkC(cid:25)DT(s)QT(s)(cid:26)=rforeverys∈C.InparticularifΣisastate-spacesystem{ABCD}ΣiscontrollableifandonlyifrkC(cid:25)sIn−ATCT(cid:26)=nforeverys∈C.THEOREM150.–“Kalmancriterionforobservability”[65].Astate-spacesystem{ABCD}isobservableifandonlyifrkΩ=nwherenisthedimensionofthestatevectorandwhereΩisthe“observabilitymatrix”deﬁnedbyΩ=(cid:23)CTATCT...(cid:15)AT(cid:16)n−1CT(cid:24)T.(7.12)REMARK151.–SincetheobservabilitymatrixΩonlydependsonAandCitcanbedenotedbyΩ(AC)andwecanexpressthatitisofranknbysayingthat“(CA)isobservable”.(OnecanalsorefermoreintrinsicallytothehomomorphismsdeﬁnedbythematricesCandAinthechosenbasesinthespiritofRemark142ofsection7.1.3.)REMARK152.–ItiseasytoshowusingTheorem150thatthecarriageofExample133(withthespeciﬁedinputandoutput)isobservable.Wecaninterpretthispropertyinthefollowingmanner:supposethatthesystemevolvesoveratimeinterval[t0t]t>t0andwecollectmeasurementsoftheinputandtheoutputoverthisinterval.Fromthesedataitispossibletoreconstructtheinitialstatex0=x(t0).(Iftheoutputisvelocityvinsteadofpositionysuchareconstructionisimpossiblesincetheinputandoutputremainunchangedifoneaddsanarbitraryconstanttothepositiony;thecontrolsystemisthereforenon-observable.)SuchanapproachofobservabilityisduetoKalman[65](incaseofdiscrete-timestate-spacesystemsseeDeﬁnition327andTheorem328(section10.4.3)).TheinterpretationofobservabilityduetoWillemsisverysimilarbutnotrelatedtothestate-spacerepresentation([96]section5.3).ThefollowingisdeducedfromTheorem150:COROLLARY153.–Controllability↔observabilityduality.(CA)isobservableifandonlyif(cid:15)ATCT(cid:16)iscontrollable.SystemsTheory(II)185PROOF.ItisclearthatΩT(AC)=Γ(cid:15)ATCT(cid:16)andhencerkΩ(AC)=rkΓ(cid:15)ATCT(cid:16)accordingtoProposition534(section13.3.4).Theabovedualityhasimportantconsequences;inparticularthefollowing:letΣbeastate-spacesystem{ABCD}ofordernandwhichisnon-observable.Letω<nbetherankofitsobservabilitymatrixΩ.LetP=(cid:25)P1P2(cid:26)∈Rn×nwhereP1isformedbyωlinearlyindependentcolumnsofΩTandP2isasub-matrixchoseninsuchawaythatPisinvertible(seePart2oftheproofofTheorem141).ThematricesˇAT=P−1ATPandˇCT=P−1CTareoftheformˇAT=(cid:27)ATo∗0AT¯o(cid:28)ˇCT=(cid:27)CTo0(cid:28)where(cid:15)ˇAToˇCTo(cid:16)iscontrollable.AsaresultˇA=PTAP−TandˇC=CP−T(whereP−Tmeans(cid:15)P−1(cid:16)T)satisfyˇA=(cid:27)Ao0∗A¯o(cid:28)ˇC=(cid:25)Co0(cid:26)where(CoAo)isobservable.Inthenewbasesthestateequationstaketheform⎧⎪⎪⎨⎪⎪⎩∂(cid:27)xox¯o(cid:28)=(cid:27)Ao0∗A¯o(cid:28)(cid:27)xox¯o(cid:28)+(cid:27)∗∗(cid:28)uy=(cid:25)Co0(cid:26)(cid:27)xox¯o(cid:28)+[∗]u.(7.13)*Wededucethefollowingresult:PROPOSITION154.–The“non-observablequotientsystem”isassociatedwiththequotientmoduleM/[yu]R(seeFootnote3ofsection7.1.3)deﬁnedbytheequation∂_¯x¯o=A¯o_¯x¯o(7.14)wherethecomponentsof_¯xarethecanonicalimagesofthecomponentsofx¯oinM/[yu]R.PROOF.Wehave(cid:27)∂Iω−AoCo(cid:28)_¯xo=0andthus_¯xo=0becausethematrix(cid:27)∂Iω−AoCo(cid:28)isleft-invertible(fromthefactthat(CoAo)isobservableandaccordingtoTheorem148).ThusM/[yu]R=(cid:25)_¯x¯o(cid:26)Randthislastmoduleisdeﬁnedbyequation(7.14).*Wecanrepresentthedecomposition(7.13)bythediagraminFigure7.3:186LinearSystems       Observable         Non observable u      yFigure7.3.Kalmanobservabilitydecompostion7.1.5.CanonicalstructureofasystemConsideracontrolsystemΣinthestate-spaceform{ABC}4whichcanbenon-controllableand/ornon-observable.Bydecomposingthissystemaccordingtocontrollabilityoneobtainstherepresentation(7.7)where(AcBc)iscontrollableandwhereA¯cistheemptymatrixifΣiscontrollable.Theoutputycanbeexpressedasy=(cid:25)CcC¯c(cid:26)(cid:27)xcx¯c(cid:28).Nowletusconsiderthecontrollablequotientsystem(7.9)andits“outputequation”˜y=Cc˜xcwherethecomponentsof˜yarethecanonicalimagesofthoseofyinthequotientM/[x¯c]R.Wecandecomposethissystemaccordingtoobservability(decompositionincludingemptymatricesifthissystemisobservable).ThequotientsystemM/[xcu]Rcanbedecomposedinthesameway.Weﬁnallyobtainthefollowingresult:THEOREM155.–“Canonicalstructureofastate-spacesystem”[65].(i)ThereexistsachangeofbasismatrixP∈Rn×nsuchthatthematrices˜A=P−1AP4.ThedirecttermmatrixDdoesnotplayanyroleinthenexttheoremandcanbeassumedtobezerowithoutlossofgenerality.SystemsTheory(II)187˜B=P−1Band˜C=CPtaketheform⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩˜A=⎡⎢⎢⎣Ac¯o∗∗∗0Aco0∗00A¯c¯o∗000A¯co⎤⎥⎥⎦˜B=⎡⎢⎢⎣Bc¯oBco00⎤⎥⎥⎦˜C=(cid:25)0Cco0C¯co(cid:26)(7.15)where(cid:21)(cid:27)Ac¯o∗0Aco(cid:28)(cid:27)Bc¯oBco(cid:28)(cid:22)iscontrollableand(cid:21)(cid:25)CcoC¯co(cid:26)(cid:27)Aco∗0A¯co(cid:28)(cid:22)isobservable.(ii)Thestate-spacesystem{AcoBcoCco}isbothcontrollableandobservable.PROOF.(i)isobtainedbyapplyingtheaboveindicatedmethod(formoredetailssee[65]([63]section3.4.3)or([96]section5.4)).(ii)Letnc¯o(resp.nco)betheorderofthematrixAc¯o(resp.Aco).AccordingtothePopov–Belevitch–Hautustest(Corollary139section7.1.3)wegetrkC(cid:27)sInc¯o−Ac¯o∗Bc¯o0sInco−AcoBco(cid:28)=nc¯o+ncoforanys∈C.ThisimpliesrkC(cid:25)sInco−AcoBco(cid:26)=ncoforanys∈Candthus(AcoBco)iscontrollable.Theobservabilityof(CcoAco)canbeshowninasimilarmanner.ThedecompositionexpressedintheabovetheoremcanberepresentedbythediagraminFigure7.4.REMARK156.–Ingeneralitisnottruethat(Ac¯oBc¯o)willbecontrollablenor(C¯coA¯co)willitbeobservable(seeExercise213section7.6).PROPOSITION157.–ThetransfermatrixofthesystemΣisG(s)=C(sI−A)−1B=Cco(sI−Aco)−1Bco.(7.16)188LinearSystemsControllable   observable Controllable Non observable Non controllable observableNon controllable Non observable +uyFigure7.4.KalmangeneraldecompositionPROOF.Theﬁrstequalityisanimmediateconsequenceofequality(2.48)(section2.5.3)sinceW(s)=0.Thesecondequalityisaconsequenceofthestructureofthematricesinequation(7.15).THEOREM158.–AcontrolsystemΣisminimal(seesection2.4.6)ifandonlyifitisbothcontrollableandobservable.PROOF.*1)Accordingtoequation(7.16)andequality(2.37)ofsection2.4.6thetransmissionorderofΣcannotexceedtheorderofthematrixAco.ThusΣisnotminimalifitisnon-observableornon-controllable.2)ConverselysupposeΣisobservable.ItisrepresentablebyaleftformD(∂)y=N(∂)uaccordingtoTheorem147(section7.1.4).IfΣisalsocontrollablethenthematrices{DN}areleft-coprimeaccordingtoTheorem138(section7.1.3).ThereforeaccordingtoRemark31(section2.4.5)thetransmissionpolesofΣaretheSmithzerosofD(s)andtheycoincidewiththepolesofΣaccordingtoRemark19ofsection2.3.7.AsaresultthecontrolsystemΣisminimal.*SystemsTheory(II)1897.2.ZerosofasystemWehaveseeninsection2.4.7thatwecandeﬁneﬁnitetransmissionpolesandzerosandalsopolesandzerosatinﬁnity.Thisisalsotrueforallothertypesofpolesandzeros.Thereadercanﬁndacompleteexpositionofthedifferenttypesofpolesandzerosatinﬁnityin[21]and[22].Inwhatfollowswewillbeinterestedonlyinﬁnitepolesandzeros.7.2.1.InvariantzerosandtransmissionzerosIntuitivelytheinvariantzeros(i.z.)ofacontrolsystemΣwithinputuandoutputycharacterizethedynamicsofthissystemwhenitsoutputismaintainedat0;thetransmissionzeroshavebeendeﬁnedinsections2.4.4and2.4.5fromthetransfermatrix(seeDeﬁnition28).*Nowletusexpressthedeﬁnitionsinthelanguageofmodules[19].ConsidertheﬁnitelypresentedR-moduleMassociatedwithΣanditsdecomposition(13.42)(section13.4.2)i.e.M=T(M)⊕Φ(7.17)whereΦisafreesubmodule.Inwhatfollowsthenotationandterminologyarethoseusedinsections13.4.1and13.4.2.AccordingtoProposition561onecanchooseΦ=Φ[yu]RwhereΦ[yu]Rissuchthat[yu]R=T([yu]R)⊕(cid:15)[yu]R∩Φ[yu]R(cid:16).Itisthischoicethatwewillusebelow.DEFINITION159.–(i)ThemoduleofinvariantzerosisT(M/[y]R).(ii)ThemoduleoftransmissionzerosisT(Φ∩[yu]R)/T(Φ∩[y]R).(iii)Theinvariantzeros(resp.thetransmissionzeros)aretheSmithzerosoftheﬁrst(resp.ofthesecond)module.*LetsystemΣbedescribedbyaRosenbrockrepresentation{DNQW}(section2.3.5).Let_¯ξ=(cid:9)_ξi(cid:10)1≤i≤rand_¯u=(cid:15)_¯ui(cid:16)1≤i≤mwhere_ξiand_¯uiarethecanonicalimagesofξianduirespectivelyinM/[y]R(ξ=(ξi)1≤i≤risthepartialstateandu=(ui)1≤i≤mistheinput).Oneobtains(cid:27)D(∂)−N(∂)Q(∂)W(∂)(cid:28)#_ξ_¯u$=0.(7.18)190LinearSystemsDEFINITION160.–ThematrixR(∂)=(cid:27)D(∂)−N(∂)Q(∂)W(∂)(cid:28)(7.19)iscalledtheRosenbrockmatrix(orsystemmatrix)ofthecontrolsystemΣ.Fromtheabovetwodeﬁnitionsequation(7.18)andsection13.2.5wegetthefollowingresult:THEOREM161.–TheinvariantzerosofΣaretheSmithzerosofitsRosenbrockmatrix.REMARK162.–SupposeΣisastate-spacesystem{ABCD}(takingcarenottoconfusethematrixDofthisstate-spacerepresentationwiththematrixD(∂)oftheRosenbrockrepresentation).ThentheRosenbrockmatrixofthesystemisR(s)=(cid:27)sIn−A−BCD(cid:28).(7.20)PROPOSITION163.–(i)Letρ(.)betherankofthematrixinparenthesesovertheﬁeldofrationalfunctionsF=R(s).TherankoftheRosenbrockmatrix(7.19)overFsatisﬁestheequalityρ(R)=ρ(D)+ρ(G)(7.21)whereGisthetransfermatrixofΣ.(ii)InparticularifRisgivenby(7.20)andifGissemi-regular(section13.1.4)weobtainρ(R)=n+min{pm}.(7.22)PROOF.Noticethat(withr=ρ(D)):(cid:27)Ir0−QD−1Ip(cid:28)R(∂)=(cid:27)D−N0G(cid:28)accordingtoequation(2.27)(section2.4.2)fromwhichweobtaintheequality(7.21)becausethematrixontheleftisinvertible.7.2.2.Input-decouplingzerosInput-decouplingzerosarealsocalled“non-controllablepoles”.Theyaredeﬁnedindependentlyofthechoiceoftheinputofthesystem.*LetusﬁrstprovideanabstractdeﬁnitionbyconsideringtheR-moduleMassociatedwithsystemΣ[19]:SystemsTheory(II)191DEFINITION164.–Themoduleofinput-decouplingzeros(i.d.z.)isT(M).Thei.d.z.saretheSmithzerosofthismodule.*TheresultbelowisanimmediateconsequenceofDeﬁnitions135(section7.1.3)and164:THEOREM165.–Asystemiscontrollableifandonlyifitdoesnothavei.d.z.s.SupposeΣisacontrolsystemdeﬁnedbyaRosenbrockrepresentation{DNQW}.Wethenhavethefollowing:PROPOSITION166.–Thei.d.z.saretheSmithzerosofthematrix(cid:25)DN(cid:26).PROOF.*ThisisobvioussinceM=[ξu]Rwhere(cid:25)D(∂)N(∂)(cid:26)(cid:27)−ξu(cid:28)=0.*SupposenowthatΣisastate-spacesystem{ABCD}andconsideritscontrollabilitydecomposition(7.7)(section7.1.3).PROPOSITION167.–Thei.d.z.saretheeigenvaluesofthematrixA¯c.PROOF.*Thisisanimmediateconsequenceofthesecondequalityof(7.7)andofProposition144(section7.1.3).*7.2.3.Output-decouplingzeros*Letusbeginbyanabstractdeﬁnition[19]consideringtheR-moduleMassociatedwiththecontrolsystemΣwithinputuandoutputy.DEFINITION168.–Themoduleofoutput-decouplingzeros(o.d.z.)isM/[yu]R.Theo.d.z.saretheSmithzerosofthismodule.*Output-decouplingzerosarealsocallednon-observablepoles.Thefollowingtheoremcanbederivedimmediatelyfromthedeﬁnitions:THEOREM169.–Acontrolsystemisobservableifandonlyifithasnoo.d.z.SupposeΣisdeﬁnedbyaRosenbrockrepresentation{DNQW}.Onehasthefollowing:192LinearSystemsPROPOSITION170.–Theo.d.z.saretheSmithzerosofthematrix(cid:27)DQ(cid:28).PROOF.*ItsufﬁcestorememberthatM/[yu]Risdeﬁnedbyequation(7.11)(section7.1.4).*LastsupposeΣisastate-spacesystem{ABCD}andletusconsideritsobservabilitydecomposition(7.13)(section7.1.4).PROPOSITION171.–Theo.d.z.saretheeigenvaluesofthematrixA¯o.PROOF.*ThisisobviousaccordingtoProposition154.*7.2.4.Input–ouputdecouplingzeros*Letusbeginbyanabstractdeﬁnition[19]:DEFINITION172.–Themoduleofinput–outputdecouplingzeros(i.o.d.z.)isT(M)T([yu]R).Thei.o.d.z.saretheSmithzerosofthismodule.*LetΣ={ABCD}beastate-spacesystemandconsideritscanonicalstructuregivenbyTheorem155(section7.1.5).Weobtainthefollowingresult:PROPOSITION173.–Thei.o.d.z.saretheeigenvaluesofA¯c¯oand{i.o.d.z.}⊂{i.d.z.}∩{o.d.z.}.PROOF.*ThemoduleT(M)/T([yu]R)isdescribedbytheequation∂˘x¯c¯o=A¯c¯o˘x¯c¯owherethecomponentsof˘x¯c¯oarethecanonicalimagesofthecomponentsofx¯c¯o(whichbelongtoT(M))inT(M)/T([yu]R)*7.2.5.Hiddenmodes*Letusbeginbygivinganabstractbut“intrinsic”deﬁnitionofhiddenmodes([19]Deﬁnition16):DEFINITION174.–Themoduleofthehiddenmodes(h.m.)isM/(Φ∩[yu]R).ThehiddenmodesaretheSmithzerosofthismodule.*SystemsTheory(II)193Thehiddenmodesaredirectlyderivedfromthei.d.z.stheo.d.z.sandthei.o.d.z.s.Wehaveindeedthefollowingresultwhereε(.)denotesthesetofelementarydivisors(takingintoaccountmultiplicities)ofthemoduleorofthematrixinparentheses(Lemma559section13.4.2).THEOREM175.–Thefollowingequalityholds:ε(h.m.)=ε(i.d.z.)˙∪ε(o.d.z.)\ε(i.o.d.z.)where˙∪isthe“disjointunion”(section13.4.2Lemma559)andA\BisthecomplementofBinA(whenB⊂A).PROOF.*WehaveaccordingtoTheorem538(section13.4.1)MΦ∩[yu]R=T(M)⊕ΦΦ∩[yu]R∼=T(M)⊕ΦΦ∩[yu]RM[yu]R=T(M)⊕Φ(T(M)∩[yu]R)⊕(Φ∩[yu]R)∼=T(M)T([yu]R)⊕ΦΦ∩[yu]R.(7.23)AccordingtoLemma559(ii)(section13.4.2)weobtainfromtheﬁrstequalityε(h.m.)=ε(i.d.z.)˙∪ε(Φ/Φ∩[yu]R)andfromthesecondequalityε(o.d.z.)=ε(i.o.d.z.)˙∪ε(Φ/Φ∩[yu]R)fromwhichwegetthedesiredresult.*Weimmediatelydeducefromtheabovethefollowingclassicresult:COROLLARY176.–Thefollowingequalityholds:{h.m.}={i.d.z.}˙∪{o.d.z.}\{i.o.d.z.}where{.}denotesthesetofelementsinbrackets(multiplicitiestakenintoaccount).LetΣ={ABCD}beastate-spacesystemandconsideritscanonicalstructuregivenbyTheorem155(section7.1.5).ThefollowingresultisadirectconsequenceofTheorem175:PROPOSITION177.–ThehiddenmodesaretheeigenvaluesofthematrixAc¯o⊕A¯c¯o⊕A¯co(countingmultiplicities).55.Ac¯o⊕A¯c¯o⊕A¯coisa“diagonalsum”ofmatrices(section13.1.4).194LinearSystems7.2.6.RelationshipsbetweenpolesandzerosDenotethesystempolesbys.p.thetransmissionpolesbyt.p.andthetransmissionzerosbyt.z..ThefollowingdeﬁnitionisduetoRosenbrock[101]:DEFINITION178.–Thesystemzeros(s.z.)aredeﬁnedbytheequality{s.z.}={t.z.}˙∪{h.m.}.Thefollowingtheoremdescribesthevariousrelationsamongthedifferentkindsofpolesandzeros;mostofthoseareowedtoRosenbrock[100][101];theﬁrstinclusionofequation(7.25)wasestablishedin[19].THEOREM179.–(i){s.p.}={t.p.}˙∪{h.m.}(7.24){t.z.}˙∪{i.o.d.z.}⊂{i.z.}⊂{s.z.}.(7.25)(ii)Ifthetransfermatrixisleft-regular{t.z.}˙∪{i.d.z.}⊂{i.z.}.(iii)Ifthetransfermatrixisright-regular{t.z.}˙∪{o.d.z.}⊂{i.z.}.(iv)Ifthetransfermatrixissquareandregular{s.z.}={i.z.}.PROOF.*Wearegoingtoprove(i)(fortheotherpointsseeExercise214or[19]).Equality(7.24)isderivedfromTheorem155(sinceaccordingtoTheorem129ofsection7.1.2theproblemcanalwayscomedowntothecasewherethesystemisgivenbyastate–staterepresentation).For(7.25)weobservethatM[y]R∼=T(M)T([y]R)⊕ΦΦ∩[y]RΦΦ∩[y]R⊃Φ∩[yu]RΦ∩[y]RandaccordingtoLemma559(i)(section13.4.2)Z(cid:21)T(M)T([y]R)(cid:22)⊃Z(cid:21)T(M)T([yu]R)(cid:22)fromwhichwededucetheﬁrstinclusion.Accordingtoequation(7.23)oneobtainsM/[yu]R∼=T1whereT1⊃T(M)T([yu]R)⊕T2T2=T(cid:21)ΦΦ∩[yu]R(cid:22)SystemsTheory(II)195andasaresult{o.d.z.}⊃Z(T2)˙∪{i.o.d.z.}.(7.26)OntheotherhandaccordingtoTheorem538(iii)(section13.4.1)andLemma559(i)(section13.4.2)T2∼=T(cid:21)ΦΦ∩[y]R(cid:22)/T(cid:21)Φ∩[yu]RΦ∩[y]R(cid:22)Z(T2)=Z(cid:21)ΦΦ∩[y]R(cid:22)\{t.z.}.LastZ(cid:21)ΦΦ∩[y]R(cid:22)={z.i}\Z(cid:21)T(M)T([y]R)(cid:22)Z(T(M)/T([y]R))⊂{i.d.z.}fromwhichwegetZ(T2)˙∪{t.z.}˙∪{i.d.z.}⊃{z.i}.(7.27)Herethesecondinclusionof(7.25)isaconsequenceof(7.26)(7.27)andofCorollary176.*7.3.StabilitystabilizabilityanddetectabilityThenotionofstabilityessentialtocontroltheorysofarhasbeendeﬁnedonlyforminimalsystems(Deﬁnition61section3.1.1).Inthegeneralcaseoneusesthefollowingdeﬁnition:DEFINITION180.–AcontrolsystemΣ(assumedtobelineartime-invariant)isstableifallitspolesarelocatedinthelefthalf-plane.REMARK181.–Accordingtosection2.3.8acontrolsystemisstableifandonlyifallvariablesofitsfreebehaviortendto0ast→+∞.(Ina“behavioralapproach”[96]thispropertyistakenasdeﬁnitionofstabilityofacontrolsystem;withthisdeﬁnitionthefollowingisatheorem:asystemisstableifandonlyifallitspolesbelongtothelefthalf-plane.6)SupposeΣisastate-spacesystem{ABCD}.ThenthepolesofΣaretheeigenvaluesofA(Theorem18section2.3.7).Wewillthenusethefollowingdeﬁnition:6.Thisremarkappearstobemadebasedonverysubtledistinctions.Theapproachaccordingtowhichalinearsystemischaracterizedbyaﬁnitelypresentedmoduleandthatinwhichsuchasystemischaracterizedbya“behavior”arenotidentical.TheequivalencebetweenthesetwoapproacheswasdeeplystudiedbyOberst[94](formorerecentresultssee[23]and[22]).196LinearSystemsDEFINITION182.–AmatrixA∈Rn×niscalledastabilitymatrixifallitseigenvaluesbelongtothelefthalf-plane.WecancompleteDeﬁnition180bythefollowing:DEFINITION183.–AcontrolsystemΣ(assumedtobelineartime-invariant)ismarginallystableifallitspoleslieintheclosedlefthalf-planethosebelongingtotheimaginaryaxis(ifany)havingalltheirstructuralindicesequalto1.REMARK184.–AccordingtoTheorem23(section3.1.1)acontrolsystemismarginallystableifandonlyifallvariablesofitsfreebehaviorarebounded.(Ina“behavioralapproach”thispropertyisusedasthedeﬁnitionofamarginallystablecontrolsystem).Withthisdeﬁnitionthefollowingisatheorem:acontrolsystemismarginallystableifandonlyifallitspolesbelongtotheclosedlefthalf-planethosethatbelongtotheimaginaryaxis(ifany)havingalltheirstructuralindicesequalto1.)ThenotionsthatfollowwillshowalltheirsigniﬁcancewhenwegettothesubjectofcontrolbystatefeedbackandthetheoryofobserversinChapters8and9respectively.DEFINITION185.–Acontrolsystemisstabilizable(resp.detectable)ifnoneofitsi.d.z.s(resp.itso.d.z.s)belongstotheclosedrighthalf-plane¯C+.WededuceimmediatelyfromPropositions166(section7.2.2)and170(section7.2.4)thefollowingresult:PROPOSITION186.–“Popov–Belevitch–Hautustestforstabilizabilityanddetectability”.LetΣbeasystemdeﬁnedbyaRosenbrockrepresentation{DNQW};thissystemisstabilizable(resp.detectable)ifandonlyifrkC(cid:25)D(s)N(s)(cid:26)=r(resp.rkC(cid:25)DT(s)QT(s)(cid:26)=r)foranys∈¯C+(wherer(cid:1)rkRD(∂)).Inparticularastate-spacesystem{ABCD}isstabilizable(resp.detectable)ifandonlyifrkC(cid:25)sIn−AB(cid:26)=n(resp.rkC(cid:25)sIn−ATCT(cid:26)=n)foralls∈¯C+.ThefollowingisanimmediateconsequenceoftheabovepropositionandisrelatedtoCorollary153(section7.1.4).COROLLARY187.–Stabilizability↔detectabilityduality.(CA)isdetectableifandonlyif(cid:15)ATCT(cid:16)isstabilizable.SystemsTheory(II)1977.4.Realization7.4.1.Introduction“Realization”meansputtingacontrolsystemintoastate-spaceform.Thistermisalittleambiguous:Theorem129(section7.1.2)showshowtoobtainarealizationofageneralcontrolsystem.Veryoftenastate-spacesystemcomesnaturallyfromphysicalequations(seetheexercises).Inthissectionwearegoingtoexaminehowtoobtainarealizationofaleftformaswellasthatofarightform.Thenitbecomeseasytodeterminea“minimalrealization”7fromatransfermatrixbecausesuchamatrixadmitsaleft-coprimefactorization(correspondingtoacontrollableleftform)andaright-coprimefactorization(correspondingtoanobservablerightform):seeRemark31(section2.4.5).Beforedoingsonoticethefollowing:let{ABCD}beastate-spacerepresentationofapropercontrolsystem.ForanyinvertiblematrixP(cid:31)P−1APP−1BCPD isalsoastate-spacerepresentationofthesamecontrolsystem(Remark7.5section7.1.2).ThisbasistransformationmatrixPcanbechosensuchthatP−1APisforexamplearationalcanonicalformofA(section13.4.3)andthereforethereexistsomeformsofrealizationthataremoreremarkablethanothers.Inadditioniftheyhavethepropertyofuniquenesstheyarecalledcanonicalforms.Thelatterhavetheparticularityofhavingthesimplestpossiblestructure.Ontheotherhandwhenthesystemisofahighorderthesecanonicalformsaretobeavoidedbecausetheirstatematrixisingeneralill-conditioned(seesection13.5.7).7.4.2.SISOsystemsAllsystemsconsideredinthisparagraphareSISO.CaseofanobservablesystemConsideracontrolsystemwhichisobservableandwhichcanthusbedescribedbytheleftform(2.17)(section2.3.5)accordingtoTheorem147(section7.1.4).AssumingthatthissystemisproperweobtainD(∂)y=N(∂)u(7.28)D(∂)=∂n+a1∂n−1+...+an(7.29)N(∂)=b0∂n+b1∂n−1+...+bn.(7.30)PerformingtheEuclideandivisionofN(∂)byD(∂)yieldsN(∂)=b0D(∂)+N(cid:2)(∂)7.Thatisarealizationintheformofaminimalstate-statesysteminthesenseofDeﬁnition33(section2.4.6).198LinearSystems1bnbna1aunxnx2x2x1x1xyFigure7.5.Diagramofanobservablecanonicalformwhered◦(N(cid:2))<d◦(D).Letξ=y−b0u;thenD(∂)ξ=N(cid:2)(∂)u(7.31)y=ξ+b0u.Itnowsufﬁcestodeterminearealizationofastrictlyproperleftform(7.31).Inordertosimplifythecalculationsweassumeinwhatfollowswithoutlossofgeneralitythatthesystemconsideredisstrictlyproper.Onecanproceedasinsection12.5.1:theleftform(7.28)(7.29)(7.30)(withb0=0)isidenticaltothelineardifferentialequationwithconstantcoefﬁcients(12.97).Deﬁningthestatesx1...xnbytherelations(12.98)(12.99)oneobtainsthesystem(12.100)(12.101)i.e.˙x=⎡⎢⎢⎢⎢⎢⎢⎢⎣−a110···0−a20............0......0.........01−an00···0⎤⎥⎥⎥⎥⎥⎥⎥⎦x+⎡⎢⎢⎢⎢⎢⎢⎢⎣b1.........bn⎤⎥⎥⎥⎥⎥⎥⎥⎦u(7.32)y=(cid:25)10······0(cid:26)x.(7.33)ThesestateequationscanberepresentedbythediagraminFigure7.5thestatesbeingtheoutputsoftheintegrators.SystemsTheory(II)199Notethatthestate-spaceform(7.32)(7.33)isonlyanotherwayofwritingtheleftform(7.28).Asthisisobservable(Theorem147section7.1.4)soisthestate-spacesystem(7.32)(7.33)(ascanbeshownusingtheKalmancriterion(Theorem1507.1.4).Anotherremarkablepropertyofthestate-spaceform(7.32)(7.33)isitssimplicity:thisrealization{AoBoCo}issuchthatAoisacompanionmatrixofthepolynomialD(∂)BohascoefﬁcientssameasthoseofthepolynomialN(∂)andthematrixCoexpressestherelationy=x1.Itisa“canonicalform”inthesensespeciﬁedinsection7.4.1.DEFINITION188.–Therealization(7.32)(7.33)iscalledtheobservablecanonicalformassociatedwiththeleftform(7.28).AccordingtotheremarksprecedingDeﬁnition188andTheorem138(section7.1.3)wehavethefollowing:PROPOSITION189.–Theobservablecanonicalform(7.32)(7.33)isalwaysobservable;itiscontrollableifandonlyifthepolynomials{DN}arecoprime.CaseofacontrollablesystemConsideracontrolsystemwhichiscontrollableandthuscanbedescribedbytherightform(2.19)(section2.3.5)accordingtoTheorem137(section7.1.3)andsupposethissystemisstrictlyproper.Wethushavetheequations(cid:20)y=N(∂)ξu=D(∂)ξ(7.34)D(∂)=∂n+a1∂n−1+...+an(7.35)N(∂)=b1∂n−1+...+bn.(7.36)Wecanwritetheﬁrstlineof(7.34)explicitlyas:∂nξ=−(cid:15)a1∂n−1+...+an(cid:16)ξ+u.Letx=(cid:25)∂n−1ξ...ξ(cid:26)T;thenitfollowsthat˙x=⎡⎢⎢⎢⎢⎢⎢⎣−a1−a2······−an10···0001..................0...00010⎤⎥⎥⎥⎥⎥⎥⎦x+⎡⎢⎢⎢⎢⎢⎢⎣10......0⎤⎥⎥⎥⎥⎥⎥⎦u(7.37)y=(cid:25)b1b2······bn(cid:26)x.(7.38)200LinearSystems1bna1au1na1nn1x2xnx1nbnbyFigure7.6.DiagramofacontrollablecanonicalformTheseequationswhicharethoseofarealization{AcBcCc}canberepresentedbythediagraminFigure7.6below;therealization{AcBcCc}isveryspecialliketheobservablecanonicalformdescribedabove:AcisacompanionmatrixofthepolynomialD(∂)thecoefﬁcientsofCcarethoseofthepolynomialN(∂)andBchasastructurethatcannotbefurthersimpliﬁed.Sinceeveryrightformiscontrollableandequations(7.37)and(7.38)representonlyarewriting(inadifferentformalism)ofarightformthestate-spacesystem{AcBcCc}isnecessarilycontrollable.DEFINITION190.–Therealization{AcBcCc}iscalledthecontrollablecanonicalformassociatedwiththerightform(7.34).ThefollowingpropositionisobtainedbyasimilarreasoningthatleadstoProposition189.PROPOSITION191.–Thecontrollablecanonicalform{AcBcCc}isalwayscontrollable;itisobservableifandonlyifthepolynomials{DN}arecoprime.Wecanverifythecontrollabilityof(AcBc)usingtheKalmancriterion(Theorem141section7.1.3).ThecontrollabilitymatrixΓ(AcBc)(seeRemark142)isformedbyiterationinthefollowingmanner:itsﬁrstcolumnisBcandits(i+1)thcolumn(1≤i≤n−1)isequaltoitsithcolumnleft-multipliedbyAc.REMARK192.–Oneshouldalwaysfollowtheaboveproceduretocalculatethecontrollabilitymatrix.Thisavoidsneedlesslycalculatingthepowersofthestatematrixwhichisverycumbersome.SystemsTheory(II)201FromtheabovemethodweobtainacontrollabilitymatrixoftheformΓc=⎡⎢⎢⎢⎢⎣1∗...∗01...............∗0...01⎤⎥⎥⎥⎥⎦whichisobviouslyofrankn.MorepreciselyarathersimplecalculationshowsthatΓcA+=InwhereA+istheuppertriangularToeplitzmatrixA+=⎡⎢⎢⎢⎢⎣1a1···an−10............a10···01⎤⎥⎥⎥⎥⎦.AsaresultΓc=A−1+.(7.39)OtherrealizationsofacontrollablesystemTherealizationsoftherightform(7.34)arealloftheform{ABC}=(cid:31)PAcP−1PBcCcP−1 accordingtoRemark132(section7.1.3)byusingachangeofbasismatrixP−1.AccordingtoRemark142(ii)wehaveΓ(AB)=PΓcandthereforeaccordingtoequation(7.39)P=Γ(AB)A+.(7.40)Itfollowsfromtheabovethatthereexistsauniquerealization{AccBccCcc}ofequation(7.34)suchthatΓ(AccBcc)=In.Indeed{AccBccCcc}=(cid:31)A+AcA−1+A+BcCcA−1+ .(7.41)LetAcc=⎡⎢⎢⎢⎢⎢⎢⎣00···0−an10···0001..................0−a20001−a1⎤⎥⎥⎥⎥⎥⎥⎦Bcc=⎡⎢⎢⎢⎢⎢⎢⎣10......0⎤⎥⎥⎥⎥⎥⎥⎦(7.42)Ccc=(cid:25)b1b2······bn(cid:26)A−1+.(7.43)OnecaneasilyverifythatA+Ac=AccA+andthusrelation(7.41)issatisﬁed.202LinearSystemsDEFINITION193.–Therealization{AccBccCcc}iscalledthecanonicalformofcontrollability.Gatheringtheresultsobtainedwehavethefollowingtheorem:THEOREM194.–(i)Let{ABC}beanyrealization(withstatex)oftherightform(7.34).Thestatexcofthecontrollablecanonicalform{AcBcCc}isdeterminedbytherelationx=PxcwherethechangeofbasismatrixPsatisﬁesequation(7.40).(ii)Thecanonicalformofcontrollability{AccBccCcc}istheuniquerealizationoftherightform(7.34)thecontrollabilitymatrixofwhichisIn.Itrelatestothecontrollablecanonicalformthroughtherelation(7.41)andisgivenbyequations(7.42)and(7.43);AccisacompanionmatrixofthepolynomialD(∂)(section13.4.3).DualitybetweencanonicalformsWehavethefollowing“dualityrelations”betweentheobservableandcontrollablecanonicalforms(theserelationsarelinkedtoCorollary153section7.1.4):Ac=AToBc=CToCc=BTo.(7.44)Wewouldliketoemphasizethefactthattheobservability↔controllabilitydualityandthustheobservablecanonicalform↔controllablecanonicalformdualityisstrictlyformalinthefollowingsense:letaphysicalsystembedescribedbyanobservablecanonicalform(thiscanbeforexampletheRLCcircuitinFigure1.1section1.1.1describedbytheleftform(2.12)ofsection2.3.3becausesuchadescriptionisequivalenttoanobservablecanonicalform);the“dualized”systemaccordingtotheformulas(7.44)doesnotingeneralcorrespondtoanyphysicalreality.7.4.3.*MIMOsystemsWewillnowdealwiththesubjectofMIMOsystemswherethecanonicalformsarenumerousandmorecomplicatedthanintheSISOcase[64].RealizationofacontrollablesystemInthefollowingonlythecaseofacontrollablesystemisconsideredbecausetherealizationofanobservablesystemcanbederivedbyduality(Corollary153section7.1.4).Wecanassumethesystemisdescribedbytherightform(2.19)(section2.3.5):(cid:20)u=D(∂)ξy=Q(∂)ξ(7.45)whereD(∂)∈Rr×risamatrixofrankr.ThereexistmatricesU(∂)andV(∂)invertibleoverRsuchthat∆=U−1DVisadiagonalmatrixdiag{β1...βr}SystemsTheory(II)203accordingtoTheorem497(section13.2.3)where0(cid:5)=βi∈Risamonicpolynomialforeveryindexi.Thematrix∆canbetheSmithformofDbutasshownbytheproofoftheaforementionedtheoremobtainingthediagonalformwithoutdivisibilityconditionontheelementsβirequireslesscalculation.Let=V−1(∂)ξv=U−1(∂)uand˜Q=QV.Relations(7.45)canbeputintheform(cid:20)v=∆(∂)y=˜Q(∂).(7.46)Hereasseenaboveisacolumnvectorwithrentriesi(1≤i≤r).Supposethatinthelist(βi)1≤i≤rtheﬁrstjelementsandonlythoseareequalto1.Forj+1≤i≤rwewritethepolynomialβiintheformβi(∂)=∂ni+βi1∂ni−1+...+βini.OntheotherhandputtingXi=⎡⎢⎢⎢⎣∂ni−1...∂1⎤⎥⎥⎥⎦j+i1≤i≤r−jweobtainaccordingtotheﬁrstrowofequation(7.46)∂Xi=C(βi)Xi+1nivj+i1≤i≤r−jwhereC(βi)isthecompanionmatrixofthepolynomialβihavingthestructureofthestatematrixofacanonicalcontrollableform(seeequation(7.37)section7.4.2)andwhere1niisacolumnmatrixwithnirowswhoseonlynon-zeroentryistheﬁrstonewhichisequalto1.SettingX=⎡⎢⎣X1...Xr−j⎤⎥⎦˜v=⎡⎢⎣vj+1...vr⎤⎥⎦oneobtainstheequation∂X=AX+diag{1ni}1≤i≤r−j˜vwithA=diag{C(βi)}1≤i≤r−j.204LinearSystemsWritingv=U−1(∂)uweobtainanexpressionoftheform(7.1)(section7.1.2)withX=η.ThereonlyremainstoimplementtheprocedurealreadyusedintheproofofTheorem129(section7.1.2)inordertogetthestateequation(7.2)(section7.1.2).Finallytheoutputy=˜Q(∂)iswrittenintheformofequation(7.3)(section7.1.2)takingintoaccounttheequalitiesi=vi1≤i≤j.EXAMPLE195.–ConsiderthetransfermatrixG(s)=#s(s+1)2−s2(s+1)2(s−1)1(s+1)2s2+s+1(s+1)2(s−1)$.ProceedingasinRemark31(ii)(section2.4.5)weobtaintheright-coprimefactorization(Q(s)D(s))ofG(s)withQ(s)=(cid:27)s011(cid:28)D(s)=(cid:27)(s+1)2s0s−1(cid:28).TheSmithformofD(s)isΣ=U−1DV=diag(cid:7)1(s+1)2(s−1)(cid:8)withU−1(s)=(cid:27)1−11−ss(cid:28)V=(cid:27)0−11(s+1)2(cid:28)whichwecanobtainbyusingthemethodof“markingoutelementaryoperations”(seesection13.2.3).ThestepsofobtainingaminimalrealizationofG(s)arenowasfollows:(i)Writetheequality(cid:27)v1v2(cid:28)=U−1(∂)(cid:27)u1u2(cid:28)=(cid:27)u1−u2u1−∂(u1−u2)(cid:28).(7.47)(ii)Accordingtotheﬁrstlineofequation(7.46)onegetsv1=1and(∂+1)2(∂−1)2=v2.ThislastequalityleadsustoputX=(cid:25)∂22∂22(cid:26)Tandweget∂X=AX+13v2withA=⎡⎣−111100010⎤⎦.(7.48)Accordingtoequation(7.47)∂X=AX+B0u+B1∂uwhereB0=⎡⎣100000⎤⎦B1=⎡⎣−110000⎤⎦.SystemsTheory(II)205(iii)FollowingthemethodusedintheproofofTheorem129letusputx=X−B1u.Weget∂x=Ax+BuwithB=AB1+B0i.e.B=⎡⎣2−1−1100⎤⎦.(7.49)(iv)Attheendy=˜Q(∂)with˜Q(∂)=Q(∂)V=(cid:27)∂011(cid:28)(cid:27)0−11(∂+1)2(cid:28)=(cid:27)0−∂1∂2+2∂(cid:28).Thereforey1=−x2andy2=1+∂22+2∂2=u1−u2+(x1−u1+u2)+2x2fromwhichwehavey=CxwithC=(cid:27)0−10120(cid:28).(7.50)ControllabilityindicesThankstotheaboveprocedureweobtainarealizationwhosestatematrixAhasquiteaparticularform(andisactuallyinarationalcanonicalformlikeinExample195)buttheBmatrixdoesnothaveaprivilegedstructure(seeequation(7.49)).Theformobtainedisthereforenotcanonical.Wenowaregoingtoexaminehowwecangofromanyrealizationtoa“canonicalformofcontrollability”.Let{ABC}beastate-spacesystemofordernandbi(1≤i≤m)betheithcolumnofB∈Rn×m.TraversethecontrollabilitymatrixΓ(AB)fromlefttorightandeliminateasonegoesalongthecolumnsthatarelinearlydependentontheprecedingones.Byre-arrangingtheremainingcolumnsweobtainamatrixoftheform(cid:25)b1...Ak1−1b1b2...Ak2−1b2...bm...Akm−1bm(cid:26)usingtheconventionthatAjbiistheemptycolumnifj<0.DEFINITION196.–Thenaturalintegerski1≤i≤marethecontrollabilityindicesofthestate-spacesystem{ABC}.REMARK197.–ThereexistsazerocontrollabilityindexkiifandonlyifΓ(AB)doesnotcontainanytermthatisdependentonbi.AsaresultifthematrixBisright-regular(i.e.rkB=m)allthekisarepositive.DEFINITION198.–[44]Thestate-spacesystem{ABC}whereA∈Rn×nB∈Rm×nandC∈Rp×nissaidtobewell-formedifrkB=mandrkC=p.88.InthecitedreferenceonlythematrixBisconsideredforthisdeﬁnition.206LinearSystemsInthefollowingthestate-spacesystemisassumedtobewell-formed.REMARK199.–Theabovehypothesisdoesnotgenerateanylossofgeneralityinreality.IndeedsupposethatrkB<mandrkC=pandtoclarifyideasletm=3andrkB=2.OneofthecolumnsofBb3forexampleisthusanR-linearcombinationoftheothercolumnsandsothereexistrealnumbersλ1andλ2suchthatb3=λ1b1+λ2b2.ConsequentlyBu=b1(u1+λ1u3)+b2(u2+λ2u3).Substitutingu(cid:2)1=u1+λ1u3andu(cid:2)2=u2+λ2u3oneisledtothecaseofawell-formedstate-spacesystemwithtwoinputsu(cid:2)1andu(cid:2)2.AsimilarreasoningcanjustifythatthematrixCcanalwaysbeassumedtobeofrankp.Seein[44]complementaryinterpretationsofthehypothesisthatrkB=m.REMARK200.–Assumingthatastate-spacesystemiswell-formedthereexistsapermutationσof{1...m}suchthatµ1(cid:1)kσ(1)≥µ2(cid:1)kσ(2)≥...≥µm(cid:1)kσ(m)≥1;σcorrespondstoaright-multiplicationofBbyapermutationmatrixQcc∈GLm(R).Theintegers{µ1...µm}areagainthecontrollabilityindicesbutarrangedina“canonicalorder”withwhichthematrixPcc=(cid:25)b(cid:2)1...Aµ1−1b(cid:2)1...b(cid:2)m...Aµm−1b(cid:2)m(cid:26)(7.51)isassociatedwhereb(cid:2)i=bσ(i).Inwhatfollowswewillcalltheﬁnitesequence(µi)1≤i≤mthelistofcontrollabilityindices.Thislistisinvariantunderchangeofbasisinthestatespaceandpermutationofsysteminputs.Theproofofthetheorembelowisdeducedinanobviousmannerfromtheconstructionofthelistofcontrollabilityindices(µi)1≤i≤minRemark200andfromTheorem141(section7.1.3):THEOREM201.–(i)Thelargestcontrollabilityindexµ1isthesmallestintegerµsuchthatρ(µ)(cid:1)rk(cid:25)BAB...Aµ−1B(cid:26)isstationaryforµ≥µ1.(ii)Wehaveρ(µ1)=(cid:11)1≤i≤mµiandthesystemiscontrollableifandonlyifρ(µ1)=n.Thereadercanﬁnda“geometricalinterpretation”ofthecontrollabilityindicesin([119]section5.7).CanonicalformofcontrollabilityDEFINITION202.–Thecanonicalformofcontrollability(withinputu(cid:2)=Q−1ccu)ofthewell-formedcontrollablestate-spacesystem{ABC}is{AccBccCcc}whereAcc=P−1ccAPccBcc=P−1ccBQccCcc=CPcc.(7.52)SystemsTheory(II)207Thisformisthefollowingwhenm=3and{µ1µ2µ3}={432}([64]section6.4):Acc=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣0∗∗∗1∗∗∗1∗∗∗1∗∗∗∗0∗∗∗1∗∗∗1∗∗∗∗0∗∗∗1∗⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦Bcc=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣100010010⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦andthematrixCccdoesnothaveapriorianyparticularform.Theunmarkedentriescontainazeroandthestarsdesignatepossiblynon-zeroelements.ItiseasytoshowthatΓ(AccBcc)=In;thisformisthusageneralizationtotheMIMOcaseofthecanonicalformofcontrollability(7.42)(section7.4.2).EXAMPLE203.–ConsideragainExample195;wehavePcc=(cid:25)b1Ab1b2(cid:26)thusthecontrollabilityindicesare{µ1µ2}={21}.ExplicitlyPcc=⎡⎣2−3−1−1210−10⎤⎦andbyapplying(7.52)wededucethefollowing:Acc=⎡⎣0−101−2−1001⎤⎦Bcc=⎡⎣100001⎤⎦Ccc=(cid:27)1−2−1011(cid:28).ControllablecanonicalformInwhatfollowsthecontrollablecanonicalformbecomesmoreusefulthanthecanonicalformofcontrollabilitybutitisalittlemoredifﬁculttoobtainintheMIMOcase(contrarytotheSISOcase).ThefollowingconstructionisduetoPopov[97](seealso[26]Chapter7).Consideragainawell-formedstate-spacesystem{ABC}.208LinearSystemsConsidertheinvertiblematrixPccgivenby(7.51).LetM=P−1ccandwewrite(assumingclarityofideasthatm=3)M=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣e11...e1µ1e21...e2µ2e31...e3µ3⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦.LetPcbethechangeofbasismatrixdeﬁnedbyP−1c=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣e1µ1Aµ1−1...e1µ1e2µ2Aµ2−1...e2µ2e3µ3Aµ3−1...e3µ3⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦.(7.53)Let(cid:31)P−1cAPcP−1cBQccCPc ={AcB(cid:2)cCc}(whereQccisthepermutationmatrixdeﬁnedinRemark200).ThematricesAcandB(cid:2)chavethefollowingstructures(form=3and{µ1µ2µ3}={432}):Ac=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣∗∗∗∗∗∗∗∗∗11100∗∗∗∗∗∗∗∗∗110∗∗∗∗∗∗∗∗∗1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦B(cid:2)c=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣1b1b200000000001b3000000001000⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(7.54)wheretheunmarkedentriesarezeroandthematrixCcdoesnothaveapriorianyparticularform.SystemsTheory(II)209NowletˇB=⎡⎣1b1b201b3001⎤⎦υ=˘Bu(cid:2)=˘BQccu(7.55)whereQccisthepermutationmatrixdeﬁnedinRemark200andu(cid:2)isasinDeﬁnition202.Weobtain(cid:7)P−1cAPcP−1cBQcc˘B−1CPc(cid:8)={AcBcCc}whereAcisgivenbyequation(7.54)andBchasthesamestructureasB(cid:2)cbutwithalltheentriesbi(1≤i≤3)equaltozeroandwhereCcdoesnothaveapriorianyparticularstructure.DEFINITION204.–{AcBcCc}isthecontrollablecanonicalform(withinputυ=˘BQ−1ccu)ofthewell-formedstate-spacesystem{ABC}.EXAMPLE205.–LetuscontinuewithExample195.Thechangeofbasismatrixgivenbyequation(7.53)isPc=⎡⎣211−1000−10⎤⎦(7.56)andweobtainwithQcc=I3Ac=⎡⎣−2−1−1100001⎤⎦B(cid:2)c=⎡⎣1−10001⎤⎦.(7.57)Cc=(cid:27)100011(cid:28).(7.58)Asaresult˘B=(cid:27)1−101(cid:28)Bc=⎡⎣100001⎤⎦.(7.59)7.5.FlatnessThenotionofaﬂatsystemwasconceivedbyFliessetal.[48]intheframeworkofnonlinearsystems;itsolvestheproblemofmotionplanningwhichisthedeterminationofanopen-loopcontrolthatwhenappliedtoasystemintheabsenceofanydisturbanceandmodelingerrorhastheeffectofhavingthissystemfollowanappropriatetrajectory.210LinearSystems7.5.1.*FlatnessofnonlinearsystemsLetΣbeanonlinearsystemdeﬁnedbyequation(2.4)section2.2.2forexamplewhereeachcomponentofFisavector-valuedrationalfunctionwithcoefﬁcientsinadifferentialﬁeldK.DEFINITION206.–Avariableσ=(σ1...σm)iscalledaﬂatoutputofΣif:(i)thecomponentsσiofσcanbeexpressedrationallyoverKasafunctionofthevariableswjofΣ(1≤j≤k)andaﬁnitenumberoftheirderivatives;(ii)converselyallthevariableswjofΣcanbeexpressedrationallyoverKasafunctionofthecomponentsσiofσandaﬁnitenumberoftheirderivatives;(iii)σisanindependentvariable(section2.3.1).Aﬂatsystemisasystemthathasaﬂatoutput.7.5.2.FlatnessoflinearsystemsLetΣbealinearsystemassociatedwithaﬁnitelypresentedR-moduleMwhereR=K[∂](withK=RifinadditionΣistime-invariant).Thenwecanreplacetheadverb“rationally”everywhereinDeﬁnition206by“linearly”;thetheorembelowfollowsfromRemark134(section7.1.3):THEOREM207.–AﬂatoutputofalinearsystemΣisabasisofitsassociatedmoduleM.Alinearsystemisﬂatifandonlyifitiscontrollable.SystematicdeterminationofaﬂatoutputLetΣbethelinearcontrollablesystemdeﬁnedbyequation(2.6)(section2.2.5)whereE(∂)∈Rr×kisleft-regular(i.e.ofrankr).SincethemoduleMisfreetheSmithformofE(∂)is(cid:25)Ir0m×r(cid:26)(seesection13.4.2)wherer=k−mistherankofthemoduleM.AsaresultthereexistmatricesU(∂)andV(∂)invertibleoverRsuchthatU−1(∂)E(∂)V(∂)=(cid:25)Ir0m×r(cid:26).LetV−1(∂)w=(cid:27)ςσ(cid:28)whereςandσhaverespectivelyrandmcomponents.Thenequation(2.6)isequivalenttoς=0andσisabasisofM;thereforeσisaﬂatoutputofΣ.“Natural”ﬂatoutputsAcontrollablesystemoftenhasaﬂatoutputwhosephysicalsigniﬁcanceisclear.Considerforexamplethelinearizedinvertedpendulum(2.7)(section2.2.7).Theabscissay1=y+lθofthemassmisaﬂatoutput.IndeedfromthesecondequalitySystemsTheory(II)211ofequation(2.7)wegetθ=¨y1/gandthereforey=y1−¨y1l/g.Fromtheﬁrstequalityof(2.7)weobtainf=¨y1(M+m)−y(4)1Ml/g.(7.60)Thevariabley1isabasisofthemoduleMsince(i)itisR-linearlyindependent(whichistrivialinthepresentcasebecauseithasonlyonecomponentthatisnotatorsionelement)(ii)alltheelementsofMareR-linearcombinationsofy1.Fromaphysicalpointofviewy1isthequantitythatprovidesthemostinformationaboutthesystem(ajugglerwhowouldliketobalancetheinvertedpendulumthroughsmallmovementsofthecarriagewouldhavehis/hereyesﬁxedonmassmthroughouthis/hernumber).Notethatthenonlinearinvertedpendulumisnotﬂat.ApplicationtomotionplanningSupposenowwewouldliketomovetheinvertedpendulumfromoneequilibriumpointtoanotherbetweentheinstantst0andt1>t0whileychangesfromvaluey(t0)=0toy(t1)>0;supposealsothatwewouldliketorealizethismovementbyimposingthattheabscissay1bestrictlyincreasing(andthuswithoutanyoscillationofthemassm).Theequilibriumconditionsexpressedasafunctionofthevariabley1canbewrittenfori∈{01}as:y1(ti)=y(ti)y(β)1(ti)=0(1≤β≤4).ThesecanbesatisﬁedifwechooseapolynomialP(t)ofdegree9andappropriatecoefﬁcientsforthetrajectoryt(cid:3)→y1(t)(followingHermite’sinterpolationmethod:seeExercise223section7.6).Theopen-loopcontroltobeappliedtothesystemisthenobtainedbyequation(7.60).(Inthepresentcasethesystembeingunstableitisalsonecessarytostabilizeitbyfeedbackaroundthe“nominaltrajectory”asdeﬁnedhere.)Thetrajectoryoftheﬂatoutputwitht0=0t1=1y(0)=0andy(1)=1isrepresentedinFigure7.7.7.6.ExercisesEXERCISE208.–ConsidertheDCmotordeﬁnedbyequation(2.14)(section2.3.3).(i)Putthissysteminstate-spaceformusinginputu=Vstatex=(cid:25)θωi(cid:26)Tandoutputofy=θ.(ii)Showthatthissystemiscontrollableandobservable.EXERCISE209.–(i)Fromtheequationsofthelinearizedinvertedpendulum((2.7)section2.2.7)determineastate-spacerealizationofthatsystembychoosingstatex=(cid:25)˙ylθl˙θy(cid:26)Tandinputu=f/M.Intheremainingpartofthisexerciseweputσ=%glandε=mMandweassumethat0<ε<1.(ii)Determinethepolesofthissystem;isthissystemstable?(iii)Showthatthissystemiscontrollable.212LinearSystems00.20.40.60.8100.20.40.60.81Time (s)Flat outputFigure7.7.Trajectoryoftheﬂatoutput(iv)SupposingthattheoutputisanR-linearcombinationofx1x2andx3isthesystemobservable?Isitdetectable?(v)Whatiftheoutputisx4?(vi)Ifx4istakenasoutputdeterminethetransmissionzerosofthesystem.Whathappenswhenε(cid:16)1?EXERCISE210.–Considerthetraindeﬁnedbyequations(1.19)(section1.11).(i)Putthissysteminastate-spaceformwithx=(cid:25)˙z1˙z2z1z2(cid:26)Tλ=km1ρ=m1m2andcontrolu=fm1.(ii)Usingthisstate-spacerepresentationdeterminethepolesofthesystem.Istherecoherencewiththeexpression(2.13)(section2.3.3)?(iii)Isthesystemcontrollable?(iv)Ifonechoosesx2asoutputisthesystemobservable?Howwouldyouinterpretthis?(v)Determinethehiddenmodeswhenx2istheoutput.(vi)Whathappensifwechoosex4astheoutput?(vii)Withthelastchoicedoesthecontrolsystemhavetransmissionzeros9?EXERCISE211.–ConsidertheRLCcircuitdescribedbytherightform(2.18)(section2.3.5).(i)Putthissystemincontrollablecanonicalform.(ii)Calculateitsobservabilitymatrix.Whathappensifthecapacitorhasacapacitancethattendsto+∞?Interpretation?EXERCISE212.–ConsiderthelinearizedinverteddoublependulumwithequationsdeterminedinExercise56(Section2.7).(i)Showthatthissystemadmitsastate-spacerepresentationwithstatex=(cid:25)θ1θ2˙θ1˙θ2(cid:26)Tandinputu=−fM.(ii)Whatarethenecessaryandsufﬁcientconditionsonthelengthsl1andl2forthesystemtobecontrollable?(iii)Choosingx1asoutputwillthesystembeobservable?9.Asmentionedinthebeginningofsection7.2weareonlyconcernedwithﬁnitetransmissionzeros.SystemsTheory(II)213EXERCISE213.–Let{ABC}beastate-spacesystemwithA=(cid:27)0010(cid:28)B=(cid:27)10(cid:28)C=(cid:25)10(cid:26).NoticingthatthissystemalreadyhasthecanonicalstructureinTheorem155(section7.1.5)showthat(Ac¯oBc¯o)isnotcontrollable.EXERCISE214.–*WecallLaplacefunctor[45]thefunctorK⊗R−whereKistheﬁeldoffractionsofR(seesection13.6.5).LetybeacolumnvectorwhoseentriesyibelongtoanR-moduleM;weseethatˆyiistheimageofyiunderthecanonicalhomomorphism(i.e.ˆyi=11yi)andthatˆyisthecolumnvectorconstitutedoftheˆyis.(a)ConsideralinearcontrolsystemassociatedwithaﬁnitelypresentedR-moduleMwithinputuandoutputyhavingmandpcomponentsrespectivelywithtransfermatrixG.Showthatˆy=Gˆu.(b)IfrkKG=pusingresultsfromthelastpartofsection13.6.5showthat[y]RisafreemoduleanddeducePart(ii)ofTheorem179(byadaptationoftheproofofPart(i)).(c)IfrkKG=mshowthatthemoduleM/[y]RistorsionandbyasimilarapproachashereaboveprovePart(iii)ofTheorem179.(d)FinallyprovePart(iv)ofthistheorem.EXERCISE215.–*LetRbeanintegraldomainandlet(Muy)beacontrolsystemwhoseringofoperatorsisR.(i)UsingtheLaplacefunctor(seeExercise214)deﬁnethetransfermatrixofthissystem(using(13.66)).(ii)AssumingthatRisacommutativeelementarydivisorring(seesection13.2.3)deﬁnetheSmith–MacMillanformofatransfermatrixthusgeneralizingDeﬁnition29.EXERCISE216.–(i)Considerthestate-spacesystem˙x1=u1˙x2=x2+u2y=x1.Calculateitso.d.z.sanditsi.z.s.(ii)Considerthestate-spacesystem˙x1=u˙x2=x2y1=x1y2=x2.Calculateitsi.d.z.sanditsi.z.s.(iii)IstherecoherencewithTheorem179?EXERCISE217.–LettherebethetransfermatrixG(s)=(cid:27)1s+11s+21s+31s+4(cid:28).(i)Showthat(Q(s)D(s))isaright-coprimefactorizationofG(s)withQ(s)=(cid:27)s+3s+4s+1s+2(cid:28)D(s)=(cid:27)s2+4s+300s2+6s+8(cid:28).(ii)DeduceaminimalrealizationofG(s)incontrollablecanonicalform.Isitunique?Ifnotwhy?214LinearSystemsEXERCISE218.–(i)AnalogoustocontrollabilityindicesdeﬁnetheobservabilityindicesofanMIMOstatesystem{ABC}.(ii)Assumingthatthissystemiswell-formeddescribeitscanonicalformofobservability{AooBooCoo}andtheobservablecanonicalform{AoBoCo}.(iii)Howdoesonegofrom{ABC}to{AooBooCoo}andthento{AoBoCo}?EXERCISE219.–LettherebethetransfermatrixG(s)=#s(s+1)2(s+2)2−s(s+2)2s(s+2)2−s(s+2)2$.(i)Showthat(D(s)N(s))isaleft-coprimefactorizationofG(s)withD(s)=(cid:27)0(s+2)2−(s+1)2(s+2)s+2(cid:28)N(s)=(cid:27)s−s0s2(cid:28).(ii)DeduceaminimalrealizationofG(s)inobservablecanonicalform(seeExercise218).Isitunique?EXERCISE220.–LetΣ={ABC}bethestate-spacesystemwithA=⎡⎣−1001−301−41⎤⎦B=⎡⎣010⎤⎦C=(cid:27)100−110(cid:28).(i)StudythecontrollabilityandtheobservabilityofΣ.(ii)Determinethepolesofthissystem.(iii)Calculate(ifany)itsi.d.z.sitso.d.z.sitsi.o.d.z.sanditshiddenmodes.Deduceitstransmissionpoles.(iv)IsΣstabilizable?detectable?(v)Calculateitsinvariantzeros.EXERCISE221.–SamequestionsasinExercise220whenA=⎡⎣−100110000⎤⎦B=⎡⎣001⎤⎦C=(cid:25)−101(cid:26).(beforeansweringQuestion(iii)calculatethetransferfunctionof{ABC}).EXERCISE222.–(i)Showthatastate-spacesystemΣ={ABCD}iscontrollableifandonlyifthereisnolefteigenvectorofAthatwillannihilateBi.e.arowvectorvT(cid:5)=0suchthatvTA=λAandvTB=0.(ii)Stateacriterionofthesametypeforobservability.SystemsTheory(II)215EXERCISE223.–Lettherebetwodistinctrealnumbersy0andy1andtwoinstantst0andt1=t0+∆∆>0.ShowthatthereexistsauniquepolynomialP(t)ofdegree2n+1thatsatisﬁesthefollowingconditions:P(ti)=yiP(β)(ti)=01≤β≤ni∈{01}.Findthesystemoflinearequationsthatwilldeterminethecoefﬁcientsofthispolynomial.(“Hermite’sinterpolation”.)EXERCISE224.–(i)Showthatz2is“naturalﬂatoutput”ofthetraindeﬁnedbyequations(1.19)(section1.11).(ii)Wewishtotransferthistrainfromoneequilibriumpositiontoanotherbetweeninstantst0andt1>t0whilez2movesfromvaluez2(t0)=z20toz2(t1)=z21(cid:5)=z20.Writedowntheequilibriumconditionsasafunctionoftheﬂatoutputz2.Chapter8StateFeedbackThestate-spaceformalismisveryusefulinprovidingbothasimpleandcompletesystemrepresentation.Thistypeofrepresentationisindeedsimplerthana“Rosenbrockrepresentation”:seesection2.3.6.Ontheotherhandwithinthisformalismacompletedescriptionofthesystemispossible(ifthelatter’s“structureatinﬁnity”[22]isleftaside).Inadditionhiddenmodesarenotoverlookedunliketherepresentationbyatransfermatrix(Proposition157section7.1.5).Butthestate-spaceformalismisespeciallyinterestinginthatitisparticularlywelladaptedtosolvingcontrolproblems.Thisisthesubjectofthisandthefollowingchapters.Firstwewillstudythecontrolbyan“elementary”statefeedback.Thisessentialpartshedsnewlightonthenotionsofcontrollabilityandstabilizability.Butthisisonlyapreliminarystep.Wewillthenexplainhowstatefeedbackcanbeusedinpracticetosolvecontrolproblems.8.1.Elementarystatefeedback8.1.1.GeneralprincipleLettherebeastate-spacesystem˙x=Ax+Bux(t)∈Rnu(t)∈Rm(8.1)y=Cxy(t)∈Rp.(8.2)Inordertocontrolthissystembystatefeedbackwewillassumeintherestofthischapterthatallthestatecomponentsaremeasured.Thishypothesisisratherstrict217Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.218LinearSystems(andwewillseeinthenextchapterhowtodowithoutit)butthereareinpracticequiteanumberofcaseswherethisissatisﬁed.Letusconsiderthesimplestexamplethatwehaveencounteredsofar:thatofthecarriage(Example133section7.1.2).Thestateofthissystemconsistsofitspositionandvelocity;inordertocontrolthissystemusingstatefeedbackwewillassumethatitisequippedwithapositionsensor(whichisnecessaryanywayifwewanttocontrolthisposition)andaspeedsensor.Wemayhoweverdispensewithaspeedsensorbyestimatingthespeedvusingthepositiony.Wehavev=˙yandhenceinpassingintotheLaplacedomainˆv(s)=sˆy(s)ify(0−)=0.Thederivativeoperatorhastransferfunctionswhichisimproperbutonecanreplaceitwithaﬁlteredderivativeoperatorwithtransferfunctions1+τsτ>0.The“estimatedspeed”ve=L−1(cid:9)s1+τsˆy(s)(cid:10)willbeasclosetotherealspeedvasthetimeconstantτissmall.AsecondexampleistheDCmotorstudiedinsection1.3whichcanbeputinstate-spaceformwithx=(cid:25)θωi(cid:26)T(seeExercise208section7.6).Ifweputinasensorforanangularpositionθandasensorforcoilcurrentionecanconsiderthatthestateismeasuredbecausetheangularspeedωisderivedfromθwithinagoodapproximationthankstotheproceduredescribedabove.Astatefeedbackforsystem(8.1)isoftheformu=v−Kx(8.3)whereK∈Rm×nisthegainmatrix(ormoresuccinctlythegain)ofthestatefeedbackandvisanexternalsignalcalculatedfromthereferencesignal.ThediagramofthefeedbacksystemisshowninFigure8.1.Notethatthestatefeedback(intheelementaryversionpresentedinthissection)doesnotdepend–atleastnotdirectly–ontheoutputy.Theequationofthefeedback-+xCyuBxAx               KvuyxFigure8.1.ControlbystatefeedbackStateFeedback219systemisobtainedbyreplacingthecontroluinequation(8.1)byitsexpression(8.3)andoneobtains˙x=(A−BK)x+Bv.(8.4)Thisexpressionisoftheformofequation(8.1)withAreplacedbyA−BKandureplacedbyv.8.1.2.PoleplacementbystatefeedbackLEMMA225.–Thei.d.z.sofasystemareinvariantunderstatefeedback.PROOF.Changingthebasisinthestate-spaceifnecessarywecanassumethatsystem(8.1)canbedecomposedaccordingtocontrollability(seesection7.1.3equation(7.7)andFigure7.2).InothertermswecansupposethatmatricesAandBareoftheformA=(cid:27)AcA120A¯c(cid:28)B=(cid:27)Bc0(cid:28)where(AcBc)iscontrollableandthesubmatrixA¯cisemptyifandonlyif(AB)iscontrollable.LetK=(cid:25)KcK¯c(cid:26)bethegainmatrixofthestatefeedback(decomposedincoherencewiththestructureofthematricesAandBabove).OneobtainsA−BK=(cid:27)Ac−BcKcA12−BcK¯c0A¯c(cid:28).(8.5)Thei.d.z.saretheeigenvaluesofA¯c(section7.2.2Proposition167)andhencetheyareinvariantunderstatefeedback.Thepolesofalineartime-invariantsystemwithrealcoefﬁcientsalwayshavethefollowingsymmetryproperty:ifp∈Cisapoleofthesystemthentheconjugate¯pofpisalsoapoleofthesystem.Thisleadsustoadoptthefollowingdeﬁnitioninaccordancewith([119]section0.8):DEFINITION226.–AsubsetPofCissaidtobesymmetric(abouttherealaxis)if:foreveryp∈Pwehave¯p∈P.LEMMA227.–ConsiderthesystemΣdeﬁnedbythestateequation(8.1)withm=1.LetP⊂Cbeasymmetricsetofnelements.IfΣiscontrollablethenthereexistsauniquestatefeedbacksuchthatthepolesofthefeedbacksystemaretheelementsofP.220LinearSystemsPROOF.SinceΣiscontrollablethematrixPdeﬁnedby(7.40)(section7.4.2)isinvertibleand(AcBc)=(cid:15)P−1APP−1B(cid:16)isinacontrollablecanonicalform;inotherwordsthematricesAcandBcarerespectivelythestatematrixandcontrolmatrixofsystem(7.37)(section7.4.2).Letxcbethestateofthiscontrollablecanonicalform.Wehavex=Pxcandhencethecontrol(8.3)iswrittenasu=v−KPxc.LetKc=KP=(cid:25)kc1kc2...kcn(cid:26).Thefeedbacksysteminthenewbasisisexpressedas˙xc=(Ac−BcKc)xc+BcuwithAc−BcKc=⎡⎢⎢⎢⎢⎢⎢⎣−a1−kc1−a2−kc2······−an−kcn10···0001..................0...00010⎤⎥⎥⎥⎥⎥⎥⎦.ThematrixAc−BcKcisacompanionofthepolynomialf(s)=sn+f1sn−1+...+fn(8.6)fi=ai+kci1≤i≤n;(8.7)f(s)isthusthecharacteristicpolynomialofAc−BcKc(section13.4.3Proposition562(i))andthereforeofA−BK.Letf(s)=’p∈P(s−p)andwewritef(s)intheformofequation(8.6).Accordingtoequation(8.6)andtheexpression(7.40)(section7.4.2)PisthesetofpolesofthefeedbacksystemifandonlyifK=(cid:25)f1−a1f2−a2...fn−an(cid:26)[Γ(AB)A+]−1.(8.8)REMARK228.–Expression(8.8)ofthegainmatrixofthestatefeedbackiscalledtheBass–Guraformula.ItisingeneralquitecumbersometouseinpracticebutithastheinterestingpointofshowingKcanbeexpressedlinearlyasafunctionofthecoefﬁcientsfi−ai1≤i≤n.LEMMA229.–ConsiderthesystemΣdeﬁnedbythestateequation(8.1)withm>1.LetP⊂Cbeasymmetricsetofnelements.IfΣiscontrollablethenthereexistsanon-uniquestatefeedbackcontrolsuchthatpolesofthefeedbacksystemaretheelementsofP.StateFeedback221PROOF.SinceΣiscontrollablewecanputthesystemincontrollablecanonicalformbychangeofbasisinthestate-spaceandinthecontrolspacei.e.inRnandRmrespectively(seesection7.4.3).Toclarifyideassupposethiscanonicalformis(7.54);denotebyxcitsstateandbyυitscontrol(seeDeﬁnition204section7.4.3).Letthestatefeedbackcontrolbeequation(8.3)whichcanbeputintheformυ=ν−Kcxcbychangeofvariable(7.55)(section7.4.3).(1)Oneﬁrstmethodconsistsofchoosingrealnumbersαi(1≤i≤4)βi(1≤i≤3)andγi(1≤i≤2)andthendetermineKcinsuchawaythatAc−BcKc=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣−α1−α2−α3−α400000111000000−β1−β2−β3001100000000−γ1−γ21⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(thisdeterminationisnotdifﬁcultduetotheformofBc).Therefore{Ac−BcKcBc}hasthesamecontrollabilityindicesas{AB}anddet(sI9−Ac+BcKc)=α(s)β(s)γ(s)α(s)=s2+α1s+α2β(s)=s3+β1s2+β2s+β3γ(s)=s4+γ1s3+...+γ4.Polynomialsα(s)β(s)andγ(s)canbearbitrarilychosenandtheirrootswhicharethepolesofthefeedbacksystemcanbearbitrarilychosentoo.(2)Thesamepolescanbeobtainedwithdifferentmethodsgivingdifferentgainmatricesandwearegoingtoshowonecaseherebelow:itconsistsofchoosingtherealnumbersαi(1≤i≤9)anddeterminingKcinsuchawaythatAc−BcKc=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣−α1−α2−α3−α4−α5−α6−α7−α8−α9111000001000001100000001001⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦;222LinearSystemsthedeterminationofKcisnotdifﬁcultforthesamereasonasgivenbefore.ThistimethematrixAc−BcKciscyclicsinceitisacompanionofthepolynomialα(s)=s9+(cid:11)1≤i≤9αis9−i.Thepolesofthefeedbacksystemaretherootsofthepolynomialα(s)andthuscanarbitrarilybechosen.Thefollowingtheoremisoneofthemostimportantinthetheoryofstate-spacesystems:THEOREM230.–ConsiderthesystemΣdeﬁnedbyequation(8.1).(i)Thefollowingtwoconditions(a)and(b)areequivalent:(a)foranysymmetricsetP⊂Cofnelementsthereexistsastatefeedback(8.3)forwhichthepolesoftheclosed-loopsystemaretheelementsofP;(b)thesystemΣiscontrollable.(ii)Thereexistsastatefeedbackforwhichtheclosed-loopsystemisstableifandonlyifΣisstabilizable.PROOF.1)(a)⇒(b):Letusprovethisimplicationbycontradiction.Ifsystem(8.1)isnotcontrollablethesetofitsi.d.z.sisnon-empty(Theorem165section7.2.2).Thesei.d.z.sarepolesofthesystemandareinvariantbyfeedbackaccordingtoLemma8.1andthusCondition(a)doesnothold.2)(b)⇒(a)accordingtoLemmas227and229.3):AssumewithoutlossofgeneralitythatthesystemΣisdecomposedaccordingtocontrollability.Sincethefeedbacksystemisgivenbyequation(8.5)thesetofitspolesisSp(Ac−BcKc)˙∪Sp(A¯c)(whereSpand˙∪denotethespectrumandthedisjointunionrespectively:seesections13.3.3and7.2.5).ThesystemΣisstabilizableifandonlyifallitsi.d.z.s(whicharetheeigenvaluesofA¯casshownintheproofofLemma225)belongtothelefthalf-plane(Deﬁnition185section7.3).Ontheotherhand(AcBc)iscontrollableaccordingtoProposition144(section7.1.3)andhencetheeigenvaluesofAc−BcKccanbearbitrarilyassigned(providedthattheyformasymmetricsubsetofCwhosecardinalisn)bychoiceofKcaccordingto(i).REMARK231.–Brunovskicanonicalform.WeusetheﬁrstmethodintheproofofLemma229andwechoosethecoefﬁcientsαiβiandγitobezero.Thefeedbacksystemthusisreducedtothreechainsofintegratorswhichare:˙x1=υ1˙xi+1=xi1≤i≤3˙x5=υ2˙xi+1=xi5≤i≤6˙x8=υ3˙x9=x8.ThisstaterepresentationiscalledtheBrunovskicanonicalformofthewell-formedsystemconsidered.Weencounterthisinparticularinthecontextofnonlinearsystemwhenweusethetechniqueof“linearizationbyfeedbackanddiffeomorphism”[62];arobustversionofthistechniquewasdevelopedin[49].ForfurtherdetailsontheBrunovskicanonicalformsee[44].StateFeedback2238.1.3.ChoiceofthepoleplacementintheSISOcaseLetΣbeacontrollableSISOstate-spacesystemofordern.AccordingtoLemma227ifonechoosesasymmetricsetP⊂Cofnelementsthereexistsauniquestatefeedbackcontrolforwhichthepolesoftheclosed-loopsystemaretheelementsofP.Thechoiceofthepoleplacementisthusreducedtothechoiceofthepolesoftheclosed-loopsystem.Bychangeofbasisinthestate-space–ifnecessary–theproblemcomesdowntothecasewhereΣisincontrollablecanonicalform.Thestateofthiscanonicalformisxc=(cid:25)∂n−1ξ...ξ(cid:26)Twhereξisthepartialstate(seesection7.4.2)andhencethestatefeedbackcontrolu=v−Kcxc=v−(cid:25)kc1...kcn(cid:26)xccanbewrittenintheformu=v−K(∂)ξ(8.9)K(∂)=(cid:12)1≤i≤nkci∂n−i.Expression(8.9)isthatofa“partialstatefeedbackcontrol”(seesection6.3.5).ToobtainamodulusmarginMmiequalto1itisthussufﬁcienttochoosethepolesπkofthefeedbacksystemasafunctionofthenpolespk(1≤k≤n)ofΣaccordingtoRule111(section6.3.5).EXAMPLE232.–Theexamplepresentedhereeventhoughitisacademicallowsustohighlightseveralimportantpoints.LetΣ={AB}beastate-spacesystemwithA=⎡⎣−120100110⎤⎦B=⎡⎣100⎤⎦.ThecontrollabilitymatrixisΓ(AB)=⎡⎣1−1301−1010⎤⎦;itsdeterminantis1andhenceΣiscontrollable.ThepolesofΣare{−201}.Rule111(section6.3.5)isthusrespectedifwechoosethepolesoftheclosed-loopsystemtobe{−2−1−1}.ThecharacteristicpolynomialofthefeedbacksystemisthenPc(s)=(s+2)(s+1)2=s3+4s2+5s+2.Nowletthestatefeedbackbeequation(8.3)(section8.1.1)withK=(cid:25)k1k2k3(cid:26).Accordingto(8.4)thestatematrixofthefeedbacksystemisA−BK=⎡⎣−1−k12−k2−k3100110⎤⎦224LinearSystemsanditscharacteristicpolynomialisf(s)=s3+s2(k1+1)+s(k2+k3−2)+k3.ByidentifyingPc(s)andf(s)termbytermweobtainK=(cid:25)352(cid:26).ThemethodusedheretocalculateKrequiressigniﬁcantlyfewercomputations(whentheyare“handcalculated”)thanthe“Bass-Guraformula”(seeRemark228).ThesensitivityfunctionSiisgivenbytherelationSi=(I+Li)−1withLi=K(sIn−A)−1B.SincethefeedbacksystemisstablewehaveSi∈(cid:1)H∞.TheBodeplotofSiisshowninFigure8.2.1Since|Si(iω)|≤1andlimω→+∞|Si(iω)|=1wehave(cid:23)Si(cid:23)∞=1andhenceMmi=1accordingtotheexpression(4.23)ofsection4.2.9.AccordingtoTheorem93(section4.2.9)thisinputmodulusmarginguaranteesaninputgainmarginof(−6dB+∞)andaninputphasemarginof(−60◦60◦)(Deﬁnition92).WecanverifythispointontheBodeplotofLishowninFigure8.3.Theactualinputgainmarginandactualinputphasemarginare(−9.5dB+∞)and(−64◦64◦)respectively.Theyarethusinthisparticularcasequiteclosetothegainmarginandphasemarginguaranteedbythemodulusmargin(whichisrelatedtothefactthatΣisunstableandthustheNyquistplotofLi(s)encirclesthecriticalpoint−1intheanti-clockwisesense:seeFigure8.4).8.1.4.*ChoiceofthepoleplacementintheMIMOcaseLetΣbeastate-spacesystemofordernwell-formedandwithmcontrolvariables.ThedifﬁcultiesAsshownbyLemma229thechoiceofthepolesofthefeedbacksystemdoesnotuniquelydeterminethestatefeedbackifm>1.EXAMPLE233.–ConsiderthestatesystemΣ={ABC}deﬁnedinExample195(section7.4.3)andmorepreciselybytheequalities(7.48)(7.49)and(7.50).AccordingtoExample205(section7.4.3)byputtingx=Pcxandυ=˘BuwherePcand˘Baregivenby(7.56)and(7.59)respectivelyweobtainthecontrollablecanonicalform{AcBcCc}withstatexcandcontrolυgivenby(7.57)(7.58)and(7.59).ThepolesofΣare{−1−11}.ThereforeRule111ofsection6.3.5isbeingcompliedwithifwechoosethepolesofthefeedbacksystemtobe{−3−3−5}.(1)WewillfollowthispoleplacementaccordingtoMethod(1)oftheproofofLemma229.WeobtainAc−BcKc=⎡⎣−6−9010000−5⎤⎦Kc=(cid:27)48−1006(cid:28).1.Thecurvesrelatedtosection8.1areplacedattheendofthissection.StateFeedback225Thegainmatrixofthestatefeedback(8.3)isK=˘B−1KcP−1c=(cid:27)56−36126(cid:28).ThesingularvaluesofthesensitivityfunctionSiareplottedinFigure8.5asafunctionofthefrequencyω.Wenotethat(cid:23)Si(cid:23)∞(cid:9)0.008dB.Thisquantityislargerthan1(byjustalittle)andhencethepropertythatholdsintheSISOcase(i.e.(cid:23)Si(cid:23)∞=1)islost.(2)WewillnowusethesamepoleplacementaccordingtoMethod(2)oftheproofofLemma229.Weget(s+3)2(s+5)=s3+11s2+39s+45.WeobtainAc−BcKc=⎡⎣−11−39−45100010⎤⎦Kc=(cid:27)938440−11(cid:28).Thegainmatrixofthestatefeedback(8.3)isK=˘B−1KcP−1c=(cid:27)45818122(cid:28).ThesingularvaluesofthesensitivityfunctionSiareplottedinFigure8.6asafunctionofω.Thistime(cid:23)Si(cid:23)∞(cid:9)11.9dBor(cid:23)Si(cid:23)∞(cid:9)4.TheinputmodulusmarginMmiisonly0.25andTheorem93(section4.2.9)givesaguaranteedgainmarginof(−1.9dB2.5dB)andaguaranteedphasemarginof(−14.4◦14.4◦).Theseareverypoorvalues.REMARK234.–(i)Theaboveexampleshowsthatforidenticalpolesoftheclosed-loopsystemthestatefeedbackcontrolcanhaverobustnesspropertiesverydependentonthemethodusedfordesigningthepoleplacement.(ii)WecanconjecturethatMethod(1)providesabettermodulusmarginthanMethod(2).ThereaderhowevershouldnotbelievethatMethod(1)willalwaysprovideamodulusmargincloseto1aslongasRule111ofsection6.3.5isbeingabidedbywithrespecttothechoiceoftheclosed-looppoles.Forexampleforthechoice{−1−1−1}whichcomplieswiththisruleweobtainK=(cid:27)121242(cid:28)Mmi(cid:9)0.62.(α)methodWewillprovideinwhatfollowsasolutiontothedifﬁcultyshownabove.Wewillcallthissolutionthe“(α)method”.226LinearSystemsLetP∈Rn×nbeasymmetricrealmatrixandAα=A+αInα∈R.Considerthestatefeedback(8.3)whereK=BTP.(8.10)Let{λ1...λn}betheeigenvaluesof˜Aα(cid:1)Aα−BK(repeatingeacheigenvalueasmanytimesasitsmultiplicity).Assumethat˜Aαisdiagonalizableandlet{ξ1...ξn}beabasisofassociatedeigenvectors.Letηi=Pξi(1≤i≤n)whichisequivalenttoη=PξandsinceξisinvertibletoP=ηξ−1(8.11)whereη=(cid:25)η1...ηn(cid:26)ξ=(cid:25)ξ1...ξn(cid:26);(8.12)andthenletνi(cid:1)(cid:27)ηiξi(cid:28)1≤i≤n.(8.13)REMARK235.–Thehypothesisthat˜Aαisdiagonalizableisonlymadetosimplifythediscussion.Thereadercanverifythattherationalewhichfollowsisalongthemainlinesandisstillvalidifthishypothesisisremovedandwedenoteabasisofgeneralizedeigenvaluesof˜Aαby{ξ1...ξn}(seeRemark531section13.3.4).*FromanumericalpointofviewitispreferabletousetheSchurformthantheJordanform([2]AnnexE).*LEMMA236.–(i)MatrixPisasolutionoftheequationPAα+ATαP−PBBTP=0(8.14)(calledanalgebraicRiccatiequation)ifandonlyifforanyi∈{1...n}thefollowingequalityholds:(cid:27)Aα−BBT0−ATα(cid:28)νi=λiνi.(8.15)Thematrixappearingontheleftofthisequality(denotedbyHinwhatfollows)iscalledtheHamiltonianmatrixandequation(8.15)expressesthefactthatνiisaneigenvectorofHassociatedwiththeeigenvalueλi.(ii)Letσbeapermutationof{1...n}ησ=(cid:25)ησ(1)...ησ(n)(cid:26)ξσ=(cid:25)ξσ(1)...ξσ(n)(cid:26)andPσ=ησξ−1σ;andwehaveP=Pσ.PROOF.(i)Foranyi∈{1...n}wehaveλiξi=(cid:15)Aα−BBTP(cid:16)ξi=Aαξi−BBTηi.(1)IfPisasolutionofthealgebraicequation(8.14)thenforanyStateFeedback227i∈{1...n}wehaveλiηi=P(cid:15)Aα−BBTP(cid:16)ξi=−ATαPξi=−ATαηiandhenceequation(8.15)issatisﬁed.(2)Converselyifforalli∈{1...n}theequality(8.15)issatisﬁedwehaveAαξi−BBTPξi=λiξiand−ATαηi=λiηiwithηi=Pξifromwhichweget(cid:15)PAα+ATαP−PBBTP(cid:16)ξi=0.AsaresultPisasolutionof(8.14).(ii)Thepermutationσcorrespondstoaright-multiplicationofηandξbyapermutationmatrixΠi.e.ησ=ηΠandξσ=ξΠ.ThereforePσ=(ηΠ)(ξΠ)−1=P.Thefollowingresultisclassicandisprovedforexamplein([64]A.41)and([119]section12.4).LEMMA237.–LetZ∈Rn×nbeasymmetricnon-negativedeﬁnitematrix(i.e.Z≥0)2andletF∈Rn×nbeastabilitymatrix(Deﬁnition182section7.3).ThentheLyapunovequationFTP+PF=−ZhasauniquesolutionP∈Rn×nandP≥0.Ifinaddition(cid:9)√ZF(cid:10)isobservable3thenP>0.THEOREM238.–(i)IfλisaneigenvalueofHsois−λ.(ii)Let{νi1≤i≤n}beasetoflinearlyindependenteigenvectorsofHletηiandξi(1≤i≤n)bethevectorsdeﬁnedbyequation(8.13)andletηandξbethematricesdeﬁnedbyequation(8.12);ifξisinvertiblePdeﬁnedby(8.11)isasolutionofthealgebraicRiccatiequation(8.14).(iii)Ofthefollowingconditions((a)&(b))⇔(c):(a)(AαB)isstabilizable;(b)Hhasnoimaginaryeigenvalues;(c)amongthematricesPdeﬁnedbyequation(8.11)thereexistsamatrixˇPforwhichAα−BK(withKgivenbyequation(8.10))isastabilitymatrix.(iv)WhenCondition(c)holdsthematrixˇPisuniqueandissymmetricrealnon-negativedeﬁnite;itisthematrixPobtainedwhentheλis(1≤i≤n)aretheeigenvaluesˇλiofHbelongingtothelefthalf-plane.(v)Let{π1...πn}bethepolesofΣ.Thepolesoftheclosed-loopsystemwiththestatefeedbackcontrol(8.3)(whereK=−BTˇP)are(cid:31)ˇλ1...ˇλn suchthatˇλi=(cid:20)πiifRe(πi)<−α−πi−2αifRe(πi)>−α.(8.16)PROOF.(i)IsobvioussincetheeigenvaluesofHarethoseofAαplusthoseof−Aα.(ii)IsareformulationofLemma236.(iii)and(iv)Accordingto(i)ifHhas2.ForasquarematrixZwithrealentriesthesymbolZ≥0(resp.Z>0)signiﬁesthatZissymmetricnon-negativedeﬁnite(resp.symmetricpositivedeﬁnite):seeDeﬁnition575(section13.5.6).ThesameholdswhenZhascomplexentrieschangingsymmetrictoHermitian.3.√ZistheHermitian(realsymmetricinthepresentcase)squarerootofZ(seesection13.5.6).ButanysquarerootofZcanbeused.228LinearSystemsnoimaginaryeigenvaluesthismatrixhasneigenvaluesbelongingtothelefthalf-planeandneigenvaluesbelongingtotherighthalf-plane.Ifinaddition(AαB)isstabilizablethereexistsauniquematrixP=ˇPoftheform(8.11)forwhichAα−BKisastabilitymatrix:see([69]Theorem1)wheretheconverseisalsoproved.Accordingtoequation(8.14)wehaveˇP(Aα−BK)+(Aα−BK)TˇP=−ZwithZ=ˇPBBTˇP≥0.AsaresultaccordingtoLemma237ˇP≥0.(v)TheeigenvaluesofHareπi+αand−πi−αandhencetheeigenvaluesofAα−BKareπi+αifRe(πi)<−αand−πi−αifRe(πi)>−α.ItremainsnowtosubtractαinordertoobtaintheeigenvaluesofA−BK.EXAMPLE239.–ConsiderthesamesystemasinExample233.Bytakingα=2thepolesofthefeedbacksystemplacedaccordingtotheabovemethodare{−3−3−5}whichareexactlythepoleschosenintheexampleconsidered.Wecanalsochooseα=1;althoughinthiscasetheHamiltonianmatrixHhaseigenvaluesattheoriginthestatefeedbackwithgainmatrix(8.10)andPgivenbyequation(8.11)arewell-deﬁnedandthepolesarethenplacedat{−1−1−3}.Thesearevaluesthatsatisfy(8.16).Theinterestinallpreviousdevelopmentcanbefoundinthetheorembelow.THEOREM240.–LettherebethestatefeedbackinTheorem238(iv).Ifα≥0theinputmodulusmarginisMmi=1.PROOF.LetL(s)=K(sIn−A)BLα(s)=L(s−α)=K(sIn−Aα)BandΦα(s)=(sIn−Aα)−1.Usethefollowingnotation:(.)∼(s)=(.)T(−s)wheretheterminparenthesesisanytransfermatrix.AndﬁnallyletP=ˇP.Bymultiplyingequation(8.14)by−1thensubtractingsPto−AαPintheobtainedequationthenaddingontotheterm−ATαPthesamequantityandlastmultiplyingthenewlyobtainedequationontheleftbyBTΦ∼α(s)andbyΦα(s)Bontherightweobtain:(Im+Lα)∼(Im+Lα)=Im;(8.17)asaresultsupRe(s)≥0¯σ(cid:9)(Im+Lα(s))−1(cid:10)≤1StateFeedback229sincethepolesofthetransfermatrix(Im+Lα(s))−1are(cid:31)ˇλ1...ˇλn whichbelongtothelefthalf-plane.WehaveontheotherhandsinceSi=(Im+L)−1(cid:23)Si(cid:23)∞=supRe(s)≥0¯σ(cid:9)(Im+L(s))−1(cid:10)=supRe(s)≥−α¯σ(cid:9)(Im+Lα(s))−1(cid:10)andhenceifα≥0(cid:23)Si(cid:23)∞≤supRe(s)≥0¯σ(cid:9)(Im+Lα(s))−1(cid:10)≤1.Sincelim|s|→+∞Si(s)=Imwehave(cid:23)Si(cid:23)∞=1andthereforeMmi=1.DEFINITION241.–The(α)methodconsistsofdeterminingthegainmatrixKofthestatefeedbackaccordingtoequation(8.10)withP=ˇPforarealnumberα≥0suchthat(i)(AαB)isstabilizable(ii)Aα(orinanequivalentmannerH)hasnoimaginaryeigenvaluesifα=0(iii)ξisinvertible(thislastconditionisalwayssatisﬁedifHhasnoimaginaryeigenvalues).EXAMPLE242.–ContinuingfromExample239weobtainˇP=⎡⎣9.916.13.928.511.213.8⎤⎦K=(cid:27)3.73.6−3.36.212.57.2(cid:28)withˇP=ˇPT.ThesingularvaluesofthesensitivityfunctionSiareplottedinFigure8.7asafunctionoftheangularfrequencyω.ThisﬁgureillustrateswellthefactthatMmi=1.REMARK243.–(i)The(α)methodiscloselyrelatedtothe“linearquadraticoptimalcontrol”(seeRemark245(ii)below).Thisisneverthelessintheformulationproposedhereamethodofpoleplacement.(ii)InthecaseofanSISOsystemTheorem240doesnotcontradictequality(4.17)ofsection4.2.8becausetherelativedegreeoftheopen-looptransferfunctionL(s)isequalto1.Ontheotherhandaccordingtoequality(4.18)oftheabove-mentionedparagraphallzerosofL(s)belongtotheclosedlefthalf-plane.ThisremarkremainsvalidinthecaseofanMIMOsystem(seesection4.2.9).Extensionofthe(α)method:theLQRmethodThe“(α)method”isaparticularcaseofthe“LQRmethod”4thealgebraiccharacteristicsofwhichwewillnowdescribewithoutinsistingonthe“optimal4.LQRstandsfor“LinearQuadraticRegulator”;seeRemark245below.ThisregulatorcorrespondstotheLQcontrol(alreadymentionedinthepreface).230LinearSystemscontrol”aspect(inaccordancewithwhatissaidinthepreface).Supposethat(AαB)isstabilizableandlettherebeastatefeedbackwithgainmatrixKgivenbyK=R−1BTP(8.18)whereR∈Rm×mR>0andwherePisarealsymmetricmatrix;theexpression(8.18)isageneralizationofequation(8.10).ConsideralsothealgebraicRiccatiequationPAα+ATαP−PBR−1BTP+Q=0(8.19)(Q∈Rn×nQ≥0)whichisageneralizationofequation(8.14).AndﬁnallyconsidertheHamiltonianmatrixH=(cid:27)Aα−BR−1BT−Q−ATα(cid:28).(8.20)LetEbeasquarerootofQ(section13.5.6)i.e.amatrixsuchthatQ=ETE.THEOREM244.–(i)Parts(i)-(iv)ofthestatementofTheorem238remainvalid(theνis1≤i≤narethegeneralizedeigenvaluesofHifthismatrixisnotdiagonalizable:seeRemarks235(section8.1.4)and531(section13.3.4))whentheexpressions(8.10)and(8.14)arereplacedrespectivelybyequations(8.18)and(8.19)andHisdeﬁnedbyequation(8.20).(ii)SupposefromnowonthatCondition(a)ofTheorem238holds.InorderthatCondition(b)ofthissametheorembesatisﬁed(forCondition(c)tohold)itsufﬁcesthatthefollowingcondition(d)besatisﬁed:(d)(EAα)isdetectable.(iii)InthiscasethereexistsauniquesolutiontothealgebraicRiccatiequation(8.19)forwhichAα−BKisastabilitymatrixanditistheabovematrixˇP;ˇPisalsotheuniquenon-negativedeﬁnitesolutiontoequation(8.19).(iv)Ifinaddition(EAα)isobservablethenˇP>0.PROOF.(i)isaneasygeneralizationofthecorrespondingpartsofthediscussionofTheorem238(seeExercise268).For(ii)see([119]section12.3)and([72]section3.4.3).(iii)isaconsequenceofLemma237.REMARK245.–(i)Condition(d)isnotnecessaryfortheexistenceofˇPandthatiswhythe(α)methodappliestounstablesystems(seeExample242).Condition(d)isonlynecessaryfortheuniquenessinTheorem244(iv)([69]Theorem2).(ii)Conditions(a)and(d)arenecessaryandsufﬁcientforthecontrolu=−Kx(withKsatisfyingequation(8.18)andP=ˇP)tominimizethequadraticcriterionJα=(cid:2)+∞0e2αt(cid:15)xT(t)Qx(t)+uT(t)Ru(t)(cid:16)dtStateFeedback231([72][2]).ThematricesQandRcanbeinterpretedasweightingmatricesallowingweightingatvariouslevelsofcertainR-linearcombinationsofthestatevariablesandofthecontrolvariables(themoresuchalinearcombinationisweightedthesmalleritsvariationswillbeovertime).Supposefromnowonthatα≥0andConditions(a)and(b)ofTheorem238hold.Lettherebethestatefeedback(8.18)withP=ˇP.UsingthesamenotationasintheproofofTheorem240weobtain(byanidenticalapproach)thegeneralizationbelowofequality(8.17)ageneralizationwhichiscalledtheKalmanequality:[Im+Lα(s)]∼R[Im+Lα(s)]=R+BTΦ∼α(s)QΦα(s)B.(8.21)Lets=iω;wehaveBTΦ∼α(iω)QΦα(iω)B≥0andasaresult¯σ(cid:9)√R(Im+Lα(iω))−1√R−1(cid:10)≤1∀ω.WededucethefollowinggeneralizationofTheorem240(whichcorrespondstothecaseQ=0andR=Im):THEOREM246.–Withthestatefeedbackdeﬁnedaboveandforanyα≥0wehave(cid:29)(cid:29)(cid:29)√RSi√R−1(cid:29)(cid:29)(cid:29)∞=1.InparticularifR=ImormoregenerallyifRisascalarmatrix5wehaveMmi=1.ThequestionthatremainstobeexaminedisthenatureofthepoleplacementcarriedoutusingtheLQRmethod.InordertodosothematrixRisreplacedinwhatfollowsbyρRwhereρ>0isarealnumber.LetG(s)=EΦ(s)BΦ(s)=(sIn−A)−1;whereG(s)isthetransfermatrixofthestate-spacesystem{ABE}.Alsolettherebethefollowinghypothesis(H):(H):G(s)∈R(s)m×mrkR(s)G(s)=mandthestate-spacesystem{ABE}isminimal.THEOREM247.–(i)Let{π1...πn}bethepolesofΣ(countingmultiplicities).Asρ→+∞thepolesˇλk(1≤k≤n)oftheclosed-loopsystemtendto(cid:20)πkifRe(πk)≤−α−πk−2αifRe(πk)>−α.5.A“scalarmatrix”R∈Rm×misoftheformrImwherer∈R.Suchamatrixisobviously>0ifandonlyifr>0.NotethatifRisascalarmatrixwecomebacktothecaseR=ImbyreplacingPbyP/randQbyQ/rinequation(8.19).232LinearSystems(ii)SupposethatHypothesis(H)isinforce.Let{ν1...νq}betheMacMillanzerosofG(s)(countingmultiplicities).Asρ→0+qpolesˇλk(1≤k≤q)oftheclosed-loopsystemtendto(cid:20)νkifRe(νk)≤−α−νk−2αifRe(νk)>−α.Theremainingn−qpolesˇλk+q(1≤k≤n−q)areunboundedandwhenm=1ˇλk+q∼skwheresk=ei(12+−1+2k2(n−q))π(cid:21)β2ρ(cid:22)12(n−q)β(cid:5)=0.(8.22)PROOF.Leta(s)=|sIn−A|andf(s)=|sIn−A+BK|(therootsofa(s)andoff(s)arethepolesoftheopen-loopsystemandoftheclosed-loopsystemrespectively).Weobtainbyconsideringthedeterminantofthetwomembersofequation(8.21)andbyputtingℵα(s)=ℵ(s−α)foranymatrixℵofrationalfunctionsf∼αfα=a∼αaαdet(cid:21)Im+1ρ√R−1G∼αGα√R−1(cid:22).(8.23)LetM=√R−1G∼αGα√R−1andr=rkR(s)G(s)≤m.WehavefromLemma519(section13.3.3)det(ρIm+M(s))=ρm+r(cid:12)k=1∆k(s)ρm−kwhere∆k(s)isthesumoftheprincipalminorsoforderkofM(s).Thereforef∼α(s)fα(s)=a∼α(s)aα(s)*1+r(cid:12)k=1∆k(s)ρ−k+.(8.24)(i)ρ→+∞.Thenwegetf∼αfα→a∼αaαf(s)beingapolynomialallrootsofwhichbelongtotheclosedlefthalf-plane.WenowonlyneedtoapplythesamerationaleasintheproofofTheorem238(iii).(ii)ρ→0+.(a)Letς(s)betheuniquemonicpolynomialtherootsofwhicharethetransmissionzerosof{ABE}andletq=degς(s)<n.ConsideringtheSmith–MacMillanformofG(s)(section2.4.5)wegetG(s)=U−1(s)(s)Ψ−1(s)V(s)wherethepolynomialmatricesU(s)andV(s)areinvertibleoverR[s].SupposingHypothesis(H)isinforcewehaver=mand(s)=diag(ε1(s)...εm(s))Ψ(s)=diag(ψ1(s)...ψm(s))ς(s)=det(s)=’mi=1εi(s)a(s)=detΨ(s)=’mi=1ψi(s)∆m(s)=detM=β2detRς∼αςαa∼αaαStateFeedback233whereβ=detU−1(s)detV(s).Asaresultfromequation(8.24)f∼α(s)fα(s)=1ρm(cid:27)β2detRς∼α(s)ςα(s)+......+a∼α(s)aα(s)*ρm+m−1(cid:12)k=1∆k(s)ρm−k+$.(8.25)(b)LetKbeacompactsubsetofthecomplexplanewhichdoesnotcontainanyMacMillanpoleofG∼αGαandtheinteriorofwhichcontainsallrootsofς∼α(s)ςα(s).The∆k(s)areboundedonKandhencetherootsoff∼α(s)fα(s)remaininginKconvergetotherootsofς∼α(s)ςα(s)asρ→0+.(c)n−qrootsoff(s)areunboundedasρ→0+accordingtotheabove.Inthecasem=1wecanassumewithoutlossofgeneralitythatR=1andtheexpression(8.25)becomesf∼α(s)fα(s)=1ρ(cid:15)β2ς∼α(s)ςα(s)+ρa∼α(s)aα(s)(cid:16).(8.26)Sincea(s)isamonicpolynomialofdegreenwehavea∼α(s)aα(s)=(−1)ns2n+s2nω1(1/s)(8.27)whereω1(1/s)tendsto0andsodoes1/s.Thesamerationaleisvalidforς(s)andhenceς∼α(s)ςα(s)=(−1)qs2q+s2qω2(1/s)(8.28)whereω2(1/s)tendsto0andsodoes1/s.Accordingtoequations(8.26)(8.27)and(8.28)therootswearelookingforasymptoticallysatisfy(−1)ns2n+β2ρ(−1)qs2q=0.Therootsofthisequationwhichbelongtotheclosedlefthalf-planearethesksdeﬁnedbyequation(8.22).REMARK248.–(i)Theconﬁgurationoftherootsskwell-knowninthetheoryofﬁltersiscalledtheButterworthconﬁgurationofordern−q:therootssk(1≤k≤n−q)areequallydistributedinthecomplexplaneonacirclecenteredattheoriginandhaveargument(cid:9)12+−1+2k2(n−q)(cid:10)π.(ii)Form>1andunderHypothesis(H)thenumberofpolesofthefeedbacksystemhavingabsolutevaluetendingto+∞isstillequalton−qi.e.tothedegreeofzeroatinﬁnityofG(s)sincethedefectofthetransfermatrixG(s)iszero(seeRemark40section2.4.7).Thesen−qpoleshavegenerallyamorecomplexbehaviorthaninthecasem=1;theirasymptoticdirectionscanbedistributedaccordingtoacombinationofseveralButterworthconﬁgurationsofdifferentorders:see([72]section3.8Example3.19).234LinearSystemsFigure8.2.BodeplotofthesensitivityfunctionFigure8.3.BodeplotofLi(s)StateFeedback235Figure8.4.NyquistplotofLi(s)Figure8.5.SingularvaluesofSi(s)–method1236LinearSystemsFigure8.6.SingularvaluesofSi(s)–method2Figure8.7.SingularvaluesofSiStateFeedback237REMARK249.–(i)SupposeHypothesis(H)isreplacedbythefollowinghypothesis(H(cid:2))whichislessrestrictive:G(s)∈Rr×mrkR(s)G(s)=r(r≤m)andthestate-spacesystem{ABE}isminimal.Thenaccordingtoequation(8.24)therootsoff∼α(s)fα(s)thatremainboundedconverge(asρ→0+)totherootsofthepolynomiala∼α(s)aα(s)∆r(s).(ii)Lettherebethestate-spacesystem{ABE}withtransfermatrixG(s)andsupposethattheabovehypothesis(H)issatisﬁedaswellasthefollowingcondition:{ABE}doesnothavetransmissionzerosintheclosedrighthalf-plane(inotherwords{ABE}isminimumphase:seeRemark68).LetalsoPρbetheunique≥0solutionofthealgebraicRiccatiequation(8.19)forα=0andRreplacedbyρRρ>0.Itisshownin([72]section3.8.3)thatPρ→0asρ→0+.8.2.Statefeedbackwithintegralaction8.2.1.InsufﬁciencyofstatefeedbackConsidertheelementaryexampleofa2nd-ordersystemhavingnozerosanddescribedbytheleftform(cid:15)∂2+a1∂+a2(cid:16)y=b2u(b2(cid:5)=0).Wecanputthissysteminstate-spaceformbysettingx=(cid:25)˙yy(cid:26)T.Astate-feedbackcontrolu=v−KxwhereK=(cid:25)k1k2(cid:26)canbewrittenasu=v−k1˙y−k2y.Itisacontroloftheproportionalandderivativekindinwhichanintegralactionismissingandthereforeazerostaticerrorcannotbeguaranteed.Whatfollowsaimsremedyingsuchaﬂaw.8.2.2.FeedbackcontrolinthepresenceofdisturbancesLettherebethesystem(cid:20)˙x=Ax+Bu+d1y=Cx+d2(8.29)wherex(t)∈Rnu(t)∈Rmy(t)∈Rpd1∈Rnandd2∈Rpwhered1andd2areconstantdisturbances(analgebraicmannerofexpressingthatthesearesignalsthewholespectrumofwhichliesinthelowfrequencies).Thesedisturbancesaresupposedtobeunmeasuredandunknown.238LinearSystemsOntheotherhandtheconstantreferencesignalr∈Rpisassumedtobeknown.Thestatexandtheoutputyaresupposedtobemeasuredateachinstant.Letebetheregulationerrordeﬁnedbye=y−r.(8.30)TheobjectiveofwhatfollowsistodesignacontrollawforwhichProperty(P)belowissatisﬁed:–limt→+∞e(t)=0–limt→+∞x(t)=const.–limt→+∞u(t)=const.∀d1∈Rnd2∈Rpbothofthemunknown∀r∈Rpknownandwithanyinitialconditions.Thehypothesesimposedonthestate-spacesystem{ABC}arethefollowing:(cid:127)(H1)(AB)iscontrollable;(cid:127)(H2)thetransfermatrixG(s)ofthesystem{ABC}issemiregularoverR(s)(seesection13.1.4).8.2.3.ResolutionofthestaticproblemThe“staticproblem”canbestatedinthefollowingmanner:Doesthereexist(foranyconstantdisturbancesd1andd2andanyconstantreferencesignalr)anequilibriumstatexeandanequilibriumcontrolueforwhichtheregulationerroriszero?Thistranslatesintotheequations(cid:20)0=Axe+Bue+d1r=Cxe+d2whichareequivalentto(cid:27)−A−BC0(cid:28)(cid:27)xeue(cid:28)=(cid:27)d1r−d2(cid:28).(8.31)Theright-handsideofthisequationisanyvectorinRn+p;asaresultthestaticproblemadmitsasolutionifandonlyifthematrixontheleftoftheaboveequalityisofrankn+p.Thisconditioncanbewrittenintheformrk(cid:27)sIn−A−BC0(cid:28)s=0=n+pStateFeedback239inawaythatrevealstheRosenbrockmatrixR(s)of{ABC}.FromHypothesis(H2)andProposition163(section7.2.1)therankρ(R)ofR(s)overR(s)isn+min(pm).FinallyaccordingtoTheorem161(section7.2.1)weobtainthefollowingresult:PROPOSITION250.–Thestaticproblemadmitsasolutionifandonlyifthefollowingtwoconditionshold:(i)m≥p;(ii)s=0isnotaninvariantzeroof{ABC}.REMARK251.–TakingintoaccountHypothesis(H2)Condition(i)ofProposition250expressesthefactthatthereareatleastasmanylinearlyindependentcontrolsaslinearlyindependentregulatedoutputs.Thisisthusamatterofnumberofdegreesoffreedom.Condition(ii)expressesthefactthatonecannotforcetheoutputofaderivatorsystemtobeanarbitraryconstant.ItissimilartothenecessaryandsufﬁcientconditionsexpressedbyProposition109(section6.3.4).8.2.4.ResolutionofthedynamicproblemBydifferentiatingequations(8.29)and(8.30)andbyputtingχ=(cid:27)∂xe(cid:28)υ=∂u(8.32)weobtainthestate-spacesystem˙χ=Fχ+Gυ(8.33)F=(cid:27)A0C0(cid:28)G=(cid:27)B0(cid:28).AccordingtoTheorem230(ii)thereexistsastatefeedbackυ=−Kχ(8.34)forwhichtheclosed-loopsystemisstableifandonlyif(FG)isstabilizable.LEMMA252.–Lettherebethecontrol(8.34).Property(P)insection8.2.2issatisﬁedifandonlyifF−GKisastabilitymatrix.PROOF.Letχ0=χ(0).Wehaveforeveryt≥0χ(t)=e(F−GK)tχ0.Property(P)isthussatisﬁedifandonlyifF−GKisastabilitymatrix.240LinearSystemsPROPOSITION253.–(FG)isstabilizableifandonlyifConditions(i)and(ii)inProposition250bothhold.Inthatcase(FG)iscontrollable.PROOF.AccordingtoProposition186(section7.3)(FG)isstabilizableifandonlyifrk(cid:25)sIn+p−FG(cid:26)=n+p∀s∈¯C+.Noticethat(cid:25)sIn+p−FG(cid:26)=(cid:27)sIn−A0BCsIp0(cid:28)∼(cid:27)sIn−A−B0C0sIp(cid:28).(i)Fors(cid:5)=0therankofthematrixontherightisequaltop+rk(cid:25)sIn−AB(cid:26)=p+nsince(AB)iscontrollable.(ii)Fors=0therankofthissamematrixisequaltotherankforthissamevalueofsoftheRosenbrockmatrixof{ABC}arankwhichisthesubjectofProposition250.IfConditions(i)and(ii)ofthispropositionbothholdwehaverk(cid:25)sIn+p−FG(cid:26)=n+p∀s∈Cand(FG)isthereforecontrollable.ThereforewecanassumethatConditions(i)and(ii)ofProposition250aresatisﬁed.Theorem230showsthatonecanchoosethecontrol(8.34)insuchamannerastoarbitrarilyassigntheeigenvaluesofF−GKinthelefthalf-plane(obeyingthesymmetrypropertywithrespecttotherealaxisinthecaseofimaginaryeigenvalues).WeputK=(cid:25)KpKi(cid:26)whereKp∈Rm×nandKi∈Rm×p.Takingintoaccountequation(8.32)equation(8.34)canbewrittenas∂u=−Kp∂x−Kieandbyintegratingthisexpressionweobtainu(t)=−Kpx(t)−Ki(cid:2)e(t)dt+const.(8.35)ThediagramofthiscontrollawcalledstatefeedbackwithintegralactionisshowninFigure8.8.REMARK254.–(i)Property(P)isstillsatisﬁedeveninthepresenceofmodelingerrorsandmorepreciselyifthematricesABandCofthesystemarereplacedbyA+∆AB+∆BandC+∆Crespectivelywhere∆A∆Band∆CareerrorssuchthatF+∆F−(G+∆G)K(where∆Fand∆GaretheadditiveerrorsonFandGrespectivelyresultingfromtheadditiveerrors∆A∆Band∆ConAStateFeedback241r-+-    Control  system .KiKpuy+Figure8.8.StatefeedbackwithintegralactionBandC)remainsastabilitymatrix.(ii)Supposesystem{ABC}iswell-formed.Ifm>pthereareseveralsolutionstothestaticproblem(8.31).Asaresultevenwithzerodisturbancesd1andd2andaﬁxedreferencertheequilibriumstatexeandcontroluearenotcontinuousfunctionsofABandC.Verysmallerrors∆A∆Band∆Ccaninduceaveryimportantchangetotheseequilibriumvaluesandconsequentlycanleadtosuchachangeinthebehaviorofthefeedbacksystem.Thisistobeavoidedanditisthuspreferableevenindispensableinpracticetoimposethat{ABC}bewell-formedandm=p[92].EXAMPLE255.–Forthesecondordersystemofsection8.2.1thecontrol(8.35)canbeputintheformu(t)=−Kp1˙y(t)−Kp2y(t)−Ki(cid:2)e(t)dt+const.andweobtainthetraditionalPIDcontroller.EXAMPLE256.–Lettherebethestate-spacesystemΣ={ABC}deﬁnedinExample195(section7.4.3)andagainconsideredinExamples233239and242ofsection8.1.4.Thissystemhasatransmissionzeroats=0(sincerkG(0)=1)andhencetheabovecontrolisnotapplicable.EXAMPLE257.–Considerthestate-spacesystemΣ={ABC}withA=⎡⎢⎢⎣−4−300100000−6−80010⎤⎥⎥⎦B=⎡⎢⎢⎣10000100⎤⎥⎥⎦C=(cid:27)13141112(cid:28)242LinearSystemsFigure8.9.StepresponsesofthefeedbacksystemwhichisaminimalrealizationincontrollablecanonicalformofthetransfermatrixofExercise217(Section7.6).Conditions(i)and(ii)ofProposition250arebothsatisﬁed.TheeigenvaluesofAare{−4−3−2−1}.Applythe(α)methodofsection8.1.4tothesystem(8.33).Withα=3/2weobtainKp=(cid:27)4360180−243−6(cid:28)Ki=(cid:27)18−36−1236(cid:28)andtheeigenvaluesofF−GKare{−4−3−3−3−2−2}.Thestepresponsesoftheclosed-loopsystemandthecorrespondingcontrolsarerepresentedinFigures8.9and8.10–seebelow.Thecurvesontheleft(resp.ontheright)arethevariationsofy1andy2(forFigure8.9)andofu1andu2(forFigure8.10)whenr1isaunitstepandr2=0(resp.whenr1=0andr2isaunitstep).Weobservenotonlythatthestaticerroriszerowhichisinlinewiththetheorybutalsoanabsenceofovershootandagooddecouplingbetweenthetwooutputs(eventhoughΣisheavilycoupled).NowitremainstoverifythatwhenthegainmatrixKisdeterminedasinExample257theinputmodulusmarginofΣisgood(andifpossibleequalto1).Theinputsensitivityfunction˜Siofthesystem(8.33)fedbackbythecontrol(8.34)issuchthat(cid:29)(cid:29)(cid:29)˜Si(cid:29)(cid:29)(cid:29)∞=1accordingtoTheorem240sinceα≥0.THEOREM258.–LetΣbefedbythecontrol(8.35).AssumethatthegainmatrixK=(cid:25)KpKi(cid:26)ischosenaccordingtothe(α)method(ormoregenerallytheLQRmethod)insection8.1.4appliedtosystem(8.33)withα≥0.ThentheinputsensitivityfunctionSiissuchthat(cid:23)Si(cid:23)∞=1.StateFeedback243Figure8.10.CorrespondingcontrolsPROOF.WehaveSi=(Im+Li)−1withLi(s)=(cid:15)Kp+1sKiC(cid:16)(sIn−A)−1B.Ontheotherhand˜Si=(cid:9)Im+˜Li(cid:10)−1withLi(s)=(cid:25)KpKi(cid:26)(cid:27)sIn−A0−CsIp(cid:28)−1(cid:27)B0(cid:28).Accordingtoequation(13.9)(section13.1.4)wehave(cid:27)sIn−A0−CsIp(cid:28)−1=(cid:27)(sIn−A)−10(1/s)C(sIn−A)−1(1/s)Ip(cid:28)andhenceLi=˜Li.ThusSi=˜Si.EXAMPLE259.–(Example257continued).ThesingularvaluesofthesensitivityfunctionSiandthoseofLiareplottedinFigures8.11and8.12respectivelyasafunctionofangularfrequencyω.Wenotethat(cid:23)Si(cid:23)∞=1inaccordancewithwhatwasconﬁrmedinTheorem258andthattheloopshapingofthesingularvaluesofLi(iω)issimilartothatinFigure4.15ofsection4.2.9.Thebalancingofthesingularvaluesintheneighborhoodof0dBiscorrect.8.3.*Internalmodelprinciple8.3.1.ProblemsettingWeconsideragainsystemΣdeﬁnedbyequation(8.29)andtheerroredeﬁnedbyequation(8.30)butinamoregeneralcontext.Wealsoconsiderthedisturbancesdi244LinearSystemsFigure8.11.SingularvaluesofSiFigure8.12.SingularvaluesofLiStateFeedback245(i=12)aswellasthereferencesignalrwhicharetorsionelementsovertheringR[∂](seesection13.4.1).ThesetofallannihilatingdifferentialpolynomialsofthesedisturbancesandofthisreferencesignalisanidealIofR[∂];theidealIisprincipalandgeneratedbyauniquemonicpolynomialdenotedbyD(∂).6Thesituationconsideredinsection8.2isaspecialcaseofthisobtainedwithD(∂)=∂.OntheotherhandthispolynomialD(∂)correspondstowhatisconsideredinsection6.4.Itisassumedthatallitsrootslieontheimaginaryaxis(seesection6.4.3Hypothesis116)andthattheserootsaresimple(thelatterconditionmakesitpossibletosimplifytheproblemsetting:seethelastparagraphofsection6.4.3).WehavethusbyhypothesisD(∂)di=0(i=12)D(∂)r=0.(8.36)AlsoHypotheses(H1)and(H2)ofsection8.2.2areinforce.Theobjectiveofwhatfollowsistodesignacontrollawforwhichthefollowingproperty(P’)issatisﬁed:–limt→+∞e(t)=0–xisbounded–uisboundedforanydisturbancesdi(i=12)andanyreferencesignalrsatisfyingequation(8.36).8.3.2.SolutionLeft-multiplying(8.29)byD(∂)weobtainputtingη=D(∂)xς=D(∂)eandυ=D(∂)u(cid:20)˙η=Aη+Bυς=Cη.Wehavee=D(∂)−1Ipς.Let{A1B1C1}beaminimalrealizationofD(s)−1.ThenaminimalrealizationofD(s)−1Ipis{AεBεCε}withAε=1≤i≤pAiBε=1≤i≤pBiCε=1≤i≤pCi6.ThispolynomialD(∂)shouldnotbeconfusedwiththedirecttermmatrixDofthestate-spacesystem.246LinearSystemswhereAi=A1Bi=B1andCi=C1foranyi∈{1...p}andwhere⊕denotesthediagonalsum(seesection13.1.4).Thetrackingerroreistheoutputofsystem{AεBεCε}withinputς.Denotingbyηεthestateofthatsystemwehave(cid:20)˙ηε=Aεηε+Bεςe=Cεηε.Nowletχ=(cid:27)ηηε(cid:28).Thenequation(8.33)issatisﬁedaswellastheoutputequatione=HχwithF=(cid:27)A0BεCAε(cid:28)G=(cid:27)B0(cid:28)H=(cid:25)0Cε(cid:26).Thefollowingresultcanbeeasilyproved:LEMMA260.–ThestatementofLemma252isstillvalidinthepresentcontextreplacing(P)by(P(cid:2)).Proposition253(section8.2.4)canbegeneralizedasfollows:PROPOSITION261.–Thepair(FG)isstabilizableifandonlyifthefollowingtwoconditionshold:(i)m≥p;(ii){ABC}hasnoinvariantzerosthatarerootsofD(s).Then(FG)iscontrollable.PROOF.Letq=d◦(D).AccordingtoProposition186(section7.3)(FG)isstabilizableifandonlyifforanys∈¯C+rkC(cid:25)sIn+pq−FG(cid:26)=n+pq.(8.37)LetV(s)=(cid:25)sIn+pq−FG(cid:26).(1)IfsisnotaneigenvalueofAd(i.e.notarootofD(s))weimmediatelyseethatCondition(8.37)holdsduetothefactthat(AB)iscontrollable.(2)SupposenowthatsisaneigenvalueofAd.WehaveV(s)=V1(s)V2(s)withV1(s)=(cid:27)In000−BεsIpq−Aε(cid:28)V2(s)=⎡⎣sIn−A0BC000Ipq0⎤⎦.StateFeedback247InadditionrkCV1(s)=n+pqbecause(AεBε)iscontrollable.OntheotherhandrkCV2(s)=pq+rkC(cid:27)sIn−A−BC0(cid:28).AccordingtoHypothesis(H2)andProposition163(section7.2.1)rkR(s)(cid:27)sIn−A−BC0(cid:28)=n+min{mp}andhencerkCV2(s)≤pq+n+min{mp}withequalityifandonlyifsisnotaninvariantzeroof{ABC}.AccordingtotheSylvesterinequality(13.7)(section13.1.4)wehaverkCV(s)≥rkCV1(s)+rkCV2(s)−(n+p+pq).Thereforethequantityontheright-handsideofthisinequalityisequalton+pqifandonlyifConditions(i)and(ii)statedaresatisﬁed.InthiscasewehaverkC(cid:25)sIn+pq−FG(cid:26)=n+pqforalls∈C.AssumingthatConditions(i)and(ii)ofProposition261holdwecanthusstabilize(8.33)byafeedbackoftheform(8.34)i.e.υ=−(cid:25)KpKε(cid:26)(cid:27)ηηε(cid:28).(8.38)Thisexpressioncanbefurtherdevelopedinthefollowingmanner:D(∂)u=−KpD(∂)x−Kεηε.Letxε=D(∂)−1Imηε.Accordingtothepreviousdiscussionweobtaintheexpressionofthe“statefeedbackwithinternalmodel”controller˙xε=Aεxε+Bεe(8.39)u=−Kpx−Kεxε.(8.40)The“internalmodel”isthesystem(8.39):itisamodelofthedisturbancesandofthereferencesignalduplicatedptimesandhavingthetrackingerroreasinput.REMARK262.–(i)ThestatexdisonlydeﬁneduptoatermannihilatedbyD(∂)andthesameistrueforthecontrolu.(ii)Thecontrol(8.40)istheexactgeneralizationofequation(8.35)(wheretheonlytermjustasmentionedaboveisaconstantofintegration).(iii)ThediagramobtainedforthecontrollawisidenticaltothatinFigure8.8withtheonlydifferencethattheintegratorneedstobereplacedbytheinternalmodel.(iv)Remark254remainsvalid.(v)ThestatementofTheorem258remainsvalidinthemoregeneralcontextthatisconsideredhere.248LinearSystems8.4.ExercisesEXERCISE263.–Showthattheinvariantzerosofastate-spacesystem{ABCD}areinvariantunderstatefeedback.EXERCISE264.–Lettherebeastate-spacesystemΣ={ABC}withA=(cid:27)021−1(cid:28)B=(cid:27)1−2(cid:28)C=(cid:25)21(cid:26).(i)StudycontrollabilityandobservabilityofΣ.(ii)CalculatethetransferfunctionG(s)ofΣitspolesanditsinvariantzeros.(iii)Withoutmakinganyadditionalcalculationindicatewhetheritispossibletomaintainthesystemoutputtoaconstantreferencebywayofastatefeedbackwithintegralaction.(iv)Determineastatefeedbackwithintegralactionforwhichthepolesoftheclosed-loopsystemare{−1−1−2}.(v)TracetheshapeoftheNyquistplotoftheopen-looptransferfunctionLi(s).EXERCISE265.–AnswerthesamequestionsasthoseofExercise264whenΣisthelinearizedinvertedpendulumwhosestate-spacerepresentationhasbeendeterminedinExercise209(Section7.6)assumingthatthecontrolledvariableisthepositionyofthecarriageandgiventhefollowing:M=10kgm=1kgl=0.981m.Forquestion(iv)choosethepolesoftheclosed-loopsystemtobe{−2−3−4−5−6}.EXERCISE266.–ConsiderthesystembelowconsistingoftwoidenticaltankstheﬁrstonedrainingintothesecondoneinFigure8.13.Thesysteminputisthedischargeofwatergoingintothe1sttanktheoutputisthewaterlevelx2inthe2ndtank.x1x2uFigure8.13.SystemoftwotanksStateFeedback249(i)Justifythefactthatwehaveastate-spacesystem{ABC}whereAandBareoftheformA=(cid:27)−α20α2−α3(cid:28)B=(cid:27)α10(cid:28)anddeterminethematrixC.Assumefromnowonthatα1=α3=0.01andα2=0.02.(ii)Isthissystemcontrollable?observable?(iii)Whatarethepolesofthissystem?Isthesystemstable?(iv)Doesthissystemhavezeros(ifyeswhatarethey)?Whatisitsstaticgain?(v)Supposeallthecomponentsofthestatearemeasuredanddetermineastatefeedbackwithintegralactionforwhichallthepolesoftheclosed-loopareplacedat−0.05.Howcansuchachoicebejustiﬁed?EXERCISE267.–ConsiderthehotairballooninFigure8.14.Let:–θbethevariationoftemperatureintheballoonwithrespecttoareferencetemperature;–ubethevariationofthequantityofheatprovidedbytheballoon(whichisthecontrolvariable);–wbetherisingspeedofthewind(whichisadisturbance);–hbethevariationofthealtitudeoftheballoonwithrespecttoareferencealtitude;–vbetheverticalvelocityoftheballoon.Aroughmodelleadstothefollowingequations:⎧⎨⎩τ1˙θ=−θ+τ1uτ2˙v=−v+στ2θ+w˙h=vFigure8.14.Hotairballoon250LinearSystemswhereτ1andτ2aretimeconstantswithvaluesof10sand1srespectively;σisacouplingfactorofvalue1takingintoaccountthechosenunits.(i)Writethissysteminstate-spaceformwithinput(cid:27)uw(cid:28)andoutputh.(ii)Determinethepolesofthissystem.Isitstable?(iii)Weatﬁrstassumethatu=0.(a)Writethesystemthusobtainedinstate-spaceformwithinputwandoutputh.(b)Isthissystemobservable?(c)Interprettheresultsphysically.(iv)Wenowassumethatw=0.(a)Repeatthequestionsof(iii)byreplacingwbyu.(b)Determinethestatefeedbackthatassignsallclosed-looppolesat−1.EXERCISE268.–*(i)LetPbeasolutionofthealgebraicRiccatiequation(8.19)letT=(cid:27)I0PI(cid:28)andletF=A−BK.(a)CalculateT−1HTwhereHistheHamiltonianmatrix(8.20).(b)ShowthatH(cid:27)IP(cid:28)=(cid:27)IP(cid:28)F.(ii)Using(i)proveParts(i)-(iii)ofTheorem244.(Thereadercanﬁndaprecisediscussionofthesolutionofthisexercisein([64]section3.4Exercises3.4-9and3.4-10.)*EXERCISE269.–Extendthetheoriesdevelopedinsections8.2and8.3tothecasewherethesystemtobecontrolledhasadirecttermi.e.the2ndequationofequation(8.29)isy=Cx+Du+d2.Chapter9ObserversInthepreviouschapterwehaveseenhowtousethesystemstatetocontrolthelatterwhenallstatecomponentsaremeasuredthusknownateachinstant.Inthischapterweconsiderthesituationwherethestatex(t)isentirelyorpartlyunknownateachinstantt.9.1.Full-orderobservers9.1.1.GeneralprincipleConsiderthestate-spacesystem{ABCD}:(cid:20)˙x=Ax+Buy=Cx+Du(9.1)wherex(t)∈Rnu(t)∈Rmandy(t)∈Rp.Thestatexisunknownwhiletheoutputyandtheinputuareassumedtobeknown.Wesuggesttodeterminefromthesedataavariablet(cid:3)→ˆx(t)suchthattheerror˜x(t)=x(t)−ˆx(t)(9.2)tendsto0whent→+∞.Thedeterminationofˆxisoftencalledthe“reconstructionofthesystemstate”andtheterms“stateobserver”and“statereconstructor”aresynonymous.Thequantity˜xdeﬁnedby(9.2)iscalledthereconstructionerrorortheobservationerror.251Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.252LinearSystemsTheformofafull-orderobserveris∂ˆx=(Aˆx+Bu)+˜K(y−Cˆx−Du)(9.3)=(cid:9)A−˜KC(cid:10)ˆx+˜Ky+(cid:9)B−˜KD(cid:10)u(9.4)where˜K∈Rn×pisthe“gainmatrixoftheobserver”(sometimescalledmoresuccinctlythe“observergain”).For˜K=0(9.3)isasimplesimulationofthestate-spacesystem{AB}.Thetermy−Cˆx−Duistheerrorweseewhenthesimulationispoor.Left-multipliedby˜Kthiserrorprovidesa“correctionterm”.Bysubtracting(9.3)fromtheﬁrstlineof(9.1)andtakingintoaccountoftheotherequationsweobtainthedifferentialequationoftheobservationerror˜x:∂˜x=(cid:9)A−˜KC(cid:10)˜x.(9.5)REMARK270.–Equation(9.5)isobtainedinthe“idealcase”wherethesystemthestateofwhichweseektoreconstructisperfectlylineartime-invariantwithmatricesABCandDperfectlyknown.IfweonlyknowapproximationsˆAˆBˆCandˆDofthesematricesequation(9.3)becomes∂ˆx=(cid:9)ˆAˆx+ˆBu(cid:10)+˜K(cid:9)y−ˆCˆx−ˆDu(cid:10)whichcomplicatesalotthedifferentialequationoftheobservationerror.Wecanalreadyconcludethatwewillcertainlybefacedwiththeproblemofrobustnessoftheobserver.Thispointwillbedealtwithlater.DEFINITION271.–(i)Thestateobserver(9.3)issaidtobestableif˜x(t)→0ast→+∞foranyinitialerror˜x(0).(ii)TheeigenvaluesofA−˜KCarecalledtheobserverpoles.THEOREM272.–Thestateobserver(9.3)isstableifandonlyifallitspoleslieinthelefthalf-plane.PROOF.ThisisanimmediateconsequenceofRemark181(section7.3).THEOREM273.–Lettherebethestateobserver(9.3).(i)Thefollowingconditions(a)and(b)areequivalent:(a)foranysymmetricsetP⊂Cofnelementsthereexistsagainmatrix˜KforwhichtheobserverpolesaretheelementsofP;(b)thepair(CA)isobservable.(ii)Thereexistsagainmatrix˜Kwithwhichtheobserverisstableifandonlyif(CA)isdetectable.Observers253PROOF.TheeigenvaluesofA−˜KCareidenticaltothoseof(cid:9)A−˜KC(cid:10)T=AT−CT˜KT(section13.3.4Proposition534).ThetheoremisthusanimmediateconsequenceofCorollary153(section7.1.4)Corollary187(section7.3)andTheorem230(section8.1.2).REMARK274.–Statefeedback↔observerduality.AsaresultoftheproofofTheorem273thedeterminationofthegainmatrixofanobserverandthatofastatefeedbackaretwo“dualproblems”.Morepreciselythegainmatrix˜Kofanobservercanbedeterminedbasedonaﬁctitiousstate-spacesystem(cid:31)ATCT .Forthissystemwedetermineaﬁctitiousstatefeedbackwithgainmatrix˜KTbyusingoneofthemethodsstudiedinChapter8.Thenweset˜K=(cid:9)˜KT(cid:10)T.REMARK275.–Expression(9.4)showsthatanobserverisafeedbacksystem:the“open-loopsystem”∂ˆx=Aˆx+eisfedbackaccordingtoe=−˜KCˆx+zwherez=˜Ky+(cid:9)B−˜KD(cid:10)u.Thereaderisaskedtodrawadiagramofthisloop.9.1.2.Statefeedback/observersynthesisLettherebeastate-spacesystem{ABCD}oftheform(9.1)where(AB)isstabilizableand(CA)isdetectable.1)Since(AB)isstabilizablewecandesignifthestatex(t)isknownateachinstanttastabilizingstatefeedbackcontrolu=v−Kx.If(AB)iscontrollablewecanmoreoverchoosethegainmatrixKinawaythatarbitrarilyassignstheeigenvaluesofA−BKinthelefthalf-plane(withtheusualsymmetryconditionwithrespecttotherealaxis):seeTheorem230(section8.1.2).2)Assumenowthatonlytheoutputy(t)andthecontrolu(t)areknownateachinstantt.Thehypothesisofdetectabilityof(CA)allowsustoafﬁrmthataccordingtoTheorem273(section9.1.1)wecanreconstructthestateusingastablefull-orderobserveroftheform(9.3).If(CA)isobservablewecanmoreoverchoosethegainmatrix˜Kinamannerastoarbitrarilyassigntheobserverpolesinthelefthalf-plane(withthesymmetryconditionwithrespecttotherealaxisasrecalledabove).3)Considerthecontrolu=v−Kˆx(9.6)254LinearSystemswhereˆxistheestimatedstateprovidedbytheobserver.Weobtainthefollowingtheoremsometimescalledthe“separationprinciple”.1THEOREM276.–Thepolesoftheclosed-loopsystemwithcontrol(9.6)aretheelementsofSp(A−BK)˙∪Sp(cid:9)A−˜KC(cid:10)(countingmultiplicities)whereSpdenotesthespectrum–seesection13.3.3–and˙∪denotesthedisjointunion–seesection13.4.2Lemma559.PROOF.Accordingto(9.2)and(9.6)wehaveu=v−K(x−˜x).Wethereforehavefrom(9.1)˙x=Ax+B(v−K(x−˜x))=(A−BK)x+BK˜x+Bv.Finallyaccordingto(9.5)thestateofaclosed-loopsystemis(cid:27)x˜x(cid:28)andthissystemisgovernedby∂(cid:27)x˜x(cid:28)=(cid:27)A−BKBK0A−˜KC(cid:28)-./0Aa(cid:27)x˜x(cid:28)+(cid:27)B0(cid:28)-./0Bavy=(cid:25)C0(cid:26)-./0Ca(cid:27)x˜x(cid:28).Wehavedet(∂I2n−Aa)=det(∂In−A+BK)det(cid:9)∂In−A+˜KC(cid:10)(seesection13.1.4)whichprovesthedesiredresult.REMARK277.–Itisclearaccordingto(9.5)thatthedynamicsoftheobserverarenotcontrollable(intheabsenceofanymodelingerrorandanydisturbance).1.*Someauthorsreservethisterminologyforthe“linearquadraticgaussiancontrol”whichisshowntobea“linearquadraticcontrol”/“Kalmanﬁlter”synthesisinthecontextofoptimalcontrol.Theterm“certaintyequivalenceprinciple”isalsousedinthatcase.*Observers255WewillnowdeterminethetransfermatrixH(s)ofacontrollerdesignedbystatefeedback/observersynthesis.Itisatransferfunctiony→−uwhenv=0andwhenthecontrolsystemissuppressedfromtheloop(the−signisduetothefactthatbyconventionthefeedbackisalwaysnegative;seeFigure4.1insection4.1.1).Accordingtoequations(9.6)and(9.4)∂ˆx=(cid:9)A−˜KC−(cid:9)B−˜KDK(cid:10)(cid:10)ˆx+˜KyandhencethetransfermatrixH(s)isgivenbyH(s)=K(cid:9)sIn−A+BK+˜KC−˜KDK(cid:10)−1˜K.(9.7)9.1.3.Statefeedback/observersynthesisandRSTcontrollerItisworthwhileseekingarelationshipbetweenthestatefeedback/observersynthesisandtheRSTcontrollerasstudiedinChapter6inthecaseofanSISOcontrolsystemassumedtobestrictlyproperinordertosimplifythenotation.Toavoidconfusionthecontrolsystemisdenotedby{FGH}inwhatfollows.Thissystemisthereforegovernedbytheequations(cid:20)˙x=Fx+Guy=Hx(9.8)whereastheobserverisgivenby∂ˆx=(Fˆx+Gu)+˜K(y−Hˆx)(9.9)andthecontrolsatisﬁesequation(9.6).ThetransferfunctionΘ(s)ofthesystem{FGH}isgivenbyΘ(s)=H(sIn−F)−1G(seeProposition157section7.1.5);thereforebyequation(13.8)(section13.1.4)wehaveΘ(s)=B(s)A(s)(9.10)A(s)=det(sIn−F)B(s)=Hadj(∂In−F)Gwhereadj(.)isthe“classicaladjointmatrix”of(.).Suppose{FGH}isminimal.Inthiscaseitsorderanditstransmissionordercoincide(section2.4.6)andthustherationalfunction(9.10)isirreduciblei.e.thepolynomialsB(s)andA(s)arecoprime.256LinearSystemsOntheotherhandletAc(∂)=det(∂In−F+GK)Ao(∂)=det(cid:9)∂In−F+˜KH(cid:10).Accordingto(9.9)wehave(cid:9)∂In−F+˜KH(cid:10)ˆx=Gu+˜Ky(9.11)andweknowthat(cid:9)∂In−F+˜KH(cid:10)−1=adj(cid:9)∂In−F+˜KH(cid:10)Ao(∂).(9.12)LetQ(∂)=Kadj(cid:9)∂In−F+˜KH(cid:10)GR(∂)=Kadj(cid:9)∂In−F+˜KH(cid:10)˜K.Wegetfromequations(9.6)(9.11)and(9.12)S(∂)u=−R(∂)y+T(∂)vwithS(∂)=Ao(∂)+Q(∂)andT(∂)=Ao(∂).Astatefeedback/observersynthesisisthereforeidenticaltoaparticularRSTcontroller.Accordingtosection6.2.1thecharacteristicpolynomialoftheclosed-loopsystemisAcl(∂)=A(∂)S(∂)+B(∂)R(∂)andbyTheorem276(section9.1.1)wehave(withthenewnotation)Acl(∂)=Ac(∂)Ao(∂).(9.13)Thislastexpressionistobecomparedwithequalities(6.20)and(6.24)ofsection6.3.5.ThetransferfunctionΘbf(s)oftheclosed-loopsystem(withinputvandoutputy)isgivenbyΘbf(s)=B(s)T(s)A(s)S(s)+B(s)R(s)=B(s)Ac(s).Thepolesoftheobserverarethushiddenmodesoftheclosed-loopsystemwhichisconsistentwithRemark277(section9.1.2).(AsacomplementtothisanalysisseeExercise301.)Observers2579.1.4.LTRmethodIntroductionWehaveshowninsections8.1.3and8.1.4thatitispossibletodesignastatefeedbackcontrolforanSISOoranMIMOstate-spacesystemΣsuchthatitwillstabilizetheclosed-loopsystemwithacertainrobustnessandmorespeciﬁcallyaninputmodulusmarginequalto1.Twoconditionsarerequiredforthis:thecontrollability(orbydefaultthestabilizability)ofΣandthepossibilitytoknowallthestatecomponentsateachinstant.Astateobserverhastheroleofrelaxingthislastconstraint.Butthereisthequestionofknowingunderwhatcondition(ifitexists)thepresenceofanobserverwillnotdestroytherobustnessoftheclosed-loop.ThisisaproblemthathasalreadybeentoucheduponintheframeworkofRSTcontrollers.IndeedsupposeΣisaminimalSISOstate-spacesystem.Astatefeedbackcontrolcanbewrittenintheformofa“partialstatefeedbackcontrol”(seesection8.1.3).Asaresultaccordingto(9.13)thechoiceinsection6.3.5of(6.20)asacharacteristicpolynomialoftheclosed-loopcorrespondstothatofastateobserverthepolesofwhicharethetransmissionzerosofΣplusfastpoles.Itisthischoicewhichallowsustoconserveinanapproximatemannerthemodulusmarginobtainedbystatefeedback(Theorem112section6.3.5).AnecessaryandsufﬁcientconditionforthesynthesisofsuchanobservertobepossibleisthatΣshouldbeminimumphase.AccordingtoProposition90(section4.2.8)thislimitationisinthenatureofthingsandisnotrelatedtothemethodused.CaseofanSISOsystemForanSISOsystemΣ(assumedtobeminimal)itispossibletodesignbypoleplacementafull-orderobserverliketheoneabove.Thisobserverisindeedentirelydeterminedbyitscharacteristicpolynomial.LetAo(∂)=B∗(∂)F(∂)(9.14)bethischaracteristicpolynomial(withthenotationusedinequation(6.24)(section9.1.3).LetL(s)(resp.Lsf(s))bethetransferfunctionoftheopenloopwhenthecontrolusedisthestatefeedback/observersynthesis(resp.thestatefeedback).(i)IfΣhasnotransmissionzerosintheclosedrighthalf-planethenB∗(∂)=B(∂)andhencethehypothesesofTheorem112aresatisﬁedand(cid:29)(cid:29)(cid:29)11+L−11+Lsf(cid:29)(cid:29)(cid:29)∞→0whentherootsofthepolynomialF(∂)tendto−∞ontherealaxis.(ii)IftheonlytransmissionzerosΣpossiblyhasintheclosedrighthalf-planehaveanabsolutevaluethatismuchlargerthanthemaximumcutofffrequencyωcoftheclosed-loopsystemthenwhentherootsofF(∂)havetheabovebehaviorL(iω)→Lsf(iω)withagoodapproximationatfrequenciesωforwhichB∗(iω)(cid:9)B(iω)i.e.suchthatω≤ωc.258LinearSystemsCaseofanMIMOsystemForasystemΣwithseveraloutputsweencounterthesamedifﬁcultyasinsection8.1.4i.e.afull-orderobserverisnotdeterminedbyitspoles(orbyitscharacteristicpolynomial).WearethusledtoconsideradualoftheLQRsolutionofsection8.1.4.Letusﬁndouttheexactnatureofthisduality.Inthetablebelowthedatathatarenecessaryforthesynthesisofastatefeedbackareshownintheleftcolumnthosethatarenecessaryforthesynthesisofanobserverareshownintherightcolumn(thematrixdenotedbyˇPinsection8.1.4isdenotedbyPinwhatfollows).AATBCTK˜KTQ˜QR˜RP˜PTheﬁrstthreerowsexpressthedualityalreadymentionedinRemark274(section9.1.1).Thenextthreerowsareinterpretedasfollows:considertheexpression(8.18)whereKRBandParereplacedby˜KT˜RCTand˜Prespectively.Wehave˜KT=˜R−1C˜Pfromwhichweget˜K=˜PCT˜R−1.(9.15)Let’sworkthesamewayasinexpression(8.19)withα=0andreplacingAbyAT.Weobtain˜PAT+A˜P−˜PCT˜R−1C˜P+˜Q=0.(9.16)REMARK278.–*(i)Expressions(9.3)(9.15)and(9.16)arethoseofthe“Kalmanﬁlter”dualofthe“linearquadraticcontrol”[2]*.(ii)By“dualizing”Theorem244ofsection8.1.4(withα=0)weobtainthetheorembelow.LettherebetheHamiltonianmatrix˜H=(cid:27)AT−CT˜R−1C−˜Q−A(cid:28)andlet˜Ebeasquarerootof˜Q.Alsolet(cid:20)˜νi(cid:1)(cid:27)˜ξi˜ηi(cid:28)1≤i≤n&beasetofgeneralizedlinearlyindependenteigenvectorsof˜Hwhere˜νiisassociatedwiththeeigenvalue˜λiof˜H.Lastlet˜η=(cid:25)˜η1...˜ηn(cid:26)˜ξ=(cid:25)˜ξ1...˜ξn(cid:26)Observers259andsupposingthat˜ξisinvertiblelet˜P=˜η˜ξ−1.(9.17)THEOREM279.–(i)Thematrix˜Pdeﬁnedby(9.17)isasolutionofthealgebraicRiccatiequation(9.16).(ii)Thefollowingconditions(a)and(b)areequivalent:(a)thereexistsamatrix˜Poftheform(9.17)suchthatA−˜KCisastabilitymatrix(thismatrix˜Pwhichisthenuniqueandsatisﬁes˜P≥0isobtainedbychoosingtheneigenvalues˜λiinthelefthalf-plane);(b)(CA)isdetectableand˜Hhasnoimaginaryeigenvalues.(iii)Supposefromhereonwardthat(CA)isdetectable.ForCondition(a)or(b)tobesatisﬁeditissufﬁcientthatthefollowingconditionholds:(c)(cid:9)A˜E(cid:10)isstabilizable.(iv)SupposeCondition(c)holds.Thenthereexistsauniquesolution˜PtothealgebraicRiccatiequation(9.16)forwhichA−˜KCisastabilitymatrix;˜Pisalsotheuniquenon-negativedeﬁnitesolutionto(9.16).(v)Ifinaddition(cid:9)A˜E(cid:10)iscontrollablethen˜P>0.Theso-called“LTR”method2consistsofchoosingthematrix˜Qasafunctionofarealnumberq≥0(itisthereforepreferabletodenotethismatrixby˜Qq)accordingtotherelation˜Qq=˜Q0+q2BVBT(9.18)whereV∈Rm×mV>0andoflettingqtendto+∞.Wehaveﬁrsttheresultbelowwhichisadirectconsequenceof([119]section3.10Theorem3.6):LEMMA280.–(i)Let˜E0beasquarerootof˜Q0.If(cid:9)A˜E0(cid:10)isstabilizable(resp.controllable)thensois(cid:21)A(cid:30)˜Qq(cid:22)foranyq≥0.(ii)If(AB)isstabilizable(resp.controllable)thensois(cid:9)AB√V(cid:10).Nowconsiderthefollowinghypothesis(HLTRu)3where˜E0isasquarerootof˜Q0andG(s)=C(sIn−A)−1B:(HLTRu):(a)Thestate-spacesystemΣ={ABC}isminimal;(b)A−BKisastabilitymatrix;(c)(cid:9)A˜E0(cid:10)iscontrollable;(d)G(s)∈R(s)m×m;2.LTR:acronymof“LoopTransferRecovery”(seeRemark283below).3.Hypothesis(HLTRu)isstrongerthanHypothesis(HLTR)usuallyconsidered(seeRemark283below);itguaranteesuniform–insteadofsimple–convergenceonthecompactsubsetsof¯C+.260LinearSystems(e)rkR(s)G(s)=m;(f)G(s)hasnoMacMillanzerosintheclosedrighthalf-plane¯C+.AssumingthatConditions(a)and(c)ofHypothesis(HLTRu)holdthealgebraicRiccatiequation(9.16)(where˜Q=˜Qq)admitsforanyq≥0auniquenon-negativedeﬁnitesolution(thissolutionisdenotedby˜Pqinwhatfollows)and˜Pq>0(accordingtoTheorem279andLemma280).Let˜Kq=˜PqCT˜R−1bethecorrespondingobservergain.Thetransfermatrixoftheopen-loopattheinputofΣusuallydenotedbyLi(s)willbedenotedbyLq(s)inwhatfollowsjusttoclearlyindicateitsdependencewithrespecttotheparameterq;itisgivenbyLq(s)=Hq(s)G(s)whereHq(s)isthetransfermatrixH(s)satisfyingequation(9.7)(seesection4.1.2whereH(s)andG(s)aredenotedbyK(s)andP(s)respectively).Theinputsensitivityfunction(ormatrix)isthusgivenbySq(s)=(Im+Lq(s))−1.ThetransfermatrixLq(s)istobedistinguishedfromwhatwewouldobtainwithastatefeedbackcontrolwhichisLsf(s)=KΦ(s)BwhereΦ(s)=(sIn−A)−1andtowhichcorrespondstheinputsensitivityfunction(ormatrix)Ssf(s)=(Im+Lsf(s))−1.Wehavethefollowingtheoremwhichdescribesthe“uniformLTReffect”:THEOREM281.–LetHypothesis(HLTRu)beinforce.ThenforanycompactsetK⊂¯C+Sq(s)−Ssf(s)→0uniformlywithrespecttos∈Kasq→+∞.PROOF.1)Foranyq>0thealgebraicRiccatiequation(9.16)canbewrittenintheform˜Pqq2AT+A˜Pqq2−˜Pqq2CT*˜Rq2+−1C˜Pqq2+˜Q0q2+(cid:9)√VBT(cid:10)T(cid:9)√VBT(cid:10)=0.(9.19)Condition(a)ofHypothesis(HLTRu)impliesthat(cid:15)ATCT(cid:16)iscontrollableand(cid:9)√VBTAT(cid:10)isobservable(accordingtoCorollary153ofsection7.1.4andLemmaObservers261280(ii)above).AccordingtoRemark249(ii)ofsection8.1.4(withρ=1q2andPρ=˜Pqq2)˜Pqq2→0asq→+∞.Thereforeasq→+∞*˜Pqq2+CT*˜Rq2+−1C*˜Pqq2+→BVBT.Accordingto(9.15)˜Kqisthereforesuchthat1q2˜Kq˜R˜KTq→BVBTandrk˜Kq=mbecause˜Pq>0.FromCorollary586(section13.5.7)thereexistsforanyq>0anorthogonalmatrixUq∈Rm×mandamatrix1(1/q)∈Rn×msuchthat%˜R˜KTq=q(cid:9)Uq√VBT+%˜R1(1/q)T(cid:10)and1(1/q)→0asq→+∞.PuttingWq=√VUTq%˜R−1itfollowsthat˜Kq=q(BWq+1(1/q)).(9.20)2)LetΨ(s)=(sIn−A+BK)−1.WehaveHq(s)=KΨ(s)(cid:9)Im+˜KqCΨ(s)(cid:10)−1˜KqandaccordingtoLemma520(section13.3.3)Hq(s)=KΨ(s)˜Kq(cid:9)Im+CΨ(s)˜Kq(cid:10)−1=KΨ(s)J(qs)(9.21)whereJ(qs)=˜Kqq*Imq+CΨ(s)˜Kqq+−1.(9.22)From(9.20)J(qs)=(BWq+1(1/q))(cid:21)Imq+CΨ(s)(BWq+1(1/q))(cid:22)−1262LinearSystemsandhencewithGc(s)=CΨ(s)BJ(qs)=BG−1c(s)(cid:21)Im+(Gc(s)Wq)−1(cid:21)CΨ(s)1(1/q)+Imq(cid:22)(cid:22)+1(1/q)G−1c(s)..(cid:21)Im+(Gc(s)Wq)−1(cid:21)CΨ(s)1(1/q)+Imq(cid:22)(cid:22).(9.23)LetK⊂¯C+beacompactset.AccordingtoConditions(d)(e)and(f)ofHypothesis(HLTRu)Gc(s)isinvertibleinK(seeChapter8Exercise263)andisboundedinthisset.FromCondition(b)ofHypothesis(HLTRu)thetransfermatrixΨ(s)isalsoboundedinK.Asaresultaccordingto(9.23)J(qs)=BG−1c(s)(Im+2(1/qs))where2(1/qs)→0uniformlywithrespecttos∈Kasq→+∞.Usingthisexpressionin(9.21)weobtain(byanidenticalrationale)Hq(s)=(Im+3(1/qs))KΨ(s)BG−1c(s)where3(1/qs)→0uniformlywithrespecttos∈Kasq→+∞.OntheotherhandΦ(s)B=Ψ(s)(cid:9)Im−BK(sIn−A+BK)−1(cid:10)−1andhencefromLemma520(section13.3.3)Φ(s)B=Ψ(s)B(Im−KΨ(s)B)−1.ThereforeLq(s)=(Im+3(1/qs))KΨ(s)B(Im−KΨ(s)B)−1andhenceSq(s)=(Im−KΨ(s)B)(Im+3(1/qs)KΨ(s)B)−1.InadditionSsf(s)=(Im+KΦ(s)B)−1=(cid:9)Im+KΨ(s)B(Im−KΨ(s)B)−1(cid:10)−1=Im−KΨ(s)B.SincethetransfermatrixKΨ(s)BisboundedinKSq(s)→Ssf(s)uniformlywithrespecttos∈Kasq→+∞.Observers263REMARK282.–IthasnotbeenprovenintheabovetheoremthatSq(s)→Imasqand|s|bothtendto+∞(withs∈¯C+).Wethuscannotassertthat(cid:23)Sq−Ssf(cid:23)∞→0asq→+∞(theauthordoesnothoweverknowofanycounterexampletothispropertyofwhichwemayconjecture).REMARK283.–The“LTR”methodpopularizedbyDoyle[38]fromatheoryduetoKwakernaakandSivan[71]canbedescribedasfollows(see[93]forarigorousproof):lettherebeHypothesis(HLTR)below:(a)Thestate-spacesystem{ABC}isstabilizableanddetectable;(b)A−BKisastabilitymatrix;(c)(cid:9)A˜E0(cid:10)isstabilizable;(d)G(s)∈R(s)p×m;(e)rkR(s)G(s)=m;(f)G(s)hasnoMacMillanzerosintheclosedrighthalf-plane¯C+.UnderthishypothesisLq(s)→Lsf(s)asq→+∞foranys∈Cwhichisnotapoleofsystem{ABC}.EXAMPLE284.–ConsidertheminimalsystemwithtransferfunctionG(s)=1s4anditscontrollablecanonicalrealization{ABC}(seesection7.4.2).(1)Letxbeameasuredstate.Thegainmatrixofthestatefeedbackcontrolforwhichthepolesoftheclosed-loopsystemareplacedat{−1−1−1−1}is:K=(cid:25)4641(cid:26).Thischoiceofthepolesoftheclosed-loopsystemcomplieswithRule111(section6.3.5).(2)Nowsupposewehavethestateobserverspeciﬁedabovewith˜Q0=I4andV=1.TheBodeplotsoftheopen-looptransferfunctionsarerepresentedinFigure9.2inthefollowingorder(goingfromlowtohighinthehighfrequencies)4:withthestatefeedback/observersynthesisandq=0q=102q=104q=106andthenwiththecompletestatefeedback.ThecorrespondingBodeplotsofthesensitivityfunctionsareshowninFigure9.3withareverseorderinthelowfrequencies.Onecanseethat(cid:23)Sq−Ssf(cid:23)∞→0asq→+∞(seeRemark282).TheBodeplotsofthetransferfunctionsHq(s)ofthecontrollerfortheabove-speciﬁedvaluesofqareshowninFigure9.4inthesameorderasinFigure9.2.Letωq=argmaxω|Hq(iω)|.Asq→+∞weseethat|Hq(iωq)|→+∞andωq→+∞.Thisphenomenonisoneofthethingsthatmakestheanalysisofthebehaviorof|Hq(iω)G(iω)|difﬁcultasbothqandωtendtoward+∞.Rememberthatacontrollerhavingalargegaininthehighfrequenciesrendersthecontrolverysensitivetomeasurementnoise(seesection4.2.6)whichistobeavoided.Itisthereforerecommendedtoincreaseparameterquntilasufﬁcientmodulusmarginisobtainedandthennottogobeyondthatvalue.4.Thecurvesrelatedtosections9.1and9.2areassembledattheendofsection9.2.264LinearSystems9.2.Statefeedback/observersynthesiswithintegralaction9.2.1.ProblemsettingInthissectionthecontrolmethodbystatefeedbackwithintegralactionstudiedinsection8.2iscombinedwithafull-orderobserver.ConsiderthefollowingsystemΣ:⎧⎨⎩∂x=Ax+Bu+d1y=Cx+d2z=Ey(9.24)wherex(t)∈Rnisthestateu(t)∈Rmisthecontroly(t)∈Rkisthemeasuredvectorandz(t)∈Rpistheregulatedvariable;d1∈Rnandd2∈Rkareconstantdisturbancesunmeasuredandunknown(seesection8.2.2).ThematricesABandCareuncertaininpracticebutmatrixEisassumedtobeperfectlyknown.Thethirdlineof(9.24)canforexamplesignifythatzisconstitutedoftheﬁrstprowsofyandinthatcaseE=(cid:25)Ip0(cid:26).Letr∈Rpbethereferencesignalassumedtobeknownandlettheregulationerrorbee=z−r.OuraiminwhatfollowsistodesignacontrollawforwhichProperty(P)ofsection8.2.2issatisﬁed.Letuswritez=E(Cx+d2)=Hx+d3withH=ECd3=Ed2.Theproblemconsideredherewouldreducetothatofsection8.2.2ifthestateweremeasured.Thereforethesamehypothesesasforthislastproblemareinforce:(H1)(AB)iscontrollable;(H2)thetransfermatrixG(s)ofSystem{ABH}issemiregularoverR(s).Asupplementarystepinthepresentcaseconsistsinreconstructingthestateusingthemeasuredvectorywhichleadsustoaddthehypothesisbelow:(H3)(CA)isobservable.Observers2659.2.2.AlgebraicsolutionWedeﬁnethevariablesχandυasinequation(8.32)ofsection8.2.4.Wegetstateequation(8.33)withF=(cid:27)A0H0(cid:28)G=(cid:27)B0(cid:28).BasedonHypotheses(H1)and(H2)andProposition253wehavethefollowing:PROPOSITION285.–(FG)isstabilizableifandonlyifthefollowingtwoconditionshold:(i)m≥p;(ii)s=0isnotaninvariantzeroofsystem{ABH}.Inthatcase(FG)iscontrollable.AssumingthatConditions(i)and(ii)ofProposition285holdthereexistsagainmatrixK∈Rm×(n+p)forwhichF−GKisastabilitymatrix.MorepreciselybychoosingKinanappropriatemannertheeigenvaluesofF−GKcanbeplacedinanarbitrarysymmetricsubsetwithn+pelementsofthelefthalf-plane(seesection8.1.2Theorem230).Letη=∂xandconsidertheobserverbelowwithaimsatreconstructingη:∂ˆη=Aˆη+Bυ+˜K(∂y−Cˆη).(9.25)Indifferentiatingtheﬁrsttwoequationsof(9.24)weobtain(cid:20)∂η=Aη+Bυ∂y=Cη;(9.26)asaresultputting˜η=η−ˆηwegetfromequations(9.25)and(9.26)∂˜η=(cid:9)A−˜KC(cid:10)˜η.(9.27)AccordingtoHypothesis(H3)thereexistsagainmatrix˜K∈Rn×kforwhichA−˜KCisastabilitymatrix.NowwritethegainmatrixKintheformK=(cid:25)KpKi(cid:26)Kp∈Rm×nKi∈Rm×pandconsiderthecontrollawυ=−Kpˆη−Kie.(9.28)266LinearSystemsLetξ=⎡⎣ηe˜η⎤⎦.Weobtainfromequations(9.26)(9.11)and(9.28)∂ξ=⎡⎣A−BKp−BKiBKpH0000A−˜KC⎤⎦ξ(cid:1)Mξ.SincethematrixMisoftheformM=(cid:27)F−GK∗0A−˜KC(cid:28)wehaveSp(M)=Sp(F−GK)˙∪Sp(cid:9)A−˜KC(cid:10)(9.29)whichprovesthatMisastabilitymatrix.Letˆx(t)=(cid:2)ˆη(t)dt˜x=x−ˆx.BythesamerationaleasintheproofofLemma252(section8.2.4)weobtainthefollowingresult:THEOREM286.–Usingthecontrollaw(9.28)wehave:limt→+∞e(t)=0limt→+∞x(t)=const.limt→+∞˜x(t)=const.limt→+∞u(t)=const.Theproblemwhichremainstobesolvedistheimplementationofthecontrollaw(9.28).SincethegainmatricesKpKiand˜Kareconstant5weobtainbyintegrating5.Thecasewherethegainsarescheduledisquitedifferent[80].Thenequations(9.28)and(9.25)mustbekeptandthecontrolumustbecalculatedbyintegratingυusinganintegratorplacedjustattheinputofthecontrolsystem.Observers267equations(9.28)and(9.25)u(t)=−Kpˆx(t)−Ki(cid:2)e(t)dt+c(9.30)∂ˆx=Aˆx+Bu+˜K(y−Cˆx)+c(cid:2)(9.31)wherecandc(cid:2)areintegrationconstants.REMARK287.–(i)Takingc(cid:2)=0equation(9.31)istheclassicexpressionofanobserverofstatex.(ii)Thecontrollaw(9.30)isnothingbutthe“controlbystatefeedbackandintegralaction”(8.35)ofsection8.2.4wherexhasbeenreplacedbyˆx.Theorem286andequality(9.29)canbeconsideredasanextensionofthe“separationprinciple”(Theorem276section9.1.2).Thediagramoftheclosed-loopsystemisshowninFigure9.1.Notethattheregulationerrore=z−risnotreconstructed(neitheristhecontrolledvariablez).9.2.3.ExtensionoftheLTRmethodSupposewechoosethegainmatrixKusingthe(α)methodortheLQRmethodofsection8.1.4.ThenaccordingtoTheorem258(section8.2.4)weobtainusingthecontrollaw(8.35)(assumingthatthestateisknown)aclosed-loopsystemwhoseinputsensitivityfunctionSisatisﬁes(cid:23)Si(cid:23)∞=1.Likeinsection9.1.4thequestiononhandnowistoknowhowtodesignthestateobserver(9.25)(orinanequivalentmannerthestateobserver(9.31))inawayasnottodegrade(toomuch)thisrobustnessproperty.WearegoingtoshowthattheLTRmethoddetailedinsection9.1.4extendstothesituationnowconsidered.Theobservergainnowdependsonparameterq≥0andisdenotedby˜Kq.Letusﬁrstdeterminethetransfermatrix(cid:25)Hqy(s)Hqe(s)(cid:26)ofthecontrollerwithinput(cid:27)ye(cid:28)andoutputuassumingthattheexternalsignals(i.e.thedisturbancesd1andd2andthereferencesignalr)arezero.Wehave(cid:9)∂In−A+˜KqC+BKp(cid:10)ˆη=˜Kq∂y−BKie.AsaresultHqy(s)=Kp(cid:9)sIn−A+BKp+˜KqC(cid:10)−1˜KqHqe(s)=(cid:27)Im−Kp(cid:9)sIn−A+BKp+˜KqC(cid:10)−1B(cid:28)Kis.268LinearSystemsWecanalsoconsiderthatonlythemeasuredvectorygoesintothecontroller(takingintoaccounttherelationz=Ey);thiscontrollerthushasasatransfermatrixHq(s)=Hqy(s)+Hqe(s)E.SupposeHypothesis(HLTR)ofRemark282(section9.1.4)isinforceandletPbethelocusofthecontrollerpoles(i.e.Sp(cid:7)A−BKp−˜KqC(cid:8)˙∪{0})whenqvariesfrom0to+∞.THEOREM288.–Letq→+∞.(i)Foranys∈¯C+(cid:2)(cid:15)P∩¯C+(cid:16)sHqe(s)→Ki.(ii)Foranys∈CwhichisnotaneigenvalueofAHqy(s)C(sIn−A)−1B→Kp(sIn−A)−1B.PROOF.ForthesakeofsimplicityassumethatHypothesis(HLTRu)isinforce(ifitisthereplacedHypothesis(HLTR)whichislessrestrictivewemustmakeuseoftheapproachdevelopedin[93]).(i):Wehavefromequation(9.20)˜KqqW−1q∼B.LetΨ(s)=(sIn−A+BKp)−1and∆q(s)=(cid:9)sIn−A+BKp+˜KqC(cid:10)−1B.Foranys∈¯C+(cid:2)(cid:15)P∩¯C+(cid:16)sisnotapoleofΨ(s)and(cid:9)In+˜KqCΨ(s)(cid:10)−1isbounded.Thereforewehavethefollowingequivalentsasq→+∞:∆q(s)=Ψ(s)(cid:9)In+˜KqCΨ(s)(cid:10)−1B∼Ψ(s)(cid:9)Im+˜KqCΨ(s)(cid:10)−1˜KqqW−1q∼Ψ(s)˜Kq(cid:9)Im+CΨ(s)˜Kq(cid:10)−1W−1qq.UsingthesamerationaleasintheproofofTheorem281wethusobtain∆q(s)∼1qΨ(s)B(CΨ(s)B)−1W−1qwhereW−1q=%˜RUq√V−1.Thelastquantityisboundedandhence∆q(s)→0asq→+∞andsHqe(s)→Ki.Theproofof(ii)canclearlybederivedfromthatofTheorem281.Observers269EXAMPLE289.–ConsideragainthesysteminExample257(section8.2.4).ThegainmatricesKpandKihavealreadybeencalculatedbutwenowassumethatthestatexisnotmeasured:themeasuredvectorisy=Cxandz=y.LetG(s)=CΦ(s)BwhereΦ(s)=(sI4−A)−1.Usingthestatefeedback/observersynthesiswithintegralactiontheopen-looptransfermatrixatinputofΣisLq(s)=Hq(s)G(s)whileusingthecontrollaw(8.35)itisequaltoLsf(s)=KpΦ(s)B+KisG(s).Thematrix˜Q0ischosentobetheidentityaswellas˜R.ThetwosingularvaluesofLq(iω)areplottedasafunctionofωinFigure9.5forq=100(–-)6q=104(..)7andq=106(--).ThetwosingularvaluesareplottedinthesameFigure(–).ThetwosingularvaluesofSq(iω)=(I2+Lq(iω))−1andthoseofSsf(iω)=(I2+Lsf(iω))−1areplottedinFigure9.6withthesameconventions.Weobservethatasq→+∞Sq(iω)→Ssf(iω)foranyω;inadditioninthecaseconsidered(cid:23)Sq−Ssf(cid:23)∞→0(seeRemark282insection9.1.4).Withq=102themodulusmarginisalreadysufﬁcient(about−3dB:seesection4.2.9)thusthisvalueofqisretained.Itisnowrelevanttoverifythetime-domainqualitiesofthefeedbacksystem.Thefollowingeventsaresimulated:(i)astepcommandofamplitude1fortheﬁrstvariableandofamplitude−1forthesecondatinstantt=1;(ii)astepdisturbanceofamplitude0.2ontheﬁrstoutputatt=5andofamplitude−0.3onthesecondoutputatt=7.Theﬁrstoutput(–)andthesecondone(--)asafunctionoftimeareshowninFigure9.9.ThecorrespondingcontrolsareshowninFigure9.10withthesameconventions.Theresponsestostepcommandsareidenticaltothoseobtainedwithacontrolbystatefeedbackandintegralaction(seeRemark277section9.1.4).Itisnotthesamefortheresponsestodisturbanceswhichneverthelessaresatisfactorywiththedesignedcontrol.Ifwechoseamuchlargervalueofq(toincreasethemodulusmargin)itwouldresultinacontrollerhavinglargergains(thetwosingularvaluesofHq(iω)areplottedasafunctionofωinFigure9.7forq=100q=104andq=106withthealreadyadoptedconventions)fromwhichasaresponsetodisturbanceswehaveontheonehandacontrolhavingtoolargeamplitudesandontheotherhandastrongcouplingbetweenthetwooutputs.Anotherquestionthatstillremainstobeanswerediswhethertheoutputmodulusmarginissuitableforthevalueretainedforparameterq(i.e.q=100).Indeedtheoutputsensitivityfunctionis(I+GHq)−1;itisdifferentfromtheinputsensitivityfunction(I+Lq)−1=(I+HqG)−1considered6.Alternatelongandshortdashes.7.Dotted.270LinearSystems-     Observer EKpsKiControl system ruyz++-Figure9.1.Closed-loopsystemuptillnow.ThesingularvaluesofthesetwosensitivityfunctionsareshowninFigure9.8:thatattheinputofΣ(..)andthatatitsoutput(--).TheoutputmodulusmarginofΣabout−8dBisclearlynotasgoodasthatattheinputmodulusmarginbutforanMIMOsystemthisvalueisstillreasonable.Figure9.2.LTReffect(1)Observers271Figure9.3.LTReffect(2)Figure9.4.LTReffect(3)272LinearSystemsFigure9.5.SingularvaluesofLiFigure9.6.Singularvalues(inputsensitivityfunction)Observers273Figure9.7.Singularvalues(controller)Figure9.8.Inputandoutputsensitivityfunctions274LinearSystemsFigure9.9.Timeresponses(Example289)Figure9.10.Timeresponses(Example289)Observers2759.3.*GeneraltheoryofobserversThegeneraltheoryofobserversisessentiallyduetoLuenberger[87][88].9.3.1.Reduced-orderobserverConsiderthestate-spacesystem{ABC}:(cid:20)˙x=Ax+Buy=Cx(9.32)wherex(t)∈Rnu(t)∈Rmandy(t)∈Rp.ThematrixCisassumedtobeleft-regular.REMARK290.–Whatfollowseasilyextendstothecasewheretheabovestate-spacesystemhasadirecttermi.e.wherethesecondequationof(9.32)isy=Cx+Du.Theonlythingtodoistoreplaceyby¯y=y−Dueverywhere.Ifthesignalyisverynoisyitisusefulforthecontrollertohaveasmallgaininhighfrequencies(seesection4.2.6).Thisiswhatwegetusingafull-orderobserversincethenthetransfermatrixH(s)givenbyequation(9.7)(withD=0)issuchthatH(iω)→0asω→+∞.Supposenowthatthesignalyislessnoisyandthatthecomponentsofyarepartofthecomponentsofx.Wecanalwayscomebacktothiscase.IndeedletJ∈Rn×(n−p)beamatrixsuchthatT=(cid:27)JC(cid:28)isinvertible.PuttingX=Txwehaveobviouslyy=(cid:25)0n×(n−p)Ip(cid:26)X.Itisthusassumedinwhatfollowsthattheabovealgebraicoperationhasalreadybeenmadeandthereforethatthestatexisoftheformx=(cid:27)xry(cid:28)(9.33)wherexr(t)∈Rn−p.REMARK291.–Neverthelesstwocasesaretobedistinguished:thecasewherethestatexis“bynature”oftheformofequation(9.33)andthecasewherethisstructureisobtainedthroughtheabovealgebraicoperation.ThelatterdoesnotproduceanexactresultifthematrixCisuncertain;the“reduced-orderobserver”thatwillbediscussedcanthenleadtoanotveryrobustcontrollaw.276LinearSystemsThefollowingdecompositionofsystem(9.32)correspondstodecomposition(9.33)ofthestate:(cid:27)˙xr˙y(cid:28)=(cid:27)ArBrCrDr(cid:28)(cid:27)xry(cid:28)+(cid:27)GrHr(cid:28)u.(9.34)Letyr=˙y−Dry−Hru.(9.35)Wehave(cid:20)˙xr=Arxr+Bry+Gruyr=Crxr.(9.36)LEMMA292.–Thepair(CrAr)isobservable(resp.detectable)ifandonlyifthepair(CA)hasthesameproperty.PROOF.Wehave(cid:27)sIn−AC(cid:28)=⎡⎣sIn−p−Ar−Br−CrsIp−Dr0Ip⎤⎦∼⎡⎣sIn−p−Ar0Cr00Ip⎤⎦fromwhichwededucethestatedresultaccordingtoProposition170andTheorem169(section7.2.3)aswellasDeﬁnition185(section7.3).Intherestofthisparagraph(CA)isassumedtobeobservable.(i)Firstletusassumethatthesignalyrisknown(whichinrealityisnotthecasesinceitsdeﬁnition(9.35)involvesthederivativeofy:seesection2.5.3).Inthiscaseafull-orderobserverforthesystem(9.36)isoftheform(seeequation(9.3))∂ˆxr=Arˆxr+Bry+˜Kr(yr−Crˆxr)+Gru(9.37)where˜Kr∈R(n−p)×pissuchthatAr−˜KrCrwillbeastabilitymatrix.(ii)Whatislefttodonowistoreplaceˆxrbyavariablezthatisgovernedbyadifferentialequationfromwhichthevariableyriseliminatedsoastoavoidthedifferentiationofy.Thereforeletz=ˆxr−˜Kry.Weobtain˙z=(cid:9)Ar−˜KrCr(cid:10)z+(cid:9)Br−˜KrDr+Ar˜Kr−˜KrCr˜Kr(cid:10)y+(cid:9)Gr−˜KrHr(cid:10)u(9.38)Observers277andthereconstructedstateˆxfurnishedbythe“reduced-orderobserver”isˆx=(cid:27)ˆxry(cid:28)=(cid:27)z+˜Kryy(cid:28)=(cid:27)In−p0(cid:28)z+(cid:27)˜KrIp(cid:28)y.(9.39)Let˜x=x−˜xbetheobservationerror.PROPOSITION293.–Wehave˜x(t)→0ast→+∞.PROOF.Wehave˜x=(cid:27)˜xr0(cid:28)with˜xr=xr−ˆxrandaccordingto(9.36)and(9.37)˜xr(t)→0ast→+∞.REMARK294.–Theobserver(9.38)(9.39)issaidtobe“ofreduced-order”becauseitisofordern−pwhilethe“full-orderobserver”(9.3)isofordern.Whenn=1onecanshowthattheobserver(9.38)(9.39)is“minimal”inthesensethattheredoesnotexistanobserveroforderlessthann−1thepolesofwhichcanbearbitrarilychosen(see[119]section3.8).9.3.2.GeneralformalismWenowproposetoreconstructavectorµ=Mx(whereM∈Rq×nisaleft-regularmatrix).Thecontrolofsystem(9.32)issupposedtobeoftheformu=−LˆµandwewriteK=LM.Remark290remainsvalid.Themostgeneralformpossiblefortheobserveris(cid:20)˙z=Fz+Gy+Huˆµ=Ez+Jywherezisavectorwithrcomponents.278LinearSystemsLetT∈Rr×nbealeft-regularmatrixandξ=z−Tx.Aftersomeelementarycalculationsweobtainthefollowingdifferentialequationfortheclosed-loopsystem:(cid:27)˙x˙ξ(cid:28)=(cid:27)A−BK−BLE0F(cid:28)(cid:27)xξ(cid:28)(9.40)H=TB(9.41)K=LET+JC(9.42)GC=TA−FT.(9.43)Equality(9.41)posesnoconstraintonsystem(9.32);thefollowingisclear:PROPOSITION295.–Theclosed-loopsystemisstableifandonlyifA−BKandFarestabilitymatricesforitspolesaretheelementsofSp(A−BK)˙∪Sp(F).Letusexaminenowunderwhatconditionthefollowingproperty(P)willbetrue:(P):ThesymmetricsetSp(A−BK)˙∪Sp(F)canbearbitrarilychoseninthelefthalf-planeandequalities(9.42)and(9.43)aresatisﬁed.Thetwocasesconsideredbelowarethefull-orderobserverandthereduced-orderobserver.Caseofafull-orderobserverInthiscaseM=E=T=InandJ=0.WritingG=˜Kequation(9.43)canbewrittenasA−˜KC=F.AsaresultProperty(P)issatisﬁedifandonlyifsystem{ABC}isminimal.Caseofareduced-orderobserverAssumingthatsystem(9.32)isputintheform(9.34)withC=(cid:25)0n×(n−p)Ip(cid:26)wehaveM=InT=(cid:25)In−p−˜Kr(cid:26)F=Ar−˜KrCrandE=(cid:27)K0(cid:28)J=K(cid:27)˜KrIp(cid:28).Equalities(9.42)and(9.43)arethussatisﬁed.AccordingtoLemma292(section9.3.1)Property(P)issatisﬁedifandonlyif{ABC}isminimal.REMARK296.–(i)AccordingtoProposition295the“separationprinciple”(Theorem276section9.1.2)isstillvalidwhenareduced-orderobserverisused.(ii)ConsidertheRSTcontrollerofsection6.4.5withp=δ0=0(usingthenotationofthecitedparagraph).ThecharacteristicpolynomialAcl(∂)isthenofdegreeObservers2792n−1.Theobtainedcontrollerisjustanotherstatefeedback/observersynthesis(seesection9.1.3)whentheobserverusedisofreduced-order(andisinfactminimalaccordingtoRemark294ofsection9.3.1).(iii)The“LTRmethod”canbeextendedtothecasewheretheobserverusedisofreduced-order[28].9.4.ExercisesEXERCISE297.–Let{ABC}bedeﬁnedasinExercise264(section8.4).(i)Determineafull-orderobserverforthissystemhavingadoublepoleat−5.(ii)Howcanyoujustifythischoice?EXERCISE298.–WeconsiderthesystemoftwotanksasdeﬁnedinExercise266(section8.4).(i)Determineafull-orderobserver(assumingthatthemeasuredvariableisthedischargeofthewaterleavingthesecondtank)thathasadoublepoleat−0.5.(ii)Howcanyoujustifythischoice?EXERCISE299.–Lettherebethestate-spacesystemΣ={ABC}whereA=(cid:27)−1210(cid:28)B=(cid:27)10(cid:28)C=(cid:25)1β(cid:26).(i)DeterminethepolesofΣandstudyitsstability.(ii)CalculatethetransferfunctionofΣaswellasitstransmissionpolesandzerosinfunctionofβ.(iii)IsΣcontrollableandforwhichvalue(s)ofβisitnon-observable?(iv)DeterminetheinvariantzerosofΣ.(v)Determineastatefeedbackcontrolu=−Kx+k0r(whereK=(cid:25)k1k2(cid:26)andwhereristhereferencesignalassumedtobeconstant)havingthefollowingproperties:(a)itplacesthepolesat{−2−1};(b)intheabsenceofdisturbancesactingontoΣtheregulationerrore=y−riszeroatsteadystate(zero“staticerror”).(vi)Inwhatfollowsβ=1.SupposeaconstantbutunknowndisturbancedisaddingtotheoutputofΣinsuchawaythattheexpressionofybecomesy=(cid:25)11(cid:26)x+d.(a)WiththecontrolcalculatedatQuestion(v)isthestaticerrorstillzero?(b)IsitpossibletodesignastatefeedbackwithintegralactionforΣ?(c)Ifyesdeterminesuchacontrolsothattheclosed-looppolesareplacedat{−2−1−1}.(d)Howcanyoujustifythischoice?(vii)Thestateisnon-measuredthuswedecidetodesignafull-orderobserverandcombineitwiththecontrollawobtainedinQuestion(vi).Wealsodecidetoplacetheobserverpolesat{−1−10}.(a)Howcanyoujustifythischoice?(b)Calculatethegainmatrix˜Kofthisobserver.EXERCISE300.–Lettherebethestate-spacesystemΣ={ABC}whereA=(cid:27)1/2−1/2−3/2−1/2(cid:28)B=(cid:27)1−1(cid:28)C=(cid:25)10(cid:26).280LinearSystems(i)Studythecontrollabilityobservabilityandstabilityofthissystem.(ii)Studyitsstabilizability.(iii)Showthatusingastatefeedbackcontrolu=−Kxitispossibletoobtainaclosed-loopsystemwhosepolesare{−1λ}whereλisarbitrarilychosenontherealaxis.Foraﬁxedλisthereuniquenessofthesolution?Giveaparameterizationofallsolutionswhenλ=−1(thisvaluewillbemaintainedinwhatfollows).(iv)Weassumefromnowonthatwewillnotmeasurethestatex.Determinethefull-orderobserverwhosepolesare{−1−10}.(v)Assumingthatonlythesecondcomponentofthestateistobereconstructeddetermineaminimalobserverbyjudiciouslychoosingitspole.(vi)Forpoleplacementisanobserver(evenaminimalone)necessary?EXERCISE301.–ConsideraminimalsystemΣwithtransferfunctionG(s)=1/(cid:15)s2+s−1(cid:16).(i)DetermineforΣtheRSTcontrollerwithintegralaction(section6.3.1)placingallclosed-looppolesats=−1andsuchthatthetransferfunctionbetweenreferencerandoutputyisoforder3inthefollowingtwocases:(a)δ0=0;(b)δ0=1.(ii)Let{FGHJ}betheobservablecanonicalformofΣ.(a)Forthisstate-spacesystemdeterminethecontrollawbystatefeedbackandintegralactionthatplacesallclosed-looppolesats=−1.(b)Determinetheminimalobserver(resp.thefull-orderobserver)whichhasallitspolesats=−1.(c)Thetwostatefeedback/observersyntheseswithintegralactionobtainedintheresponsestoQuestions(ii)(a)and(ii)(b)canbewrittenintheformoftwoRSTcontrollers.Whatarethey?(ComparewiththoseobtainedinQuestion(i).)(iii)Whatisthepossibleﬂawintheabovecontrollers?Howcanyoucorrectit?EXERCISE302.–*Furtherdevelopthetheorydiscussedinsection8.3byconsideringthecasewherethestatexisnotmeasured.SupposethatΣisdeﬁnedby(9.24)whereyisthemeasuredvectoranddenotebyϕ(∂)thepolynomialwithminimaldegreewhichannihilatesthedisturbancesd1andd2aswellasthereferencer.ConsiderthecasewhereΣhasadirecttermi.e.thesecondequationof(9.24)isy=Cx+Du+d2.*Chapter10Discrete-TimeControl10.1.IntroductionTheprinciplespresentedinChapters568and9forthedesignofcontrollawsaregeneralthoughtheyleadto“continuous-time”(alsocalled“analog”)controls.Foralongtimeinnumerousbranchesofindustryimplementationofcontrollawshasbeendonethroughcomputers.Theresultingcontrolsarecalled“discrete-time”or“digital”becausethesetofinstantsatwhichthesignalscanbedeliveredtocomputersandatwhichthosecomputerscandeliverthecontrolisdiscrete.TheseinstantsareoftheformkTwherek∈ZandTisarealpositivenumber;theseinstantsarecalledthesamplinginstantswhileTisthesamplingperiod(fs=1/Tandωs=2πfsarethenthesamplingfrequencyandtheangularsamplingfrequencyrespectively).Digitalsignalshaveanotherparticularity:theirvalueateachinstantiscodedinaﬁnitenumberofbits.Thiscodingoperationiscalledquantization.Entirebooksaredevotedtodiscrete-timecontrolandsystemse.g.[4].10.2.Discrete-timesignals10.2.1.DiscretizationofasignalAcontinuous-timesignalwithvaluesinRnisafunctionoftherealvariablex:R→Rn:t(cid:3)→x(t).ThediscretizedsignalatsamplingperiodTisthesequencexdofelementsinRndeﬁnedbyxd(k)=x(kT)k∈Z(sequencesare281Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.282LinearSystemsdenotedinthischapterasfunctionsdeﬁnedinZ).1Inrealityasmentionedinsection10.1thecomponentsofxdarequantizedbuttoremainassimpleaspossiblewewillnotdealwiththeproblemofquantization–besidesitsimportancediminishesastheperformanceofcomputersincreases;see[4]onthissubject.Signalxdisadiscrete-timesignal.Itisassumedintherestofthissectionthatn=1exceptwhenotherwisestated.Itiseasytoextendthetheorytothecasewherencanbeanypositiveintegerbydoingthesamerationaleforeachcomponent.10.2.2.z-transformThe(two-sided)z-transformofadiscrete-timesignalxdisthez-transformofthesequencexd(seesections12.3.5and12.4.4)andisdenotedbyXinwhatfollows.WehaveX(z)=+∞(cid:11)k=−∞xd(k)z−k(10.1)forz∈CcwhereCcistheannulusofconvergenceofX.10.2.3.SampledsignalLetx:R→Rbeacontinuous-timesignalandletT>0.ThissignalsampledatperiodTisx∗=xT(10.2)whereTistheDiraccomb(seesection12.2.3).Denotingdistributionslikefunctionswethushavex∗(t)=x(t)T(t)=x(t)+∞(cid:12)k=−∞δ(t−kT).IfxbelongstothespaceOMofindeﬁnitelydifferentiablefunctionswhosederivativesofallorders(includingorder0)areslowlyincreasingweobtainfromexpression(12.20)ofsection12.2.3x∗(t)=+∞(cid:12)k=−∞x(t)δ(t−kT)=+∞(cid:12)k=−∞x(kT)δ(t−kT)=+∞(cid:12)k=−∞xd(k)δ(t−kT).(10.3)1.ThesignalxcanbeonlydeﬁnedinanintervalofR;inthatcasexdisdeﬁnedinanintervalofZ.Discrete-TimeControl283REMARK303.–Relation(10.3)isexactformoregeneralfunctionsxthanthosebelongingtoOM.SincetheDiracdistributionisameasurewithsupportreducedto{0}itsufﬁcesthatthereexistafunctiony∈OMandforallk∈ZaneighborhoodNkofksuchthatx|Nk=y|Nk.Inthatcasexd∈s(cid:2)thusx∗∈S(cid:2)(seesection12.3.2).InwhatfollowswedenotebyDTthesetofallfunctionsx:R→Rthatsatisfytheabovecondition.ItisimmediatelyclearthatDTisanR-vectorspacewhichwewillcallthespaceof“T-discretizablesignals”.NotallfunctionsbelongingtoDTarecontinuous.Thereexistsalinearbijectionxd˜(cid:3)→x∗andthatiswhythesetwoquantitiesareoftenidentiﬁed.Anyhowwewilldistinguishinwhatfollowsthediscretizedsignalxdwhichisaconcreteentityandthesampledsignalx∗whichispurelyamathematicalobjectusefulformanycalculations.The(two-sided)Laplacetransformofx∗satisﬁesˆx∗(s)=+∞(cid:12)k=−∞xd(k)(cid:2)+∞−∞δ(t−kT)e−stdt=+∞(cid:12)k=−∞xd(k)e−skTandhenceaccordingto(10.1)ˆx∗(s)=X(cid:15)esT(cid:16)i.e.ˆx∗(s)=X(z)z=esT.(10.4)10.2.4.PoissonsummationformulaLetFbetheFouriertransformandx∈DT.Accordingtoequation(10.2)andthesecondExchangetheorem(seesection12.3.1)wehaveFx∗=12π(Fx)∗(FT)undertheconditionthattheFouriertransformFxbewithcompactsupport.Underthishypothesisaccordingtorelation(12.41)(section12.41)(Fx∗)(ω)=12π(cid:21)(Fx)∗2πTωs(cid:22)(ω)=1T+∞(cid:12)k=−∞(Fx)(ω−kωs).Ontheotherhandfromequation(10.4)(Fx∗)(ω)=X(cid:15)eiωT(cid:16)andbysubstitutingiω=swehavetheequalityX(cid:15)esT(cid:16)=1T+∞(cid:11)k=−∞ˆx(s−ikωs)(10.5)(whereˆxstandsforthebilateralLaplacetransformofx)whichisanexpressionofthePoissonsummationformula.284LinearSystems10.2.5.SamplingtheoremLetx∈DTbea(continuous-time)signalsuchthatFxhasacompactsupportandletωmax=inf{ω≥0:supp(Fx)⊂[−ωω]}(wheresuppisthesupport).ThissignalxisdiscretizedattheperiodT=1/fsandwewritexd(k)=x(kT)(k∈Z).DEFINITION304.–ThefrequencyfN=fs/2andtheangularfrequencyωN=2πfNarecalledtheNyquistfrequencyandtheNyquistangularfrequencyrespectively.Wehavethefollowingresultcalledthe“samplingtheorem”orthe“Shannontheorem”.Shannonindeedpointedouttheimportanceofthisresultintheproblemofsampling.2THEOREM305.–Asufﬁcientconditionunderwhichwecanreconstructthesignalxfromthediscretizedsignalxdis:ωN>ωmax.PROOF.Inordertomakeitmorereadabletheproofiscarriedoutinawaythatalldistributionsencounteredaretreatedasfunctions.Wehaverelation(10.5)whichcanalsobewrittenasX(cid:15)eiωT(cid:16)=1T+∞(cid:11)k=−∞ˆx[i(ω−kωs)].(10.6)AssumingthatωN>ωmaxwehaveforallω∈[0ωN]X(cid:15)eiωT(cid:16)=1Tˆx(iω).(10.7)Letthe“normalizedangularfrequency”beθ=ωT.(10.8)The“function”θ(cid:3)→X(cid:15)eiθ(cid:16)istheFouriertransformofthesequencexd∈s(cid:2)(seesection12.3.3).ThereforeknowingxdweknowX(cid:15)eiωT(cid:16)forallω∈[0ωN]and2.AccordingtosomeauthorsCauchyalreadyknewthisresultin1841.ThisisquestionnableandmoreprobablytheﬁrsttodiscoverthisconditionwasWhittakerin1915.Discrete-TimeControl285thusthe“function”ω(cid:3)→ˆx(iω)accordingtoequation(10.7).ThisistheFouriertransformofxandthereforex∈S(cid:2)isknown(seesection12.3.1).TheelementsofS(cid:2)aredistributions;*sincethedistributiondeﬁnedbyxisknownthefunctionxisknownalmosteverywhere.*REMARK306.–(i)Apriorisamplingcancausealossofinformation.ThesamplingtheoremshowsthattheconditionωN>ωmax(calledthe“Shannoncondition”)issufﬁcientforalltheinformationcontainedinxtobealsocontainedinxd.(ii)Equality(10.7)istherelationthatexistsbetweenFxdandFx.Usingthenormalizedangularfrequency(10.8)wecanwriteX(cid:15)eiθ(cid:16)=1Tˆx(iω)andX(cid:15)eiθ(cid:16)=(Fxd)(θ)ˆx(iω)=(Fx)(ω)(seesections12.3.3and12.3.4).RememberthatFxdis2π-periodic.Since(Fxd)(−θ)=(Fxd)(θ)Fxdisdeterminedbyitsrestrictionto[0π]andπisthevalueofθwhenthevalueofωisωN.LetusnowseeinamoreexplicitmannerhowxcanbereconstructedfromxdwhentheShannonconditionissatisﬁed.Letusdeﬁnethe“sinecardinal”functiondenotedbysincassincϕ=(cid:20)sinϕϕifϕ(cid:5)=01ifϕ=0.TheshapeofthisfunctionisshowninFigure10.1.10.80.40.60.20–0.2–0.4–25–20–15–10–50510152025Figure10.1.Sinecardinalfunction286LinearSystemsPROPOSITION307.–SupposetheShannonconditionissatisﬁed(seeRemark306).ThenwehavetheShannoninterpolationformulax(t)=+∞(cid:11)k=−∞xd(k)sinc[ωN(t−kT)].(10.9)PROOF.TheFouriertransformofxisgivenby(10.7).Byinversetransform(section12.3.1)weobtainx(t)=12π(cid:2)+∞−∞ˆx(iω)eiωtdω=T2π(cid:2)ωN−ωNX(cid:15)eiωT(cid:16)eiωtdω.Accordingto(10.1)wehavex(t)=2ωN+∞(cid:12)k=−∞xd(k)(cid:2)ωN−ωNeiω(t−kT)dωfromwhichwededuce(10.9).REMARK308.–LetusexaminewhatwillhappeniftheShannonconditionisnotsatisﬁed.(i)SupposexisasinusoidalsignalsampledatperiodTandwithangularfrequency3ωN/2.Itsspectrumconsistsoftworaysatfrequency−3ωN/2and3ωN/2(seerelation(12.42)ofsection12.3.1).Accordingtoequation(10.6)thedistributionω(cid:3)→X(cid:15)eiωT(cid:16)(denotedlikeafunctionforconvenience)is2ωN-periodic.Therayatangularfrequency−3ωN/2ofthespectrumofxthusgeneratesbythesummationin(10.6)arayatangularfrequencyωN/2.Asaresultthediscretizedsignalxdisidenticaltowhatwewouldobtainbydiscretization(atthesameperiodT)ofasinusoidalsignalwithangularfrequencyωN/2.Throughtheinterpolationformula(10.9)itisthislastsignalthatwegenerateinsteadofx.(ThisphenomenonwasobservedbyNyquistandthisisthereasonwhyfN=fs/2iscalledtheNyquistfrequency.)(ii)Takeasecondexamplebyconsideringthecontinuous-timesignalxwhosespectrumisshowninFigure10.2.ThespectrumofthediscretizedsignalxdisshowninFigure10.3whentheShannonconditionissatisﬁedandinFigure10.4whenitisnot.Samplinginthislastcasehasdestroyedtheinformationwhichwascontainedinxbyaphenomenonwecallspectrumaliasing.REMARK309.–Theinterpolationformula(10.9)isnon-causalbecauseinordertocalculatex(t)accordingtothisexpressiononemustknowallthexd(k)includingthoseforwhichkT>t(Figure10.5showsatypicalexampleofreconstructionofsignalxfromthediscretizedsignalxd).Insomeapplicationswherethesignalisprocessedoff-linethisdoesnotpresentanydifﬁcultybutthecontrolengineerwhoalwayshastoworkinreal-timedoesnothavetheappropriatecrystalball.Itisthereforenecessarytomakeacausalapproximationofformula(10.9);suchanapproximationisonlyvalidifωN≥λωmaxwhereλ>1.Afactorλ=5or10iscommonlyused.Discrete-TimeControl287maxmaxixˆFigure10.2.Spectrumofthecontinuous-timesignalNNTieXFigure10.3.Spectrumofthediscretizedsignal(Shannonconditionsatisﬁed)NNTieXFigure10.4.Spectrumofthediscretizedsignal(spectrumaliasing)288LinearSystemsFigure10.5.Shannoninterpolation10.2.6.HoldAholdprovidingasolutiontotheproblemwasbroughtupinRemark309(section10.2.5)i.e.thenon-causalnatureoftheinterpolationformula(10.9).Onewaytoapproximatelyreconstructasignalxwhichhasbeendiscretizedistoholditsvalueovereachsamplingperiod.Thisholdoperationisobviouslycausalandconsistsofanextrapolation.Aholdofordern(n≥0)makesitpossibletodetermineanestimateofx(t)t∈[kT(k+1)T)fromthevaluesxd(k)...xd(k−n).Zero-orderholdThezero-orderhold(Z.O.H.)isthemostcommonlyusedbecauseitrequirestheleastcalculations.Letxdbeadiscretizedsignal(xd(k)=x(kT)).The“sampled-and-heldsignal”(withZ.O.H.)isdeﬁnedbyxh0(t)=xd(k)t∈[kT(k+1)T).Thefunctionxh0isastaircasefunction(suchafunctionisdiscontinuousingeneral).Supposexisdifferentiable;thenaccordingtothemeanvalueformula|x(t)−xh0(t)|≤Tsupt|˙x(t)|andhencetheinterpolationerrorissmallifxvariessufﬁcientlyslowly.First-orderholdThesampled-and-heldsignalwithaﬁrst-orderholdisdeﬁnedbyxh1(t)=xd(k)+t−kTT(xd(k)−xd(k−1))t∈[kT(k+1)T).Notethatxh1isacontinuousfunction.Discrete-TimeControl289Txx*H(s) hxZ.O.H Figure10.6.Sample-and-holdTransferfunctionofzero-orderholdIneverythingthatfollowswelimitourselvestothecaseofazero-orderhold.Thesampled-and-heldsignalxh0isdenotedbyxh.ItsLaplacetransformisgivenbyˆxh(s)=(cid:2)+∞−∞xh(t)e−stdt=+∞(cid:12)k=−∞x(kT)(cid:2)(k+1)TkTe−stdt.Theintegralontheright-handsideisequaltoe−skTH(s)whereH(s)=1−e−sTs.(10.10)Wethusobtainˆxh(s)=+∞(cid:11)k=−∞x(kT)e−skTH(s)andwealsohaveˆxh(s)=ˆx∗(s)H(s).(10.11)ThetransferfunctionH(s)isthusthatofthezero-orderholdandthesignalxhcanberepresentedasinFigure10.6;itissaidtobeobtainedfromxbymeansofasample-and-hold.10.3.Discrete-timesystems10.3.1.GeneraldescriptionLetΣbeacontinuous-timesystemwithtransfermatrixG(s);Σisassumedtobelineartime-invariantthroughoutthischapter.BydiscretizingtheoutputyofΣatsamplingperiodTweobtainthediscrete-timesignalyd.Thissignalcanthenbeprocessedbyacalculatorinordertogenerateadiscrete-timecontrolud.ThiscontrolsignalcanbeconvertedusingaZ.O.H.intoacontinuous-timesignaluh(notethatuh:t(cid:3)→uh(t)isnotacontinuousfunction).Thefeedbackby“discrete-timecontrol”isachievedbyusingthissignaluhasinputtoΣ.290LinearSystemstytuhkudkydDAC+ZOHG(s) ADCFigure10.7.DiscretizedsystemItisthennecessarytoconvertydintoanormalizedsignalwhosecomponentsvarybetween−10Vand+10Vforexample;thecombinationofthediscretizationandthisnormalizationiscalledtheanalog-to-digitalconversion(ADC).Ontheotherhandthesignaludmustbeheldandthenampliﬁed.Adigital-to-analog(DAC)conversionisthusnecessary.WethusarriveatthediagramofthediscretizedsystemΣdwithinputudandoutputydasrepresentedinFigure10.7.ThesystemΣdisalineartime-invariantdiscrete-timesystem.Itadmitsastate-spacerepresentation(theproofofTheorem129ofsection7.1.2extendstothecaseofdiscrete-timesystemswithoutdifﬁculty–see[15]or[22]formoredetails)andasaresultatransfermatrixF(z).AsaﬁrststepwearegoingtoshowhowtodetermineF(z)fromG(s)andthenwewillshowhowtodetermineastate-spacerepresentationofΣdfromthatofΣ.REMARK310.–ThediscretizedoutputydisonlyrepresentativeoftheoutputyiftheShannonconditionissatisﬁed(seeTheorem305).Asaresultitisessentialtoincorporatealow-passﬁlterwithlowercutofffrequencyωNintoΣattheoutputofthissystem.Thisﬁlteriscalledan“anti-aliasingﬁlter”sinceitspurposeistoavoidspectrumaliasing(seesection10.2.5Remark308).10.3.2.SampledsystemThenotionofthesampledsystem(aswewillrefertointhistext)ispurelyabstractaswellasthatofasampledsignal.ButitisusefultocalculateF(z)asdiscussedabove.SupposeΣ0isacontinuous-timesystemwithtransfermatrixˆf(s)receivingasampled-signalu∗(t)(withperiodT)atitsinput.Theoutputy(t)ofthissystemissampledatperiodTanditresultsinasampledoutputy∗(t)(seeFigure10.8).Discrete-TimeControl291Tuu*F(s)Tyy*Figure10.8.SampledsystemLetf(t)=L−1(cid:7)ˆf(s)(cid:8)betheimpulseresponseofΣ0(seesection2.5.2).Assumingthattheinitialconditionsarezerotheoutputyisgivenbyy=f∗u∗=f∗(Tu)andthesampledoutputy∗thussatisﬁesy∗=Tf∗(Tu)=f∗∗u∗(thesecalculationsarevalidifbothfandubelongtoDT∩S(cid:2)(Γ)whereΓisanon-emptyintervalofR:seesections12.3.4and10.2.3).AccordingtotheExchangetheoremwethushaveˆy∗(s)=ˆf∗(s)ˆu∗(s)andasaconsequenceof(10.4)Y(z)=F(z)U(z)(10.12)whereY(z)U(z)andF(z)arethez-transformsofthesequences(yd(k))(ud(k))and(fd(k))respectively.10.3.3.DiscretizedsystemConsiderthesysteminFigure10.8withinputud=(ud(k))andoutputyd=(yd(k)).Thisisadiscrete-timesystembutitcannotbeconsideredadiscretizedsystemyet(accordingtothedeﬁnitiongiveninsection10.3.1)becausetheinputofthecontinuous-timesystemΣ0doesnotreceiveasampled-and-heldsignal.WecandecomposethesystemΣdresultingfromthediscretizationofΣatperiodTaccordingtothediagraminFigure10.9.Thediscrete-timesignalud=(ud(k))representedbythesampledsignalu∗accordingto(10.3)isnowheldbeforeenteringΣ.ThesysteminFigure10.9isidenticaltotheoneinFigure10.8whenˆf(s)=G(s)H(s).Wededucethefollowingtheorem:292LinearSystemsu*TTy*H(s) G(s) Z.O.H sfˆFigure10.9.DiscretizedsystemTHEOREM311.–ThetransfermatrixGd(z)ofthediscretizedsystemΣdcanbeexpressedasafunctionofthetransfermatrixG(s)ofthecontinuous-timesystemΣbytherelationGd(z)=(cid:15)1−z−1(cid:16)Z(cid:23)L−1(cid:7)G(s)s(cid:8)(cid:24).(10.13)PROOF.Accordingto(10.12)Y(z)=Gd(z)U(z)whereGd(z)=Z[f(t)]3f(t)=L−1{G(s)H(s)}=L−1(cid:7)G(s)1−e−sTs(cid:8).Leth(t)=L−1(cid:7)G(s)s(cid:8).Thenf(t)=h(t)−h(t−kT)andhenceGd(z)=(cid:15)1−z−1(cid:16)H(z).SomeexamplesoftransferfunctionsofdiscretizedsystemsaregiveninTable(10.14).G(s)Gd(z)1sTz−11s2T2(z+1)2(z−1)2e−Tsz−1as+a1−e−aTz−e−aT(10.14)EXAMPLE312.–ConsiderthethirdrowofTable(10.14).SinceG(s)=1s2(doubleintegrator)Gd(z)=(cid:15)1−z−1(cid:16)Z(cid:25)L−1(cid:31)1s3 (cid:26).Accordingtorelation(12.83)ofsection12.4.4h(t)=L−1(cid:20)1s3&=t221(t).3.Weslightlyabusethelanguagehere.MorepreciselyoneshouldwriteGd(z)=Z[fd(k)]=Z[f(kT)].Discrete-TimeControl293Asaresulthd(k)=h(kT)=T22k21(k)andaccordingtotable(12.64)ofsection12.3.5Z(cid:25)L−1k21(k)(cid:26)=z(z+1)z−1.FinallyGd(z)=T2(z+1)2(z−1)2.10.3.4.State-spacerepresentationofadiscrete-timesystemDiscretizationofastate-spacesystemLettherebeacontinuous-timesystem{ABCD}:(cid:20)˙x=Ax+Buy=Cx+Du.Iftheinputuofthissystemisasampled-and-heldsignal(withZ.O.H.)wehaveu(t)=ud(k)t∈[kT(k+1)T).TheintegrationofthestateequationbetweentheinstantskTand(k+1)Tprovidesaccordingtorelation(12.112)ofsection12.5.2xd(k+1)=eATxd(k)+(cid:2)(k+1)TkTeA[(k+1)T−t]Bu(t)dt=eATxd(k)+(cid:2)(k+1)TkTeA[(k+1)T−t]dtBud(k)wherexd(k)=x(kT).Puttingyd(k)=y(kT)andAd=eATBd=(cid:2)T0eAtBdt(10.15)weobtain(changingtto(k+1)T−tintheintegral)(cid:20)xd(k+1)=Adxd(k)+Bdud(k)yd(k)=Cxd(k)+Dud(k).(10.16)State-spacerepresentationofadiscrete-timesystemAdiscrete-timesystemsuchasequation(10.16)isnotalwaysobtainedbydiscretizationofacontinuous-timesystem.Usingtheshift-forwardoperatorq(seesection12.3.5)equation(10.16)iswrittenas(cid:20)qxd=Adxd+Bdudyd=Cxd+Dud.(10.17)294LinearSystemsEquations(10.17)constituteastate-spacerepresentationofacausaldiscrete-timesystem.Fromapurelyformalpointofviewtheseequationshavethesamestructureasequations(7.4)ofsection7.1.2:weonlyneedtoreplacethedifferentialoperator∂bytheshift-forwardoperatorq.Theterminologyintroducedinsection7.1.2(statematrixcontrolmatrixetc.)isthereforemaintained.Anotnecessarilycausaldiscrete-timesystemhasastate-spacerepresentationoftheform(cid:20)qxd=Adxd+Bdudyd=Cxd+Wd(q)ud(10.18)whereWd(q)isapolynomialmatrix.ThissystemiscausalifandonlyifWd(q)isaconstantmatrixD(possiblyzero)i.e.ifthefuturevaluesoftheinputhavenoinﬂuenceonthepresentoutput.Thecontrolengineeronlyhastodowithcausalsystems(seesection10.2.5Remark309).System(10.18)issaidtobestrictlycausalifWd(q)=0.Thismeansthatthesysteminputhasnoimmediateinﬂuenceontheoutput.Thenotionsofcausalsystemandstrictlycausalsystemrelativetodiscrete-timesystemscorrespondtothenotionsofpropersystemandstrictlypropersystemrelativetocontinuous-timesystems(seesection2.5.3).4Usingthenotionoftransfermatrixweareledtothefollowing:DEFINITION313.–Adiscrete-timesystemΣdissaidtobecausal(resp.strictlycausalbicausal)ifitstransfermatrixG(z)isproper(resp.strictlyproperbiproper).(Seesection13.6.1.)REMARK314.–Itisalsopossibleandtosomeextentpreferabletorepresentadiscrete-timesystemusingtheoperatorδ=q−1T(10.19)(whereTisthesamplingperiodifthesystemconsideredisadiscretizedsystemandwhereTisanyrealnumber>0–forexampleT=1–otherwise)becauseδislike∂=ddta“differentialoperator”.Thisremarkonlyhasfullsigniﬁcanceintheframeworkoflineartime-varyingsystems:see[15]or[22](whereT=1).Usingthisoperatoradiscrete-timesystemadmitsastate-spacerepresentation(cid:20)δxd=˜Adxd+˜Bdudyd=Cxd+˜Wd(δ)ud4.Thisisthereasonwhysomeauthorscallcausal(resp.strictlycausal)acontinuous-timesystemwhichintheterminologyofthisbookiscalledproper(resp.strictlyproper).Weshouldavoidthisabuseoflanguageinouropinion.Forgeneralconsiderationsoncausalitysee[41].Discrete-TimeControl295where˜Ad=Ad−InT˜Bd=BdT.Suchasystemiscausal(resp.strictlycausal)ifandonlyif˜Wd(δ)=D(resp.˜Wd(δ)=0).CalculationsLetusconsideradiscretizedsystemandseehowonecaninpracticecalculatethematricesAdandBddeﬁnedbyequation(10.15).ThematrixAdisanexponentialmatrixandthuscanbecalculatedbyoneofthemethodsdiscussedinsection12.5.2.ThesamemethodsmakeitpossibletocalculateΨ=(cid:2)T0eAτdτ=InT+AT22+A2T33!+...andwethenhaveAd=In+AΨBd=ΨB.IntheparticularcasewhereAisinvertibleΨ=A−1(Ad−In)andwethushaveBd=A−1(Ad−In)B.InthegeneralcaseletΦ(t)=eAtandΓ(t)=Ψ(t)B.Wehaveddt(cid:21)Φ(t)Γ(t)0Im(cid:22)=(cid:21)Φ(t)Γ(t)0Im(cid:22)(cid:21)AB00(cid:22)andhence(cid:21)AdBd0Im(cid:22)=exp(cid:20)(cid:21)AB00(cid:22)T&.(10.20)Equality(10.20)hastheadvantageofexpressingverysyntheticallytherelationthatexistsbetweenthestateandcontrolmatricesofthediscretizedsystemandthoseofthecontinuous-timesystembutitdoesnotgenerallyconstitutethemosteconomicwaytomakecalculations.10.3.5.CalculationofthestateofadiscretizedsystemConsidertheﬁrstequationofequation(10.16).Lettherebeaninitialconditionxd(0)=x0.296LinearSystemsWehavexd(1)=Adx0+Bdud(0)xd(2)=Adxd(1)+Bdud(1)=A2dx0+(cid:25)BdAdBd(cid:26)(cid:27)ud(1)ud(0)(cid:28)andbyinductionweeasilyestablishthatxd(k)=Akdx0+(cid:25)BdAdBd...Ak−1dBd(cid:26)⎡⎢⎢⎢⎣ud(k−1)...ud(1)ud(0)⎤⎥⎥⎥⎦.(10.21)10.4.Structuralpropertiesofdiscrete-timesystems10.4.1.PolesandzerosThedeﬁnitionsofthevariouskindsofpolesandzerosofdiscrete-timesystemsareidenticaltothoseofcontinuous-timesystems.5Letusrecallsomeoftheessentialpointsregardingdiscrete-timesystemsinstate-spaceforms:–ThepolesofthesystemΣddescribedby(10.18)aretheeigenvaluesofAdandtheorderofΣdisequaltothenumberofitspoles(seesection2.3.7).ThesepolesarealsotheSmithzerosofthematrixzIn−Adandthisobservationallowsonetodeﬁnetheirstructuralindicestheirordersandtheirdegrees(seesection13.2.5).–TheinvariantzerosofΣdaretheSmithzerosofthe“Rosenbrockmatrix”(cid:27)zIn−Ad−BdCWd(z)(cid:28).–Thetransmissionpoles(resp.zeros)ofΣdaretheMacMillanpoles(resp.zeros)ofthetransfermatrixGd(z)=C(zIn−Ad)−1Bd+Wd(z)(seesections2.4.22.4.4and2.4.5).–Thenon-controllablemodes(alsocalledtheinput-decouplingzerosi.d.z.s)ofΣdaretheSmithzerosof(cid:25)zIn−Ad−Bd(cid:26)5.Aunitarydeﬁnitionofpolesandzerosofcontinuous-timeanddiscrete-timesystems(possiblytime-varying)hasbeengivenin[15]and[22]usingmoduletheory.Discrete-TimeControl297anditsnon-observablepoles(alsocalleditsoutput-decouplingzeroso.d.z.s)aretheSmithzerosof(cid:27)zIn−AdC(cid:28)(seesections7.2.2and7.2.3).–Thehiddenmodescanbedeterminedby{hiddenmodes}={systempoles}(cid:2){transmissionpoles}(seesection7.2.6Theorem179(i))andaccordingtoCorollary176ofsection7.2.5wecandeterminetheinput–outputdecouplingzeros(i.o.d.z.)by{i.o.d.z.}={i.d.z.}˙∪{o.d.z.}(cid:2){hiddenmodes}.–TheblockingzerosofΣd(ifany)arethecomplexnumberszsuchthatGd(z)=0(seesection2.4.4Deﬁnition27)whilethezerosofΣdremaindeﬁnedaccordingtoDeﬁnition178(section7.2.6).Inwhatfollowsthediscrete-timesystemΣdisassumedtobecausaldeﬁned(exceptwhenotherwisestated)byastate-spacerepresentationsuchasthatofequation(10.17)andisdenotedby{AdBdCD}(or{AdBdC}ifD=0).10.4.2.ControllabilityControllabilityofadiscretetimestate-spacesystemDEFINITION315.–Adiscretetimesystem{AdBdCD}iscontrollable(resp.0-controllable)ifthereexistsacontrolsequencethatallowsonetotransferitsstatexdfromanyinitialvaluexd(0)toanyﬁnalvaluex∗d(resp.totheorigin)inﬁnitetime.REMARK316.–Numerousauthorscallreachability(resp.controllability)thenotioncalledcontrollability(resp.0-controllability)above:see[64]forexample.Weusehereagaintheterminologyadoptedin[22]inordertobetterrevealtheunitybetweencontinuous-timeanddiscrete-time.THEOREM317.–(i)ThesystemΣd={AdBdCD}iscontrollableifandonlyifrkΓ=nwherenistheorderofΣdandwhereΓdisthecontrollabilitymatrixΓd=(cid:25)BdAdBd...An−1dBd(cid:26)(10.22)(“Kalmancriterionforcontrollability”).(ii)Σdis0-controllableifandonlyifimAnd⊂imΓd.66.Withaslightabuseoflanguage:thesesymbolsdenotetheimagesofthelinearmappingsrepresentedbythematricesΓdandAndinthecanonicalbases.298LinearSystemsPROOF.(i)Accordingto(10.21)ΣdiscontrollableifandonlyifthereexistsanintegerNsuchthatrkΓdN=nwhereΓdN=(cid:25)BdAdBd...AN−1dBd(cid:26).NowaccordingtotheCayley–Hamiltontheorem(section13.3.4Theorem537)rkΓdN=rkΓdnforanyN≥nsinceAndisalinearcombinationwithrealcoefﬁcientsofInAd...An−1d.(ii)IfrkAnd⊂rkΓdforanyxd(0)∈Rnthereexistsaﬁnitesequence(u(k))0≤k≤nsuchthatAndxd(0)+Γd⎡⎢⎣u(n−1)...u(0)⎤⎥⎦=0andhenceΣdis0-controllable.TheconversealsoholdsaccordingtotheCayley–Hamiltontheorem.PROPOSITION318.–Thefollowingtwoconditionsareequivalent:(a)imAnd⊂imΓd;(b)foranyrowvT=(cid:25)v1...vn(cid:26)ifvTAidBd=00≤i≤n−1thenvTAnd=0.PROOF.Condition(a)meansthatforanyx∈imAndx∈imΓ.NowwehavevTAidBd=00≤i≤n−1ifandonlyifforanyx∈imΓvTx=0.AlsowehavevTAnd=0ifandonlyifforanyx∈imAndvTx=0fromwhichwearriveattheequivalenceasstated.*Intrinsicdeﬁnitionsofcontrollabilityand0-controllabilityDeﬁnition315isofcourseonlyvalidforadiscrete-timesysteminstate-spaceform.Moregenerallyadiscrete-timesystemΣdcanbedeﬁnedasaﬁnitelypresentedR-moduleofMwhereR=R[q](seesection2.2.5Remark8).WearethusledtothefollowingdeﬁnitioninwhichtheinputofΣditsoutputandthewayitisrepresented(state-spaceformRosenbrockrepresentationetc.)donotplayanyrole.DEFINITION319.–ThesystemΣdiscontrollableifthemoduleMisfree(orinanequivalentmannertorsion-freesinceRisaprincipalidealdomain).InthecasewhereΣdisastate-spacesystemDeﬁnitions319and315areequivalentaccordingtoTheorem317.Indeedalltheproofsofsection7.1.3canbetransposedtothecaseofdiscrete-timesystems.ThecanonicaldecompositionaccordingtocontrollabilityandTheorem165ofsection7.2.2(i.e.Σdiscontrollableifandonlyifithasnoi.d.z.)remainvalid.REMARK320.–ThestatementofTheorem129(section7.1.2)remainsvalidmutatismutandis:alldiscrete-timecontrolsystemsadmitastate-spacerepresentation(oftheformofequation(10.18)).Discrete-TimeControl299LettherebethemultiplicativesetS={qnn≥0}andletA=S−1Rbetheringconsistingofallelementsoftheformr/qnr∈Rn≥07;inadditionlet˘MbetheA-moduleA⊗RMconsistingofallelementsoftheformm/qnm∈Mn≥0(seesection13.6.5).Considerthefollowingdeﬁnitionof0-controllabilitywhichisintrinsiclikeDeﬁnition319ofcontrollability:DEFINITION321.–ThesystemΣdis0-controllableiftheA-module˘Misfree.REMARK322.–TheringAisaprincipalidealdomainandsoisRandhencethemodule˘Misfreeifandonlyifitistorsion-free(seeCorollary555ofsection13.4.2).PROPOSITION323.–Foradiscrete-timestate-spacesystemΣd={AdBdCD}Deﬁnitions315and321of0-controllabilityareequivalent.PROOF.TheA-module˘Misfreeifandonlyifthematrix(cid:25)In−q−1Adq−1Bd(cid:26)(whichisapresentationmatrixof˘M)isright-invertible(seetheproofofTheorem141insection7.1.3).Thisconditionmeansthatif˘vT=(cid:25)˘v1...˘vn(cid:26)isarowofelementsofarightA-moduletheequality˘vT(cid:25)In−q−1Adq−1Bd(cid:26)=0implies˘vT=0.Theﬁrstoftheseequalitiesisequivalentto(a)˘vT=˘vTq−1Adand(b)˘vTq−1Bd=0.Right-multiplying(a)byq−1weobtain˘vTq−1=˘vTq−2Adandhence˘vTq−2AdBd=0accordingto(b).Thislastequalityinturnimplies˘vTq−3A2dBd=0etc.Foranyi∈{0...n−1}wecanright-multiplytheequality˘vTq−i−1AidBd=0byqi+1and(sinceqisaninvertibleelementofA)thisequalityisequivalentto˘vTAidBd=0.Theequality˘vT(cid:25)In−q−1Adq−1Bd(cid:26)=0isthusequivalentto˘vTAidBd=00≤i≤n−1.AsaresultifimAnd⊂imΓwehave˘vTAnd=0accordingtoProposition318andhencefrom(a)wehave˘vT=0and˘Misfree.Converselyif˘Misfreetheequalities˘vTAidBd=00≤i≤n−1imply˘vT=0andhenceimAnd⊂imΓ.FromDeﬁnitions319and321the“Popov–Belevitch–Hautustestforcontrollability(resp.0-controllability)”isstatedasfollows:PROPOSITION324.–Thediscrete-timestate-spacesystemΣd={AdBdCD}iscontrollable(resp.0-controllable)ifandonlyifrkC(cid:25)zIn−AdBd(cid:26)=nforanycomplexnumberz(resp.foranycomplexnumberz(cid:5)=0).ControllabilityanddiscretizationLetΣbeacontinuous-timecontrolsystemtheinputofwhichisanindependentvariablewithmcomponents(seesection2.3.1)discretizedatasamplingperiodofT.LetΣdbethediscretizedsystem.7.*TheringAistheringofLaurentpolynomialsinqandisdenotedbyR(cid:25)qq−1(cid:26).*300LinearSystemsTHEOREM325.–ForΣdtobecontrollableitisnecessarythatΣbecontrollableanditissufﬁcientthatΣbecontrollableandhasnopolesλ1λ2suchthatλ1−λ2=2πki/Twherekisanynon-zerointegerandi=√−1.PROOF.(A)Preliminarycalculations.AccordingtoTheorem129(section7.1.2)wecanassumethatΣisastate-spacesystem{ABCD}.Letε(s)=esTψ(s)=(cid:2)T0estdtwhicharetwoentirefunctions(seesection12.4.2).Accordingtoequation(10.15)wehaveAd=ε(A)Bd=ψ(A)B.Wegetψ(s)=(cid:15)esT−1(cid:16)/sfors(cid:5)=0andψ(s)=Tfors=0andhenceψ(s)=0ifandonlyifesT=1withs(cid:5)=0i.e.s=2πki/Twherekisanon-zerointeger.Ontheotherhandε(s1)=ε(s2)ifandonlyifs1−s2=2πki/Twherekisaninteger.Thepair(AB)isnon-controllableifandonlyifthereexistsv∈Cn(cid:2){0}andλ∈Csuchthat(a)vTA=vTλ;(b)vTB=0(seesection7.6Exercise222).Likewise(AdBd)isnon-controllableifandonlyifthereexistsv∈Cn(cid:2){0}andλ∈Csuchthat(c)vTAd=vTeλT;(d)vTBd=0.AssumingthatCondition(a)holdsvTε(A)=vTε(λ)andvTψ(A)=vTψ(λ)(seesection12.4.2Proposition439).AsaresultvTAd=vTeλTandvTBd=vTψ(A)B=vTψ(λ)B=ψ(λ)vTB.InadditionassumingthatCondition(c)holdsvTeAT=vTeλTi.e.vTe(A−αIn)T=vTe(λ−α)Twhereα>max{Reβ:β∈Sp(A)}.AccordingtoPropositions439and441(section12.4.2)thisisequivalenttovTA=vT(λ+2πki/T)wherekisaninteger.(B)Proofbycontradiction.Lettherebeconditions(i)(ii)and(iii)below:(i)Σdiscontrollable;(ii)Σiscontrollable;(iii)Σhasnopolesλ1λ2suchthatλ1−λ2=2πki/Tk(cid:5)=0.1)Supposethat(ii)doesnotholdandletv(cid:5)=0λ∈Cbesuchthat(a)and(b)hold.ThenvTAd=vTε(λ)andvTBd=vTBψ(λ)=0andhencewehave(c)and(d)anditfollowsthat(i)doesnothold.2)Suppose(i)doesnothold.AlleigenvaluesofAdareoftheformeλTwhereλisaneigenvalueofA.ThusletλbeaneigenvalueofAandv(cid:5)=0besuchthat(c)and(d)hold.AccordingDiscrete-TimeControl301to(c)vTA=vT(λ+2πki/T)i.e.λ+2πki/TisaneigenvalueofAandvTisanassociatedleft-eigenvector.Asaresulteither(iii)doesnotholdork=0.Supposek=0;accordingto(d)vTBd=vTBψ(λ)=0andhencevTB=0orψ(λ)=0.Intheﬁrstcase(ii)doesnothold;inthesecondλ=2πli/Tl(cid:5)=0and(iii)doesnothold.3)Suppose(iii)doesnotholdandm=1.TherethusexistvT1andvT2suchthatvTjA=λjvTj(j=12)withλ1−λ2=2πki/Tk(cid:5)=0.ThereforevT1(cid:5)=vT2accordingto(13.29)(seesection13.3.3)andeλ1T=eλ2T(cid:1)µfromwhichvTjAd=µvTj(j=12).AsaresulttheeigenvalueµofAdhasgeometricmultiplicityatleast2andrk(cid:25)µIn−AdBd(cid:26)≤n−2+m<nthus(i)doesnothold.Weobservethatifλ=πki/T(k∈Z(cid:2){0})isaneigenvalueofAsois¯λandλ−¯λ=2πki/T.Wededucethefollowing:Letωmax>0andletEωmaxbethesetofallcontrollablecontinuous-timesystemsthepolesofwhichhaveanimaginarypart≤ωmax.LetDωmaxbethesetofsystemsobtainedbydiscretizationatsamplingfrequencyfsofallsystemsbelongingtoEωmax;letωs=2πfsandωN=ωs/2.TheresultbelowwhichisapplicabletosystemsisaconsequenceofTheorem325andisanalogoustothesamplingtheorem(section10.2.5Theorem305)whichisapplicabletosignals:COROLLARY326.–AnecessaryandsufﬁcientconditionforallsuchsystemsbelongingtoDωmaxtobecontrollableisωN>ωmax.10.4.3.ObservabilityObservabilityofadiscrete-timestate-spacesystemThe“behavioral”deﬁnitionofobservabilityofadiscrete-timesystemisasfollows:DEFINITION327.–Adiscrete-timestate-spacesystemΣd={AdBdCD}isobservable(resp.0-observable)iffromitsinputsandoutputsud(k)andyd(k)k∈{0...N−1}forasufﬁcientlylargeNwecandeterminethestatexd(0)(resp.xd(N)).THEOREM328.–Lettherebethe“observabilitymatrix”Ωd=(cid:23)CTATdCT...(cid:15)ATd(cid:16)n−1CT(cid:24)TwherenistheorderofΣd.(i)SystemΣdisobservableifandonlyifrkΩd=n(“Kalmancriterionforobservability”).(ii)Σdis0-observableifandonlyifkerΩd⊂kerAnd.88.Withthesameabuseoflanguageaspreviouslymentioned.302LinearSystemsPROOF.Accordingtoexpression(10.21)ofsection10.3.5wehaveyd(k)=CAkdx0+(cid:25)BdAdBd...Ak−1dBd(cid:26)⎡⎢⎢⎢⎣ud(k−1)...ud(1)ud(0)⎤⎥⎥⎥⎦+Dud(0).Bywritingtheseequalitiesfromk=0tok=κ−1(κ≥1)wethusobtainanexpressionoftheform⎡⎢⎢⎢⎣yd(0)yd(1)...yd(κ−1)⎤⎥⎥⎥⎦=Ωdκx0+Tκ⎡⎢⎢⎢⎣ud(0)ud(1)...ud(κ−1)⎤⎥⎥⎥⎦(10.23)whereΩdκ=⎡⎢⎢⎢⎣CCAd...CAκ−1d⎤⎥⎥⎥⎦.AccordingtotheCayley–HamiltontheoremrkΩdN=rkΩdnforanyN≥nandhencewecanrestricttothecaseκ=N=n.LetΩd=Ωdn.(1)Accordingtoequation(10.23)wecandeterminex0asafunctionofud(k)andyd(k)k∈{0...n−1}ifandonlyifΩdisleft-invertiblei.e.ofrankn.(2)Accordingto(10.21)xd(n)isdeterminedinauniquemannerasafunctionofud(k)andyd(k)k∈{0...n−1}ifandonlyifAndx0isdeterminedinauniquemannerasafunctionofthesamequantities.IfkerΩdisnotincludedinkerAndthereexistx0andx(cid:2)0suchthatx0−x(cid:2)0∈kerΩdandx0−x(cid:2)0/∈kerAnd.WethushaveΩdx0=Ωdx(cid:2)0andAndx0(cid:5)=Andx(cid:2)0andthereforeud(k)andyd(k)k∈{0...n−1}donotallowonetodetermineAndx0andΣdisnot0-observable.ConverselysupposekerΩd⊂kerAnd.Letf:x(cid:3)→Andxandlet¯fbethelinearmappinginducedbyfonRn/kerΩd(seesection13.3.2Remark518).Wehave¯f(¯x0)=Andx0where¯x0=x0+kerΩdandhenceΣdis0-observable.*Intrinsicdeﬁnitionsofobservabilityand0-observabilitySupposenowthatthediscrete-timecontrolsystemΣdischaracterizedbyaﬁnitelypresentedR-moduleM(withR=R[q])andthatΣdhasinputudandoutputyd(seesection7.1.1).Usinganapproachsimilartothatfollowedinsection10.4.2(andpreservingthenotationintroducedinthissection)weareledtothefollowingdeﬁnitionwhichgeneralizesDeﬁnition327:DEFINITION329.–SystemΣdisobservable(resp.0-observable)ifM=[ydud]R(resp.A⊗RM=A⊗R[ydud]R).Discrete-TimeControl303Popov–Belevitch–HautustestWededuceimmediatelyfromDeﬁnition329thefollowingcriterion(“Popov–Belevitch–Hautustest”):PROPOSITION330.–Thediscrete-timestate-spacesystemΣd={AdBdCD}isobservable(resp.0-observable)ifandonlyifrkC(cid:25)zIn−ATdCT(cid:26)=nforanycomplexnumberz(resp.foranycomplexnumberz(cid:5)=0).COROLLARY331.–Adiscrete-timesystemΣdisobservable(resp.0-observable)ifandonlyifithasnoo.d.z.(resp.ifallitso.d.z.s–ifany–arezero).ObservabilityanddiscretizationTheorem325canbetransposedwithoutdifﬁcultytothecaseoflossofobservabilityduetodiscretization.Thedetailsarelefttothereader.EXAMPLE332.–Lettherebeaminimalcontinuous-timesystemwithtransferfunctionπ2s2+π2+1s+1.A“natural”state-spacerepresentationofthissystemis{ABC}withA=⎡⎣−10000−π2010⎤⎦B=⎡⎣110⎤⎦C=(cid:25)10π2(cid:26).Ifthisstate-spacesystemisdiscretizedatperiodT=2weobtainthediscrete-timesystem{AdBdC}whereAd=⎡⎣e−200010001⎤⎦Bd=⎡⎣1−e−200⎤⎦(asuitablemethodforcalculatingAdistheuseoftheinverseLaplacetransform–seesection12.5.2–andwecanalsocalculateBdusingthesecondequalityofequation(10.15).System{AdBdC}isneithercontrollablenorobservable(therankofitscontrollabilitymatrixis1andthatofitsobservabilitymatrixis2).Thestepresponseofthecontinuous-timesystem(–)andtheinterpolatedoneofthediscretizedsystem(--)arerepresentedinFigure10.10.304LinearSystemsFigure10.10.Stepresponses10.4.4.RosenbrockrepresentationTheRosenbrockrepresentationandinparticulartheleftandrightformsdetailedinsection2.3.5forcontinuous-timesystemsareidenticalinthecaseofdiscrete-timesystemsreplacingthe∂operatorbytheqoperator(see[15]or[22]).Aleftformisalwaysobservablewhilearightformisalwayscontrollable.10.4.5.StabilityStabilityofadiscrete-timecontrolsystemΣdisdeﬁnedanalogouslytothatofacontinuous-timecontrolsystemΣ.Assumewithoutlossofgenerality(accordingtoRemark320ofsection10.4.2)thatΣdisastate-spacesystem{AdBdCD}.ThefreebehaviorofΣdisthereforeaccordingtoDeﬁnition20(section2.3.8)thevectorspacespannedbyitsstatexdwhenitsinputudiszero.Inthiscaseaccordingto(10.21)xdisthesequence(xd(k))deﬁnedasafunctionoftheinitialstatex0byxd(k)=Akdx0.(10.24)Remarks181and184ofsection7.3leadustothefollowingdeﬁnition(froma“behavioral”pointofview):DEFINITION333.–Adiscrete-timelineartime-invariantsystemΣdisstable(resp.marginallystable)ifallthevariablesofitsfreebehaviortendto0(resp.arebounded)asktendsto+∞.Discrete-TimeControl305LEMMA334.–Adiscrete-timestate-spacesystemΣd={AdBdCD}isstable(resp.marginallystable)ifandonlyiflimk→+∞Akd=0(resp.thesequence(cid:15)Akd(cid:16)isbounded).THEOREM335.–Thediscrete-timelineartime-invariantsystemΣdisstable(resp.marginallystable)ifandonlyifallitspoleslieintheopenunitdisk|z|<1(resp.intheclosedunitdisk|z|≤1thosethatbelongtothecircle|z|=1–ifany–havingalltheirstructuralindicesequalto1).PROOF.Assumewithoutlossofgenerality(accordingtoRemark320ofsection10.4.2)thatΣdisastate-spacesystem{AdBdCD}.ChangingthebasisifnecessarythestudyofAkdcomesdowntothecasewhereAdisinJordanformAd=λlJλl(diagonalsum):seesection13.3.4Theorem530.WethushaveAkd=λl(Jλl)k.Foranyk≥l−1wehaveJλl=λI+J0landsincethematricesλIandJ0lcommute(Jλl)k=k(cid:12)j=0(cid:21)kj(cid:22)λk−j(J0l)j=l−1(cid:12)j=0(cid:21)kj(cid:22)λk−j(J0l)jwherethelastequalityisduetothefactthat(J0l)l=0.Thusweobtainthestatedresult.DEFINITION336.–ThestatematrixAdofadiscrete-timesystemisastabilitymatrixifallitseigenvaluesbelongtotheopenunitdisk|z|<1.SupposenowthatΣdhasbeenobtainedbythediscretizationofacontinuous-timesystemΣ.Wehavethefollowing:THEOREM337.–ThesystemΣdisstable(resp.marginallystable)ifandonlyifΣhasthesameproperty.PROOF.Supposewithoutlossofgenerality(seeTheorem129section7.1.2)thatΣisastate-spacesystem{ABCD}.LetT>0bethesamplingperiod.SystemΣdisthusastate-spacesystem{AdBdCD}whereAd=exp(AT).Changingthe306LinearSystemsbasis(ifnecessary)theproblemcomesdowntothecasewhereAisinJordanformi.e.A=λlJλl.AsaresultAd=λlexp(JλlT)=λleλTeJ0lTandeJ0lTisgivenbyexpression(12.107)ofsection12.5.2(replacingtbyT)whichprovesthetheorem.DEFINITION338.–LetΣdbeaminimaldiscrete-timesystemandsupposethatthissystemisbicausal(Deﬁnition313).ThesystemΣd(orabusingthelanguageitstransfermatrix)issaidtobebistableifitstransmissionpolesandzerosalllieintheopenunitdisk.10.5.Pseudocontinuoussystems10.5.1.BilineartransformThe“bilineartransform”isdeﬁnedbyw=λz−1z+1λ>0(10.25)InterpretationThistransformcanbeinterpretedasanapproximationtointegrationbytrapezoidalrule.Indeedconsiderthedifferentialequation∂y=u(10.26)andletˆy(s)andˆu(s)bethebilateralLaplacetransformsofyandurespectively.Theysatisfytherelationˆy(s)=1sˆu(s).(10.27)Ontheotherhandbyintegratingequation(10.26)betweentheinstantskTand(k+1)Tweobtainy[(k+1)T]−y(kT)=(cid:2)(k+1)TkTu(t)dt.(10.28)Discrete-TimeControl307tutTkTk1Figure10.11.TrapezoidalruleSupposethattherestrictionofutotheinterval[kT(k+1)T)hasthegraphrepresentedbythecurveinFigure10.11.Theintegralﬁguringintheright-handsideof(10.28)istheareasubtendedbythisgraph.Anapproximationofthisareaisthatsubtendedbythesegmentjoiningthepoints(kTu(kT))and((k+1)Tu[(k+1)T])andhencewehavey[(k+1)T]−y(kT)(cid:9)Tu[(k+1)T]+u(kT)2andtherefore(q−1)yd(cid:9)T2(q+1)ud.LetY(z)andU(z)bethebilateralz-transformsofydandudrespectively.Accordingtoexpression(12.67)ofsection12.3.5weobtainY(z)(cid:9)T2z+1z−1U(z).(10.29)Thetrapezoidalruleleadsthereforetoapproximation(10.29)ofequation(10.27).Returningtothecomplexvariablewdeﬁnedby(10.25)weobtainw(cid:9)swithλ=2T.(10.30)PropertiesPROPOSITION339.–(i)Thebilineartransformisadiffeomorphism9fromtheinterioroftheunitdisk|z|<1ontothelefthalf-planeRew<0fromtheexterioroftheunitdisk|z|>1ontotherighthalf-planeRew>0exceptthepointλandfrom9.Thatmeansitisadifferentiablebijectionwhoseinversefunctionisalsodifferentiable.308LinearSystemstheunitcircle|z|=1exceptthepoint−1ontotheimaginaryaxis.Theinversediffeomorphismisgivenbyz=1+w/λ1−w/λ.(10.31)(ii)Wehavez=eiθ(−π<θ<π)ifandonlyifw=iωwithω=λtanθ2.PROOF.Letz=ρeiθ.Thenw=λρeiθ−1ρeiθ+1andweeasilyseethatRew=λ(cid:15)ρ2−1(cid:16)ρ2+2ρcosθ+1whichproves(i)accordingto(10.31).Forρ=1w=λeiθ/2−e−iθ/2eiθ/2+e−iθ/2=iλtanθ2.REMARK340.–Forsmallvaluesofθ2(θbeingexpressedinradians)tanθ2(cid:9)θ2andhencetherelation(10.8)ofsection10.2.5issatisﬁedwithagoodprecisionforλ=2T(withthisvaluethebilineartransformiscalledthe“Tustintransform”).Ifwewantthisrelationtobeveriﬁedwithagoodprecisionintheneighborhoodofθ0(cid:5)=0|θ0|beinglargeenoughfortheapproximationtanθ02(cid:9)θ02tobetooroughwetakeλsuchthatλtanθ02=ω0=θ0Tandhenceλ=θ0Ttanθ02.(10.32)Withthisvaluethebilineartransformiscalledthe“Tustintransformwithprewarping”.Ifwetakeθ0tendingto0inequation(10.32)wegetbacktotheclassicvalueλ=2Tinequation(10.30).10.5.2.PseudocontinuousrepresentationsLetΣdbeadiscrete-timesystemwithtransfermatrixGd(z)andletˇG(w)=Gd(cid:21)1+w/λ1−w/λ(cid:22).Discrete-TimeControl309ThetransfermatrixˇGis“analogous”tothatofacontinuous-timesystembecauseaccordingto(10.30)thevariablewis“analogous”totheLaplacevariables.WecallˇG(w)the“pseudocontinuousform”ofthetransfermatrixGd(z).Itislikewisepossibletodeterminea“pseudocontinuousstate-spacerepresentation”ˇΣofadiscrete-timesystemΣdbyusingtheoperator∆=λq−1q+1(10.33)thatis“analogous”tothedifferentialoperator∂.Notethatq=1+∆/λ1−∆/λ.(10.34)PROPOSITION341.–(i)If−1isnotapoleofΣdapseudocontinuousrepresentationofthissystemisˇΣgivenby(cid:20)∆ηd=ˇAηd+ˇBudyd=ˇCηd+ˇDud(10.35)whereηd=q+1λTxd∆ηd=δxd(withδ=q−1T)ˇA=λ(Ad−In)(Ad+In)−1ˇB=2T(Ad+In)−1BdˇC=TλC(Ad+In)−1ˇD=(cid:23)D−C(Ad+In)−1Bd(cid:24).(ii)ThetransfermatrixofˇΣisˇG(w)whereˇG(w)=ˇC(cid:15)wIn−ˇA(cid:16)−1ˇB+ˇD.(10.36)PROOF.(i)Wehaveηd=1Tλ(Ad+In)xd+1TλBdudandhencexdcanbeexpressedasafunctionofηdifandonlyif−1isnotaneigenvalueofAdandinthatcasexd=Tλ(Ad+In)−1ηd−(Ad+In)−1Bdud.310LinearSystemsItfollowsthat∆ηd=1T[(Ad−In)xd+Bdud]=ˇAηd+ˇBud.Weobtainyd=ˇCηd+ˇDudbyananalogousrationale.(ii)WehavewithzeroinitialconditionsZ(∆ηd)=wZ(ηd).Accordingto(10.35)Z(∆ηd)=ˇAZ(ηd)+ˇBU(z)andhence(cid:15)wIn−ˇA(cid:16)Z(ηd)=ˇBU(z)fromwhichwededuce(10.36).DEFINITION342.–Supposethat−1isnotapoleofΣd.ThenˇΣdeﬁnedby(10.35)isapseudocontinuousstate-spacerepresentationofΣd.REMARK343.–(i)Theabovepseudocontinuousstate-spacesystemˇΣisproperbutnotstrictlyproperevenifΣdisstrictlycausal.(ii)IfΣdisobtainedbydiscretizationofacontinuous-timestate-spacesystemΣ={ABCD}withsamplingperiodTthesetofpolesofΣdis(cid:31)esTs∈P wherePisthesetofpolesofΣandhence−1isnotapoleofΣd.Nowtakeλ=2/TandT→0+;thenAd=eAT=In+TA+o(T)Bd=(cid:2)T0eAtdtB=TB+o(T)(10.37)whereo(T)/T→0.AsaresultˇA→AˇB→BˇC→CˇD→D.10.5.3.*IntrinsicdeﬁnitionofapseudocontinuoussystemTheconsiderationsthatfollowcalluponnotionsofextensionandrestrictionoftheringofscalarsandoffunctor(seesection13.6.5).Fromthealgebraicpointofviewconsiderthediscrete-timesystemΣdasassociatedwithaﬁnitelypresentedR-moduleM(RdenotingtheprincipalidealdomainR[q]:seesection10.4.2).Accordingtoequation(10.33)inorderthat∆beabletoactontheR-moduleMweneedtobeabletodividetheelementsofMbyq+1.Discrete-TimeControl311ThuslettherebethemultiplicativesetQ={(q+1)nn≥0}⊂RandletB=Q−1Rbetheprincipalidealdomainconsistingofallelementsoftheformr/(q+1)nr∈Rn≥0(seesection13.6.5).Since∆∈BtheprincipalidealdomainC=R[∆]isasubringofB.Converselyaccordingto(10.34)1−∆/λ=2/(q+1).LettherebethemultiplicativesetT={(1−∆/λ)nn≥0}⊂CandletD=T−1C.Accordingtoequation(10.34)q∈DandhenceRisasubringofD.WecanthusgofromanR-moduletoaC-moduleandconverselybymeansoftwofunctorsdenotedbyGandHrespectivelyanddeﬁnedasfollows:–FunctorG:extensionoftheringofscalarsfromRtoBthenrestrictionoftheringofscalarsfromBtoC(seesection13.6.5).–FunctorH:extensionoftheringofscalarsfromCtoDthenrestrictionoftheringofscalarsfromDtoR.Thesetwofunctorsareexactastheyarethecompositionsoftwoexactfunctorsaccordingto([102]Corollary3.74andTheorem9.31).LetMbeanR-module.ThefunctionθM:M→B⊗RMdeﬁnedbym(cid:3)→1⊗m(where1istheunitelementofB)isR-linearandaccordingtoequation(13.66)section13.6.5:kerθM={m∈M:(q+1)nm=0forsufﬁcientlylargen}.LikewiseletˇMbeaC-module.ThefunctionψˇM:ˇM→D⊗CˇMdeﬁnedbyˇm(cid:3)→1⊗ˇmisC-linearandkerψˇM=(cid:31)ˇm∈ˇM:(1−∆/λ)nˇm=0forsufﬁcientlylargen .LEMMA344.–LetMbeaﬁnitelypresentedR-module.(i)IfMisfreeofrankn(resp.torsion)thenGMisaﬁnitelypresentedC-modulewhichisfreeofrankn(resp.torsion).(ii)LetNbeasubmoduleofM.ThenGNisasubmoduleofGMandGM/GN=G(M/N).PROOF.See([10]sectionsII.1andII.5)and([102]Theorem3.76).DEFINITION345.–LetΣdbeadiscrete-timesystemi.e.aﬁnitelypresentedR-moduleM.ThepseudocontinuoussystemˇΣassociatedwithΣdistheﬁnitelypresentedC-moduleˇM=GM.ConverselyletˇΣbeapseudocontinuoussystemi.e.aﬁnitelypresentedC-moduleˇM.Thediscrete-timesystemassociatedwithˇΣistheﬁnitelypresentedR-moduleM=HˇM.312LinearSystemsPROPOSITION346.–LetΣdbeadiscrete-timecontrolsystemwithinputud=[ud1...udm]Tandoutputyd=(cid:25)yd1...ydp(cid:26)T.ThenthepseudocontinuoussystemˇΣisacontrolsystemwithinputˇud=[ˇud1...ˇudm]Tandoutputˇyd=(cid:25)ˇyd1...ˇydp(cid:26)Twhereˇudi=θM(udi)andˇydj=θM(cid:15)ydj(cid:16)(1≤i≤m1≤j≤p).IfΣdiscontrollablethensotooisˇΣ.PROOF.Lemma344impliesthatProperty(ii)ofsection7.1.1holds.IfΣdiscontrollablethissystemisafreeR-moduleMandhenceˇΣistheC-moduleGMwhichisfreeaccordingtoLemma344.WewillillustrateDeﬁnition345throughthreeexamplestheﬁrstiselementaryandthenexttwoare“pathological”:EXAMPLE347.–LettherebethesystemΣd:qyd=udandletMbetheassociatedR-module.TheB-moduleB⊗RMisdeﬁnedforanyvalueofn≥0byq(q+1)nˇyd=1(q+1)nˇudwhereˇyd=θM(yd)andˇud=θM(ud).Bytakingn=1weobtain(1+∆/λ)ˇyd=(1−∆/λ)ˇudwhichdeﬁnesˇM=GM.InpracticeweobtainˇΣbyreplacingtheoperatorqbyitsexpression(10.34)asafunctionof∆.EXAMPLE348.–LettherebethesystemΣd:(q+1)yd=0.ThemoduleˇM=GMisreducedto0.EXAMPLE349.–LettherebethesystemΣd:(cid:15)q2−1(cid:16)yd=udwhichadmitsastrictlycausalstate-spacerepresentationoforder2thestatematrixofwhichhaseigenvalues−1and1(Lemma341isthereforenotapplicable).UsingthefunctorGweobtain4(∆/λ)ˇyd=(1−∆/λ)2ˇud.ToputˇΣinstate-spaceformwecansetˇxd=4ˇyd+(2−∆/λ)ˇudandweobtain(cid:20)∆ˇxd=ˇudˇyd=14ˇxd+14(∆/λ−2)ˇudwhichisanimproperﬁrst-orderstate-spacerepresentation.Discrete-TimeControl31310.5.4.StructuralpropertiesofpseudocontinuoussystemsConsiderapseudocontinuoussystemˇΣ.Itsstructuralproperties(stabilitycontrollabilityobservabilityetc.)aredeﬁnedinthesamemannerasthoseofacontinuous-timesystemandtheyarethusmadeexplicitbyreplacing(inaformalway)theoperator∆by∂.LetΣdbeadiscrete-timecausalcontrolsystemandletˇΣbetheassociatedpseudocontinuouscontrolsystem.Supposethat−1isnotapoleofΣd.Wethenhavethefollowing:THEOREM350.–ThepseudocontinuoussystemˇΣisstable(resp.controllableobservableetc.)ifandonlyifΣdhasthesameproperty.PROOF.WecanassumewithoutlossofgeneralitythatΣdisdeﬁnedbyastate-spacerepresentation{AdBdCD}accordingtoRemark320(section10.4.2).ThereforeˇΣisthestate-spacesystem(cid:31)ˇAˇBˇCˇD givenbyProposition341.ThespectrumofˇAisincludedinthelefthalf-planeifandonlyifthatofAdisincludedintheopenunitdiskaccordingtoProposition339(section10.5.1)andProposition439(section12.4.2).Ontheotherhandweshowtheequivalencebetweenthecontrollabilityof(AdBd)andthatof(cid:15)ˇAˇB(cid:16)byapplyingthePopov–Belevitch–Hautustest.Itisthesamefortheequivalencebetweentheobservabilityof(CAd)andthatof(cid:15)ˇCˇA(cid:16).10.6.Synthesisofdiscrete-timecontrol10.6.1.DirectapproachesThedesignmethodsofcontinuous-timecontrolsstudiedintheprecedingchapters(PIDandRSTcontrollersstatefeedbackcontrolwithintegralaction–ormoregenerally“internalmodel”statefeedback/observersynthesisetc.)canbereformulatedinthecontextofdiscretetime.Inthiswayweareledtosynthesesofdiscrete-timecontrolswhichwecanqualifyas“directapproaches”.Theseapproachesarediscussedinarathercompletemannerin[4];theywillnotbedevelopedhere.10.6.2.DiscretizationbyapproximationItisalsopossibletoderiveadigitalcontrollerfromananalogcontrollerbyapproximation.TheEulerapproximation∂(cid:9)q−1T(10.38)isthesimplestone.Itcomesdowntowritingifxisadifferentiablecontinuous-timesignal˙x(t)(cid:9)x(t+T)−x(t)T314LinearSystemsandthatt=kT.ThisapproximationisofcourseonlyvalidifxdoesnotvarytoorapidlybetweentwosamplinginstantsandhenceifthesamplingperiodTissufﬁcientlysmall.ConsiderforexampleananalogPIDcontrollerwhosetransferfunctionisK(s)=k*1+1TIs+Tds1+TdNs+.Withzeroinitialconditionstheapproximation(10.38)isequivalenttos(cid:9)z−1T.WethusobtainthedigitalPIDcontrollerwithtransferfunctionKd(z)=k*1+TTI1z−1+TdTz−11+TdNT(z−1)+.10.6.3.PassagethroughapseudocontinuoussystemTheeffectivenessoftheapproachdetailedherebelowhasbeenpointedoutinamoregeneralcontext[99].GeneralprocedureLetΣdbeadiscrete-timecontrolsystemnothavingapoleat−1andhavingtheappropriatestructuralproperties(controllabilityobservabilityetc.)andletˇΣbetheassociatedpseudocontinuoussystem(whichhasthesamestructuralpropertiesaccordingtoTheorem350).Weapplyoneofthedesignmethodsofcontinuous-timecontroldiscussedinsection10.6.1toˇΣ(theoperator∂beingreplacedby∆).WethusobtainasuitablepseudocontinuouscontrollerˇΘforwhichthepseudocontinuousclosed-loopsystemisstable.NowletΘdbethediscrete-timecontrollerassociatedwithˇΘ(i.e.Θd=HˇΘwiththenotationinsection10.5.3*).AccordingtoTheorem350ΣdfedbackbyΘdisstable.REMARK351.–ThepseudocontinuouscontrolsystemˇΣisproperbutnotstrictlyproper(seesection10.5.2Remark343(i)).Asaresultfortheprocedureweareproposingtobeapplicablethemethodsusedtodesigncontinuous-timecontrolshavetoapplytothistypeofsystem(seesection6.5Exercises121and128;section8.4Exercise269;section9.3.1Remark290;section9.4Exercise302).Discrete-TimeControl315ConsiderationofthecomputingtimeTheabovecontrollerΘdiscausalbutnotstrictlycausal(evenifˇΘisstrictlyproper):seeExercise358.Ifthecomputingtimeofthecontrolud(k)isnotnegligiblerelativetothesamplingperiod(whichdependsbothonthecontrollercomplexityandthecomputingrapidity)itisnecessarywhenimplementingthecontrollawinrealtimetoonly“send”thecontrolud(k)atinstant(k+1)Tthususingastrictlycausalcontroller.EXAMPLE352.–Considerthecontinuous-timesystemΣdeﬁnedbytheleftform(6.25)(section6.3.6).The(Z.O.H.)discretizedsystemΣdwithsamplingperiodT=0.1isdescribedbytheleftformAd(q)yd=Bd(q)udwhereAd(q)=q2−1.72q+0.74Bd(q)=−0.016q+0.051.WewishtodesignadigitalRSTcontrollerwithequationSd(q)ud=Rd(q)(rd−yd)(10.39)havingcharacteristicssimilartothoseofthecontinuous-timeRSTcontrollerwhosepolynomialsaredeﬁnedbyequalities(6.28)and(6.29).(i)Caseofanon-strictlycausalcontroller.Asmentionedabovetheimplementationofsuchacontrollerispossibleifthecomputingtimeofthecontrolud(k)issmallcomparedwiththesamplingperiod(uptoabout20%ofthisperiod).Themorepowerfulcomputersarethemorefrequentthissituationbecomes.ThepseudocontinuoussystemˇΣassociatedwithΣdforλ=2TisdescribedbytheleftformˇA(∆)ˇy=ˇB(∆)ˇuwithˇA(∆)=∆2+2.99∆+1.99ˇB(∆)=0.019∆2−0.587∆+3.98.NotethatthecoefﬁcientsofthesepolynomialsareclosetothoseofthepolynomialsA(∂)andB(∂).Wechooselikeinsection6.3.6ˇAcl(∆)=ˇAc(∆)(∆+10)2withˇAc(∆)=(∆+2)2.ThepolynomialsˇS(∆)andˇR(∆)whichrealizethedesiredpoleplacementwhilesatisfyingconditionˇS(0)=0(integrator)areˇS(∆)=∆3+21.40∆2+209.54∆ˇR(∆)=83.02∆2+266.24∆+200.83anddeﬁnethepseudocontinuouscontrollerˇΘ.ThisonehereisassociatedwiththedigitalcontrollerΘddeﬁnedbytheleftform(10.39)whereSd(q)=q3−1.367q2+0.542q−0.175Rd(q)=1.867q3−1.315q2−1.828q+1.353.316LinearSystems(ii)Caseofastrictlycausalcontroller.IfwewanttoobtainastrictlycausalcontrollerweﬁrstreplaceAd(q)by˜Ad(q)=qA(q)thusthesystemΣdbythesystem˜Σdwhichhasasupplementaryunitydelay.Thepseudocontinuoussystemassociatedwith˜Σdis(againwithλ=2T)describedbytheleftformˇA2(∆)ˇy=ˇB2(∆)ˇuwhereˇA2(∆)=∆3+22.99∆2+61.84∆+39.83ˇB2(∆)=−0.019∆3+0.97∆2−15.72∆+79.67.ChoosenowˇAcl(∆)=ˇAc(∆)(∆+10)2(∆+2/T)2whereeachroot∆=−2/Tcorrespondstoasupplementarypoleq=0introducedbythecomputingdelay.WeobtainthepolynomialsˇS(∆)=∆4+44.65∆3+539.05∆2+4858.88∆ˇR(∆)=84.71∆3+1963.91∆2+5593.19∆+4016.66thatdeﬁneastrictlyproperpseudocontinuouscontroller˜Θwhichisassociatedwithadiscrete-timecontroller˜Θd.Thelatterisdeﬁnedbyaleftformwhosepolynomialscanbedenotedby˜Sd(q)andRd(q).Thedigitalcontrollerwearelookingforisdeﬁnedbytheleftform(10.39)whereSd(q)=q˜Sd(q).Weﬁnallyobtain(aftersimpliﬁcationoftherootq=0commontoSd(q)andRd(q))10Sd(q)=q4−1.398q3+0.637q2−0.145q−0.095Rd(q)=1.903q3−1.344q2−1.863q+1.383.(iii)InthetwocasesSd(1)=0whichmeansthatthecontrollerisanintegratorsystem(Sd(1)isthesumofthecoefﬁcientsofthepolynomialSd(q));inadditionRd(−1)=0;thisisduetothefactthatsinceeachpseudocontinuouscontrollerisstrictlyproperitszeroatinﬁnity(i.e.atw=∞)istransformedintoazeroatz=−1throughtheinversebilineartransform(10.31)(itisimportantthatthisconditionberespectedforthedigitalcontrollertohavezerogainattheNyquistfrequencywhichplaystheroleof“highfrequencies”fordiscrete-timesystems:seesection4.2.6).Itnowremainstoverifythegoodbehavioroftheclosed-loopsystem.Theeventsareasfollows:(i)unitstepcommandatinstantt=0;(ii)disturbancestepofamplitude0.3addingtoyatinstantt=5.TheoutputydandthecontroludarerepresentedasfunctionsoftimeinFigure10.12withtheﬁrst(--)andthesecond(–)controller–seebelow.Thetworesponsesarealmostidentical;thesecondcontrollernonethelessgeneratesaslightdelaycomparedwiththeﬁrst(whichwaspredictable).TheseresponsesarealsoverysimilartothoseinFigures6.6and6.8ofsection6.3.6(thistimehoweverthesimulationisperformedwithoutmeasurementnoise).10.Thereaderisrequestedtoshowthatduetothefactor∆+2/TinˇAcl(∆)thepolynomialsSd(q)andRd(q)musthavethisrootincommon(thedigitalRSTcontrollerthusis0-controllablebutnotcontrollablebeforetheindicatedsimpliﬁcationisperformed).Discrete-TimeControl317Figure10.12.Timeresponses(Example352)EXAMPLE353.–Considerthesamecontinuous-timesystemΣasinExample352butinthesituationspeciﬁedinsection6.4.7no.1.Wemustnotonlyensureacontrolwithoutanystaticerrorbutalsorejectasinusoidaldisturbancewithangularfrequencyω0=1rad/s.Thenotationusedhereisidenticaltothatinthecitedparagraph(mutatismutandis).Weassumethatthecomputingtimeissmallcomparedwiththesamplingperiodandthatitisthereforeuselesstodesignastrictlycausaldigitalcontroller(otherwisethemethodalreadydetailedinExample352(ii)canbeused).Theeasiestwayistousethebilineartransformwithprewarpingatthenormalizedangularfrequencyθ0=ω0Twithcoefﬁcientλgivenbytheexpression(10.32)(foranotherapproachseeExercise361).ThereforeD1(∆)=∆2+2ςω0∆+ω20withς=0.005forexample.ThepolynomialsAs(∆)andAcl(∆)arerespectivelyAs(∆)=(∆+1+iω0)(∆+1−iω0)(∆+10)2Acl(∆)=As(∆)(∆+2)(∆+1)2andthecorrespondingpolynomialsˇR(∆)ˇS(∆)andˇT(∆)areˇR(∆)=99.58∆4+360.90∆3+437.50∆2+276.91∆+100.58ˇS(∆)=∆5+21.08∆4+221.38∆3+23.27∆2+220.17∆ˇT(∆)=0.50∆4+11.06∆3+71.41∆2+120.70∆+100.58.318LinearSystemsFigure10.13.Timeresponses(Example353)Notethatthispseudocontinuouscontrollerisstrictlyproper(seeExample352(iii))andthatˇR(0)=ˇT(0)(seesection6.4.2).ThepolynomialˇS(∆)canbeputintheformˇS(∆)=∆D1(∆)(∆+β)(cid:15)∆+¯β(cid:16)withβ=−10.53+10.45iandˇT(∆)=As(∆)ˇR(0)As(0).Thispseudocontinuouscontroller˜Θisassociatedwithadiscrete-timecontrollerΘdusingthetransform(10.33)withthevalueofλasspeciﬁedabove.ThiscontrollerΘdisadigitalRSTcontrollerwhosepolynomialsareRd(q)=2.274q5−6.043q4+3.081q3+4.462q2−5.356q+1.581Sd(q)=q5−3.334q4+4.208q3−2.598q2+0.9139q−0.1905Td(q)=10−2(2.396q5−3.514q4−0.809q3+3.315q2−1.567q+0.217).WehaveSd(1)=0Rd(−1)=0(seeExample352(iii))andRd(1)=Td(1)sinceˇT(0)=ˇR(0)(thisistheconditionforthestaticerrortobezero).Thesimulationoftheclosed-loopsystemyieldstheresponsesinFigure10.13(withthesameconditionsasinsection6.4.7n◦1).TheseresponsesarealmostidenticaltothoseinFigure6.12Discrete-TimeControl31910.7.ExercisesEXERCISE354.–Establishalltheresultsintable(10.14).EXERCISE355.–Lettherebethesinusoidx(t)=sin(πt).(a)Whatisitsfrequency?(b)Thissinusoidisbeingdiscretizedatfrequencyfe=1andhencethesamplinginstantsaretherationalintegers.Whatistheobtaineddiscretizedsignalxd?(c)CanwereplacethestrictinequalitybyaninequalitywhichisnotstrictinthestatementofTheorem305?EXERCISE356.–Considerthecontinuous-timestate-spacesystemΣ={ABC}withA=(cid:27)−201−2(cid:28)B=(cid:27)10(cid:28)C=(cid:25)01(cid:26).(i)DeterminethepolesofΣ.(ii)ThesystemΣisdiscretizedwithsamplingperiodT=0.2;determinethediscretizedsystemΣd={AdBdCd}.(iii)CalculatethetransferfunctionofΣdanditspoles.EXERCISE357.–ThealtitudeθofasatellitesatisﬁestheequationJd2θdt2=Γ(t)whereJisthemomentofinertiaandΓ(t)isthetorqueexertedatinstantt.Weputy=θandu=Γ/J.(i)Determinetheobservablecanonicalform{ABC}ofthissystemΣ.(ii)ThesystemΣisdiscretizedwithperiodT.Determinethestate-spacerealization{AdBdC}ofthediscretizedsystemΣdbyapplyingrelation(10.15)ofsection10.3.4.Isitanobservablecanonicalform?(iii)DeterminetheleftformD(q)yd=N(q)udwhichgovernsthesystemΣd.(iv)Wenowassumethatthetorqueexertedtothesatelliteisnolongeru(uptofactorJ)butu(t−τ)whereτisthetransmissiondelayoftheinformationfromEarth.Byputtingτ=3TwritetheleftformwhichgovernsΣdtakingthisdelayintoaccount.(v)WritetheobservablecanonicalformofΣdinthissituation.(vi)DeterminetheblockdiagramofΣdanalogoustothatinFigure7.5(section7.4.2)butwheretheintegratorsarereplacedbydelaysq−1.(vii)Generalizetheabovetothecasewhereτ=nTn≥1.(viii)ConsideringanRSTcontrollercalculatedaccordingtothemethodspresentedinExamples352and353isitwelladaptedforthecontrolofatime-delaysystemsuchasthisonehere?Ifyesisitnotakindofpredictivecontrol?EXERCISE358.–LetˇΣbeapseudocontinuoussystemdeﬁnedbyastate-spacerepresentation(cid:31)ˇAˇBˇCˇD .Determinethecorrespondingdiscrete-timestate-space320LinearSystemssystemΣd={AdBdCD}bymakingtheappropriatehypothesisontheeigenvaluesofˇA.InthegeneralcaseisΣdstrictlycausal?EXERCISE359.–LetΣbeacontinuous-timestate-spacesystemofordernwithequation˙x=Ax+BuanddiscretizedatperiodT.WedenotethestateandcontrolmatricesofthediscretizedsystemΣdbyAandBrespectively.(i)ShowthatforT→0+Ad→InBd→0andthatallthepolesofΣdtendto1.(ii)Wedenotetheoperator(q−1)/Tasδ.Determinethestateandcontrolmatrices˜Adand˜BdofΣdwhenitsstate-spaceequationisexpressedusingtheoperatorδaccordingtoδxd=˜Adxd+˜Bdud.Byabuseoflanguagewewillcallthisequationthe“deltatransform”ofthestateequationofΣd[4].(iii)Determinethelimitsof˜Adand˜BdasT→0+;deducethatΣdisbetterrepresentedusingthedeltaoperator(insteadoftheshift-forwardoperatorq)afterquantizationwhenthesamplingperiodisverysmall.(iv)Inordertoimplementthecontrollawdoestheoperatorδposeanyproblem?(Comparewiththeoperator∆.)EXERCISE360.–LetΣbethecontinuous-timesystemdeﬁnedbytheleftform(∂+1)y=u.(i)ThissystemisdiscretizedatperiodT(cid:16)1.DeterminethetransferfunctionGd(z)ofthediscretizedsystemΣdandthenthetransferfunctionˇG(w)oftheassociatedpseudocontinuoussystemˇΣ(withλ=2/T).(ii)DeterminelimT→0ˇG(w).Whatisremarkableaboutthislimit?(iii)WehypothesizethatthecomputingtimeisnegligiblecomparedwiththesamplingperiodT=0.2.DeterminethepseudocontinuousRSTcontrollerhavingthefollowingproperties:(a)itisanintegratorsystem;(b)ˇR/ˇShasrelativedegreeδ0=1;(c)allthepolesofthepseudocontinuousclosed-loopsystemareplacedat−10/3;(d)thetransferfunctionbetweenthereferencesignalandtheoutputisoforder1.(iv)Determinethecorrespondingdiscrete-timeRSTcontroller.CalculateSd(1)Rd(1)Td(1)andRd(−1).Interpretation?(vi)Examinetheproblemagaininthecasewherewecannotneglectthecomputingtimecomparedwiththesamplingperiod.EXERCISE361.–HowdoyoutreatExample353usingabilineartransformwithoutprewarping?EXERCISE362.–(i)Canadiscretizedsystemhavepolesinthesubset(−∞0)oftherealline?(ii)Canithavepolesat0?EXERCISE363.–*Considerthediscrete-timesystemdeﬁnedbytheleftform(q+1)qyd=(q+1)ud.(i)Isthissystemstabilizable?(ii)Showthattheassociatedpseudocontinuoussystemiscontrollable.DoesthisresultcontradictTheorem350orProposition346?(iii)Canthissystembeobtainedbydiscretizationofacontinuous-timesystem?*Chapter11IdentiﬁcationA“visual”methodofidentiﬁcationhasalreadybeendiscussedinthisbook(seeExercise71section3.6).Othermethodsthatarealittlelessrusticexist(Strejc’sandBroïda’sforexample[77])whoseresultshoweverarenotsigniﬁcantlybetter.Wepresenthereoneofthewidelyusedandefﬁcientidentiﬁcationapproachesi.e.theparametricidentiﬁcation.TheideaistoﬁrstchooseaprioriamodelM(θ)ofthesystem(forexampleonthebasisofphysicalconsiderations)whereθistheparametervectortobeestimated.Theestimationofθisdonebyminimizingacostfunction(or“criterion”)forexamplethenorml2ofthe“predictionerror”(seesection12.2.1forthedeﬁnitionofthisnorm).Toclarifyideasconsiderthediscrete–timesystem(cid:15)q2+a1q+a2(cid:16)yd=(b1q+b2)ud.Thisequationdeterminesthemodelstructure(whichisadiscrete–timeleftformofthesecondorder).Thecoefﬁcientstobeestimatedaretheai’sandbi’s(1≤i≤2).Puttingθ=(cid:25)a1a2b1b2(cid:26)TtheidentiﬁcationproblemnowcomesdowntoestimatingθbyminimizinganappropriatecriterionJ(θ).11.1.RandomsignalsThenaturalframeworkusedtodiscussparametricidentiﬁcationmethodsisthatofrandomsignals(alsocalledstochasticprocesses).Thetheoryofrandomsignalsisbasedonthetheoryofprobabilities(seesection12.7).Wewillbeginwiththestudy321Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.322LinearSystemsofpseudo-randomsignals(inthesenseof[5])whichsharemanypropertieswithergodicrandomsignals(seesection11.1.5)andwewilllimitourdiscussiontotheformer(generalstochasticprocessesarestudiedine.g.[8]and[37]).11.1.1.Momentsoforder1and2Letx={x(t)}t∈Zbeadiscrete-timesignalwithvaluesinK=RorC(i.e.x∈KZ).Themean(ormomentoforder1)ofthissignalisx(t)=limN→+∞12N+1N(cid:11)t=−Nx(t).(ifitexistsandwithanabuseoflanguagebecausex(t)doesnotdependont).REMARK364.–Themeanofasignalonlydeﬁnedfort≥0isx(t)=limN→+∞1NN−1(cid:12)t=0x(t).Themomentoforder2ofx(ifitexists)alsocalleditsmeanenergyis|x(t)|2=limN→+∞12N+1N(cid:11)t=−N|x(t)|2.DEFINITION365.–Asignalxissaidtobecenteredifx(t)=0.11.1.2.Correlationandcross-correlationDEFINITION366.–Letx∈KZ.Itscorrelationsequence(orabusingthelanguageitscorrelationfunction)isrxx={rxx(τ)}τ∈Zsuchthatrxx(τ)=x∗(t)x(t+τ)=limN→+∞12N+1N(cid:11)t=−Nx∗(t)x(t+τ)ifitexists1inwhichcasethesignalxissaidtobecorrelatable.Lety∈KZ.Thecross-correlationsequence(orfunction)ofxandyisthesequencerxy={rxy(τ)}τ∈Zsuchthatrxy(τ)=x∗(t)y(t+τ)=limN→+∞12N+1N(cid:11)t=−Nx∗(t)y(t+τ)1.Inthischapterx∗denotestheconjugateofx.Identiﬁcation323ifitexists.Twosignalsxy∈KZaresaidtobecross-correlatableiftheyarecorrelatableandrxyexistsanduncorrelatedifrxy=0.PROPOSITION367.–(i)ryxexistsifandonlyifrxyexistsandryx(τ)=r∗xy(−τ)(insuchawaythattherelation“beingcross-correlatable”issymmetric).(ii)Ifxandyarecross-correlatablethentheyareofﬁnitemeanenergyand|rxy(τ)|≤%rxx(0)(cid:30)ryy(0).PROOF.(i)isobviousand(ii)isaconsequenceoftheSchwarzinequality.Wewriter(cid:2)xy(τ)=rxy(τ)√rxx(0)√ryy(0)andaccordingtoProposition367wehave))r(cid:2)xy(τ)))≤1.DEFINITION368.–Thesequencer(cid:2)xxisthenormalizedcorrelationsequence(orfunction)ofxandr(cid:2)xyisthenormalizedcross-correlationsequence(orfunction)ofxandofy.PROPOSITION369.–AsetAofpairwisecross-correlatablesignalsisaK-vectorspace.PROOF.Itsufﬁcestonoticethatifz=µx(µ∈K)rzz=|µ|2rxxandthatrx+yx+y=rxx+rxy+ryx+ryy.DEFINITION370.–Letf={f(t)}t∈Zbeadiscrete-timesignal.Thissignalissaidtobeofpositivetype(writtenasf(cid:18)0)ifforanysequence(ck)ofcomplexnumbersandanysequence(τk)ofelementsofZwehave(cid:12)(kl)∈I×Ickc∗lf(τk−τl)≥0foreveryﬁnitesetofindicesI.PROPOSITION371.–Letxbeacorrelatablesignal.Thenrxx(cid:18)0.324LinearSystemsPROOF.Let(ck)beasequenceofcomplexnumbersand(τk)beasequenceofelementsofZ.ForanyﬁnitesetofindicesIwehave(cid:12)(kl)∈I2ckc∗lN(cid:12)t=−Nx∗(t)x(t+τk−τl)=(cid:12)(kl)∈I2ckc∗lN+τk(cid:12)t=−N+τkx∗(t−τk)x(t−τl)andN+τk(cid:12)t=−N+τk=N(cid:12)t=−N+N+τk(cid:12)t=N+1−−N+τk−1(cid:12)t=−N.Thetwosumsontheright-handsidedividedby2N+1tendto0asN→+∞.Ontheotherhand(cid:12)(kl)∈I×Ickc∗l2N+1N(cid:12)t=−Nx∗(t−τk)x(t−τl)=12N+1N(cid:12)t=−N)))))(cid:12)k∈Ic∗kx(t−τk))))))2≥0.*ThefollowingresultisavariantoftheBochnertheorem[5]theproofofwhichcanbefoundin([37]ChapterX).THEOREM372.–Letxbeacorrelatablesignal.(i)rxxcanbewrittenintheformofaFourier–Stieltjesintegralrxx(τ)=12π(cid:2)π−πeiωτdΦxx(ω)whereΦxxisaboundednon-decreasingfunctioncalledthespectralfunctionofx.2(ii)rxx(τ)=σ(0)=Φ(0+)−Φ(0−)≥0.(iii)|rxx(τ)|2=14π2(cid:11)ωσ2(ω)whereσ(ω)=Φ(ω+)−Φ(ω−)≥0(σcanonlybenon-zeroinacountablesetofpoints).(iv)AsaresultifΦisacontinuousfunctionthenrxx(τ)=|rxx(τ)|2=0.*2.Inthischapterthenormalizedangularfrequency(see(10.8)section10.2.5)isdenotedbyωandnotθtoavoidconfusionwiththeparametervector.Identiﬁcation325PROPOSITION373.–Letxbeacorrelatablesignal.Thenrxx(t)=)))x(t))))2+ryy(t)wherey(t)=x(t)−x(t).PROOF.Writingm=x(t)wehaverxx(τ)=(m∗+y∗(t))(m+y(t+τ))=|m|2+ryy(τ)becausey(t)=0.OntheotherhandtheBochnertheoremshowsthatryy(t)exists.11.1.3.Pseudo-randomsignalsDEFINITION374.–Apseudo-randomsignalisacorrelatablesignalwhosespectralfunctionΦxxisabsolutelycontinuous(seesection12.2.3).Thederivativeϕxxofthisspectralfunctioniscalledthespectraldensityofx.THEOREM375.–(i)Apseudo-randomsignalisacorrelatablesignalsuchthatrxx(τ)=12π(cid:2)π−πeiωτϕxx(ω)dω=(cid:15)F−1ϕxx(cid:16)(τ);itsspectraldensityϕxxissuchthatϕxx(ω)=(Frxx)(ω)≥0andϕxx∈L1[−ππ](whereL1[−ππ]denotesthespaceofintegrablefunctionson[−ππ]).(ii)Itscorrelationsequencerxxissuchthatrxx(τ)=|rxx(τ)|2=0.(iii)Apseudo-randomsignaliscentered.PROOF.(i)and(ii)areimmediatelyclearfromDeﬁnition374andtheBochnertheorem.(iii)isderivedfromProposition373.DEFINITION376.–Twopseudo-randomsignalsxandyaresaidtobeabsolutelycross-correlatableiftheyarecross-correlatableandifrxy=F−1ϕxywhereϕxy∈L1[−ππ]iscalledthecross-spectraldensityofxandofy.LetRxy(z)bethez-transformofrxy.PROPOSITION377.–Ifxandyareabsolutelycross-correlatablethenyandxarealsoabsolutelycross-correlatableandRyx(z)=Rxy(cid:9)z∗−1(cid:10)∗;inparticularϕyx(ω)=ϕ∗xy(ω).IfK=RRyx(z)=Rxy(cid:15)z−1(cid:16);inparticularϕ∗xy(ω)=ϕxy(−ω).326LinearSystemsPROOF.Supposexandyareabsolutelycross-correlatable.Thenϕyx(ω)=+∞(cid:12)τ=−∞ryx(τ)z−τ=+∞(cid:12)τ=−∞r∗xy(−τ)z−τ(accordingtoProposition367(i))thusϕyx∈L1[−ππ]andϕyx(ω)=ϕ∗xy(ω).Theotherassertionsareclear.THEOREM378.–AsetMofpairwiseabsolutelycross-correlatablepseudo-randomsignalsisaK-vectorspace.PROOF.AccordingtotheproofofProposition369ifz=µx(µ∈K)thenϕzz=|µ|2ϕxxandϕx+yx+y=ϕxx+ϕyy+ϕxy+ϕyx.11.1.4.FilteringandfactorizationWecalladigitalﬁlteradiscrete-timelineartime-invariantsystemfunctioningfrominitialinstantt0→−∞withzeroinitialconditions.Fromthemathematicalpointofviewadigitalﬁlterischaracterizedbyaninput–outputrelationwhichisaconvolutionoperatoru(cid:3)→y=g∗ui.e.y(t)=+∞(cid:12)τ=−∞g(t−τ)u(τ).Theimpulseresponseofthisﬁlter(i.e.itsresponsetoaunitimpulseδ0–seesection12.3.5)isthesequenceg=(g(t))t∈ZanditstransferfunctionisG(z)=Z{g}.IfweassumethatthefunctionG(z)isrationalthisﬁlteriscausalandstableifandonlyiftherationalfunctionG(z)isproperandallitspoleslieintheopenunitdiscwhichisequivalenttosayingthatgispositivelysupportedandbelongstol1(seeTheorem446section12.4.4).InterferenceformulaLettherebetwostablecausaldigitalﬁlterswithrationaltransfersfunctionsG1(z)andG2(z).Letx1(resp.x2)betheinputtotheﬁrst(resp.second)ﬁlterandy1(resp.y2)beitsoutput(seeFigure11.1).THEOREM379.–Ifx1andx2areabsolutelycross-correlatablethensoarey1andy2andRy1y2(z)=G1(cid:15)z−1(cid:16)G2(z)Rx1x2(z)(11.1)whereRy1y2(z)andRx1x2(z)arethez-transformsofry1y2andrx1x2respectively.Relation(11.1)iscalledtheinterferenceformula.Identiﬁcation3271x1y2x2yzG2zG1Figure11.1.Pseudo-randomsignalﬁlteringPROOF.Letgi=Z−1{Gi}(i=12).AccordingtotheExchangetheorem(section12.3.5)yi(t)=+∞(cid:12)k=−∞gi(t−k)xi(k)asaresulty∗1(t)y2(t+τ)=+∞(cid:12)k=−∞+∞(cid:12)j=−∞g1(t−k)g2(t+τ−k+j)x∗1(k)x2(k+j)fromwhichwegetry1y2(τ)=+∞(cid:12)k=−∞+∞(cid:12)j=−∞g1(t−k)g2(t+τ−k+j)rx1x2(j).OntheotherhandRy1y2(z)=+∞(cid:12)τ=−∞ry1y2(τ)z−τandwethusobtain(11.1)bywriting−τ=(t−k)−(t−k+τ−j)−j.Pseudo-whitenoiseApseudo-whitenoiseisacorrelatablesignalwsuchthatrww=λδ0.Wethushaverww(τ)=(cid:20)λifτ=00otherwise.(11.2)Sincerww(0)=|w(t)|2≥0λisarealnon-negativenumbercalledthevarianceofthepseudo-whitenoisew;σ=√λisitsstandarddeviation.328LinearSystemsDEFINITION380.–Thepseudo-whitenoisewithcorrelationsequence(11.2)issaidtobenormalizedifitsvarianceisequalto1.Theproofofthefollowingtheoremisobviousanditsdetailsarelefttothereader:THEOREM381.–Apseudo-whitenoisewithvarianceλisapseudo-randomsignalwithconstantspectraldensityequaltoλ.Apseudo-randombinarysequence(PRBS)isapseudo-randomsignalwsuchthatw(tn)∈{−σσ}where(tn)isastrictlyincreasingsequenceofelementsofZchoseninsuchawaythatrww(τ)(cid:9)0ifτ(cid:5)=0(formoredetailssee[110]Chapter5).Suchasignalisthereforeanapproximationofapseudo-whitenoise.SpectralfactorizationDEFINITION382.–Apseudo-randomsignalxissaidtoberationalifRxx(z)isarationalfunction.Itissaidtobepersistentlyexcitingifϕxx(ω)>0foranyω∈[−ππ]andtobepersistentlyexcitingoforderN(N∈{12...}∪{+∞})ifthereexistNdistinctvaluesωj∈[0π]forwhichϕxx(ωj)>0.THEOREM383.–(Spectralfactorizationtheorem).Letxbeapersistentlyexcitingrealrationalpseudo-randomsignal(i.e.K=R).Thereexistsapseudo-whitenoisewwithwell-determinedvarianceaswellasauniquebistableandbicausalﬁltertheimpulseresponseandthetransferfunctionofwhicharedenotedbygandG(z)respectivelysuchthatx=g∗wandg(0)=1.PROOF.AccordingtoProposition377ifzk(resp.pk)isazero(resp.apole)ofRxx(z)then1/zkz∗kand1/z∗k(resp.1/pkp∗kand1/p∗k)areagainzeros(resp.poles)ofRxx(z).OntheotherhandRxx(z)hasnopolesontheunitcirclebecauseϕxx∈L1[−ππ]andRxx(z)hasnozerosontheunitcirclebecauseϕxx(ω)>0forallω∈[−ππ].WecanthuswriteRxx(z)=λ(k∈K(z−zk)(z−z∗k)(z−1/zk)(z−1/z∗k)(j∈J(z−pj)(cid:15)z−p∗j(cid:16)(z−1/pj)(cid:9)z−1/p∗kj(cid:10)λ>0.Wecanassumewithoutlossofgeneralitythat|zk|<1and|pj|<1k∈Kj∈J.ThuswehaveG(z)=(k∈K(z−zk)(z−z∗k)(j∈J(z−pj)(cid:15)z−p∗j(cid:16)z2nIdentiﬁcation329wheren=card(J)−card(K).ThereforeG(z)isbiproperandbistable.Wehaveg(0)=lim|z|→+∞G(z)=1accordingtotheInitialvaluetheorem(seesection12.3.5).ThistransferfunctionG(z)isuniquelydetermined.Thereexistsasignalwsuchthatx=g∗w;thissignalwisgivenbyw=Z−1(cid:20)X(z)G(z)&.Itisapseudo-randomsignalaccordingtoTheorem379andsinceRxx(z)=G(cid:15)z−1(cid:16)G(z)λ(11.3)wehaveRww(z)=1G(z−1)G(z)Rxx(z)=λwhichprovesthatwisapseudo-whitenoise.Relation(11.3)expressesthe“bicausalspectralfactorization”thathasbeendone.Pseudo-colorednoiseJustaswhitelightisasuperimpositionofallcolorsandthushasaspectrumthatcontainsallvisiblefrequenciesapseudo-whitenoisehasaconstantspectraldensity.Ifwhitelightgoesthroughacolorﬁlter(apieceofcoloredglassforexample)weobtaincoloredlightbehindthisﬁlter.Byanalogyapseudo-colorednoiseistheoutputofadigitalﬁlterexcitedbyapseudo-whitenoise.Theorem383showsthateveryrationalpseudo-randomsignalxisapseudo-colorednoise.11.1.5.ErgodicrandomsignalsErgodicityofuptosecondorderLet(ΩFP)beaprobabilityspace.Adiscrete-timerandomsignalxisasequenceofrandomvariablesx(t):ω∈Ω→x(tω)∈(KB)(witht∈ZandwhereBistheBorelianσ-algebraofK).ThissignalissaidtoberealifK=R330LinearSystemsandstationaryifitslawofprobabilityνx(t)isindependentoft.Itisergodicofﬁrstorderifallrandomvariablesx(t)areofﬁrstorderandifforallω∈Ω3E[x(t)]=x(tω).Thesignalxisergodicuptosecondorderifinadditionallrandomvariablesx(t)areofsecondorderand(forallω∈Ω)E[x∗(t)x(t+τ)]=x∗(tω)x(t+τω).(11.4)Inwhatfollows“ergodic”means“ergodicuptosecondorder”.Wefurtherassumethatforanyω∈Ωx(.ω)isapseudo-randomsignalinthewayspeciﬁedinsection11.1.3(thisimpliesinparticularthatE[x(t)]=0i.e.xiscentered).WhitenoiseandcolorednoiseAdiscrete-timewhitenoisewisanergodicrandomsignalwhoseallrealizationsw(.ω)arepseudo-whitenoisesofthesamevarianceE(cid:23)|w(t)|2(cid:24)=λ≥0(seerelation(11.2)section11.1.4).4Weincludeasanadditionalhypothesisthattherandomvariablesw(t)andw(τ)areindependentwhenevert(cid:5)=τ.5LetxbearandomsignalergodicuptothesecondordersuchthatRxx(z)=Z{rxx}isarationalfunction.Therealizationsofxarepseudo-colorednoises(seesection11.1.4)andxisthuscalledacolorednoise.TemporallawLetXbearealergodicrandomsignal.Itstemporallawisthesetofprobabilitylawsofallrandomvariables(X(t1)X(t2)...X(tn))where{t1...tn}spansthesetofallstrictlyincreasingﬁnitesequencesofelementsofZ.Inparticulararandomsignal(assumedtoberealergodic)issaidtobeGaussianifitstemporallawisGaussianthatisallthevariablesdiscussedhereareGaussian.3.MorepreciselyforalmosteveryωinthesenseoftheprobabilitymeasurePi.e.“almostsurely”.Thatishowtheexpression“forallω∈Ω”shouldbeinterpretedinwhatfollows.4.Someauthorscallapseudo-whitenoisewhatwecallhereadiscrete-timewhitenoise.5.Theserandomvariablesareofcoursenon-correlated.Twoindependentrandomvariablesarenon-correlatedbuttheconversedoesnotholdexceptinparticularcases(theGaussiancaseforexample).Identiﬁcation33111.2.Open-loopidentiﬁcation11.2.1.NotationHereandinsubsequentsectionsallsystemsarelineardiscrete-timeonesandarerepresentedbymeansofthedelayoperatorq−1inplaceoftheshift-forwardoperatorq(wethusworkovertheringAintroducedinsection10.4.2).LetG(cid:15)q−1(cid:16)bearationalfunctionwithindeterminateq−1.TheleftmultiplicationbyG(cid:15)q−1(cid:16)representstheconvolutionbytheimpulseresponsegoftheﬁlterwithtransferfunctionG(cid:15)z−1(cid:16).InotherwordsifG(cid:15)q−1(cid:16)=+∞(cid:12)τ=−∞g(τ)q−τwherethesequence(g(τ))τ∈Zispositivelysupportedforcausalityoftheﬁltertheoutputy(t)isgivenbyy(t)=G(cid:15)q−1(cid:16)u(t)=+∞(cid:12)τ=−∞g(τ)u(t−τ).NotethatinthisformalismtheInitialvaluetheoremcanbewritteninthefollowingparticularlysimpleform:g(0)=G(0).REMARK384.–Inthissectionwhereopen-loopsystemsareconsideredweassumethatthesesystemsarestableinordertoavoidthedivergenceoftheoutputsignal{y(t)}.11.2.2.LeastsquaresmethodDeterministicapproachConsiderastablesystemwiththefollowingmodel:A(cid:15)q−1(cid:16)y(t)=q−rB(cid:15)q−1(cid:16)u(t)+e(t)(11.5)A(cid:15)q−1(cid:16)=1+nA(cid:12)k=1aiq−iB(cid:15)q−1(cid:16)=nB(cid:12)k=1biq−i.332LinearSystemswhereuandyarerespectivelytheinputandtheoutputofthesystemandeistheerrorduetotheuncertaintyonthecoefﬁcientsaiandbiwhicharetobeidentiﬁedthusaprioriarenotpreciselyknown(possiblytotallyunknown).Integerr≥0isthepuredelay(addingtothedelayduetothediscretizationwithZ.O.H.).Letθ=(cid:25)a1...anAb1...bnB(cid:26)TφT(t−1)=(cid:27)−y(t−1)...−y(t−nA)u(t−r−1)...u(t−r−nB)(cid:28).Therelation(11.5)isthuswrittenasy(t)=φT(t−1)θ+e(t).(11.6)DEFINITION385.–Thevectorφ(t−1)(thatcontainsthedatay(τ)andu(τ−r)uptoinstantt−1)iscalledtheregressionvector.TheleastsquaresmethodconsistsofminimizingthecriterionJ(θt)=1tt(cid:12)τ=1e(τ)2.(11.7)wheret>1.LetΦ(t−1)=⎡⎢⎣φT(0)...φT(t−1)⎤⎥⎦(11.8)Y(t)=(cid:25)y(1)...y(t)(cid:26)T(11.9)E(t)=(cid:25)e(1)...e(t)(cid:26)T.ThenY(t)=Φ(t−1)θ+E(t)andJ(θt)=1tET(t)E(t)=1t(Y(t)−Φ(t−1)θ)T(Y(t)−Φ(t−1)θ).ForJ(.t)tobeminimumatpointˆθ(t)itisnecessarythattheEulercondition∂J∂θ(cid:9)ˆθ(t)t(cid:10)=0besatisﬁed(seesection12.6.4Proposition453)with∂J∂θ(θt)=−2tΦT(t−1)(Y(t)−Φ(t−1)θ).Identiﬁcation333Asaresultˆθ(t)willbeasolutionoftheso-callednormalequationΦT(t−1)Φ(t−1)θ=ΦT(t−1)Y(t).Thisequationalwaysadmitsasolution[29]butitsuniquenessisnotguaranteedunlessΦT(t−1)Φ(t−1)isinvertiblethatisrkΦ(t−1)=nA+nB.(11.10)Supposethisconditionholds.WethenhaveHJ(θt)=2tΦT(t−1)Φ(t−1)>0(whereHJistheHessianmatrixofJ:seesection12.6.2)thusJ(.t)isstrictlyconvex(section12.6.4Proposition454).AccordingtoTheorem455(section12.6.4)J(.t)thusadmitsastrictglobalminimumatpointˆθ(t)suchthatˆθ(t)=(cid:15)ΦT(t−1)Φ(t−1)(cid:16)−1ΦT(t−1)Y(t)=Φ†(t−1)Y(t)(forthesecondequalityseesection13.5.7Proposition583).Thissolutioncanbemademoreexplicitusing(11.8)and(11.9):ˆθ(t)=(cid:21)t(cid:11)τ=1φ(τ−1)φT(τ−1)(cid:22)−1t(cid:11)τ=1φ(τ−1)y(τ).(11.11)Weobtainthefollowingtheorem:THEOREM386.–Theleastsquaresproblemhasauniquesolutionifandonlyifcondition(11.10)holds(thatisifthedatacontainsufﬁcientinformation).Thissolutionisgivenby(11.11).RecursiveleastsquaresLetP(t)=*t(cid:12)τ=1φ(τ−1)φT(τ−1)+−1.Theinconvenientofsolution(11.11)isthatitisnecessarytocalculateP(t)whichistheinverseofamatrixofsize(nA+nB)×(nA+nB).WehavetherecurrenceformulaP−1(t)=P−1(t−1)+φ(t−1)φT(t−1).334LinearSystemsApplyingthe“Inversionlemma”(section13.1.4Lemma493)withA=P−1(t−1)B=φ(t−1)C=1andD=φT(t−1)weget(cid:19)P(t)=P(t−1)−K(t−1)φT(t−1)P(t−1)K(t−1)=P(t−1)φ(t−1)1+φT(t−1)P(t−1)φ(t−1)(11.12)Ontheotherhandaccordingto(11.11)ˆθ(t)=P(t)t(cid:12)τ=1φ(τ−1)y(τ).(11.13)Using(11.12)weobtainˆθ(t)=ˆθ(t−1)+K(t−1)(cid:23)y(t)−φT(t−1)ˆθ(t−1)(cid:24).(11.14)Relations(11.12)and(11.14)deﬁnetherecursiveleastsquaresalgorithmwhichdoesnotrequireanymatrixinversion.Letusstudythisfromastochasticpointofview.FormulationbasedonpredictionerrorAllrandomsignalsareassumedtoberealergodic.TheinputandoutputofthesystemarenowconsideredtobesuchsignalsandvectorθisarandomvariablewithvaluesinRn(n=nA+nB).Considerthe“stochasticversion”ofmodel(11.5)knownasthe“ARXmodel”6A(cid:15)q−1θ(cid:16)y(t)=q−rB(cid:15)q−1θ(cid:16)u(t)+w(t)(11.15)wherey(t)=φT(t−1)θ+w(t).(11.16)LetFt−1betheσ-algebrageneratedbytherandomvariablesy(t−i)u(t−i)i≥1andθ(seesection12.7.2).(Regardingtheinputonlythevaluesu(t−r−i)i≥1areactuallyinvolvedbutwedonottakethisintoaccountforthesakeofclarityinthefollowingdiscussion.)DEFINITION387.–Wecallone-stepoptimalpredictionofy(t)therandomvariablegivenbyˆy(t|t−1)=E[y(t)|Ft−1](11.17)6.ARXstandsforAutoRegressive(whichisthepartA(cid:15)q−1(cid:16)y(t)=w(t))andXfortheeXternalpartq−rB(cid:15)q−1(cid:16)u(t)(uisindeedanexternalsignal).Identiﬁcation335andpredictionerrortherandomvariableε(t)=y(t)−ˆy(t|t−1).(11.18)LetustakethefollowingtwohypothesestheﬁrstofwhichmakesmoreexplicitRemark384(section11.2.1).(H0):Theco-domainoftherandomvariableθisasubsetDofRnsuchthatforanyθ0∈DA(cid:15)z−1θ0(cid:16)(cid:5)=0forany|z|≥1andthepolynomialsA(cid:15)q−1θ0(cid:16)andB(cid:15)q−1θ0(cid:16)areco-prime.(H1):wisawhitenoisewithvarianceE(cid:23)w(t)2(cid:24)=λandissuchthatforeveryinstanttw(t)isindependentoftheσ-algebraFt−1.THEOREM388.–UnderHypotheses(H0)and(H1)theone-stepoptimalpredictionofy(t)isgivenby:ˆy(t|t−1)=φT(t−1)θ.PROOF.Letz(t)=φT(t−1)θandlety•(t)beapredictionofy(t)thatis(likez(t))anelementofL2(ΩFt−1P).WehaveE(cid:23)|y(t)−y•(t)|2(cid:24)=E(cid:23)|z(t)−y•(t)+w(t)|2(cid:24).Therandomvariablesz(t)−y•(t)andw(t)areindependentthuswehaveaccordingtoProposition470(ii)(section12.7.2)E(cid:23)|y(t)−y•(t)|2(cid:24)=E(cid:23)|z(t)−y•(t)|2(cid:24)+λ≥λ.Theminimumoftheleft-handsideisattainedwheny•(t)=z(t)asaresultz(t)=E[y(t)|Ft−1]accordingtoDeﬁnition474(section12.7.3).REMARK389.–(i)Theaboveoptimalpredictionˆy(t|t−1)canbeexpressedlinearlyasafunctionofθthusitiscalledalinearregressioninstatistics.(ii)AccordingtoTheorem388theleastsquarescriterion(11.7)isalsowrittenasJ(θt)=1tt(cid:11)τ=1(ε(τ))2.(11.19)Accordingtotheergodichypothesis(11.4)ast→+∞J(θt)→E(cid:23)(ε(τ))2(cid:24).BiasoftheestimatorLetwetakeHypothesis(H2):336LinearSystems(H2):Thereexistsauniquevalue˘θ∈Dofθsuchthatthedatayandusatisfytherelationy(t)=φT(t−1)˘θ+w(t).(11.20)REMARK390.–(i)Hypothesis(H2)meansthattheidentiﬁedsystemactuallyhastheARXstructure(11.15)andthat˘θisthe“truevalue”ofparameterθ(seesection11.2.5formoredetails).(ii)Toberigorousoneshoulddistinguishbetweenthenoise{w(t)}ofthemodel(whichisonlyﬁctitious)andthenoise{˘w(t)}oftheactualsystem(andtowritey(t)=φT(t−1)˘θ+˘w(t)inplaceof(11.20)).Butthiscomplicatesthederivationwithoutchangingtheconclusion(seeExercise428);forthesakeofsimplicitywand˘warenotdistinguishedinwhatfollows.Letˆθ(t)=argminθJ(tθ)(11.21)(assumingthatJ(t.)admitsastrictglobalminimumthatiscondition(11.10)holds).DEFINITION391.–Theestimatorthatprovidestheestimateˆθ(t)(orabusingthelanguagetheestimatorˆθ(t))issaidtobenon-biasedifE(cid:23)ˆθ(t)(cid:24)=˘θandasymptoticallynon-biasediflimt→+∞E(cid:23)ˆθ(t)(cid:24)=˘θ.THEOREM392.–UnderHypotheses(H0)to(H2)theleastsquaresestimatorisnon-biased.PROOF.Accordingto(11.13)andHypothesis(H2)wehavebyputtingΓ(t)=tP(t)ˆθ(t)=Γ(t)1tt(cid:12)τ=1φ(τ−1)(cid:9)φT(τ−1)˘θ+w(τ)(cid:10)=˘θ+Γ(t)1tt(cid:12)τ=1φ(τ−1)w(τ).(11.22)Therandomvariablesφ(τ−1)andw(τ)areindependentaccordingtoHypothesis(H1)andw(τ)iscenteredthusaccordingtoProposition470(i)(section12.7.3)E[φ(τ−1)w(τ)]=E[φ(τ−1)]E[w(τ)]=0thereforeE(cid:23)ˆθ(t)(cid:24)=˘θ.Identiﬁcation337ConsistencyoftheestimatorDEFINITION393.–TheestimatorinDeﬁnition391issaidtobeconsistentiflimt→+∞ˆθ(t)=˘θalmostsurely.(11.23)REMARK394.–Aconsistentestimatorisasymptoticallynon-biased.Letustakethetwofollowinghypotheses:(H3):Theinput{u(t)}isapersistentlyexcitingsignalofinﬁniteorder.(H4):Foranyinstantstandτthenoisew(t)isindependentoftheinputu(τ).REMARK395.–Hypothesis(H4)isrealisticinthecontextofopen-loopidentiﬁcationwhichisthesubjectofthissection.Iftheidentiﬁcationismadeinclosed-loopthefeedbackintroducesacross-correlationbetweenu(t)andthenoisew(τ)τ≤t.THEOREM396.–UnderHypotheses(H0)to(H4)theleastsquaresestimatorisconsistent.PROOF.WehaveΓ(t)−1=1tP−1(t)=1tt(cid:12)τ=1φ(τ−1)φT(τ−1)(seesection11.1.1Remark364).AsaresultifE(cid:23)φ(t−1)φT(t−1)(cid:24)isinvertiblewehaveast→+∞Γ(t)→(cid:20)E(cid:23)φ(t−1)φT(t−1)(cid:24)−1&.TheinvertibilityofE(cid:23)φ(t−1)φT(t−1)(cid:24)isensuredbyHypotheses(H3)and(H4)([110]Chapter5Complement5.2);wewillcomebacktothispointinthemoregeneralcontextofsection11.2.5.Ontheotherhand1tt(cid:12)τ=1φ(τ−1)w(τ)→E[φ(t−1)w(t)]=0(seeproofofTheorem392).Thetheoremisthusaconsequenceof(11.22).ResidueanalysisTHEOREM397.–UnderHypotheses(H0)to(H4)ifˆθ(t)=˘θthenthepredictionerror{ε(t)}isawhitenoisesuchthatforanyinstantstandτε(t)isindependentoftheinputu(τ).338LinearSystemsPROOF.Itsufﬁcestonotethatforθ=˘θε(t)=w(t).EXAMPLE398.–Letustakethecontinuous-timeminimalsystemwithtransferfunctionG(s)=s−2s2+0.2s+1.(11.24)Discretizingthissystem(withZ.O.H.)atsamplingperiodTs=0.5s(toclarifyideaswecanindeedassumethattheunitoftimechosenisthesecond)weobtainthediscrete-timesystemwithtransferfunctionGd(z)=0.2193z−0.6853z2−1.6718z+0.9048=0.2193z−1−0.6853z−21−1.6718z−1+0.9048z−2(cid:1)˘B(cid:15)z−1(cid:16)˘A(z−1).ThesysteminputisaPRBSwithamplitude1andthe“equationnoise”wisaGaussianwhitenoisewithstandarddeviation1.Theidentiﬁcationisdoneover1000points.(ThesamplingperiodTschosencanseemquitelarge–takingintoaccounttheundampedcharacteristicfrequencyω0=1rad/softhesystem–butthisorderofmagnitudeisessentialtoobtainacorrectidentiﬁcation.WithrespecttothecontrolTscanbemuchsmallerandwecanconceivetwodifferentsamplingperiods:alargeenoughonefortheidentiﬁcationanothersmalleroneforthecontrolsothatthelatterwillquicklyreacttoadisturbance.)Theinputandtheoutputovertheﬁrst200points(thustheﬁrst100seconds)areshowninFigure11.2(–)7aswellasthesimulatedoutputbasedontheidentiﬁedmodelandintheabsenceofnoise(--);thismakesitpossibletogetaﬁrstappreciationofthequalityoftheidentiﬁcationandalsoofthenoiselevel.ResidueanalysisisshowninFigure11.3.8Accordingtosomestatisticcriteriathatwearenotgoingtodetailherenormalizedcorrelationsequencer(cid:2)εεandnormalizedcross-correlationsequencer(cid:2)εucanbeconsideredtobezero(exceptr(cid:2)εε(0))iftheystayinsidetheconﬁdenceintervalsrepresentedbydarkerareaswhichisthecasehere.ThetransferfunctionˆG(s)ofthecontinuous-timeidentiﬁedsystemisobtainedbyinvertingformula(10.13)ofsection10.3.3basedonthetransferfunctionofthediscrete-timeidentiﬁedsystem(inpracticethisinversionisdonethroughastate-spacerealizationbyinvertingrelation(10.20)ofsection10.3.4).InthiswayweobtainˆG(s)=1.163s−1.952s2+0.2161s+0.962whichisverycloseto(11.24).Thestepresponsesoftheactualsystem(–)andoftheidentiﬁedsystem(--)areshowninFigure11.4.Thecurvesarealmostmerge.EXAMPLE399.–ThisexampleisthesameasExample398exceptinthewaythatthewhitenoisew(t)affectsthesystem:hereitisnolongeran“equationnoise”asin7.Figures11.2to11.7areattheendofsection11.2.8.Where“lag”standsfortheargumentτofryu(τ).Identiﬁcation339relation(11.15)butameasurementnoise.Thismeansthatthesystemsatisﬁes(cid:20)y(t)=y0(t)+w(t)˘A(cid:15)q−1(cid:16)y0(t)=q−r˘B(cid:15)q−1(cid:16)u(t).(11.25)Thisistheonlychangecomparedwiththepreviouscase.Notethatthissituationismorerealistic.Theinputandoutputfortheﬁrst200pointsareshowninFigure11.5(–)aswellastheoutputsimulatedbasedontheidentiﬁedmodelandintheabsenceofnoise(--).AccordingtotheresidueanalysisshowninFigure11.6theidentiﬁcationisaprioripoor.Thetransferfunctionofthecontinuous-timeidentiﬁedsystemisindeedˆG(s)=6.099s−9.491s2+4.354s+4.001andthusisquitedifferentfrom(11.24).Thestepresponsesoftheactualsystem(–)andoftheidentiﬁedsystem(--)showninFigure11.7conﬁrmthepoorqualityoftheidentiﬁcation.BiasanalysisTHEOREM400.–Ifthenoisewisameasurementnoisetheleastsquaresestimatorisingeneralbiased.PROOF.LetusfollowagaintheapproachusedintheproofofTheorem392.Recallﬁrstthataccordingto(11.25)˘A(cid:15)q−1(cid:16)y(t)=˘B(cid:15)q−1(cid:16)u(t−r)+˘A(cid:15)q−1(cid:16)w(t).(11.26)Polynomial˘A(cid:15)q−1(cid:16)isoftheform˘A(cid:15)q−1(cid:16)=1+q−1˘A•(cid:15)q−1θ(cid:16).Asaresultif˘θisthetruevalueoftheparameterwehavefrom(11.13)writingv(t)=A•(cid:15)q−1θ(cid:16)w(t)E(cid:23)ˆθ(t)(cid:24)=˘θ+Γ(t)1tt(cid:12)τ=1E(cid:23)φT(τ−1)v(τ−1)(cid:24)thusE(cid:23)ˆθ(t)(cid:24)(cid:5)=˘θbecause(exceptforaspecialcase)thecomponentsofφ(τ−1)arecorrelatedwithv(τ−1).Theorem400–aswellasExample11.15–showsthattheleastsquaresmethodbasedonanARXmodeldoesnotmakeitpossibletocorrectlyresolveallidentiﬁcationproblems.Forthisreasonweshallstudymorecomplexmethodsinthefollowingsections.340LinearSystems11.2.3.ModelsandpredictionConsidertheidentiﬁcationproblemofasysteminthepresenceofmeasurementnoise(seeExample399).Assumingthatthismeasurementnoiseiswhitetheinputu(t)theoutputy(t)andthewhitenoisew(t)satisfytherelation(11.26).Thisexpressionhasastructurethatcanbeputintoseveraldifferentforms.ModelsARMAXmodel9ThismodelisA(cid:15)q−1θ(cid:16)y(t)=q−rB(cid:15)q−1θ(cid:16)u(t)+C(cid:15)q−1θ(cid:16)w(t).(11.27)AddingtheconstraintA(cid:15)q−1θ(cid:16)=C(cid:15)q−1θ(cid:16)(11.28)equation(11.26)hasthisstructure.IngeneralwecanassumethatpolynomialC(cid:15)z−1θ(cid:16)ofanARMAXmodelissuchthatC(cid:15)z−1θ(cid:16)(cid:5)=0for|z|≥1accordingtothespectralfactorizationtheorem(section11.1.4Theorem383).OutputerrormodelTheOE(OutputError)modelisy(t)=q−rB(q−1θ)F(q−1θ)u(t)+w(t).(11.29)ThisisthestructurethatcorrespondstotheprobleminExample11.15(withA(cid:15)q−1θ(cid:16)replacedbyF(cid:15)q−1θ(cid:16)).BoxandJenkinsmodelWecanconsiderthemoregeneralcasewherethemeasurementnoisenisacolorednoise.10Accordingtothespectralfactorizationtheoremncanbeassumedtobegeneratedbythemodeln(t)=C(cid:15)q−1θ(cid:16)D(q−1θ)w(t)9.ARMAstandsforAutoRegresiveMovingAverageandXisfromeXternal.10.DonotconfusethenoisenwiththeintegerninHypothesis(H0).Identiﬁcation341wherethetransferfunctionH(cid:15)q−1θ(cid:16)=C(cid:15)q−1θ(cid:16)D(q−1θ)isbicausalandbistableandsuchthatH(0θ)=1.WethusobtaintheBoxandJenkinsmodel(BJ)y(t)=q−rB(q−1θ)F(q−1θ)u(t)+C(q−1θ)D(q−1θ)w(t).(11.30)PredictionerrormodelWegatherallmodelsobtainedaboveintothegeneralstructureA(cid:15)q−1(cid:16)y(t)=q−rB(q−1θ)F(q−1θ)u(t)+C(q−1θ)D(q−1θ)w(t)(11.31)calledthepredictionerrormodel(PEM).Wearenowgoingtoseewhy.One-stepoptimalpredictionGeneralsystem(11.31)(inwhichpolynomialsA(cid:15)q−1θ(cid:16)B(cid:15)q−1θ(cid:16)C(cid:15)q−1θ(cid:16)D(cid:15)q−1θ(cid:16)andF(cid:15)q−1θ(cid:16)appearexplicitly;thecoefﬁcientsofthesepolynomialsarethecomponentsoftheparameter-vectorθtobeidentiﬁed)canbeputinthemoreconciseformy(t)=G(cid:15)q−1θ(cid:16)u(t)+H(cid:15)q−1θ(cid:16)w(t).(11.32)ThefollowinghypothesiswhichgeneralizesHypothesis(H0)(section11.2.3)isinforce;nisthenumberofcomponentsofθ.(H0’):TherandomvariableθtakesitsvaluesinasubsetDofRnsuchthatforanyθ0∈DG(cid:15)q−1θ0(cid:16)isthetransferfunctionofastableandstrictlycausalﬁlterH(cid:15)q−1θ0(cid:16)isthetransferfunctionofabistableandbicausalﬁlterandH(0θ0)=1(seesection11.1.4Theorem383andsection11.2.1);GandHarerationalfunctionswithrespecttotheindeterminateq−1andthecomponentsofθ0.REMARK401.–MoreconciselyG(cid:15)q−1θ(cid:16)andH(cid:15)q−1θ(cid:16)canbedenotedinwhatfollowsasG(cid:15)q−1(cid:16)andH(cid:15)q−1(cid:16)respectivelywhenthisisnotambiguous.LetFt−1betheσ-algebrageneratedbytherandomvariablesy(t−i)u(t−i)i≥1andθ.Theone-stepoptimalpredictionˆy(t|t−1)andthepredictionerrorε(t)aredeﬁnedasinDeﬁnition387(section11.2.2).342LinearSystemsTHEOREM402.–UnderHypothesis(H0’)andHypothesis(H1)ofsection11.2.2theone-stepoptimalpredictionofy(t)isgivenbyˆy(t|t−1)=(cid:23)1−1H(q−1)(cid:24)y(t)+G(q−1)H(q−1)u(t).(11.33)PROOF.Letz(t)betheleft-handsideof(11.33)andy•(t)beapredictionofy(t)i.e.(likez(t))anelementofL2(ΩFt−1P).Wehavey(t)−y•(t)=z(t)−y•(t)+w(t)thuswecanconcludeasintheproofofTheorem388.11.2.4.OutputErrormethodandARMAXmethodTheidentiﬁcationmethodnowconsistsofdeterminingˆθ(t)deﬁnedby(11.21)thusofminimizingcriterion(11.19)(seesection11.2.2Remark389).ThisminimizationisusuallycarriedoutusingtheLevenberg–Marquardtalgorithm(seesection12.6.5)whichrequirestheknowledgeofthegradientofthecriterionwithrespecttotheparameterstobeidentiﬁed.Letθibetheithcomponentofθ;wehave∂J∂θi(tθ)=2tt(cid:12)τ=1ε(τ)∂ε∂θi(τ).Whatneedstobedonenextistoevaluatethepartialderivatives∂ε∂θi.ToillustratethispointwewillcalculatethesepartialderivativesfortheOutputErrormodelandfortheARMAXmodel(seealsoExercise422).CaseofanOutputErrormodelWehaveG(cid:15)q−1(cid:16)=q−rB(cid:15)q−1(cid:16)F(q−1)H(cid:15)q−1(cid:16)=1.Asaresultaccordingto(11.33)ε(t)=y(t)−q−rB(cid:15)q−1(cid:16)F(q−1)u(t).Therefore∂ε∂bi(t)=−q−r−iF(q−1)u(t)∂ε∂fi(t)=q−r−iB(cid:15)q−1(cid:16)F(q−1)2u(t).Identiﬁcation343CaseofanARMAXmodelWehaveG(cid:15)q−1(cid:16)=q−rB(cid:15)q−1(cid:16)A(q−1)H(cid:15)q−1(cid:16)=C(cid:15)q−1(cid:16)A(q−1)andthereforefrom(11.33)ε(t)=A(cid:15)q−1(cid:16)C(q−1)y(t)−q−rB(cid:15)q−1(cid:16)C(q−1)u(t).Wededucethat∂ε∂ai(t)=q−iC(q−1)y(t)∂ε∂bi(t)=−q−r−iC(q−1)u(t)∂ε∂ci(t)=−q−iC(q−1)ε(t).EXAMPLE403.–ThisexampleisidenticaltoExample399(section11.2.2)butthistimethesystemisidentiﬁedusingtheOutputErrormethod.Theinputandtheoutputfortheﬁrst200pointsareshowninFigure11.8(–)aswellasthesimulatedoutputbasedontheidentiﬁedmodelintheabsenceofanoise(--).Thelatterclearlybetter“follows”theactualoutputthaninFigure11.5.TheresidueanalysisshowninFigure11.9isgood.Forveriﬁcationthetransferfunctionoftheidentiﬁedcontinuous-timesystemisˆG(s)=0.9465s−1.991s2+0.1933s+0.9941andisthusverycloseto(11.24).Thestepresponsesoftheactualsystem(–)andoftheidentiﬁedsystem(--)showninFigure11.10conﬁrmthegoodqualityoftheidentiﬁcation.The(veryimportant)problemofidentiﬁcationinthepresenceofawhitemeasurementnoiseisthuscorrectlyresolvedbytheOutputErrormethod.EXAMPLE404.–ThisexampleisidenticaltoExample403butthesystemisidentiﬁedusingtheARMAXmethod.Theinputandoutputfortheﬁrst200pointsareshowninFigure11.11(–)aswellasthesimulatedoutputbasedontheidentiﬁedmodelintheabsenceofanoise(--).TheresidueanalysisshowninFigure11.12iscorrect.Thepolynomialsoftheidentiﬁeddiscretizedsystemarethefollowing:ˆA(cid:15)q−1(cid:16)=1−1.6750q−1+0.9052q−2ˆB(cid:15)q−1(cid:16)=0.1907q−1−0.6495q−2ˆC(cid:15)q−1(cid:16)=1−1.6886q−1+0.9237q−2.344LinearSystemsThefactthatˆC(cid:15)q−1(cid:16)isveryclosetoˆA(cid:15)q−1(cid:16)makesitpossibletoconcludethatthenoiseisdeﬁnitelyameasurementnoise(accordingtorelation(11.28)).Thestepresponsesoftheactualsystem(–)andoftheidentiﬁedsystem(--)showninFigure11.13conﬁrmtheexcellentqualityoftheidentiﬁcation.TheproblemofidentiﬁcationinthepresenceofawhitemeasurementnoiseisthusalsoresolvedbytheARMAXmethod.EXAMPLE405.–ThedifferencebetweenthisexampleandExample403isthatthistimethemeasurementnoiseisverymuchcolored.ItisrepresentedinFigure11.14overtheﬁrst100points.TheidentiﬁcationiscarriedoutusingtheOutputErrormethod.Theinputandtheoutputovertheﬁrst200pointsareshowninFigure11.15(–)aswellasthesimulatedoutputbasedontheidentiﬁedmodelintheabsenceaofnoise(--).TheresidueanalysisshowninFigure11.16demonstratesthattheOutputErrormodeldoesnotcorrespondtothecasetreated.Indeedthepredictionerrorwasnotwhitenedattheendoftheoptimization.Neverthelessthestepresponsesoftheactualsystem(–)andtheidentiﬁedsystem(--)inFigure11.17showthatthesystemwascorrectlyidentiﬁed.Analysisofthisphenomenonisthesubjectofthefollowingsection.11.2.5.ConsistencyoftheestimatorandresiduesConsistencyConsiderthegeneralmodel(11.32)andsupposethattheactualsystemisgovernedbytheequationy(t)=˘G(cid:15)q−1(cid:16)u(t)+˘H(cid:15)q−1(cid:16)w(t)(11.34)wheretheﬁlter˘G(cid:15)q−1(cid:16)isstableandstrictlycausalandtheﬁlter˘H(cid:15)q−1(cid:16)isbicausalbistableandsuchthat˘H(0)=1.Hypothesis(H2)(section11.2.2)isahypothesisofidentiﬁabilityaccordingtothefollowing:DEFINITION406.–Thesystemissaidtobeidentiﬁableifthereexistsauniquevalue˘θ∈DofθsuchthatG(cid:9)q−1˘θ(cid:10)=˘G(cid:15)q−1(cid:16)andH(cid:9)q−1˘θ(cid:10)=˘H(cid:15)q−1(cid:16).11Supposethatnowmodel(11.32)canbewrittenasy(t)=G(cid:15)q−1θs(cid:16)u(t)+H(cid:15)q−1θν(cid:16)w(t)(11.35)11.Thereareseveralnotionsofidentiﬁabilityintheliterature.Thenotionpresentedhereisavariantoftheso-calledstructuralidentiﬁability.Anotherconceptofidentiﬁabilityispresentedin[110].Identiﬁcation345sothattheparameterθtobedeterminedisseparatedintotwoindependentpartsθsandθνtheﬁrstpartcharacterizingonlythedeterministicpartofthesystemtobeidentiﬁedandthesecondpartcharacterizingonlythestochasticpart.Wehaveθ=(θsθν).(11.36)Letˆθ(t)=(cid:9)ˆθs(t)ˆθν(t)(cid:10)whereˆθ(t)isdeﬁnedby(11.21)(seesection11.2.2).DEFINITION407.–Thedeterministicpartofthesystemissaidtobeidentiﬁableifthereexistsauniquevalue˘θsofθssuchthatG(cid:9)q−1˘θs(cid:10)=˘G(cid:15)q−1(cid:16).THEOREM408.–LetHypothesis(H0’)(section11.2.3)beinforceaswellasHypotheses(H1)(H3)and(H4)(section11.2.2).(i)Ifthedeterministicpartofthesystemisidentiﬁablethentheestimatorofthispartisconsistenti.e.limt→+∞ˆθs(t)=˘θsalmostsurely.(ii)Ifthewholesystemisidentiﬁableandifthevarianceλ=σ2ofthewhitenoisewisnon-zerothentheestimatorofthesystemisconsistenti.e.limt→+∞ˆθ(t)=˘θalmostsurely.PROOF.Theone-stepoptimalpredictioncanbewrittenasˆy(t|t−1)=(cid:27)1−1H(q−1θν)(cid:28)y(t)+G(cid:15)q−1θs(cid:16)H(q−1θν)u(t)whereastheactualsystemisdeﬁnedby(11.34).Thepredictionerrorε(t)=y(t)−ˆy(t|t−1)thussatisﬁesε(t)=∆G(cid:15)q−1θs(cid:16)H(q−1θν)u(t)+˘H(cid:15)q−1(cid:16)H(q−1θν)w(t)(11.37)where∆G(cid:15)q−1θs(cid:16)=˘G(cid:15)q−1(cid:16)−G(cid:15)q−1θs(cid:16).Ast→+∞J(tθ)→E(cid:23)ε(t)2(cid:24)(seesection11.2.2Remark389)andaccordingtoHypothesis(H4)whichimpliesthattheright-handsideof(11.37)isthesumoftwoindependentrandomvariableswehaveE(cid:23)ε(t)2(cid:24)=E⎡⎣*∆G(cid:15)q−1θs(cid:16)H(q−1θν)u(t)+2⎤⎦+E⎡⎣*˘H(cid:15)q−1(cid:16)H(q−1θν)w(t)+2⎤⎦.346LinearSystemsLetQ1betheﬁrstquantityontheright-handsideoftheaboveequalityandQ2thebesecondquantity.WehaveE(cid:23)ε(t)2(cid:24)=rεε(0)=12π(cid:2)π−πϕεε(ω)dω.ThereforeaccordingtotheInterferenceformula(seesection11.1.4Theorem379)Q1=12π(cid:2)π−π)))))∆G(cid:15)e−iωθs(cid:16)H(e−iωθν))))))2ϕuu(ω)dωQ2=12π(cid:2)π−π)))))˘H(cid:15)e−iω(cid:16)H(e−iωθν))))))2ϕww(ω)dωwhereϕuu(ω)>0foraninﬁnitenumberofdistinctangularfrequenciesω∈[−ππ]accordingtoHypothesis(H3)andϕww(ω)=λ.(i)Supposethatthedeterministicpartisidentiﬁable.AccordingtoHypothesis(H3)Q1isminimum(andequalto0)ifandonlyif∆G(cid:15)e−iωθs(cid:16)=0foraninﬁnitenumberofdistinctfrequenciesω∈[−ππ]thatisθs=˘θs;indeedanon-zerorationalfunctioncanonlyhaveaﬁnitenumberofzeros.(ii)Assumethatthepreviousconditionissatisﬁedandλ>0.WethenhaveE(cid:23)ε(t)2(cid:24)=λ2π(cid:2)π−π)))))˘H(cid:15)e−iω(cid:16)H(e−iωθν))))))2dω.Let˜H(cid:15)q−1θν(cid:16)=˘H(cid:15)q−1(cid:16)H(q−1θν).Thisisthetransferfunctionofabicausalbistableﬁltersuchthat˜H(0θν)=1;itcanbewrittenas˜H(cid:15)q−1θν(cid:16)=1++∞(cid:12)τ=1˜h(τ)q−τ.AccordingtotheParsevalequality(12.45)ofsection12.3.3E(cid:23)ε(t)2(cid:24)=λ*1++∞(cid:12)τ=1˜h(τ)2+.Sinceλ>0E(cid:23)ε(t)2(cid:24)isminimumifandonlyifthel2normof(cid:9)˜h(τ)(cid:10)isalsominimum.Nowbyassumingthatthesystemisentirelyidentiﬁablethe˜h(τ)τ≥1areallzeroifandonlyifH(cid:15)q−1θν(cid:16)=˘H(cid:15)q−1(cid:16)thatistosayθ=˘θ(takingintoaccountthefactthatθs=˘θs).Identiﬁcation347REMARK409.–ThestatementofTheorem408(ii)remainsvalidifwedonothavetheseparation(11.36)ofθintotwopartsθsandθν(asthereadercancheckbyslightlymodifyingtheproofofthistheorem)andExample404isanillustrationofthisremark.ResidueanalysisThepredictionerrorhastheimportantpropertystatedinthetheorembelow(whichgeneralizesTheorem397).THEOREM410.–UnderthehypothesesofTheorem408ifthesystemisidentiﬁableandiftheparameterθhasitstruevalue˘θthenthepredictionerror{ε(t)}isawhitenoisesuchthatforanyinstantstandτε(t)isindependentoftheinputu(τ).PROOF.Wehaveε(t)=w(t)andsothetheoremisaconsequenceofHypothesis(H4).11.2.6.FilteringofdataItisrareforthe“deterministicpart”ofthesystemasdeﬁnedinsection11.2.5tobeidentiﬁableintheabsolute.Indeedthissubsystem(whosemodelise.g.todesignacontrollaw)isinfactverycomplex(seesection4.2.1).Thereforeitmakessensetoidentifyonlythe“usefulpart”ofthissubsysteminaboundedintervaloffrequencies(thatconsistsoffrequenciesatwhichthecontrolhasenergy).Let[ωminωmax]besuchaninterval.Ifwerestrictthebehaviorofthedeterministicpartto[ωminωmax]thentheabove-mentioned“usefulpart”becomesidentiﬁableifboththisintervalandthemodelstructurearecorrectlychosen.InpracticewethereforeﬁlterthedatabyabandpassﬁlterwithtransferfunctionHbp(cid:15)z−1(cid:16)assumedtobebicausalandbistableforreasonsdiscussedbelow.Inotherwordsthedataprovidedtothealgorithmareinsteadoftheinput/outputu(t)andy(t)theﬁlteredinput/outputuf(t)=Hbp(cid:15)q−1(cid:16)u(t)yf(t)=Hbp(cid:15)q−1(cid:16)y(t).SupposethetransferfunctionoftheentiredeterministicpartisG(cid:15)q−1(cid:16)=Gu(cid:15)q−1(cid:16)+Gn(cid:15)q−1(cid:16)whereGu(cid:15)q−1(cid:16)isthe“usefulpart”andGn(cid:15)q−1(cid:16)istheparttobeneglected.ThebandpassﬁlterischoseninawaythatHbp(cid:15)e−iω(cid:16)Gu(cid:15)e−iω(cid:16)(cid:9)Gu(cid:15)e−iω(cid:16)Hbp(cid:15)e−iω(cid:16)Gn(cid:15)e−iω(cid:16)(cid:9)0.348LinearSystemsInabsenceofnoisetheinputsandoutputssatisfytherelationy(t)=(cid:25)Gu(cid:15)q−1(cid:16)+Gn(cid:15)q−1(cid:16)(cid:26)u(t).MultiplyingthetwosidesofthisequalitybyHbp(cid:15)q−1(cid:16)weobtaintherelationsatisﬁedbytheﬁlteredinputsandoutputsyf(t)=(cid:25)Gu(cid:15)q−1(cid:16)+Gn(cid:15)q−1(cid:16)(cid:26)uf(t)whichyieldsyf(t)(cid:9)Gu(cid:15)q−1(cid:16)uf(t).Letusnowexaminehowthisisformalizedinthepresenceofanoise.Considerthegeneralmodel(11.32)ofsection11.2.3andletH(cid:15)q−1θ(cid:16)=Hi(q−1θ)Hbp(q−1)(11.38)whereHi(cid:15)q−1θ(cid:16)isthatpartofthestochastictermwhichistobeidentiﬁed;inH(cid:15)q−1θ(cid:16)thispartismultipliedby1/Hbp(cid:15)q−1(cid:16)whichisthetransferfunctionofabandstopﬁlter.AccordingtoTheorem402thepredictionerrorisgivenbyε(t)=1Hi(q−1θ)yf(t)−G(cid:15)q−1θ(cid:16)Hi(q−1θ)uf(t).Asaresulttheheuristicmethodthatconsistsofsupplyingtheﬁltereddatatothealgorithmcanbeinterpretedasaparticularchoiceofthenoisemodel(accordingtorelation(11.38)).Figure11.2.Simulateddataandoutput–Example398Identiﬁcation349Figure11.3.Residueanalysis–Example398Figure11.4.Stepresponses–Example398350LinearSystemsFigure11.5.Simulateddataandoutput–Example399Figure11.6.Residueanalysis–Example399Identiﬁcation351Figure11.7.Stepresponses–Example399Figure11.8.Simulateddataandoutput–Example403352LinearSystemsFigure11.9.Residueanalysis–Example403Figure11.10.Stepresponses–Example403Identiﬁcation353Figure11.11.Simulateddataandoutput–Example404Figure11.12.Residueanalysis–Example404354LinearSystemsFigure11.13.Stepresponses–Example404Figure11.14.Measurementnoise–Example405Identiﬁcation355Figure11.15.Simulateddataandoutput–Example405Figure11.16.Residueanalysis–Example405356LinearSystemsFigure11.17.Stepresponses–Example40511.3.Closed-loopidentiﬁcation11.3.1.DirectandindirectapproachConsiderasystemwithequation(11.34)wheretheﬁlter˘G(cid:15)q−1(cid:16)isstrictlycausal(butmaybeunstable)andtheﬁlter˘H(cid:15)q−1(cid:16)isbicausalbistableandsuchthat˘H(0)=1.Thecasewherethevarianceλofthenoiseiszeroistrivialthereforewewillassumeλ>0inwhatfollows.Inthepresentsectionwewilldiscusstheproblemofidentiﬁcationofthesystemwhenitisfedbackbyacausalregulatoraccordingtotheequationu(t)=r(t)−K(cid:15)q−1(cid:16)y(t);(11.39)thefeedbacksystemisassumedtobestableandthereferencesignal{r(t)}satisﬁesHypothesis(H5)below:(H5):Foranyinstantstandτr(t)isindependentofthenoisew(τ).Hypothesis(H0’)ofsection11.2.3isreplacedbythehypothesis(H0”)hereafter(wherendenotesthenumberofcomponentsofθ).(H0”):TherandomvariableθtakesitsvaluesinasubsetDofRnsuchthatforanyθ0∈DG(cid:15)q−1θ0(cid:16)isthetransferfunctionastrictlycausalﬁlterstabilizedbythecontroller(11.39);H(cid:15)q−1θ0(cid:16)isthetransferfunctionofabistableandbicausalﬁltersuchthatH(0θ0)=1;GandHarerationalfunctionswithrespecttoq−1andtothecomponentsofθ0.Hypothesis(H1)ofsection11.2.2whichisstillrealisticisinforce.Identiﬁcation357Letusnowlookatwhatdifﬁcultyarisesintheidentiﬁcationprocessduetothefeedbackloop.Sincey(t)iscorrelatedwiththew(τ)τ≤t(duetoequation(11.32))relation(11.39)introducesacross-correlationbetweenu(t)andthew(τ)τ≤t(seesection11.2.2Remark395).ThereforeHypothesis(H4)ofsection11.2.2isnotsatisﬁedwhichinvalidatestheproofofTheorem408(section11.2.5).Itisthereforenecessarytoreconsidertheproblemoftheconsistencyoftheestimator.DirectapproachThedirectapproachofclosed-loopidentiﬁcationconsistsinidentifyingthesystemusingtheinputandoutputsignals{u(t)}and{y(t)}whilethefeedbackloopisignored.Besidesthisisunavoidablewhenthecontrollerisunknown.Theanalysisofthedirectapproachiscarriedoutinsection11.3.2.IndirectapproachTheindirectapproachwhichisonlypossiblewhenthecontrollerisknownconsistsoftwosteps.1)Theﬁrststepconsistsinidentifyingthefeedbacksystem.Todothisvariousmethodsareofcoursepossibleinparticulartheonethatconsistsofminimizingthel2normofthepredictionerror.Forthefollowingdiscussionwewillassumethatthisisthemethodbeingused;themodelofthefeedbacksystemdeducedfrom(11.32)and(11.39)canthenbewrittenasy(t)=Gc(cid:15)q−1θ(cid:16)r(t)+Hc1(cid:15)q−1θ(cid:16)w(t)(11.40)whereGc=G1+GKHc1=H1+GK.Letθ0∈D;wecancarryoutthebicausalspectralfactorizationofthesignalHc1(cid:15)q−1θ0(cid:16)w(t)(seesection11.1.4Theorem383)ifthissignalispersistentlyexcitingthatisifλ(cid:1)ϕww>0andHypothesis(H6)belowisinforce:(H6):Foranyθ0∈DtheﬁltersG(cid:15)q−1θ0(cid:16)andK(cid:15)q−1(cid:16)havenopolesontheunitcircle.Underthishypothesismodel(11.40)canbeputintheformy(t)=Gc(cid:15)q−1θ(cid:16)r(t)+Hc(cid:15)q−1θ(cid:16)w(cid:2)(t)(11.41)whereaccordingtoHypothesis(H0”)foranyθ0∈DGc(cid:15)q−1θ0(cid:16)isthetransferfunctionofastableandstrictlycausalﬁlterHc(cid:15)q−1θ0(cid:16)isthetransferfunctionofa358LinearSystemsbicausalbistableﬁltersuchthatHc(0θ0)=1andw(cid:2)isawhitenoisewithvariancesameasw;wandw(cid:2)arenotdistinguishedinthesequel.Inotherwordsthemodel(11.41)satisﬁesHypothesis(H0’)(section11.2.3).AccordingtoHypothesis(H1)mentionedabovewecandeterminetheoptimalone-steppredictionˆy(t|t−1)basedonmodel(11.41);thispredictionisgiven(accordingtoTheorem402section11.2.3)byˆy(t|t−1)=(cid:27)1−1Hc(q−1)(cid:28)y(t)+Gc(cid:15)q−1(cid:16)Hc(q−1)r(t).(11.42)Hypothesis(H5)meansthatHypothesis(H4)isalsoinforceformodel(11.41)(mutatismutandis).Let˘Gc=˘G1+˘GK˘Hc1=˘H1+˘GKandassumingthat˘GandKdonothavepolesontheunitcirclelet˘Hc(cid:15)q−1(cid:16)beabistablebicausalﬁltersuchthat˘Hc(cid:15)q−1(cid:16)˘Hc(q)=˘Hc1(cid:15)q−1(cid:16)˘Hc1(q)and˘Hc(0)=1.WehaveGc=˘GcandHc=˘HcifandonlyifG=˘GandH=˘H;consequently:LEMMA411.–Thefeedbacksystemisidentiﬁablewithmodel(11.41)ifandonlyiftheopen-loopsystemisidentiﬁablewithmodel(11.32).NowletusconsiderHypothesis(H3’)below:(H3’):Thesignal{r(t)}persistentlyexcitingofinﬁniteorder.REMARK412.–(i)Thesysteminputuisgivenbyu(t)=11+˘G(q−1)K(q−1)r(t)−K(cid:15)q−1(cid:16)˘H(cid:15)q−1(cid:16)1+˘G(q−1)K(q−1)w(t).(11.43)Letthesensitivityfunctionbe˘So(cid:15)z−1(cid:16)=11+˘G(z−1)K(z−1).WehaveaccordingtoHypothesis(H5)ϕuu=)))˘So)))2ϕrr+)))K˘H˘So)))2λ(11.44)Identiﬁcation359(wherethetransferfunctionsareevaluatedontheunitcircle).ThereforeifHypothesis(H3’)isinforceorifK(cid:5)=0{u(t)}isapersistentlyexcitingsignalofinﬁniteorder.(ii)Accordingto(11.43)thetransferfunctionbetweenwanduisG1=−K˘H/(cid:9)1+˘GK(cid:10).ApplyingtheInterferenceformula(11.1)(section11.1.4)withx1=x2=wandG2=1wegetRuw(z)=G1(cid:15)z−1(cid:16)λ;thusaccordingtoProposition377(section11.1.3)ϕuwϕwu=)))K˘H˘So)))2λ2.(11.45)Letˆθ(t)betheestimatordeﬁnedby(11.21)whereJisdeﬁnedby(11.19)and(11.18)andˆy(t|t−1)isgivenby(11.42).TheresultbelowfollowsimmediatelyfromtheaboveandfromTheorem408:COROLLARY413.–SupposeHypotheses(H0”)(H1)(H3’)(H5)and(H6)areinforce.Iftheopen-loopsystem(11.34)isidentiﬁablewithmodel(11.32)thentheestimatorˆθ(t)ofthefeedbacksystem(designedbasedonmodel(11.41))isconsistent.REMARK414.–Supposemodel(11.32)canbeputintheform(11.35).ThenGcdependsonlyonθs(i.e.onthatpartoftheparameterθwhichonlycharacterizesthedeterministicpartoftheopen-loopsystem)andthereforecanbewrittenasGc(cid:15)q−1θs(cid:16).OntheotherhandeventhoughHonlydependsonθν(i.e.thatpartoftheparameterθthatonlycharacterizesthestochasticpartoftheopen-loopsystem)Hcdependsonbothθνandθsthusentirelyonparameterθ.AsaresultifwereplacethehypothesisofsystemidentiﬁabilityinthestatementofCorollary413bythehypothesisofidentiﬁabilityofthedeterministicpartoftheopen-loopsystem(withtheotherhypothesesremainingunchanged)wecannotguaranteetheconsistencyoftheestimatorofthispart(seetheproofofTheorem408).Butthehypothesisoftotalsystemidentiﬁability(includingthestochasticpart)issometimestoostrongthuswewillrevisitthissubjectlater(seesection11.3.3).2)ThesecondstepconsistsofdeterminingusingtheestimateˆGc(cid:15)q−1(cid:16)oftheactualclosed-looptransferfunction˘Gc(cid:15)q−1(cid:16)anestimateˆG(cid:15)q−1(cid:16)oftheactualopen-looptransferfunction˘G(cid:15)q−1(cid:16).Wehavetheequality˘G=˘Gc1−˘GcK.AsaresultˆG=ˆGc1−ˆGcK(11.46)360LinearSystemsisanestimateof˘G(cid:15)q−1(cid:16)anditisthisonethatisusedintheclassicindirectapproach.IftheestimateˆGc(cid:15)q−1(cid:16)of˘Gc(cid:15)q−1(cid:16)wereperfect(thatisˆGc(cid:15)q−1(cid:16)=˘Gc(cid:15)q−1(cid:16))(11.46)wouldcertainlybeaperfectestimateof˘G(cid:15)q−1(cid:16).Howeverinpractice(11.46)mightwellbeapoorestimateof˘G(cid:15)q−1(cid:16)duetothefactthattheestimateˆGc(cid:15)q−1(cid:16)of˘Gc(cid:15)q−1(cid:16)isinevitablyspoiltbyerrors.ThusˆGc(cid:15)q−1(cid:16)mayhavetoohighanordergenerateinstabilitiesnotcorrespondtoanythingrealetc.11.3.2.ConsistencyofestimatorinthedirectapproachIdentiﬁcationoftheglobalsystemInthedirectapproachinastrictsenseaswasalreadybeenmentionedthecontrollerisnotsupposedtobeknown.UnderHypotheses(H0”)and(H1)theone-stepoptimalpredictionˆy(t|t−1)andthepredictionerroraregivenby(11.33)and(11.37)respectively.Thislastexpressioncanbewrittenintheformε(t)=1H(q−1θ)z(t)+w(t)z(t)=∆G(cid:15)q−1θ(cid:16)u(t)−∆H(cid:15)q−1θ(cid:16)w(t)(11.47)where∆G(cid:15)q−1θ(cid:16)=˘G(cid:15)q−1(cid:16)−G(cid:15)q−1θ(cid:16)∆H(cid:15)q−1θ(cid:16)=˘H(cid:15)q−1(cid:16)−G(cid:15)q−1θ(cid:16).Letˆθ(t)betheestimatordeﬁnedby(11.21)whereJisdeﬁnedby(11.19)and(11.18).LEMMA415.–SupposeHypotheses(H0”)and(H1)areinforce;iftheintegralinexpression(11.48)belowadmitsaminimumthenˆθ(t)→θ•ast→+∞wherewith∆G=∆G(cid:15)e−iωθ(cid:16)∆H=∆H(cid:15)e−iωθ(cid:16)(.)∗=conjugateof(.)θ•∈argminθ∈D12π(cid:2)π−π1|H(e−iωθ)|2ϕzz(ω)dω(11.48)ϕzz(ω)=(cid:25)∆G−∆H(cid:26)ϕςς(ω)(cid:27)∆G∗−∆H∗(cid:28)(11.49)ϕςς(ω)=(cid:27)ϕuu(ω)ϕuw(ω)ϕwu(ω)λ(cid:28).(11.50)PROOF.AccordingtoHypothesis(H1)therandomvariablesz(t)andw(t)areindependentthusE(cid:23)ε(t)2(cid:24)=1|H|2E(cid:23)z(t)2(cid:24)+λfromwhichwederive(11.48)ifaminimumsuchasthatmentionedexists(seesection12.6.4);forthisitissufﬁcientIdentiﬁcation361thatsystem(11.34)isidentiﬁablebasedonthemodel(11.32)).Theexpressionofϕzz(ω)isdeducedfrom(11.47).Ifsystem(11.34)isidentiﬁablebasedonthemodel(11.32)Lemma415showsthatitsestimatorˆθ(t)givenby(11.21)isconsistentifϕςς(ω)>0foraninﬁnitenumberofvaluesofωin[−ππ].Letϕruu(cid:1))))˘So)))2ϕrrϕwuu(cid:1))))K˘H˘So)))2λ(11.51)whereaccordingto(11.44)ϕuu=ϕruu+ϕwuu;ϕruu(resp.ϕwuu)isthecontributionofsignalr(resp.ofnoisew)tothespectraldensityϕuuofu.From(11.45)ϕwuu=ϕuw(ω)ϕwu(ω)/λ.PROPOSITION416.–Ifλ>0thefollowingconditionsareequivalent:(i)Hypothesis(H3’)issatisﬁed.(ii)ϕruu(ω)>0foraninﬁnitenumberofvaluesofωin[−ππ].(iii)ϕςς(ω)>0foraninﬁnitenumberofvaluesofωin[−ππ].PROOF.(i)isequivalentto(ii)accordingto(11.51);(ii)isequivalentto(iii)accordingtoProposition576(section13.5.6)(11.50)andtheequalityϕruu=ϕuu−ϕuw(ω)ϕwu(ω)/λ.TheresultbelowisthusclearandconsistentwithCorollary413(section11.3.1):THEOREM417.–UnderHypotheses(H0”)(H1)(H3’)(H5)andλ>0theestimatorˆθ(t)isconsistentiftheopen-loopsystem(11.34)isidentiﬁablebasedonmodel(11.32).IdentiﬁcationofthedeterministicpartInpracticewemostoftenseektoidentifythedeterministicpartofthesystem(whichisthesubjectofinterestofTheorem408(i)inthecontextofopen-loopidentiﬁcation).Thehypothesisoftheidentiﬁabilityoftheglobalsystem(i.e.includingthestochasticpart)isoftentoostrong(seesection11.3.1Remark414).Considernowthesituationwhereonlythedeterministicpartofthesystemisidentiﬁable.Inthefollowingsectionthesignal{u(t)}isassumedtobepersistentlyexciting(seeRemark412(i)section11.3.1).Accordingto(11.49)(11.50)andProposition576(section13.5.6)writingϕrww(ω)(cid:1)λ−ϕwu(ω)ϕuw(ω)/ϕuu(ω)362LinearSystemsB(cid:15)e−iω(cid:16)(cid:1)∆H(cid:15)e−iω(cid:16)ϕuw(ω)/ϕuu(ω)(11.52)weget(cid:25)∆G−∆H(cid:26)ϕςς(ω)(cid:27)∆G∗−∆H∗(cid:28)=(cid:25)∆G+B−∆H(cid:26)(cid:27)ϕuu00ϕrww(cid:28)(cid:27)(∆G+B)∗−∆H∗(cid:28).Notethataccordingto(11.44)(11.45)and(11.51)ϕrww=λϕruuϕuu.(11.53)From(11.48)wegetθ•∈argminθ∈D12π(cid:2)π−πϕuu|H|2(cid:20)|∆G+B|2+|∆H|2ϕrwwϕuu&dω(11.54)(i)Ifthestochasticpartisnotidentiﬁablewecannothave∆H=0whichimpliesB(cid:5)=0accordingto(11.52).Itfollowsthatwewillnothave∆G→0andthustheestimatorofthedeterministicpartisasymptoticallybiased.AfortioriitisthusnotconsistentaccordingtoRemark394(section11.2.2).(ii)Neverthelessnotethataccordingto(11.52)and(11.53)|B|2=|∆H|2λϕuuϕwuuϕuu|∆H|2ϕrwwϕuu=|∆H|2λϕuu(cid:21)1−ϕwuuϕuu(cid:22)thusif|∆H|boundedifλ/ϕuu→0andifϕwuu/ϕuu→0(inotherwordsifthesignal-to-noiseratiobecomesinﬁnitelylarge)thentheintegrandof(11.54)tendstoϕuu|H|2|∆G|2.Thusifthedeterministicpartoftheopen-loopsystemisidentiﬁabletheintegralin(11.54)becomesminimumwhen∆G=0andtheestimatorofthedeterministicpartbecomesconsistent(butthisisalmostatautology).Thereforethedirectapproachleadstothesameconclusionastheindirectapproach(seeRemark414):exceptforthesituationwherethesignal-to-noiseratioissolargethatwecanconsiderthattheidentiﬁcationisrealizedintheabsenceofanoiseinordertoidentifythedeterministicpartoftheopen-loopsystemitisnecessarytoidentifythissystemgloballythusthissystemneedstobeentirelyidentiﬁable.Identiﬁcation36311.3.3.AthirdpathModiﬁeddirectapproachThepurposeofthisapproachistoresolvethedifﬁcultiesoutlinedintheprevioussection;eventhoughitisbasedonthedirectapproachitutilizesexplicitlytheknowledgeofthecontroller.Themodeloftheopen-loopsystem(11.34)isassumedtobeoftheform(11.32)withG=G(cid:15)q−1θs(cid:16)(ontheotherhandHisassumedtobeentirelydependentontheparameterθwhichmakesthedifferencewiththemodel(11.35)).Inadditionweassumethatθ=(θsθβ)wheretherandomvariablesθsandθβareindependentandthatthesetDdeﬁnedinHypothesis(H0”)isoftheformDs×Dβ.Accordingto(11.37)underHypotheses(H0”)and(H1)thepredictionerrorisgivenbyε(t)=1H(q−1θ)(cid:25)∆G(cid:15)q−1θs(cid:16)u(t)+v(t)(cid:26)wherev(t)=˘H(cid:15)q−1(cid:16)w(t)andu(t)satisﬁes(11.39).Usingthislastexpressionweobtainε(t)=˘So(cid:15)q−1(cid:16)H(q−1θ)(cid:25)∆G(cid:15)q−1θs(cid:16)r(t)+(cid:15)1+G(cid:15)q−1θs(cid:16)K(cid:15)q−1(cid:16)(cid:16)v(t)(cid:26).ThereforeunderHypothesis(H5)thespectraldensityϕεεof{ε(t)}satisﬁes(with∆Gθs=∆G(cid:15)e−iωθs(cid:16)Hθ=H(cid:15)e−iωθ(cid:16)etc.)ϕεε=1|Hθ|2|∆Gθs|2)))˘So)))2ϕruu+1|Hθ|2|1+GθsK|2)))˘So)))2ϕvv.(11.55)TheresultbelowcanbeproventhesamewayasTheorem383(section11.1.4):LEMMA418.–UnderHypothesis(H6)thereexistsauniquerationalfunctionR(cid:15)z−1θs(cid:16)suchthatforanyθs∈DstheﬁlterR(cid:15)q−1θs(cid:16)isbicausalbistablesuchthatR(0θs)=1and|Rθs|2=|1+GθsK|2.12Thecriteriontobeminimizedbecomesast→+∞J(θ)=12π(cid:2)π−πϕεε(ω)dω.ChooseH(cid:15)q−1θsθβ(cid:16)oftheformH(cid:15)q−1θsθβ(cid:16)=R(cid:15)q−1θs(cid:16)H(cid:15)q−1θβ(cid:16)(11.56)12.Thevariableθ0sisdenotedbyθsformoresimplicity.364LinearSystemswhereforeveryθβ∈DβtheﬁlterH(cid:15)q−1θβ(cid:16)isbicausalbistableandsuchthatH(0θβ)=1.WethushaveJ(θ)=J1(θsθβ)+J2(θβ)(11.57)J1(θsθβ)=12π(cid:2)π−π1|Hθ|2|∆Gθs|2)))˘So)))2ϕruudωJ2(θβ)=12π(cid:2)π−π1))Hθβ))2)))˘So)))2ϕvvdω.THEOREM419.–SupposeHypotheses(H0”)(H1)(H3’)(H5)and(H6)areinforceandthedeterministicpartoftheopen-loopsystem(11.34)isidentiﬁable.Assumealsothatthemodelchosenfortheidentiﬁcationis(11.32)withHoftheform(11.56).Thentheestimatorˆθs(t)ofthedeterministicpartofthesystemisconsistent.PROOF.Accordingto(H0”)and(H1)theoptimalone-steppredictionˆy(t|t−1)isgivenby(11.33).Accordingto(H5)ϕεεisoftheform(11.55)andfrom(H3’)ϕruu(ω)>0foraninﬁnitenumberofvaluesofωin[−ππ](seeProposition416).Hypothesis(H6)allowsonetowrite(11.56)andsincethedeterministicpartofthesystemisidentiﬁableJ1(θsθβ)isminimum(andequalto0)ifandonlyif∆G(cid:15)q−1θs(cid:16)=0i.e.θs=˘θs.(Ontheotherhandast→+∞ˆθβ(t)→ˆθβ•whereˆθβ•∈argminθβ∈DβJ2(θβ)ifJ2(θβ)admitsaminimum.)RelationwiththeindirectapproachTheone-steppredictor(11.33)fedbackbythecontroller(11.39)canbewritten(inclosed-loop)ˆy(t|t−1)=#1−1+G(cid:15)q−1θs(cid:16)K(cid:15)q−1(cid:16)H(q−1θ)$y(t)+G(cid:15)q−1θs(cid:16)H(q−1θ)r(t).(11.58)1)Supposethatforeveryθs∈DsG(cid:15)q−1θs(cid:16)isstableandK(cid:15)q−1(cid:16)aswell.ThenR(cid:15)q−1θs(cid:16)=1+G(cid:15)q−1θs(cid:16)K(cid:15)q−1(cid:16)andaccordingto(11.56)(11.58)canbewrittenasˆy(t|t−1)=(cid:27)1−1H(q−1θβ)(cid:28)y(t)+Gc(cid:15)q−1θs(cid:16)H(q−1θβ)r(t)anexpressionthatbecomesidenticalto(11.42)ifinthislastequationwereplaceHc(cid:15)q−1θ(cid:16)byH(cid:15)q−1θβ(cid:16).2)Converselyifwemakeuseoftheindirectapproachofsection11.3.1with(i)G=G(cid:15)q−1θs(cid:16)andthusGc=Gc(cid:15)q−1θs(cid:16)and(ii)Hc(cid:15)q−1θ(cid:16)replacedbyIdentiﬁcation365H(cid:15)q−1θβ(cid:16)(inotherwordsprovidedthatthedeterministicpartandthestochasticpartoftheclosed-loopsystemarecharacterizedbyindependentparameters)Theorem408(i)(section11.2.5)showsthatunderHypotheses(H0”)(H1)(H3’)and(H5)13weobtainforthedeterministicpartoftheclosed-loopsystemaconsistentestimatorˆθs(t)(theproblemmentionedinRemark414isthusresolved).Ontheotherhandthedifﬁcultyinherenttothesecondstepoftheindirectapproachinitsclassicformulationisavoided.Indeedˆθs(t)isanestimatorofthedeterministicpartoftheopen-loopsystemaswellasofthedeterministicpartoftheclosed-loopsystem.EXAMPLE420.–LetustakeExample399(section11.2.2)again.Thistimethesystemisfedbackbyaproportionalcontrollerwithgaink=0.4(accordingtotherelationu=k(r−y)).ThesystemisidentiﬁedinadirectapproachbytheARMAXmethod(withnA=nB=nC=2likeinExample404wherenPdenotesthenumberofunknownparametersofpolynomialP=ABorC).ThereferencesignalisaPRBSofamplitude1themeasurementnoisewisaGaussianwhitenoiseofstandarddeviation1andtheidentiﬁcationiscarriedoutover1000points.Theorem417isapplicable.Theinputandtheoutputovertheﬁrst200pointsarerepresentedinFigure11.18(–)aswellastheoutputsimulatedbasedontheidentiﬁedmodelintheabsenceofanoise(--).TheresidueanalysisisshowninFigure11.19.Weseethatthepredictionerror{ε(t)}iswhitenedcorrectly;ontheotherhandε(t)isnotcorrelatedwiththeinputu(τ)τ>t(butiscorrelatedwiththeu(τ)τ≤tinaccordancewiththeobservationmadeatthebeginningofsection11.3.1).Forveriﬁcationpurposesthetransferfunctionoftheidentiﬁedcontinuous-timesystemisˆG(s)=1.139s−1.974s2+0.1495s+0.9825andthusiscloseto(11.24).Thestepresponsesoftheactualsystem(–)andoftheidentiﬁedsystem(--)showninFigure11.20conﬁrmthegoodqualityoftheidentiﬁcation.NeverthelessthecomparisonwiththeresultsobtainedinExample404showsthattheclosed-loopidentiﬁcationis(duetothecross-correlationbetweentheinputandtheoutput)anunfavorablecircumstance.11.4.ExercisesEXERCISE421.–ConsidertheARXmodely(t)+ay(t−1)=bu(t−1)+w(t).13.Hypothesis(H6)thusbecomesuseless.366LinearSystemsFigure11.18.Simulateddataandoutput–Example420Figure11.19.Residueanalysis–Example420Identiﬁcation367Figure11.20.Stepresponses–Example420(i)Letθ=(cid:25)ab(cid:26).Expressˆθ(t)asafunctionofryyryuandruuwhent→+∞.(ii)Weobtainexperimentallyryy(0)=0.03ryy(1)=−10−3ruu(0)=0.05ryu(0)=−0.02andryu(−1)=−0.036.Calculater(cid:2)yy(1)r(cid:2)yu(0)andr(cid:2)yu(−1).Aretheresultscoherent?(iii)Determinetheidentiﬁedcoefﬁcientsˆaandˆb.(iv)Istheidentiﬁedsystemstable?Whatisitsstaticgain?EXERCISE422.–Considerthe“PEMmodel”(11.31).Determinethepredictionerrorasafunctionofthedataanditsvariouspolynomialsandthenthepartialderivativesofthiserrorwithrespecttothecoefﬁcientsofthesepolynomials.EXERCISE423.–WeconsiderthesystemofExample398andwetrytoidentifythissystemusingtheOutputErrormethod.AssumingthatthedeterministicpartisidentiﬁableistheestimatorofthispartconsistentwhenHypotheses(H1)and(H3)ofsection11.2.2areinforce?EXERCISE424.–WeconsiderthesystemofExample405andwetrytoidentifythissystemusingtheARMAXmethod.(i)Assumethatthedeterministicpartisidentiﬁable(thevectorθsofDeﬁnition407beingcomposedofthecoefﬁcientsofthepolynomialsAandBoftheARMAXmodel).ArethehypothesesofTheorem408(i)satisﬁed?*(ii)Isitstillpossibletoobtainacorrectestimationofthe“deterministicpart”?(Ifyesindicatehow).*368LinearSystemsEXERCISE425.–ConsiderasystemhavingthestructureA(cid:15)q−1(cid:16)y(t)=q−rB(cid:15)q−1(cid:16)u(t)+C(cid:15)q−1(cid:16)D(q−1)w(t)(11.59)withtheusualconventions.(a)ShowthatA(cid:15)q−1(cid:16)y(t)=q−rB(cid:15)q−1(cid:16)u(t)+1D1(q−1)w(t)(11.60)whereD1(cid:15)q−1(cid:16)isapowerseriesinq−1.(b)Justifythefactthat(11.60)isacorrectapproximationof(11.59)whenD1(cid:15)q−1(cid:16)isapolynomialofsufﬁcientlyhighdegreesuchthat))D1(cid:15)z−1(cid:16)))>0for|z|≥1.(c)Multiplyingthetwosidesof(11.60)byD1(cid:15)q−1(cid:16)showthatwecanidentifywithagoodprecision˜A(cid:15)q−1(cid:16)(cid:1)A(cid:15)q−1(cid:16)D1(cid:15)q−1(cid:16)and˜B(cid:15)q−1(cid:16)(cid:1)B(cid:15)q−1(cid:16)D1(cid:15)q−1(cid:16)usingtheleastsquaresmethod(andpossiblyrecursiveleastsquares).(Thisprocedureisoneofthevariantsofthegeneralizedleastsquaresmethodcalledtheindirectprocedure.)EXERCISE426.–Letustakesystem(11.32)whereG(cid:15)q−1θ(cid:16)=Gc(cid:15)q−1(cid:16)Gi(cid:15)q−1θ(cid:16)H(cid:15)q−1θ(cid:16)=Hc(cid:15)q−1(cid:16)Hi(cid:15)q−1θ(cid:16)andwhereGc(cid:15)z−1(cid:16)andHc(cid:15)z−1(cid:16)areknowntransferfunctionsGi(cid:15)z−1θ(cid:16)andHi(cid:15)z−1θ(cid:16)aretobeidentiﬁed(usualhypothesesareinforce:Gc(cid:15)q−1(cid:16)andGi(cid:15)q−1θ(cid:16)arecausalandstableHc(cid:15)q−1(cid:16)andHi(cid:15)q−1θ(cid:16)arebicausalbistableandsuchthatHc(0)=Hi(0θ)=1).Howcanweestimateθ?EXERCISE427.–(i)IsitpossibletotreatExample420correctlyusingtheOutputErrormethod?(ii)Accordingtothetheorydiscussedinsection11.3.3isitpossibletotreatcorrectlyExample420withanARMAXmodelwhosestochasticparthaslesscoefﬁcients(nC=1forexample)?EXERCISE428.–Gooverthedevelopmentofthischapteragainandcarefullydistinguishbetweennoisesw(t)and˘w(t)ofthemodelandofthesystemrespectivelyandthusjustifyRemark390(ii).Chapter12Appendix1:AnalysisWhatispresentedhereisabriefsummaryofafewelementsofanalysiswhichareveryusefulincontroltheory.Theneedforconcisenessisnotcompatiblewiththerigorrequiredinpuremathematics.Moreprecisepresentationsincludingproofscanbefoundinnumerousbooks([35][103][83]amongothers).ThetheoryofmeasureandintegrationisonlytoucheduponinRemark431below;thisshouldnotbeaninconvenienceforthereaderwhohasnotstudiedthissubjectwhichisdevelopedingreatdetailin([35]vol.II)orin[103]usingadifferentapproach.*Followingaresomeofthesimpliﬁcationsmade:weareonlyconcernedwithHausdorfftopologicalspaceswithacountablebasewhosetopologyiscompletelydescribedbytheconvergenceofsequences1;twofunctionsthatareequalalmosteverywhereinLebesgue’ssenseareconsideredequal(inotherwordsafunctionis“identiﬁed”withitsLebesgueclass);adistributionisexpressedthesamewayasafunctionandthe“integralnotation”(12.14)usedsystematicallybelow.*12.1.Topology12.1.1.TopologicalspacesOpensetsAtopologicalspaceEisasetembeddedwithatopologyconsistingofopensets.Theopensetssatisfythefollowingaxioms:∅andEareopenanyunionofopensetsisopenaswellasanyﬁniteintersectionofopensets.IfAisasubsetofEthe1.Inthecaseoftopologicalspaceswithuncountablebase–inparticularinspacesofdistributions–onehasonlytoreplacesequencesbynetsorequivalentlybyﬁlters.369Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.370LinearSystemstopologyofAsaidtobe“inducedbythatofE”ifitsopensetsaretheΩ∩AwheretheΩ’saretheopensetsofE.SomefundamentalconceptsAclosedsubsetofEisthecomplementofanopensubsetofE.IfAisanon-emptysubsetofE(possiblyreducedtoonepoint)aneighborhoodofAinEisasubsetofEcontaininganopensetcontainingA.TheclosureofasubsetAofEisthesmallestclosedsubsetcontainingAthatistheintersectionofallclosedsubsetscontainingA.AtopologicalspaceEissaidtobecompactiffromanycoveringofEbyafamilyofopensetsonecanextractaﬁnitesubcovering;onecanshowthatacompactsubsetofEisnecessarilyclosedinE.ThetopologicalspaceEisconnectediftheonlysubsetsofEwhicharebothopenandclosedare∅andE;roughlyspeakingthismeansthatEis“allinonepiece”.LetEbeanytopologicalspaceandx∈E;theunionofallconnectedsubsetsofEcontainingxisaconnectedsetC(x)calledtheconnectedcomponentofxinE.TheintersectionoftwodistinctcomponentsofEisobviouslyemptyandEistheunionofitsconnectedcomponents.ConvergenceandcontinuityWesaythatasequence(xn)inthetopologicalspaceEconvergestoy∈EifforanyneighborhoodVof{y}(alsocalledaneighborhoodofy)thereexistsanintegerNsuchthatxn∈Vwhenevern≥N.AsubsetAofEissaidtobedenseinEifeverypointofEisalimitofasequenceofpointsofA.LetEandFbetwotopologicalspacesandletf:E→Fbeafunction.ThisfunctionissaidtobecontinuousifforanyopensubsetΩFofFtheinverseimagef−1(ΩF)isopeninE.Inthiscaseforanypointy∈Eandforanysequence(xn)whichconvergestoyinEthesequence(f(xn))convergestof(y)inF;andconverselyifthisconditionholds(*withthesimpliﬁcationsmentionedabove*)fiscontinuous.Iff:E→FiscontinuousandEiscompactthenf(E)⊂Fiscompact.MetricspacesMetricspacesareamongthemostimportanttopologicalspaces.AmetricspaceisasetEequippedwithafunctiond:E×E→R+calledadistancesuchthat(i)d(xy)=d(yx)andd(xx)=0(ii)d(xy)+d(yz)≤d(xz)(“triangleinequality”)(iii)ifd(xy)=0thenx=y.2(Ifdonlysatisﬁes(i)and(ii)andifits2.Quantiﬁersareunderstoodwhenthereisnoambiguity.Appendix1:Analysis371codomainisR+∪{+∞}thisfunctioniscalledapseudometric.)If(Ed)isametricspacetheopen(resp.closed)ballwithcenterxandradiusr(x∈Er≥0)isthesetofally∈Esuchthatd(xy)<r(resp.d(xy)≤r).Ofcourseanopenballwithzeroradiusistheemptyset.Letx∈EandVnx={y∈E:d(xy)≤1/n}.ThesetofallVnxn≥1isafundamentalsystemofneighborhoodsofxinotherwordsasetWxisaneighborhoodofxifandonlyifthereexistsn≥1suchthatVnx⊂Wx.ThereforeanopensetofEisaunionofopenballs.CompletenessAsequence(xn)inametricspace(Ed)iscalledaCauchysequenceifd(xnxm)tendsto0asnandmbothtendtoinﬁnity.AllconvergentsequencesareCauchybuttheconverseisfalseingeneral.AmetricspaceinwhicheveryCauchysequenceconvergesissaidtobecomplete.12.1.2.TopologicalvectorspacesGeneraldeﬁnitionLetEbeavectorspacedeﬁnedovertheﬁeldK=RorC(RdenotestheﬁeldofrealnumbersandCtheﬁeldofcomplexnumbers).Wewillequipthisspacewithatopologywhichiscompatiblewithitsalgebraicstructurei.e.suchthatthefollowingtwoconditionshold:–if(λn)isasequenceofnumbersconvergingtoλinKandif(xn)isasequenceofelementsofEconvergingtoxinEthenthesequence(λnxn)convergestoλxinE;–if(yn)isanothersequenceofelementsofEconvergingtoapointyinEthenthesequence(xn+yn)convergestox+y.Notethataccordingtotheabovecompatibilityconditions(xn)convergestoxifandonlyifxn−xconvergesto0.AK-vectorspaceequippedwithsuchatopologyiscalledatopologicalvectorspace.IfEandFaretwotopologicalvectorspacesanisomorphismfromEontoFisabijectivelinearfunctionofE→Fwhichiscontinuousaswellasitsinversefunction.NormedvectorspaceAseminormonEisafunction(cid:23).(cid:23):E→R+suchthatthefollowingtwoconditionshold:–(cid:23)λx(cid:23)=|λ|(cid:23)x(cid:23)(whereλisascalari.e.anelementofK);–(cid:23)x+y(cid:23)≤(cid:23)x(cid:23)+(cid:23)y(cid:23)(“triangleinequality”).372LinearSystemsTherefored(xy)=(cid:23)x−y(cid:23)isapseudometric.Thefunction(cid:23).(cid:23)isanormifthefollowingsupplementaryconditionholds:–if(cid:23)x(cid:23)=0thenx=0(thatisiftheabovepseudometricisadistance).Thetopologyofanormedvectorspacecanbecharacterizedasfollows:asequence(xn)ofelementsofEconvergesto0inEifandonlyifthesequence((cid:23)xn(cid:23))tendsto0inR.(Howeverthetopologyofatopologicalvectorspaceisnotalwaysdeﬁnedbyanormnorbyadistance.Inparticularthisisthecaseofdistributionspaceswhichwillbestudiedlater.)AboundedsubsetofEisasetincludedinaball.ACauchysequenceinEisbounded.3Avectorxsuchthat(cid:23)x(cid:23)=1issaidtobeunitaryandthesetofunitaryvectorsformsthe“unitsphere”.LetEbeanormedvectorspace.AcompactsubsetofEisnecessarilyclosedandbounded(butifEisinﬁnite-dimensionalthereexistclosedandboundedsubsetsofEwhicharenotcompact).IfEiscompleteitiscalledaBanachspace.CaseofRandCInparticularanopenballofRisanopenintervaloftheform(ab)−∞<a≤b<+∞.InthecomplexplaneCtheopenballwithcenterzandradiusr≥0isnothingbuttheopendiscwithcenterzandradiusrwhichisthesetofs∈Csuchthat|z−s|<r.DualityAlinearformfonEisalinearfunctionfromEintoK.Foranyx∈Ethescalarf(x)isoftendenotedby fx!(“dualitybracket”).Thesetofalllinearforms(resp.ofallcontinuouslinearforms)onEisaK-vectorspacewhichwecallthealgebraicdual(resp.thetopologicaldual)ofEandwedenotebyE∗(resp.E(cid:2)).IfEisﬁnite-dimensionalE∗andE(cid:2)coincide.IfEisanormedvectorspacewecandeﬁneanormonE(cid:2)(seesection12.1.3).Thetopologydeﬁnedbythisnormiscalledthe“strongtopology”ofE(cid:2).WecandeﬁneonE(cid:2)anothertopologycalledtheweak*topologyinthefollowingmanner:let(x(cid:2)n)beasequenceofelementsofE(cid:2);wesaythatthissequenceweakly*convergesto0inE(cid:2)ifforanyy∈Ethesequence( x(cid:2)ny!)tendsto0.HilbertspacesLetK=RorC.Apre-HilbertspaceEovertheﬁeldKisaK-vectorspaceequippedwithascalarproductdenotedas ..!.Thisscalarproductisapositive3.ThisdoesnotholdforaCauchynet.Appendix1:Analysis373deﬁniteHermitianformi.e.afunctionfromE×EintoKforwhichthefollowingconditionshold:– x+x(cid:2)y!= xy!+ x(cid:2)y!and xy+y(cid:2)!= xy!+ xy(cid:2)!;– xλy!=λ xy!;thusthefunction x.!:y(cid:3)→ xy!islinear;4– xy!= yx!(HermitiansymmetryifK=CsymmetryifK=R);– xx!∈R+and xx!=0ifandonlyifx=0.Onecaneasilyshowthatthefunctionx(cid:3)→% xx!(cid:1)(cid:23)x(cid:23)isanormonEcalledapre-Hilbertnorm.Apre-HilbertspaceisthusaparticulartypeofnormedK-vectorspace.WehavetheSchwarzinequality:| xy!|≤(cid:23)x(cid:23)(cid:23)y(cid:23).(12.1)NotethatifK=Cthefunctionx→ xy!isantilinear5whereasitwouldbelinearif ..!wereadualitybracket.Acompletepre-HilbertK-spaceiscalledaHilbertspace.Apre-HilbertnormonaHilbertspaceiscalledaHilbertnorm.Thefollowingisthe“orthogonalprojectiontheoreminHilbertspaces”:THEOREM429.–LetEbeapre-HilbertspaceandF(cid:5)={0}beacompletesubspaceofE.Foranyx∈EthereexistsoneandonlyonepointofFdenotedbyˆxsuchthat(cid:23)x−ˆx(cid:23)=miny∈F(cid:23)x−y(cid:23).Thispointischaracterizedbytherelation yx−ˆx!=0forally∈FandisthustheorthogonalprojectionofxontoF.Thefunctionx(cid:3)→ˆxisK-linear(itisalsocontinuouswithnorm1:seesection12.1.3).Finite-dimensionalspacesAccordingtotheRiesztheoremifEisaﬁnite-dimensionalK-vectorspacethereexistsauniquetopologyonEcompatiblewithitsstructureofK-vectorspace.InadditionifFisatopologicalvectorspaceandiffisalinearfunctionfromEintoFfisnecessarilycontinuous.IntheelementsofalgebrainChapter13weareonlyconcernedwithﬁnitedimensionthereforethenotionofcontinuity(whichisnotalgebraic)isneverdiscussed;fromacertainpointofviewwecanconsiderthatitisunderstood.AK-vectorspaceofdimensionn<+∞isidentiﬁedwith4.Accordingtoanequivalentbutdifferentconventionthefunction(cid:8).y(cid:9):x(cid:10)→(cid:8)xy(cid:9)isassumedtobelinear.5.Afunctionf:x(cid:10)→f(x)isantilineariff(x1+x2)=f(x1)+f(x2)andf(λx)=¯λf(x).374LinearSystemsKnonceabasisischosen(seesection13.3.1).Thetopologyofthisspacecanthusbedeﬁnedbyoneofthefollowingnorms(amongothers):(cid:23)x(cid:23)1=(cid:11)ni=1|xi|(cid:23)x(cid:23)∞=max1≤i≤n|xi|(withx=(x1...xn)).Asubsetofaﬁnite-dimensionalK-vectorspaceiscompactifandonlyifitisclosedandbounded.Everyﬁnite-dimensionalK-vectorspaceiscompleteandischaracterizedbythefactthatitislocallycompactthatistosaythereexistcompactneighborhoodsof0(theunitballifthespaceisnormedforexample).EuclideanandHermitianspacesLetEbeaﬁnite-dimensionalpre-HilbertspaceoverK.IfK=REiscalledanEuclideanspaceandapre-HilbertnormonsuchaspaceissaidtobeEuclidean.IfK=CEiscalledaHermitianspaceandapre-HilbertnormonsuchaspaceissaidtobeHermitian.ObviouslyanEuclideanspaceoraHermitianspaceisaHilbertspace;converselyaﬁnite-dimensionalHilbertspaceisaEuclideanspaceoraHermitianspace.AHilbertK-spaceofdimensionnisidentiﬁedwithKninanorthonormalbasis;thevectorsofthisspacealsoidentiﬁedwiththecolumnvectorswithnentriesinK;thenthe“standardscalarproduct”is xy!=x∗y(seeaboveaswellassection13.5.1).Thepre-Hilbertnorm(cid:23)x(cid:23)2associatedwiththisscalarproductiscalledthe“standardEuclideannorm”ifK=Randiscalledthe“standardHermitiannorm”ifK=C.12.1.3.ContinuouslinearoperatorsLetEandFbetwonormedK-vectorspacesandletubeaK-linearfunctionfromEintoF(uisalsocalledalinearoperatorfromEintoF).Wesaythatuiscontinuous(inthenormtopology)if(cid:23)u(cid:23)<+∞where(cid:23)u(cid:23)(cid:1)supx(cid:12)=0(cid:10)u(x)(cid:10)(cid:10)x(cid:10)=sup(cid:10)x(cid:10)≤1(cid:23)u(x)(cid:23)=sup(cid:10)x(cid:10)=1(cid:23)u(x)(cid:23)(12.2)(thereadercanprovethetwoequalitiesontheright-handsideasanexercise).ThecontinuouslinearoperatorsfromEintoFformaK-vectorspacedenotedbyL(EF)andthequantity(12.2)deﬁnesanormonthisspace.Thisnormissometimescalledthe“operatornorminducedbythenormsofEandF”(andthe“operatornorminducedbythenormofE”ifE=F).FortheexplicitcalculationofthisnorminthecasewhereEandFareﬁnite-dimensionalHilbertspacesseesection13.5.7.Appendix1:Analysis375Ifu∈L(EF)andv∈L(FG)whereEFandGarenormedK-vectorspaceswehave(cid:23)vu(cid:23)≤(cid:23)v(cid:23)(cid:23)u(cid:23)andweexpressthispropertybysayingthatanoperatornormismultiplicative.OnecanshowthatL(EF)iscompletethusaBanachspaceifFisaBanachspace.InparticularE(cid:2)=L(EK)isaBanachspaceequippedwiththenorm(12.2);thisnormdeﬁnesthe“strongtopology”ofE(cid:2).Ifasequence(x(cid:2)n)ofelementsofE(cid:2)convergestozerointhistopologythissequenceissaidtostronglyconvergetozero.IfEisﬁnite-dimensionalalllinearoperatorofEintoFarecontinuous(thisremainstrueifFisanytopologicalvectorspace).12.2.Sequencesfunctionsanddistributions12.2.1.Sequenceslpspaces–Casep∈[1+∞)WedenotebylptheK-vectorspaceconsistingofallsequences(xn)n∈ZofelementsofKsuchthat+∞(cid:11)n=−∞|xn|p<∞.Then(cid:23)x(cid:23)p(cid:1)(cid:21)+∞(cid:11)n=−∞|xn|p(cid:22)1/p(12.3)isanormonlpandthatspaceequippedwiththenorm(12.3)isaBanachspace.Inthecasep=2wedeﬁneonthespacel2thescalarproduct xy!2(cid:1)+∞(cid:12)n−∞¯xnyn(12.4)where¯xnistheconjugateofxn.Thespacel2isaHilbertspace.–Casep=+∞Thespacel∞isthatofallboundedsequences;thisspaceisequippedwiththenorm(cid:23)x(cid:23)∞(cid:1)supn|xn|.(12.5)ThisisaBanachspace.376LinearSystemsThefollowingresultcaneasilybeshown:THEOREM430.–If1≤q≤p≤+∞thenlq⊂lp.SequenceswithpositivesupportThesupportofasequence(xn)isthesetofallintegersnforwhichxn(cid:5)=0.Thissequence(xn)issaidtobewithpositivesupportorpositivelysupportedifxn=0forn<0.ConvolutionoftwosequencesGeneralcaseLethandubetwosequencesofelementsofK.Theirconvolutionproductisthesequenceydenotedy=h∗uanddeﬁnedbyyn=+∞(cid:11)k=−∞hn−kuk=+∞(cid:11)k=−∞hkun−k(12.6)undertheconditionthattheseriesconvergesforallvaluesofn.Notethatthefunction(hu)(cid:3)→h∗uisbilinear(likeeveryproduct)andsymmetric(h∗u=u∗h).ConvolutionandlpspacesAseasilyshownifh∈l1thentheaboveconvolutionproductiswell-deﬁnedwheneveru∈lpp∈[1+∞]andinthiscaseh∗u∈lp.InadditionwehavetheYounginequality(cid:23)h∗u(cid:23)p≤(cid:23)h(cid:23)1(cid:23)u(cid:23)p.(12.7)ThelinearoperatorHdeﬁnedbyHu=h∗uiscalledtheconvolutionoperatorwithkernelh.TheYounginequalityshowsthatifh∈l1thenHiscontinuousfromlpintolpandthat(cid:23)H(cid:23)≤(cid:23)h(cid:23)1.ConvolutionofsequenceswithpositivesupportIfhanduaretwopositivelysupportedsequencesofcomplexnumberstheirconvolutionproductymustexistandisalsopositivelysupportedsinceyn=n(cid:12)k=0hn−kuk.Appendix1:Analysis37712.2.2.FunctionsLebesguespacesREMARK431.–LetSbeasubsetofRandxbeafunctionfromSintoKnorfromSinto[−∞+∞].Thefollowingnotionsareusedbelow:(a)themeasurabilityofS(inLebesgue’ssense);(b)themeasurabilityofx(inLebesgue’ssense);theintegral(cid:1)Sx(t)dt(inLebesgue’ssense);theLebesgueclassofx.(i)IfSisanintervalofRthemeasurabilityofxisnotastrongassumption.IndeedthereisnoexplicitexampleofafunctionwhichisnotLebesgue-measurablealthoughusingtheaxiomofchoiceonecanprovethatsuchfunctionsexist[103].(ii)ThesetSisLebesgue-measurableifandonlyifsoisitscharacteristicfunctionκS:R→RdeﬁnedbyκS(t)=1ift∈SandκS(t)=0otherwise.ThesetSissaidtobeof(Lebesgue)measurezeroifitisof“zerolength”.ThishappensinparticularifSiscountable.Forexamplethesubset[01]∩Qof[01]isofmeasurezero(althoughQisdenseinR)thusitscomplement[01]∩(R\Q)isofmeasure1.(iii)LetS⊂RbeLebesgue-measurable.TwofunctionsxandyfromSintoKnorfromSinto[−∞+∞]aresaidtobeequalalmosteverywhereifZ={t∈S:x(t)(cid:5)=y(t)}isof(Lebesgue)measurezero.TheLebesgueclassofxconsistsofallfunctionsywhichareequaltoxalmosteverywhere.(iv)LetSbeanintervalofRandletxbeafunctionfromSintoKnorfromSinto[−∞+∞].IfthereexistsintheLebesgueclassofxafunctionysuchtheusualRiemannintegral(cid:1)Sy(t)dtexiststhentheLebesgueintegral(cid:1)Sx(t)dtexists(i.e.xisLebesgue-integrableonS)andisequalto(cid:1)Sy(t)dt.Ontheotherhandletx:S→Kn;theLebesgueintegral(cid:1)Sx(t)dtexistsifandonlyifxismeasurableand(cid:1)S(cid:23)x(t)(cid:23)dt<+∞.TheLebesguespacesLp(p∈[1+∞])arespacesoffunctionswhichhavepropertiesverysimilartothoseoflpspaces.–Casep∈[1+∞)WedenotebyLpthespaceofallfunctionsx:R→KwhichareLebesgue-measurableandsuchthat(cid:1)+∞−∞|x(t)|pdt<∞.ProvidedthatxisnotdistinguishedfromitsLesbesgueclassLpiswrittenLpandthefunctionx(cid:3)→(cid:23)x(cid:23)pasdeﬁnedbelowisanormonLp:(cid:23)x(cid:23)p=(cid:21)(cid:2)+∞−∞|x(t)|pdt(cid:22)1/p.(12.8)Inthecasep=2wedeﬁneonspaceL2thescalarproduct xy!2(cid:1)(cid:2)+∞−∞¯x(t)y(t)dt(12.9)378LinearSystemswhere¯x(t)istheconjugateofthecomplexnumberx(t).Wehave(cid:23)x(cid:23)2=√<xx>2.ThefunctionsbelongingtoL2aresaidtobesquareintegrable.Insignaltheorythequantity(cid:23)x(cid:23)22iscalledtheenergyofthesignalx.AsaresultL2canbeinterpretedasthespaceofsignalswithboundedenergy.–Casep=+∞Afunctionx:R→KissaidtobeessentiallyboundedifitismeasurableandthereexistsaﬁniteconstantM>0suchthat|x(t)|≤Malmosteverywhere.ThegreatestlowerboundofalltheconstantsMsatisfyingtheaboveconditionisdenotedas(cid:23)x(cid:23)∞(cid:1)ess.supt∈R|x(t)|.(12.10)ThespaceL∞isthatofallessentiallyboundedfunctionsandthefunctionL∞(cid:20)x(cid:3)→(cid:23)x(cid:23)∞∈R+isaseminorm.AgainprovidedthatxisnotdistinguishedfromitsLesbesgueclassL∞iswrittenL∞andtheaboveseminormisthenanormonL∞.FromthepointofviewofsignaltheoryL∞canbeinterpretedasthespaceofsignalswithboundedabsolutevalue.AccordingtotheFisher–RiesztheoremallLpspaces(p∈[1+∞])areBanachspacesandL2isaHilbertspace.Theorem430doesnothaveananalogueinthecaseoffunctions.FunctionswithpositivesupportThesupportofafunctionistheclosureofthesetofpointsatwhichitisnon-zero.Incontroltheoryweareessentiallyinterestedinfunctionswithpositivesupportthatisinfunctionsxsuchthatx(t)=0fort<0.Hereiswhy.ConsideracontrolsystemanditsinputudeﬁnedinR;itisthusafunctionfromRintoKm(K=RorC).Ifweareonlyconcernedwiththebehaviorofthesystemfromaninitialinstantt0whichwecanassumetobezero(bytranslationoftheoriginoftime)thenwealsoareonlyconcernedwiththerestrictionofutoR+.Theknowledgeofthisisequivalenttothatoftheproductu+=u1where1istheunitstepdeﬁnedasfollows:1(t)=(cid:20)0t<01t≥0.Thisfunctionu+iswithpositivelysupported.Appendix1:Analysis379ConvolutionoftwofunctionsGeneralcaseLethandubetwofunctionsfromRintoK.TheirconvolutionproductisthefunctionyfromRintoKdenotedasy=h∗uanddeﬁnedbyy(t)=(cid:2)+∞−∞h(t−τ)u(τ)dτ=(cid:2)+∞−∞h(τ)u(t−τ)dτ(12.11)undertheconditionthattheintegralconvergesforallvaluesoft.Thefunction(hu)→h∗uisbilinearandsymmetric.ConvolutionandLebesguespacesOnecanshowthatifh∈L1thentheaboveconvolutionproductiswelldeﬁnedwheneveru∈Lpp∈[1+∞]andinthiscaseh∗u∈Lp.InadditionwehavetheYounginequality(cid:23)h∗u(cid:23)p≤(cid:23)h(cid:23)1(cid:23)u(cid:23)p.(12.12)LetHbethelinearoperatordeﬁnedbyHu=h∗ucalledtheconvolutionoperatorwithkernelh.TheYounginequalityshowsthatifh∈L1thenHiscontinuousfromLpintoLpandthat(cid:23)H(cid:23)≤(cid:23)h(cid:23)1.Tocalculate(cid:23)H(cid:23)inthecasep=2seesection13.6.2Theorem589.AlgebraK+WedenotebyKthesetofalllocallyintegrablefunctionsfromRintoK(i.e.ofthosefunctionswhichareintegrableonallcompactintervals)andbyK+thesetconsistingofallpositivelysupportedfunctionsbelongingtoK.Forexamplethefunctiont→eαt1(t)belongstoK+andthefunctiont(cid:3)→eet1(t)aswell.ForanyhandubelongingtoK+integral(12.11)iswrittenasy(t)=(cid:2)t0h(t−τ)u(τ)dτthusitconverges;inadditionh∗u∈K+.ThesetK+isacommutativeK-algebra(seesection13.1.1);indeeditisbothaK-vectorspaceandacommutativeringwithconvolutionasaproduct.Wethuscallitaconvolutionalgebra.AnotherexampleofconvolutionalgebraisL1.Allthesealgebrasarecommutativesincetheconvolutionproductiscommutative.ContinuityofaconvolutionproductIfhandubelongtoK+andifinadditionhisalocallyboundedfunction(thatishisboundedonanycompactinterval)onecanshowthattheirconvolutionproducty=h∗uisacontinuousfunction.380LinearSystems12.2.3.DistributionsIntroductiontothenotionofdistributionLetTbeavectorspaceoffunctionsφfromRintoCwhichareveryregularinthefollowingsense:–theyareindeﬁnitelydifferentiable;–eithertheyhaveacompactsupportandinthiscaseTisdenotedasD;orandtheydecreaseveryrapidlytoward0atinﬁnitymorepreciselyφanditsderivativesofallordersdecreasemorerapidlyintheneighborhoodofinﬁnity(inabsolutevalue)thananyfunctionoftheform1|tk|(wherekisanypositiveinteger):inthiscaseTisdenotedbyS.SpaceSiscalledthe“spaceofrapidlydecayingfunctionsatinﬁnity”.WecallTaspaceoftestfunctions.NowletfbeafunctionfromRintoC.Letusformtheintegral Tfφ!(cid:1)(cid:2)+∞−∞f(t)φ(t)dt.(12.13)ForT=Dthisintegralconvergeswheneverf∈K.ForT=SitconvergesforanyfunctionfinKthatisnotincreasing(inabsolutevalue)fasterthananypolynomialintheneighborhoodofinﬁnity;wedenotethesetofthesefunctionsbyOandwecallthisthespaceof“slowlyincreasingfunctionsatinﬁnity”.ForexampleallpolynomialsintbelongtoO.ThereforeweassociatewithafunctionfthelinearformTf:φ→ Tfφ!∈CwhichbelongstothedualT(cid:2)ofT(f∈KorOdependingonwhetherT=DorS).IftwofunctionsfandgaresuchthatTf=Tgonecanshowthatf=galmosteverywherethusfandgareidentiﬁed(byabuseoflanguage).TheknowledgeoffisequivalentthentothatofTfwhichmakesitpossibletoidentifyfwithTf.Intheintegralof(12.13)fandφplayasymmetricroleifthesefunctionsbelongtoL2accordingto(12.9):thedualofL2isL2itself.ButTisaspacemuchsmallerthanL2thereforeitsdualT(cid:2)ismuchlarger(indeedthestrongertheconditionsuponφthelargerthesetoffunctionsfforwhichtheintegral(12.13)converges).InparticularD(cid:2)islargerthanKandS(cid:2)islargerthanO.ThereforewehaveembeddedthespacesoffunctionsKandOinthelargerspacesD(cid:2)andS(cid:2)respectively.TosumupO⊂S(cid:2)⊂D(cid:2)O⊂K⊂D(cid:2).Appendix1:Analysis381SpacesofdistributionsThespaceD(cid:2)iscalledthespaceofdistributions.ThespaceS(cid:2)iscalledthespaceoftempereddistributions.AdistributionisthusalinearformT:φ(cid:3)→ Tφ!(T∈T(cid:2)).Thespacesofdistributionsareequippedwiththefollowingtopologicalstructure:asequence(Tn)ofT(cid:2)convergestoT∈T(cid:2)ifforanytestfunctionφ∈T Tnφ!convergesto Tφ!.6WiththisdeﬁnitionTcanbeprovedtobedenseinT(cid:2)andinLpp∈[1+∞).Takingintoaccountrelation(12.13)weﬁnditconvenienttodenotethedualitybracket<Tφ>byanintegrali.e. Tφ!=(cid:2)+∞−∞T(t)φ(t)dt(12.14)wherethedistributionTthusappearsthesamewayasafunctiont(cid:3)→T(t).Butitisa“generalizedfunction”7whichonlyhasmeaningunderasummationasin(12.14).Adistributionwhichisnotafunctionissaidtobe“singular”.SupportofadistributionWesaythatadistributionTiszeroinanopensubsetΩofRif Tφ!=0foranyfunctionφ∈TthesupportofwhichisincludedinΩ.TheunionofallopensubsetsinwhichTiszeroisthelargestopensetinwhichTvanishes.ItscomplementSwhichisclosedisbydeﬁnitionthesupportofTandisdenotedbysuppT.NowletΩbeanopenneighborhoodofS=suppTandφ1φ2betwofunctionsofTthatareequalinΩ.Thenφ=φ1−φ2iszeroinΩthereforesuppφisincludedinthecomplementofΩwhichisitselfincludedinS.Asaresult Tφ!=0whichimpliesthat Tφ1!= Tφ2!.Thisshowsthat Tφ!onlydependsontherestrictionofφtoanyopenneighborhoodofS.ForexampleifThasasupportincludedin[t1t2](resp.[t1+∞))wecanwrite Tφ!=(cid:2)t+2t−1T(t)φ(t)dt(resp. Tφ!=(cid:2)+∞t−1T(t)φ(t)dt)(12.15)6.*Thisisthedeﬁnitionweak*topology(seesection12.1.2).ButduetotheveryspeciﬁctopologicalpropertiesofT(TandthusT(cid:3)areMontelspaces)weak*convergenceandstrongconvergenceofsequencescoincideinT(cid:3)[106].*7.SoarealsoSato’shyperfunctions[22].382LinearSystemswherebyconvention(cid:2)t+2t−1(cid:1)limε1→0+ε2→0+(cid:2)t2+ε2t1−ε1.Thisgeneralizesinthecaseofdistributionsthenotionofsupportdeﬁnedforfunctionsexceptthatinthecaseofafunctiont−1andt+2canbereplacedbyt1andt2respectively.Aseasilyseeneverycompactlysupporteddistributionistempered.WedenotebyT(cid:2)+thesubspaceofT(cid:2)consistingofpositivelysupporteddistributionsandforanysuchdistributionTwethushave Tφ!=(cid:1)+∞0−T(t)φ(t)dtforanyφ∈T.DifferentiationofdistributionsDeﬁnitionLetT∈T(cid:2);wecandeﬁneformallythederivativeofthisdistributionbyapplyingtheintegrationbypartsto(12.14):(cid:2)+∞−∞˙T(t)φ(t)dt=−(cid:2)+∞−∞T(t)˙φ(t)dtisawell-deﬁnedexpressionbecause˙φ∈T.Asaresult<˙Tφ>(cid:1)−<T˙φ>.(12.16)Wededucethatalldistributionsareindeﬁnitelydifferentiable.DerivativeofafunctioninthesenseofdistributionsLetfbealocallyintegrablefunctionfromRintoC(thatisf∈K).ThisfunctionisgenerallynotdifferentiableatallpointsofRintheusualsense.Forexamplethefunctionf(t)=t1(t)isdifferentiablein(−∞0)(withazeroderivative)andin(0+∞)(withaderivativeof1)butnotat0.Ontheotherhandifwedealwithfthesamewayaswithadistributionfbecomesdifferentiable.Itsderivativeisthustaken“inthesenseofdistributions”.Thisderivativecanbeafunctionorasingulardistribution.Intheexamplepresentedherewehave˙f=1thus˙fisafunction.AbsolutelycontinuousfunctionLetfbealocallyintegrablefunctionandlet˙fbeitsderivativeinthesenseofdistributions.If˙fisalocallyintegrablefunctiononecanshowthatf(t)−f(t0)=(cid:2)tt0˙f(τ)dτAppendix1:Analysis383(seeforexample[35]sectionXVII.5).Suchafunctionfissaidtobeabsolutelycontinuous.Itisimmediatethatanabsolutelycontinuousfunctioniscontinuous.Anexampleofabsolutelycontinuousfunctionisthefunctionf(t)=t1(t)consideredabove.Thisfunctionisnotdifferentiableintheusualsense.TheunitsteptheDiracdistributionanditsderivativesDerivativeoftheunitstepTheunitstepisnotdifferentiableintheusualsenseoffunctionsduetoitsdiscontinuityat0.Inadditionthisfunctionisnotabsolutelycontinuoussinceitisnotcontinuous.Itsderivativeinthesenseofdistributionsisthusasingulardistributionwhichwearegoingtodetermine.Wehaveforanyφ∈T1˙1φ2=−31˙φ4=−(cid:2)+∞0˙φ(t)dt=φ(0)DeﬁnitionofδThederivativeoftheunitstepistheDiracdistributionδdeﬁnedby δφ!=φ(0).(12.17)Usingthe“integralnotation”wehave(cid:2)+∞−∞δ(t)φ(t)dt=φ(0).(12.18)Obviouslythesupportofδis{0}andthisdistributionisthereforetempered.InterpretationofδAccordingto(12.18)wecaninterpretδasageneralizedfunctionbeingeverywherezeroexceptat0whereitis+∞andsuchthat(cid:2)+∞−∞δ(t)dt=lim→0+(cid:2)+−δ(t)dt=1.(12.19)Wecanjustify(12.19)inthefollowingmanner:let>0;wehaveaccordingto(12.19)and(12.15):φ(0)=(cid:1)+−δ(t)φ(t)dt.Sinceδisa“non-negative(generalized)function”wehave(cid:1)+−δ(t)φ(t)dt=φ(η)(cid:1)+−δ(t)dtforsomeη∈[−]accordingtothemeanvalueformula.Wethusobtainthesecondequalityof(12.19)bytaking→0+andtheﬁrstbyusingthefactthatδ(t)=0fort(cid:5)=0.384LinearSystemsNotethatherewerepeatedlyabusethelanguage:speciﬁcallyafunctionwhichiseverywhereequaltozeroexceptat0hasazerointegral(Remark431).Thus(12.19)wouldbeimpossibleifδwereactuallyafunctionasthenotationandtherationaleimplyit;butthecalculationswhichhavejustbeenmaderighthereareexactandtheycanbegivenaprecisemathematicalsense.ConvergencetoδInordertogiveaprecisemeaningoftheaboveletusstatethefollowingresult:THEOREM432.–Let(fn)beasequenceofmeasurablefunctionsfromRtoR+suchthat(cid:1)+∞−∞fn(t)dt=1andforeveryneighborhoodVof0inR(cid:1)R\Vfn(t)dtconvergesto0.Then(fn)convergestoδinD(cid:2).DerivativesofδAccordingto(12.16)wehave3˙δφ4=−˙φ(0)andbyiteration3δ(n)φ4=(−1)nφ(n)(0).ProductofdistributionsOnecannotingeneraldeﬁnetheproductoftwodistributionsTandU.OnecandeﬁnetheproductofadistributionTbyafunctionfaccordingtotheformula Tfφ!= Tfφ!undertheconditionthatfφbelongstoTforanyfunctionφofT.IfT=Dthisholdswheneverfisanindeﬁnitelydifferentiablefunction;ifT=SthisholdswheneverfisafunctionbelongingtothespaceOMofindeﬁnitelydifferentiablefunctionswhosederivativesofallorders(zerothorderincluded)8belongtoO.IfT∈S(cid:2)+thisformulaismeaningfulaccordingto(12.15)wheneverfisanindeﬁnitelydifferentiablefunctionwhosederivativesofallordersareslowlyincreasingast→+∞(onesuchfunctiondoesnotgenerallybelongtoOMbecausewearenotconcernedwithitsbehaviorast→−∞).Inparticularletεαbethefunctiondeﬁnedbyεα(t)=e−αtα>0.Itsatisﬁestheabove-mentionedpropertythustheproductεαTexists.ThiswillbeusefullatertodeﬁnetheLaplacetransform.ForT=δwehaveδf=f(0)δ.(12.20)8.Thezerothorderderivativeoffisfitselfofcourse.Appendix1:Analysis385ConvolutionofdistributionsTheconvolutionproductoftwodistributionsgeneralizestheconvolutionproductoftwofunctions.Forthisproducttobemeaningfulitisnecessarythatthesupportsofthesedistributionssatisfycertainproperties.Inparticularifhisacompactlysupporteddistributiontheproducth∗ucanbedeﬁnedandthemapu(cid:3)→h∗uiscontinuousconvolutionoperator(withkernelh)fromD(cid:2)intoD(cid:2).Wecancalculateh∗uwhenu∈Dandthenwhenu∈D(cid:2)byusingthedensityofDinD(cid:2).Usingthe“integralnotation”ofdistributionswecanformallywritethat(whenh∗uiswell-deﬁned)(h∗u)(t)=(cid:1)+∞−∞h(t−τ)u(τ)dτasifbothhanduwerefunctions.Forexampleifu∈D(cid:1)+∞−∞δ(τ)u(t−τ)dτ=u(t)accordingto(12.18)thusδ∗u=u(12.21)andthisresultcannowbeextendedtothecaseu∈D(cid:2).AsaresulttheDiracdistributionisthe“unitelement”fortheconvolutionproduct.ConvolutionalgebrasofD(cid:2)+andS(cid:2)+IfbothhandubelongtoT(cid:2)+(T=DorS)thentheconvolutionproducth∗uiswell-deﬁnedandbelongstoT(cid:2)+.ThismakesD(cid:2)+andS(cid:2)+convolutionalgebras(thesealgebrasarecommutativelikeK+).SinceδbelongstothesetwosetsD(cid:2)+andS(cid:2)+areunitaryalgebras(seesection13.1.1)contrarytoK+forδ/∈K+.Usingthe“integralnotation”weobtainwhenhandubelongtoT(cid:2)+:(h∗u)(t)=(cid:2)t+0−h(t−τ)u(τ)dτ.(12.22)DerivativeandconvolutionWehavealsoddt(h∗u)=dhdt∗u=h∗dudt.(12.23)Inparticulartakingh=δ˙u=˙δ∗uInotherwordsdifferentiatingadistributionisjustastakingitsconvolutionproductwith˙δ.386LinearSystemsTranslationofadistributionConsiderﬁrstafunctionf:t(cid:3)→f(t).Thefunctionf(τ)deﬁnedbyf(τ)(t)=f(t−τ)isatranslation(orshift)offoftimeτ.Forτ>0thisshiftisadelayorlag;forτ<0itisanadvanceoralead.Letusnowdeﬁnethe“Diracdistributionatτ”:itisthetempereddistributionδ(τ)withsupport{τ}suchthat1δ(τ)φ2=φ(τ)φ∈S.Theintegralexpressionofthisdeﬁnitionisφ(τ)=(cid:1)+∞−∞δ(τ)(t)φ(t)dt.Thusφ(τ)=(cid:1)+∞−∞δ(t)φ(t+τ)dt=(cid:1)+∞−∞δ(t−τ)φ(t)dt(afterachangeofvariable)thus(abusingthelanguageaswealreadydid)δ(τ)(t)=δ(t−τ).(12.24)Letu∈D;weget(δ(τ)∗u)(t)=(cid:2)+∞−∞δ(τ)(ς)u(t−ς)dς=u(t−τ).Inotherwordstheshiftoperatoru(cid:3)→u(τ)isnothingbuttheconvolutionoperatoru(cid:3)→δ(τ)∗u.Wecannowextendthisdeﬁnitiontothecasewhereu∈D(cid:2)bydensityofD(cid:2)inD.AsaresultforanydistributionT∈D(cid:2)theshifteddistributionT(τ)isdeﬁnedbyT(τ)=δ(τ)∗T.(12.25)DiraccombLetT>0bearealnumberwhichweinterpretasaperiod.TheDiraccombTisdeﬁnedbyT=+∞(cid:12)n=−∞δ(nT).Thereforeaccordingto(12.24)T(t)=+∞(cid:11)n=−∞δ(t−nT).(12.26)OnecaneasilyshowthatthisseriesconvergesinS(cid:2)thusdeﬁningatempereddistribution.Appendix1:Analysis38712.3.FourierLaplaceandztransforms12.3.1.FouriertransformsofdistributionsFouriertransforminL1Letu∈L1;thuswecancalculateforevery“angularfrequency”ωFu(ω)=(cid:1)+∞−∞u(t)e−iωtdt.(12.27)ThefunctionFu(fromRintoC)iscalledtheFouriertransformofuandFistheFouriertransform.ThistransformisC-linear.Wehave|(Fu)(ω)|≤(cid:1)+∞−∞))u(t)e−iωt))dt=(cid:1)+∞−∞|u(t)|dtandthusFu∈L∞and(cid:23)Fu(cid:23)∞≤(cid:23)u(cid:23)1.REMARK433.–ThereexistsvariousdeﬁnitionsoftheFouriertransform.Forexamplewecandeﬁneitasafunctionoffrequencyν=ω/2πinsteadoftheangularfrequencyω;thisleadsustoreplace(12.27)byFu(ν)=(cid:2)+∞−∞u(t)e−i2πνtdt.(12.28)FouriertransforminSSinceS⊂L1whatwehavediscussedsofarapplies.Butonecanalsoshowthefollowingremarkableproperties:–TheimageofSbyFisSandmorespeciﬁcallyFisanisomorphismfromthespaceSontoitself(i.e.anautomorphismofS).–ForanyfunctionˇubelongingtoS(ˇu:ω(cid:3)→ˇu(ω))wewrite(Fˇu)(t)=(cid:2)+∞−∞ˇu(ω)eiωtdω.Thenwehavethereciprocityformulavalidforanyfunctionu∈S:12πFFu=uinotherwords:F−1=12πF.(12.29)REMARK434.–If(12.28)isusedasthedeﬁnitionoftheFouriertransformweobtainthesimplerrelationF−1=F.388LinearSystemsFouriertransforminS(cid:2)IfbothuandφbelongtoSweimmediatelyget Fuφ!= uFφ!.(12.30)Thisrelationisnowtakenasadeﬁnitioninthecasewhereu∈S(cid:2);thismakesFanautomorphismofS(cid:2).Usingthisdeﬁnitionitiseasilyshownthat(Fδ)(ω)=1;(Fδ(τ))(ω)=e−iωτ(12.31)anditfollowsthat(FT)(ω)=+∞(cid:11)n=−∞e−iωnT.(12.32)Thereciprocityformula(12.29)isvalidinS(cid:2).Byapplyingitto(12.31)weget(cid:2)+∞−∞eiω(t−τ)dω=2πδ(t−τ)(12.33)anexpressionthatbecomesclearerfromthepointofviewofphysicsifweexchangetherolesplayedbythetimeandtheangularfrequencyandbychangingtto−t:(cid:1)+∞−∞eit(ω0−ω)dt=2πδ(ω−ω0).(12.34)TheFouriertransformofthesinusoidalsignalt(cid:3)→eiω0tisthusthedistributionω(cid:3)→2πδ(ω−ω0)(denotedhereasafunctionwiththeusualabuseoflanguage).ThesupportoftheFouriertransformofasignaliscalleditsspectrum.Thespectrumofthesinusoidalsignalconsideredaboveisthepoint{ω0}.Suchaspectrumiscalleda“rayspectrum”(withauniquerayatω0).FouriertransforminL2LetxandybetwofunctionsbelongingtoS.Wehave FuFy!2=(cid:2)+∞−∞(cid:2)+∞−∞(cid:2)+∞−∞_u(t)y(τ)eiω(t−τ)dtdτandaccordingto(12.33)weobtainthePlancherel–Parsevalformula: FuFy!2=2π uy!2.ThisequalityallowsustoextendtheFouriertransformtoL2(forSisdenseinL2)andthePlancherel–Parsevalformulaisstillvalidinthatspace.ThismakesFanautomorphismofL2.REMARK435.–WithDeﬁnition(12.28)oftheFouriertransformthePlancherel–Parsevalrelationbecomes FuFy!2= uy!2.Appendix1:Analysis389ExchangetheoremAccordingto(12.12)L1isaconvolutionalgebraandweknowthattheFouriertransformisdeﬁnedinL1.LethandubetwofunctionsofL1;thenF(h∗u)(ω)=(cid:2)+∞−∞(cid:2)+∞−∞(cid:2)+∞−∞h(τ)u(t−τ)e−iωτe−iω(t−τ)dtdτdωandthusF(h∗u)=FhFu.(12.35)ThisistheExchangetheoremwhichisoneofthefundamentalpropertiesoftheFouriertransform(transformationoftheconvolutionproductintoanordinaryproduct).Weseethatthistheoremalsoappliestothefollowingcases:–bothhandubelongtoS(cid:2)+;–hisacompactlysupporteddistributionanduisatempereddistribution.TheSecondExchangetheoremLet˜h=Fhand˜u=Fu.Accordingto(12.35)and(12.29)weobtaintheSecondExchangetheorem(whichisvalidwhen˜hand˜usatisfythesamehypothesesashanduaboverespectively):F(cid:9)˜h˜u(cid:10)=12πF˜h∗F˜u.(12.36)12.3.2.FourierseriesPeriodicdistributionsAfunctionudeﬁnedinRisperiodicwithperiodT>0(orinabbreviationisT-periodic)ifu(t+nT)=u(t)foreveryrealnumbertandeveryrationalintegerninotherwordsifallshiftedfunctionsu(nT)=δ(nT)∗uareequaltou.Thisdeﬁnitionwiththislastformulationcanbeextendedtothecaseofadistributionu∈D(cid:2).Everyperiodicdistributionistempered.TrigonometricseriesLet(an)beasequenceofcomplexnumbers.Wesaythatthissequenceisslowlyincreasingifthereexistaconstantcandanintegerksuchthat|an|≤c|n|k(12.37)390LinearSystemsforalln(cid:5)=0.ThesetofallslowlyincreasingsequencesisaC-vectorspacedenotedbys(cid:2)(forajustiﬁcationofthisnotationsee[95]).Foreverysequence(an)∈s(cid:2)theseriesu(t)=+∞(cid:11)n=−∞ane−i2πTnt(12.38)convergesinS(cid:2).(Intheexpression(12.38)weabusethelanguageandthecorrectformulationisu=(cid:11)+∞n=−∞ane−i2πTn•.)Indeedletφ∈S;then3e−i2πTn•φ4(cid:1)(cid:1)+∞−∞e−i2πTntφ(t)dt=Fφ(cid:15)2πTn(cid:16).WeknowthatFφ∈SthusthesequencewithgeneraltermanFφ(cid:15)2πTn(cid:16)decreasesfasterthan1n2atinﬁnityforexampleandthustheseries(cid:11)+∞n=−∞an3e−i2πTn•φ4isabsolutelyconvergent.Wethuscannowdeﬁnethesumofseries(12.38)by: uφ!=(cid:11)+∞n=−∞an3e−i2πTn•φ4asusual.WeimmediatelyseethatuisaT-periodicdistribution.OnecanshowthatanyT-periodicdistributionuisofthisformandseries(12.38)iscalledtheFourierseriesexpansionofu.FouriercoefﬁcientsTheabovecoefﬁcientsanaretheFouriercoefﬁcientsofu.Thedifﬁcultyincalculatingthemisthatseries(12.38)isingeneralnotconvergentinthe“classic”sense.1)ConsiderﬁrstthecasewheretheT-periodicdistributionuiswrittenasu=(cid:12)n∈ZU(nT)(withU(nT)=δ(nT)∗U)wherethedistributionUhasitssupportincludedinanopenintervaloftheform(αα+T);wethenhaveforeverynan=1T(cid:2)α+Tαu(t)ei2πTntdt.(12.39)2)Inthegeneralcasewecanproceedasfollows:integratetheseriesu−a0termbytermk+2times(whereksatisﬁes(12.37)).Wethenobtainaquantityfwhichisaprioriatempereddistributionandiswrittenasf=(cid:12)n(cid:12)=0bne−i2πTn•Appendix1:Analysis391wherebn=an(−i2πTn)k+2.Thesequence(bn)tendsto0asfastas1n2asntendstoinﬁnitythustheseriesfisnormallyconvergent(thatistosayifwewritegn=bne−i2πTn•theseries(cid:11)n(cid:23)gn(cid:23)∞=(cid:11)n|bn|isconvergent)andwecanwriteforanyrealnumberα(cid:2)α+Tαf(t)ei2πTmtdt=(cid:12)n(cid:12)=0bn(cid:2)α+Tαe−i2πT(n−m)tdt=TbmthereforetheFouriercoefﬁcientsbnoffcanbecalculatedaccordingtobn=1T(cid:2)α+Tαf(t)ei2πTntdt.NotethatfisacontinuousT-periodicfunction.Oncethebn’sareknownwehavefromtheabovean=(cid:15)−i2πTn(cid:16)k+2bnn(cid:5)=0.CaseofDiraccomb;PoissonsummationformulaTheDiraccombTisobviouslyaT-periodicdistribution.Wecanapplyformula(12.39)withα=−T/2andweobtainan=1T(cid:2)T2−T2+∞(cid:12)k=−∞δ(t−kT)ei2πTntdt=1T(cid:2)T2−T2+∞(cid:12)k=−∞δ(t−kT)ei2πnkdt=1T(cid:2)T2−T2δ(t)dtthusan=1Tforeveryn.Asaresult+∞(cid:12)n=−∞δ(t−nT)=1T+∞(cid:12)n=−∞e−i2πTnt.(12.40)ThusreplacingtbyωandTby2πT:+∞(cid:12)n=−∞e−iωnT=2πT+∞(cid:12)n=−∞δ(ω−n2πT).Consequentlyaccordingto(12.32)wehavethePoissonsummationformula:F(cid:20)+∞(cid:11)n=−∞δ(t−nT)&(ω)=2πT+∞(cid:11)k=−∞δ(ω−k2πT).(12.41)392LinearSystemsInotherwordstheFouriertransformofthe(timedomain)T-periodicDiraccombisuptoafactorof2πTthe(frequencydomain)2πT-periodicDiraccomb(becauseweworkwithangularfrequencies:ifweworkwithfrequenciesν=ω2πthisperiodicitybecomes1T).REMARK436.–ThePoissonsummationformulaisoftenexpressedinadifferentbutequivalentmanner.Accordingto(12.30)wehaveforanyfunctionφ∈S FTφ!= TFφ!anequalitywhichmakesitpossibletodetermineFTfromT.(i)WithT=2πitcanbewrittenaccordingto(12.41)as(cid:12)k∈Zφ(k)=(cid:12)n∈ZFφ(2nπ).(ii)IftheFouriertransformisdeﬁnedby(12.28)thissummationformulaissimpler(andthisisthemostclassicform):(cid:12)k∈Zφ(k)=(cid:12)n∈ZFφ(n).Itremainsvalidmoregenerallyincasesthanφ∈S:see([35]sectionXXII.12).FourierseriesexpansionandFouriertransformTheT-periodicdistributionudeﬁnedby(12.38)admitsaFouriertransformandaccordingto(12.33)wehave(Fu)(ω)=+∞(cid:12)n=−∞an(cid:2)+∞−∞e−i(ω−2πTn)tdt=+∞(cid:12)n=−∞2πanδ(cid:21)ω−n2πTt(cid:22)(12.42)where(an)∈s(cid:2).Converselyif(an)∈s(cid:2)theright-handsideof(12.42)istheFouriertransformofaT-periodicdistributionuthusthisFouriertransformbelongstoS(cid:2).Theformula(12.42)whichgeneralizes(12.41)showsthecloserelationshipthatexistsbetweentheFourierseriesexpansionandtheFouriertransformwhenwedealwithdistributions.Wehaveindeedobtainedthefollowingresult:THEOREM437.–AT-periodicdistribution(whichwecaninterpretasatime-domainsignal)hasarayspectrumtheseraysareintheangularfrequencydomainseparatedby2πTandweightedwiththeFouriercoefﬁcientsan.Appendix1:Analysis39312.3.3.FouriertransformsofsequencesDeﬁnitionLeta=(an)∈s(cid:2).ItsFouriertransformisthesumoftheseries(Fa)(θ)=+∞(cid:11)n=−∞ane−inθ.(12.43)Thisisseries(12.38)whereonereplaces2πtTwithθ.AccordingtotheaboveFaisa2π-periodicdistribution.ItthusadmitsaFourierseriesexpansionofform(12.43).AsaresultFisanisomorphismfroms(cid:2)ontothespaceof2π-periodicdistributions.ThecalculationoftheinverseFouriertransformisnothingbutthecalculationoftheFouriercoefﬁcientsofa2π-periodicdistribution.Inthecasewherethesequence(an)convergesto0asfastas1n2forntendingtoinﬁnitywehaveshownthatan=12π(cid:1)π−π(Fa)(θ)einθdθ.(12.44)Fouriertransforminl2Ifbotha=(an)andb=(bn)belongtol2wecancalculateaccordingto(12.4) ab!2=(cid:11)+∞n−∞anbn.By(12.44)thisquantityiswritingα=Faandβ=Fb:+∞(cid:12)n−∞anbn=12π(cid:2)π−π¯α(θ)β(ς)+∞(cid:12)n−∞ein(ς−θ)dθdς.Inadditionaccordingto(12.40)+∞(cid:12)n−∞ein(ς−θ)=+∞(cid:12)n=−∞δ(θ−ς−2πn).WeﬁnallyobtainthePlancherel–Parsevalformula ab!2=12π(cid:1)π−π(cid:15)Fa(cid:16)(θ)(Fb)(θ)dθ.(12.45)OnecanshowthattheFouriertransformisanisomorphismfroml2ontothespaceof2π-periodicfunctionswhicharesquare-integrableon[−ππ]whenthisspaceisequippedwiththescalarproductintheright-handsideof(12.45).Theconvolutionalgebras(cid:2)+Lets(cid:2)+bethespaceofallpositivelysupportedsequencesbelongingtos(cid:2).Oneeasilyshowsthatforanytwoelementsaandbofs(cid:2)+theconvolutionproducta∗biswell-deﬁnedandbelongstos(cid:2)+;thuss(cid:2)+isaconvolutionalgebrahavingthesequenceδ0asaunitelement;thisoneisdeﬁnedby(δ0)n=1ifn=0and(δ0)n=0ifnot.394LinearSystemsFouriertransformofaconvolutionproductConsidertwosequencesa=(an)andb=(bn)theconvolutionproductandtheFouriertransformofwhicharewell-deﬁned;aandbcanbeelementsofs(cid:2)+forexample.Wehave[F(a∗b)](θ)=+∞(cid:12)n=−∞+∞(cid:12)k=−∞akbn−ke−inθ=+∞(cid:12)k=−∞+∞(cid:12)n=−∞ake−inθkbn−ke−i(n−k)θ=(Fa)(θ)(Fb)(θ);asaresultwegettheExchangetheorem(inthesameformas(12.35)):F(a∗b)=FaFb.(12.46)12.3.4.LaplacetransformIntroductionTheLaplacetransform(offunctionsorofdistributions)isanextensionoftheFouriertransform.Asaﬁrststepwewilldiscusstheunilateral(orone-sided)Laplacetransformapplicabletopositivelysupportedfunctionsordistributions.Thebilateral(ortwo-sided)Laplacetransformwhosetheoryismoredifﬁcultwillonlybebrieﬂymentionedattheendofthissection(foracompletediscussionofthisquestionsee[106]ChapterVIII).Wehavedeﬁnedinsection12.2.3thefunctionεα:t→e−αt.LetT∈D(cid:2)+andsupposearealnumberα0existssuchthatεα0T∈S(cid:2)+.Alsoletα≥α0.Wehaveεα=εα0εα−α0anditisobviousthatεα0εα−α0T∈S(cid:2)+.ThusεαT∈S(cid:2)+foranyα≥α0.ThuswecandeﬁnethesetA+ofalldistributionsT∈D(cid:2)+forwhichthereexistsarealnumberα0suchthatεαT∈S(cid:2)+wheneverα≥α0.ItisclearthatA+isacommutativeconvolutionalgebra.Itisunitarybecauseδ∈A+;furthermoreS(cid:2)+⊂A+⊂D(cid:2)+.AswewillseeA+isthespaceofdistributionsadmittingaunilateralLaplacetransform.Appendix1:Analysis395DeﬁnitionLetu∈A+andγ=inf(cid:31)α∈R:εαu∈S(cid:2)+ .TherealnumberγiscalledtheabscissaofconvergenceoftheLaplacetransformofu.Thesetofthoseα∈Rforwhichεαu∈S(cid:2)+isthereforetheinterval|γ+∞)anintervalthatcanbeeitheropenorclosedatγ.TheLaplacetransformˆu=Luisafunctionofthecomplexvariabledeﬁnedforα∈|γ+∞)byˆu(α+iω)=F{εαu}(ω).(12.47)Thereforeaccordingto(12.27)and(12.15)ˆu(s)=(cid:2)+∞0−u(t)e−stdtRe(s)∈|γ+∞).(12.48)Strictlyspeakingthisexpressiononlymakessensewhenuisafunctionbutusingthe“integralnotation”(12.14)itcanbeextendedtothecasewhereuisadistribution.Onecanshowthatˆuisholomorphic(seesection12.4.1)intheopenhalf-planeRe(s)>γ.ItisclearthattheLaplacetransformationLisC-linear.Alsonotethatifu∈S(cid:2)+thenˆu(iω)=(Fu)(ω).UsefulLaplacetransformsuˆu(s)γ11s0δ1−∞δ(τ)e−τs−∞˙δs−∞δ(n)sn−∞e−αt1(t)1s+α−αtme−αt1(t)m!(s+α)m+1−αe−αtsin(ωt)1(t)ω(s+α)2+ω2−αe−αtcos(ωt)1(t)s+α(s+α)2+ω2−α396LinearSystemsExchangetheoremWeeasilyextendthe“Exchangetheorem”toLaplacetransforms:ifhandubothbelongtoA+andiftheirLaplacetransformshaveabscissaeofconvergenceγ1andγ2respectivelythenh∗u∈A+andthisconvolutionproducthasaLaplacetransformwhoseconvergenceabscissaisγ=max(γ1γ2);furthermoreL(h∗u)=LhLu.(12.49)LaplacetransformsofderivativesCaseofapositivelysupporteddistributionLetu∈A+.Weknowthat˙u=˙δ∗uthusL(˙u)(s)=sˆu(s)(12.50)Byinductionweobtainforanynon-negativeintegernL(cid:15)u(n)(cid:16)(s)=snˆu(s).(12.51)CaseofadifferentiablefunctionConsidernowafunctionudeﬁnedanddifferentiableinanopenneighborhoodof[0+∞)andthederivativeofwhichislocallyintegrable.Bydeﬁnitionthe(unilateral)LaplacetransformofuistheLaplacetransformofthepositivelysupporteddistributionu+=1uifu+belongstoA+.AsaresultLu(cid:1)Lu+.ThisLaplacetransformisthusgivenbytheformula(12.48)(where0−canbereplacedby0).Wehavedu+dt=˙u1+uδ(Leibnizformula)fromwhichaccordingto(12.20)du+dt=˙u1+δu(0).Accordingto(12.50)L(cid:9)du+dt(cid:10)(s)=s(Lu+)(s)thuss(Lu+)(s)=L(˙u)(s)+u(0)i.e.L(˙u)(s)=sˆu(s)−u(0).(12.52)Appendix1:Analysis397Extension:caseofthesumofanntimescontinuouslydifferentiablefunctionandofapositivelysupporteddistributionWeareoftenledincontroltheorytoconsidersignalsoftheformu=f+T(12.53)wherefisafunctionthatisdeﬁnedandntimescontinuouslydifferentiableinanopenneighborhoodof[0+∞)(n≥1)9andwhereT∈A+.Asintheabovethe(unilateral)LaplacetransformofucanbedeﬁnedastheLaplacetransformofthepositivelysupporteddistributionu+=f++Tandisthusgivenbytheformula(12.48)butthistimeitisessentialuse0−asthelowerlimitofintegration.Nowuandfhavethesamerestrictiontoeveryopeninterval(−ε0)10whereε>0isasufﬁcientlysmallrealnumber.Thereforetherestrictionofutosuchanintervalisanntimescontinuouslydifferentiablefunctionandfurthermore:u(i)(cid:15)0−(cid:16)=f(i)(0)0≤i≤n−1.(12.54)Writing(12.53)andthentakingtheLaplacetransformoftheobtainedexpressionwegetL(˙u)(s)=L(cid:9)˙T(cid:10)(s)+L(cid:9)˙f(cid:10)(s).Asaresultaccordingto(12.50)and(12.52)L(˙u)(s)=sˆT(s)+sˆf(s)−f(0)thusfrom(12.54)L(˙u)(s)=sˆu(s)−u(cid:15)0−(cid:16).Byinductionwecanestablishthat:L(cid:9)u(n)(cid:10)(s)=snˆu(s)−n−1(cid:12)i=0sn−1−iu(i)(cid:15)0−(cid:16).9.Morepreciselyitsufﬁcestoassumethatuisntimesdifferentiableinanopenneighborhoodof[0+∞)andthatitsnthderivativeislocallyintegrable.10.ThisisintuitivelyclearsinceTispositivelysupported.Forageneraldeﬁnitionofthenotionofrestrictionofadistributiontoanopenset(see[35]section17.13).398LinearSystemsDenotethequantityu(i)(0−)(i≥0)by∂i0u0;theninparticular∂00=1andu0=u(0)(0−)(∂0canviewedasadifferentialoperatoronthevaluesatinstant0−).11Wehave:n−1(cid:12)i=0sn−1−iu(i)(cid:15)0−(cid:16)=n−1(cid:12)i=0sn−1−i∂i0u0=sn−∂n0s−∂0u0.thereforeL(cid:15)u(n)(cid:16)(s)=snˆu(s)−sn−∂n0s−∂0u0.(12.55)InverseLaplacetransformBromwich–SchwartztheoremThistheoremcharacterizestheLaplacetransformsofdistributionsandisstatedinthefollowingmanner([106]sectionVIII.4):considerafunctionofthecomplexvariables(cid:3)→f(s)whichisholomorphicinthehalf-planeRe(s)>γ;thisfunctionistheLaplacetransformofadistributionu∈A+ifandonlyifthereexistsarationalintegernandahalf-planeRe(s)(cid:3)β>γsuchthatthefunctions→(1+|s|)−nf(s)isboundedinthishalf-plane.InversionofaLaplacetransformCase1:BromwichformulaSupposethatintheaboven=−2thusf(s)decreasesattheinﬁnityatleastasfastas1s2(iff(s)isarationalfunctionthismeansthatitisofrelativedegreeofatleast2:seesection13.6.1).Thusthereexistsaconstantcsuchthat|f(α+iω)|≤cα2+ω2forα>min(β0).Thereforethefunctionoftherealvariablefα:ω→f(α+iω)belongstoL1andwecancalculate12π(cid:15)Ffα(cid:16)(t)=12π(cid:2)+∞−∞fα(iω)eiωtdω.Afterthechangeofvariables=α+iωweget:12π(cid:15)Ffα(cid:16)(t)=e−αtu(t)11.Thisnotionisimproperbecauseforexample˙u(cid:15)0−(cid:16)cannotbederivedfromu(cid:15)0−(cid:16).Itwouldbemorerigorous–butmisleading–todenotetheabovequantity∂i0u0as∂i0u.Appendix1:Analysis399whereu(t)=12πiα+i∞(cid:2)α−i∞f(s)estds.(12.56)ThusF{εαu}(ω)=f(α+iω)whichshowsthatf(s)=ˆu(s)and(12.56)istheformulaoftheinverseLaplacetransformcalledtheBromwichformula.Wewillseeinsection12.4.4thatthisfunctionuiscontinuousandpositivelysupported.Case2:Schwartz’smethodSupposenowthatn>−2.Wehaves−nf(s)=s2g(s)whereg(s)=s−(s+2)f(s).Thefunctions(cid:3)→s2g(s)isboundedinthehalf-planeRe(s)(cid:3)α>min(β0)thuswecanapplytheBromwichformulatothefunctiong:thepositivelysupportedfunctionvdeﬁnedbyv(t)=12πi(cid:2)α+i∞α−i∞g(s)estdsadmitsaLaplacetransformg(s).Butsincef(s)=sn+2g(s)f(s)istheLaplacetransformofthedistributionv(n+2).ThisshowsthateverydistributionbelongingtoA+isaﬁniteorderderivativeofacontinuousfunction.ThisalsoshowsthattheLaplacetransformisabijectionfromA+ontothesetoffunctionsofthecomplexvariablecharacterizedbytheBromwich–SchwartztheoremasetwhichwecanthusdenoteasLA+.SinceA+isaconvolutionalgebraandtheLaplacetransformtransformstheconvolutionproductintotheordinaryproductLA+isanalgebrafortheordinaryproduct(ascanalsobedirectlyveriﬁed).InitialvaluetheoremandﬁnalvaluetheoremLetubeafunctionadmittingaLaplacetransformˆu.Onecanprovethefollowingresults[95](becarefulaboutthehypotheses!):1)Ifuissuchthatlimt→+∞u(t)existsthentheabscissaofconvergenceγofˆusatisﬁesγ≤0andwehavetheﬁnalvaluetheorem:lims∈Rs→0+sˆu(s)=limt→+∞u(t).2)Ifuadmitsalimitat0fromtherightdenotedasu(0+)wehavetheinitialvaluetheorem:lims∈Rs→+∞sˆu(s)=u(0+).400LinearSystemsBilateralLaplacetransformLetusnowgeneralizethemethodusedinthebeginningofthissection.Letu∈D(cid:2);thesetofrealnumbersαsuchthatεαu∈S(cid:2)isanintervalΓ=|γ−γ+|ofR(whichcanbedependingonthecaseeitheropenorclosedatγ−oratγ+).IfΓ(cid:5)=∅thesetBc={s∈C:Re(s)∈Γ}isthebandofconvergenceoftheLaplacetransformofu.Thistransformdenotedasˆu=Luisafunctionofthecomplexvariabledeﬁnedforα∈|γ−γ+|byrelation(12.47)oralsoby(withtheusualabuseoflanguage)ˆu(s)=+∞(cid:2)−∞u(t)e−stdts∈Bcwhichgeneralizes(12.48);ˆu(s)isholomorphicintheinterior˚BcofBc(if˚Bc(cid:5)=∅).Ifu∈D(cid:2)+thenγ+=+∞andthenwecomebacktotheunilateralLaplacetransform.ExtensionoftheExchangetheoremLetΓbeanon-emptyopenintervalofRandletS(cid:2)(Γ)bethesetofalldistributionsu∈D(cid:2)suchthatεαu∈S(cid:2)foreveryα∈Γ.OnecanshowthatS(cid:2)(Γ)isacommutativeconvolutionalgebra.The“Exchangetheorem”(12.49)isstillvalidwhenbothhandubelongtoS(cid:2)(Γ).Itfollowsthat(12.51)isalsostillvalidbecauseδ(n)∈S(cid:2)(Γ)foreverynon-emptyopenintervalΓofRcontainingzero.*Paley–Wiener–SchwartztheoremWedenotebyEthespaceofindeﬁnitelydifferentiablefunctionsfromRintoC.Thisspaceisequippedwiththefollowingtopology:asequence(ϕn)convergestoϕinEifforeveryintegerk≥0ϕ(k)nconvergesuniformlytoϕ(k)oneverycompactset.OnecanshowthatEisaFréchetspace.ItsdualE(cid:2)isthespaceofcompactlysupporteddistributions.UsingthePaley–Wiener–Schwartztheorem([106]sectionVII.8)([35]sectionXXII.18)onecancharacterizetheelementsofE(cid:2)usingtheirLaplacetransforms.Foranentirefunctionf(s)(seesection12.4.1below)tobetheLaplacetransformofadistributionwithsupportincludedin[−aa](a≥0)itisnecessaryandsufﬁcientthatthereexistsanintegern≥0andaconstantc≥0suchthatforeverys∈C|f(s)|≤c(1+|s|)nea|Res|.(comparewiththeBromwich–Schwartztheorem.)Appendix1:Analysis40112.3.5.z-transformThez-transformplaysthesameroleforsequencesastheLaplacetransformfordistributions.LiketheLaplacetransformitisageneralizationoftheFouriertransform.Weﬁrstwillconsiderthe“unilateral”z-transform.The“bilateral”z-transformisdiscussedattheendofthissection.DeﬁnitionLetx=(xn)n∈Zbeasequenceofcomplexnumbersassumedtobepositivelysupported(section12.2.1).Letρ=inf(cid:31)r>0:(cid:15)xnr−n(cid:16)∈s(cid:2) andsupposeρ<+∞.Foreveryr∈|ρ+∞)(wheretheinterval|ρ+∞)isopenorclosedatρdependingonthesituation)thesequence(xnr−n)belongstos(cid:2).ThissequencethusadmitsaFouriertransformF(cid:31)xnr−n (θ)=+∞(cid:12)n=0xnr−ne−inθ=+∞(cid:12)n=0xn(cid:15)reiθ(cid:16)−n.Letustakethepowerseriesinz−1X(z)=+∞(cid:12)n=0xnz−n.(12.57)Accordingtotheabovethisseriesconvergesforeveryzsuchthat|z|∈|ρ+∞)sinceX(cid:15)reiθ(cid:16)=F(cid:31)xnr−n (θ).(12.58)ThefunctionX:z(cid:3)→X(z)ofthecomplexvariableziscalledthe(unilateral)z-transformofxitisdeﬁnedfor|z|∈|ρ+∞)andisholomorphicintheopenset|z|>.Inthisopensettheseries(12.57)isabsolutelyconvergent.TherealnumberiscalledtheradiusofconvergenceofX.Thez-transformZ:x(cid:3)→XisC-linear.z-transformofanadvancedsequenceWedeﬁnetheadvanceoperatorqalsocalledtheshift-forwardoperatoractsonsequencesofcomplexnumbersinthefollowingmanner:qxn=xn+1.(12.59)402LinearSystemsThez-transformofthesequenceqxisZ{qx}(z)=(cid:11)+∞n=0xn+1z−n=z[X(z)−x0].Generalizingthisrationale:Z(cid:31)qkx (z)=zk#X(z)−k−1(cid:12)n=0xnz−n$.(12.60)ExchangetheoremLetx=(xn)andy=(yn)betwopositivelysupportedsequencesthez-transformsofwhichhavearadiusofconvergenceof1and2respectively.Thenthepositivelysupportedsequencex∗yhasaz-transformwhoseradiusofconvergenceisρ=max(1ρ2)andZ{x∗y}(z)=X(z)Y(z).(12.61)Inparticularletδk(k≥0)bethesequencedeﬁnedbyδk(n)=1ifn=kδk(n)=0ifnot12.Thesequenceδkispositivelysupporteditsz-transformisz−k.Foreverysequencex=(xn)δk∗xisthedelayedsequenceq−kxn=xn−k.AsaresultZ(cid:31)q−kx (z)=Z{δk∗x}(z)=z−kX(z).(12.62)Inversez-transformAccordingtoequation(12.58)whichestablishestherelationshipbetweentheFouriertransformandthez-transformwehaveforanyr>xnr−n=F−1X(cid:15)reiθ(cid:16)=12π(cid:2)π−πX(cid:15)reiθ(cid:16)einθdθfromwhichafterthechangeofvariablez=reiθxn=12πi5|z|=rX(z)zn−1dz.(12.63)12.Inordernottocomplicatethenotationwewritethesequenceδkhereasafunctionofn.Appendix1:Analysis403Usefulz-transformsWedeﬁnetheunitstep1asthesequencedeﬁnedby1n=1ifn≥01n=0ifn<0.(xn)X(z)δ010δkz−k01zz−11(an1n)zz−a|a|(n1n)z(z−1)21(cid:15)n21n(cid:16)z(z+1)(z−1)31sin(nωT)1nzsinωTz2−2zcosωT+11cos(nωT)1nz2−zcosωTz2−2zcosωT+11(12.64)InitialvaluetheoremandﬁnalvaluetheoremOnecanshowthefollowingresults[95]:1)lim|z|→+∞X(z)=x0(initialvaluetheorem).2)Iflimn→+∞xnexiststhenX(z)hasaradiusofconvergence≤1andlimz∈Rz→1+(z−1)X(z)=limn→+∞xn(ﬁnalvaluetheorem).Bilateralz-transformLetx=(xn)beasequenceofcomplexnumberslet))ρ−ρ+))=(cid:31)r>0:(cid:15)xnr−n(cid:16)∈s(cid:2) (12.65)andsuppose|ρ−ρ+|(cid:5)=∅.Foreveryr∈|ρ−ρ+|thesequence(xnr−n)belongstos(cid:2)andthusadmitsaFouriertransformF(cid:31)xnr−n (θ)=+∞(cid:12)n=−∞xnr−ne−inθ=+∞(cid:12)n=−∞xn(cid:15)reiθ(cid:16)−n.TheseriesX(z)=+∞(cid:12)n=−∞xnz−n(12.66)404LinearSystemsconvergesforallzsuchthat|z|∈|ρ−ρ+|andsatisﬁestherelation(12.58).Forz=reiθwithrequaltoρ−orρ+(ifoneoftheseelementsbelongsto|ρ−ρ+|)theseries(12.66)convergencesinS(cid:2).LetCc={z∈C:|z|∈|ρ−ρ+|}.ThissetCciscalledtheannulusofconvergenceofX.ThefunctionofthecomplexvariableX:z(cid:3)→X(z)iscalledthe(bilateral)z-transformofxandisdeﬁnedinCc.Intheinterior˚CcofCc(ifitisnon-empty)theseries(12.57)isabsolutelyconvergentandX(z)isholomorphic.Ifthissequencexispositivelysupported(12.66)becomesidenticalto(12.57)andρ+=+∞.ExtensionoftheExchangetheoremWearenowgoingtoextendtheExchangetheorem(12.61)tothecaseofthebilateralz-transformaswehavedoneatsection12.3.4forthebilateralLaplacetransform.LetΓbeanon-emptyopenintervalincludedin[0+∞)andlets(cid:2)(Γ)(resp.l1(Γ))bethesetofsequencesofcomplexnumbers(xn)suchthat(r−nxn)∈s(cid:2)(resp.(r−nxn)∈l1)foreveryr∈Γ.Aseasilyshowns(cid:2)(Γ)=l1(Γ)andsincel1isaconvolutionalgebra(section12.2.1)s(cid:2)(Γ)isonetoo.TheExchangetheorem(12.61)isalsovalidwhenxandybothbelongtos(cid:2)(Γ).WenowdeducethatZ(cid:31)qkx (z)=zkX(z)(12.67)whereZdenotesthebilateralz-transform(ofcoursethisequalitydoesnotcontradict(12.60)andthereaderisrequestedtowonderwhy).12.4.FunctionsofonecomplexvariableAsseenaboveaLaplacetransformandaz-transformarefunctionsofonecomplexvariableanditisthusimportanttostudysomeofthepropertiesofthesefunctions.12.4.1.HolomorphicfunctionsDeﬁnitionofaholomorphicfunctionAfunctions→f(s)isholomorphicinanon-emptyopensubsetΩofCifforanys0∈Ω˙f(s0)=lims→s0s(cid:12)=s0f(s)−f(s0)s−s0exists.ThesetofallholomorphicfunctionsinΩisdenotedbyO(Ω).Appendix1:Analysis405AnalyticfunctionAfunctionofthecomplexvariables→f(s)isanalyticinΩifforanypointzofCthereexistsanon-emptyopendiscDzr={s∈C:|s−z|<r}containedinΩsuchthatfisthesumofapowerseriesins−zwhichconvergesinDzrthatistosayfadmitsaTaylorseriesexpansionintheneighborhoodofz:f(s)=+∞(cid:12)k=0(s−z)kk!f(k)(z).(12.68)IdentityofthetwonotionsLetpbeacomplexnumberandletjbeapositiveinteger.Wehave1(s−p)j=1(−p)j1(cid:9)1−sp(cid:10)j.Thisfunctionisthesumofapowerseriesinswithradiusofconvergence|p|.Bychangingtheoriginoftheplaneitfollowsthatifp(cid:5)=zthissamefunctionisthesumofapowerseriesins−zwithradiusofconvergence|p−z|.Nowletf(z)beanyrationalfunctionandletPbethesetofitsdistinctpolessothatf(s)isholomorphicintheopensetC\P(whereC\PdesignatesthecomplementofPinC).From(13.60)(13.61)andtheaboveitfollowsthatf(s)isanalyticinC\P.Asaresultholomorphyandanalyticityaresynonymousasfarasrationalfunctionsareconcerned.AccordingtoGoursat’stheorem([35]sectionIX.10Problem2)thisholdstrueforallfunctionsofonecomplexvariable.Inadditionletz∈C\Pandr=minp∈P|z−p|.Powerseries(12.68)hasaradiusofconvergenceofrthusitconvergesuniformlyinthecloseddisc|s−z|≤foranyρ<r.AnalyticcontinuationprincipleThefollowingisprovedine.g.([25]IV.2.3)and([35](9.4.3)):THEOREM438.–LetΩbeanopenconnectedsubsetofthecomplexplaneandf∈O(Ω).Thefollowingconditionsareequivalent:(i)thereexistsapointz∈Ωsuchthatforallintegersn≥0f(n)(z)=0;(ii)thereexistapointz∈ΩandanopenneighborhoodN⊆Ωofzsuchthatf|N=0;(iii)thereexistsacompactinﬁnitesubsetHofΩsuchthatf(z)=0forallz∈H.(iv)f=0(i.e.f(s)=0foralls∈Ω).406LinearSystemsEntirefunctionsIfafunctionofonecomplexvariableisholomorphicinCthenseries(12.68)convergesuniformlyinanyboundedsubsetofC.Suchafunctionissaidtobeentire.ThusthesetofallentirefunctionsisO(C).MeromorphicfunctionsLetΩbeanopenconnectedsubsetofC.AcomplexfunctionhissaidtobemeromorphicinΩifthereexistfg∈O(Ω)g(cid:5)=0suchthath=f/g.Letz∈Ω;thereexistsarationalintegerqsuchthath(s)=(s−z)ql(s)wherel(z)(cid:5)=0andthisrepresentationisunique.Ifq<0hhasapoleoforder−qatz;andifq>0hissaidtohaveazerooforderqatz.Thereexistsarealr>0suchthath(s)=(cid:12)ν≥qaν(s−z)νaq(cid:5)=0foranys∈Ωsuchthat|s−z|<r;theseriesintheright-handmemberoftheaboveequalityiscalledaLaurentseries(andistheLaurentexpansionofhatz).ThisLaurentseriesisabsolutelyconvergentforeverysintheopendisk∆(z;r)withcenterzandradiusrandisnormally–thusuniformly–convergentinanycloseddisk¯∆(z;ρ)withcenterzandradiusρ0<ρ<r.Ifq<0i.e.ifzisapoleofhthena−1iscalledtheresidueofhatzandisdenotedasRes(h;z).12.4.2.FunctionsofamatrixLetfbeananalyticfunctionintheneighborhoodof0andletρbetheradiusofconvergenceofthepowerseriesf(s)=+∞(cid:12)k=0akskak=f(k)(0)k!.(12.69)Inotherwordsρ=sup(cid:19)c≥0:+∞(cid:12)k=0|ak|ck<+∞6.LetA∈Cn×n.WecallspectralradiusofmatrixAthequantityr(A)=max{|λ|:λ∈Sp(A)}whereSp(A)isthespectrumofA(section13.3.3).Onecanshowthattheseriesf(A)=+∞(cid:12)k=0akAkAppendix1:Analysis407isconvergentifr(A)<ρ([52]sectionV.4).Assumingthatthisconditionholdswehavethefollowingresult:PROPOSITION439.–Letλ∈CbeaneigenvalueofAandletx∈CnbeaneigenvectorofAassociatedwithλ.Thenf(λ)isaneigenvalueoff(A)andxisaneigenvectoroff(A)associatedwithf(λ).PROOF.WehaveAx=λxthusf(A)x=(cid:11)+∞k=0akAkx=(cid:11)+∞k=0akλkx=f(λ)x.ExponentialofmatrixTheexponentialfunctionf(s)=escanbeexpandedasfollows:es=+∞(cid:12)k=0skk!withaninﬁniteradiusofconvergence(i.e.itisanentirefunction);asaresultifA∈Cn×ntheexponentialofthismatrixisdeﬁnedbytheserieseA=+∞(cid:12)k=0Akk!(12.70)whichconvergesforanysquarematrixA.WehaveinparticulareλIn=eλInandthuse0n×n=In.(12.71)OntheotherhandonecaneasilyshowthatiftwosquarematricesBandCareofthesameorderandcommutetheneB+C=eBeC.(12.72)InparticularBand−Bcommutethereforee0n×n=eBe−Bwhichprovesthate−B=(cid:15)eB(cid:16)−1.FinallyifBandCaretwosquarematricesconsidertheirdiagonalsumB⊕C(seesection13.1.4).ItisimmediatethateB⊕C=eB⊕eC(12.73)408LinearSystemsLogarithmofmatrixThefunctionf(s)=ln(1+s)(s∈R)canbeexpandedasfollows:ln(1+s)=+∞(cid:12)k=1(−1)k−1skk(12.74)withradiusofconvergenceρ=1.DEFINITION440.–Thepowerseriesintheright-handsideof(12.74)whichconvergesforallcomplexnumbersssuchthat|s|<113istheprincipalbranchofln(1+s).Inthecomplexplanethereareseveral“branches”ofthelogarithm“inverse”oftheexponentialfunctionasshownherebelow(formoredetailssee[25]).PROPOSITION441.–Letf(s)betheprincipalbranchofln(1+s).Allotherdeterminationsofln(1+s)areoftheformf(s)+2kπi(wherekisaninteger).PROOF.Considertwocomplexnumbersxandysuchthatex=ey.Thisequalityisequivalenttoex−y=1i.e.tox−y=2kπi(k∈Z).IfA∈Cn×nwecanthusdeﬁneln(In+A)bytheexpressionln(In+A)=+∞(cid:12)k=1(−1)k−1Akkprovidedthatr(A)<1.12.4.3.IntegrationinthecomplexplaneIntegrationalongapathPathApathisapiecewisecontinuouslydifferentiablefunctionfromaninterval[ab]ofRintothecomplexplaneC.Itisthusa“sufﬁcientlyregularly”parametrizedcurveofthecomplexplanewhichwecandenoteasγ:[ab](cid:20)t(cid:3)→γ(t)∈C.Itisorientedbecausewhenthevariablettraverses[ab]fromatobthepointγ(t)traversesthiscurveinawell-determinedsense.IfthereexistsanopensubsetΩofCsuchthatγ([ab])⊂ΩthenγiscalledapathinΩ.Thispathγisclosedifγ(a)=γ(b).13.Itconvergesalsofors=1.Appendix1:Analysis409IntegrationLetf(s)beameromorphicfunctionintheopensetΩ⊂CandletPbethesetofitsdistinctpoles.LetalsoγbeapathinΩandwhichdoesnotpassthroughanypointofP.Thenwedeﬁnetheintegraloffalongthepathγ:thisisthequantity(cid:2)γf(s)ds=(cid:2)baf(γ(t))˙γ(t)dt.IndexofapointwithrespecttoaclosedpathLetpbeapointofthecomplexplaneandletγ:[ab](cid:20)t(cid:3)→γ(t)∈Cbeaclosedpathnotpassingthroughp.Thequantityj(p;γ)=12πi(cid:2)γdss−p(12.75)iscalledtheindexofpwithrespecttoγ.Thisindexisarationalintegerandisinterpretedasthenumberofturnsthatγgoesaroundp(inthedirectsense);equivalentlywecansaythatj(p;γ)isthenumberofturnsthatsgoesaroundpinthedirectsensebyfollowingtheclosedpathγ.Indeedletnbethenumberofturnsasdeﬁnedaboveandwriteh(t)=(cid:2)ta˙γ(τ)γ(τ)−pdτ.Writeγ(τ)−p=(τ)eiθ(τ)(τ)>0(polarformofγ(τ)−p).Then˙γ(τ)γ(τ)−p=˙(τ)(τ)+iθ(τ)andthush(b)=i∆θwhere∆θisthevariationoftheargumentofs−pwhenstraversestheclosedpathγ.Wehave∆θ=2πn.Noticethatifγ1γ2:I→Caretwoclosedpathswhichdonotpassthrough0thenγ1γ2:I(cid:20)t(cid:3)→γ1(t)γ2(t)∈Cisagainaclosedpathwhichdoesnotpassthrough0andj(0;γ1γ2)=j(0;γ1)+j(0;γ2).(12.76)410LinearSystemsIntegrationofelementaryfunctionsalongaclosedpathLetk≥0.Then(cid:1)skds=sk+1k+1+const.Thereforeforanyclosedpathγ(cid:1)γskds=0.WededucethatforanypolynomialQ(s)(cid:1)γQ(s)ds=0.Ontheotherhandleth(s)=1(s−p)kk≥2Wehave(cid:2)h(s)ds=11−k(s−p)1−k+const.Asaresultforanyclosedpathγnotpassingthroughp(cid:1)γh(s)ds=0.ResiduetheoremThistheoremisoneofthemostimportantonesinthetheoryoffunctionsofonecomplexvariable.Asaresultof(13.60)(13.61)(12.75)andoftheaboveiff(s)isarationalfunctionandifγisaclosedpathnotpassingthroughanypoleoff(s)wehave(cid:2)γf(s)ds=2πi(cid:12)p∈Pj(p;γ)Res(f;p).(12.77)AsubsetΩofCisconvexifwhenevertwopointsaandbbelongtoΩthesegment[ab]={(1−t)a+tb:0≤t≤1}isincludedinΩ.Theequality(12.77)stillholdswhenfisameromorphicfunctioninanopenconvex14setΩ⊂CandwhentheclosedpathγisinΩ(Cauchy’sresiduetheorem)[103].Cauchy’sintegralformulaLetfbeaholomorphicfunctioninanon-emptyopenconvex15subsetΩofthecomplexplane.LetzbeapointofΩandγbeaclosedpathinΩwhichdoesnotpassthroughz.14.SeeRemark442.15.SeeRemark442.Appendix1:Analysis411Thefunctionf(s)s−zismeromorphicinΩandhasauniquepolez;theresidueofthisfunctionatzisf(z).Byapplyingtheresiduetheorem(12.77)tof(s)s−zwethusobtain“Cauchy’sintegralformula”:j(z;γ)f(z)=12πi(cid:2)γf(s)s−zds(12.78)Wecangeneralizethisformulausingtheexpansion(12.68)whichconvergesuniformlyinthecloseddisk|s|≤rifr>0issmallenoughforthisdisktobeincludedinΩ.Theresidueoff(s)(s−z)k+1atzisf(k)(z)k!;thereforeweget:j(z;γ)f(k)(z)=k!2πi(cid:2)γf(s)(s−z)k+1dsk≥0.(12.79)REMARK442.–*AnopenconnectedsubsetΩofthecomplexplaneiscalledsimplyconnectedifroughlyspeakingeveryclosedpathinΩcanbecontinuouslyshrunkuntiltobereducedtoonepoint;suchasubsetΩisan“openconnectedsubsetwithoutholes”(soisanyconvexopenset).Intheabove“convex”canbeeverywherereplacedby“simplyconnected”.*MaximummodulusprincipleLEMMA443.–Iff(s)isholomorphicinΩ={s:|s−s0|<r}andif|f(s)|≤|f(s0)|foralls∈Ωthenf(s)isconstantinΩ.PROOF.Wemayassumethatf(s0)(cid:5)=0.Accordingto(12.78)whenever0<ρ<rf(s0)=12π(cid:2)2π0f(cid:15)s0+ρeiθ(cid:16)dθorequivalently(cid:2)2π0(cid:15)1−f(cid:15)s0+ρeiθ(cid:16)/f(s0)(cid:16)dθ=0.Therealpartoftheintegrandis≥0andiszeroonlywhenf(s0)=f(cid:15)s0+ρeiθ(cid:16).Nowwecanstatethemaximummodulusprinciple:THEOREM444.–LetΩbeanopensubsetofthecomplexplanelet¯Ωbeitsclosureandletfbeafunctionwhichisbothcontinuousin¯ΩandholomorphicinΩ.If|f(s)|admitsamaximumin¯Ωthismaximumisattainedonthefrontier∂Ω=¯Ω\Ω.PROOF.*Assumethatthemaximumisattainedataninteriorpointz.TheconnectedcomponentC(z)ofzinΩisopenforΩislocallyconnectedand∂C(z)⊂∂Ω.HencebyTheorem438andLemma443fisconstantinΩthusfisconstantin¯Ωbycontinuityandtherefore|f(s0)|=|f(z)|atsomepoints0of∂Ω.*412LinearSystems12.4.4.ApplicationstoinversetransformsCalculationsandpropertiesoftheinverseLaplacetransformLetustakearationalfunctionf(s)andsupposethatf(s)isofrelativedegreeatleast2(withoutlossofgeneralityaccordingtosection12.3.4).LetPbethesetofitsdistinctpolesandletαbearealnumbersuchthatα>Re(p)∀p∈P.Thenweknowaccordingto(12.56)thatf(s)istheLaplacetransformofafunctionuandthatu(t)=12πiα+i∞(cid:2)α−i∞f(s)estds.Wewillshowontheonehandhowwecaneasilycarryoutthiscalculationusingresiduesandontheotherhandthatthisfunctionuispositivelysupported.LetustaketheintegralJ(tA)=(cid:1)α+iAα−iAf(s)estdsA>0.1)Firstlett≥0.Wecompletethepaths=α+iθθ∈[−AA]bythesemi-circleCα−deﬁnedbys=α+Aeiϕϕ∈(cid:25)π23π2(cid:26)suchthataclosedpathγA−isformed(seeFigure12.1).Thereexistsaconstantc>0suchthat|f(s)|≤cA2foranys∈Cα−providedthatAissufﬁcientlylarge.Ontheotherhandforanys∈Cα−))est))=)))eRe(st)+iIm(st))))=)))eRe(st))))≤eαt(12.80)AiAiCAFigure12.1.ClosedpathAppendix1:Analysis413therefore)))(cid:1)Cα−f(s)estds)))≤ceαtA2(cid:1)Cα−ds=ceαtA2πA.Thisquantitytendsto0asA→+∞.ThuslimA→+∞J(tA)=limA→+∞(cid:1)γA−f(s)estds.InadditiontheclosedpathγA−turnsinthedirectsenseonetimearoundallthepolesoff(s)whenAissufﬁcientlylargeandthusalsoaroundthepolesoff(s)estsinceestisentire.Applyingtheresiduetheorem(12.77)wethenobtainu(t)=(cid:12)p∈PRes(f(s)est;p)t≥0.(12.81)Forexampleletf(s)=1(s−p)nn≥2.Wehaveest=epte(s−p)t=ept(cid:11)+∞k=0(s−p)kk!tk.Thisfunctionhasauniquepoles=panditsresidueatthispoleistn−1ept(n−1)!.AsaresultL−1(cid:20)1(s−p)n&(t)=tn−1ept(n−1)!t≥0.(12.82)Notethatthisformularemainsvalidforn=1.2)Nowsupposet<0.Theupperbound(12.80)isnolongervalidinCα−whichwewillreplacebythecomplementarysemi-circleCα+deﬁnedbys=α+Aeiϕϕ∈(cid:25)−π2π2(cid:26).Wenowintegratef(s)estalongtheclosedpathγA+deﬁnedastheunionofthepaths=α+iθθ∈[−AA]andofCα+.Thisclosedpathdoesnotencloseanyofthepolesoff(s)andthus12πi(cid:1)α+i∞α−i∞f(s)estds=0fort<0.Thereforewecanspecify(12.82)bywritingL−1(cid:7)1(s−p)n(cid:8)(t)=tn−1ept(n−1)!1(t).(12.83)THEOREM445.–Letˆf(s)bearationalfunctionletPbethesetofitsdistinctpolesandletρ∈Zbeitsrelativedegree(seesection13.6.1).i)Thefunctionˆf(s)istheLaplacetransformofadistributionf∈A+theabscissaofconvergenceofthisLaplacetransformisγ=maxp∈PRe(p)andthisdistributionisoftheformf=−ρ(cid:12)i=0qiδ(i)+g1(byconventionthesumiszeroifρ>0)wherethefunctiongisanalyticandoftheform(cid:11)kjtkepjt.414LinearSystemsii)Thisdistributionfistempered(thusadmitsaFouriertransform)ifandonlyifγ≤0.iii)Therationalfunctionˆf(s)isproper(inotherwordswehaveρ≥0)ifandonlyifthedistributionfisoftheformq0δ+gwhereq0(cid:1)lim|s|→∞f(s)(thecasewhereˆf(s)isstrictlypropercorrespondstothecasewhereq0=0).iv)Foreveryp∈[1+∞)thefollowingpropertyholds:thefunctiongbelongstoLpifandonlyifγ<0;thisisalsoanecessaryandsufﬁcientconditionforgtoadmitaboundedFouriertransform.Proofi)Accordingto(13.60)wehaveˆf(s)=ˆq(s)+ˆg(s)whereˆq(s)isapolynomialandwhereˆg(s)isastrictlyproperrationalfunction.Thepolynomialˆq(s)isnon-zeroifandonlyifρ≤0anditisthenoftheformˆq(s)=−ρ(cid:12)i=0qisi.ItsinverseLaplacetransformisthusq=−ρ(cid:12)i=0qiδiwhichisatempereddistribution.Wecandecomposethestrictlyproperrationalfunctionˆg(s)intosimpleelements;eachofthesesimpleelementsistheLaplacetransformofafunctionoftheform(12.83)andthecorrespondingabscissaofconvergenceisRe(p).Wededucefromtheretheexpressionofγ.Inadditioneachofthesefunctionsiscontinuous(andevenanalytic)exceptattheorigin;thesameistrueforg=L−1{ˆg}.ii)Thefunctiongisslowlyincreasing–thusfisatempereddistribution–ifandonlyifγ≤0.iii)Therationalfunctionˆf(s)isproper(resp.strictlyproper)ifandonlyifthepolynomialˆq(s)isaconstant(resp.iszero).iv)Itisagainsufﬁcienttothinkofthedecompositionofˆg(s)intoitssimpleelementsandthenmakeuseoftheirinverseLaplacetransforms.Notethatifγ<0theng∈L∞buttheconverseisnottrue.ForexampleL−1(cid:31)1s =1(t):thisfunctionisboundedbutγ=0.Moreoveronecanhaveγ=0andg/∈L∞:forexampleL−1(cid:31)1s2 =t1(t).Considerthefollowingexample:L−1(cid:31)s+1s =δ+1:thisisatempereddistribution.Thefunctionginthiscaseisdeﬁnedbyg(t)=1t≥0;itisaslowlyincreasingfunction.Wehave(Fg)(ω)=(Lg)(iω)=1iωifω(cid:5)=0.ThisFouriertransformisnotbounded(sinceγ=0)andisnotevenafunction:thisisthedistribution1iP1ωwhereP1ωisthe“principalvalue”of1ω[106].Appendix1:Analysis415Calculationoftheinversez-transformApplicationoftheresiduetheoremLetf(z)bearationalfunction.Accordingto(12.57)f(z)isthez-transformofapositivelysupportedsequence(xn)ifandonlyiff(z)isaproperrationalfunction.LetPbethesetofdistinctpolesoff(z)andlet=maxp∈P|p|.Letr>andletγbetheclosedpathz=reiθθ∈[02π].Formula(12.63)canbewrittenas:xn=12πi5γf(z)zn−1dz.(Wecanalsodirectlyestablishthisformulafromtheresiduetheoremafterthechangeofvariables=z−1;f(z)admitsapowerseriesexpansionins=z−1.)Accordingtotheresiduetheorem(12.77):xn=(cid:12)p∈PRes(cid:15)f(z)zn−1;p(cid:16)n≥0.(12.84)DecompositionintosimpleelementsandpowerseriesexpansionLetusnowdiscussanothermethodwhichisoftenfasterforthecalculationofinversez-transforms.Theproperrationalfunctionf(z)isalsoarationalfunction(notnecessarilyproper)ins=z−1.Indeedf(z)isanirreduciblerationalfunctionN(z)D(z).Letn=d◦(D).WehaveN(z)D(z)=N(z)znD(z)zn=N∗(cid:15)z−1(cid:16)D∗(z−1)(12.85)whereN∗andD∗areobviouslypolynomials.Wecannowapplythedecomposition(13.60)(13.61)tof∗(cid:15)z−1(cid:16)=f(z).Writethisdecompositionintheformf∗(cid:15)z−1(cid:16)=Q∗(cid:15)z−1(cid:16)+(cid:12)p∈P(cid:12)1≤j≤nPαpj(1−pz−1)jwherePisthesetofdistinctpolesoff(z)andnPisthemultiplicityofthepolep∈P.ThepolynomialQ∗(cid:15)z−1(cid:16)isoftheformQ∗(cid:15)z−1(cid:16)=m(cid:12)k=0ckz−k416LinearSystemsanditsinversez-transformistheﬁnitesequencec={c0...cm}.Wethusknowhowtoﬁndtheinversez-transformoff(z)ifweknowhowtocalculatetheinversez-transformofatermoftheform1(1−pz−1)j.Wehave1(1−pz−1)j=+∞(cid:12)k=0(cid:21)j+k−1k(cid:22)pkz−kwhere(cid:15)j+k−1k(cid:16)isthebinomialcoefﬁcient(j+k−1)!(j−1)!k!.Asaresultforanyk≥0Z−1(cid:19)1(1−pz−1)j6(k)=(cid:21)j+k−1k(cid:22)pk.(12.86)ConsequencesEssentialconsequencesoftheaboveinparticularof(12.86)aregatheredinthefollowingtheorem:THEOREM446.–Arationalfunctionf(z)whosesetofdistinctpolesisPisthez-transformofapositivelysupportedsequencex=(xn)ifandonlyiff(z)isproper.Theradiusofconvergenceofthisz-transformisthenρ=maxp∈P|p|.Thesequence(xn)isslowlyincreasingthusadmittingaFouriertransformifandonlyifρ≤1.Inadditionthefollowingconditionsareequivalent:(i)ρ<1;(ii)x∈l1;(iii)theFouriertransformFxisbounded.12.4.5.ArgumentprincipleTheargumentprincipleplaysakeyroleinthestudyofthestabilityofclosed-loopsystems.Letustaketheirreduciblerationalfunctionf(s)=(mk=1(s−zk)(nk=1(s−pk)(12.87)wherethezk’sandpk’sarethezerosandpolesoff(s)respectively.Whentheyaremultipletheyappearseveraltimesin(12.87).Thelogarithmicderivativeoff(s)isdf(s)f(s)(thisisthederivativeoflnf(s)).Wehavedf(s)f(s)=m(cid:12)k=1dss−zk−n(cid:12)k=1dss−pk.(12.88)Appendix1:Analysis417Supposenowγ:[ab]→Caclosedpathnotpassingthroughanyofthezk’snorpk’s.Theimageofγbyfistheclosedpathηdeﬁnedbyη(t)=f(γ(t)).Integratingtheleft-handmemberof(12.88)alongthepathγwehave(cid:2)γdf(s)f(s)=(cid:2)ba˙f(γ(t))˙γ(t)f(γ(t))dt.Nowletusintegratedssalongη:(cid:2)ηdss=(cid:2)ba˙η(t)η(t)dt=(cid:2)ba˙f(γ(t))˙γ(t)f(γ(t))dt.Thereforebyintegrating(12.88)alongγandbyusing(12.75)weobtainj(0;η)=m(cid:12)k=1j(zk;γ)−m(cid:12)k=1j(pk;γ).Letusnowchoosetheclosedpathγinsuchawaythatmorespeciﬁcallyγencirclesclockwise(i.e.intheindirectsense)onetimenzzerosandnppolesoff(s)(takingintoaccountmultiplicities)whereasitdoesnotenclosetheotherpolesandzerosofthisrationalfunction(anddoesnotpassthroughanyofthesepolesandzeros).Wethenhavej(zk;γ)=j(pk;γ)=−1forthezerosandpoleswhichareenclosedandj(zk;γ)=j(pk;γ)=0forthosethatarenot.Asaresult:j(0;η)=np−nz.(12.89)WecannowstateCauchy’sargumentprinciple:THEOREM447.–Thenumberoftimestheimageγ{f(s)}turnsaroundtheorigininthedirectsenseisequaltonp−nzwherenpandnzarethenumberofpolesandzerosrespectively(multiplicitiesincluded)enclosedbyγintheclockwisesense.12.5.DifferentialequationsBelowthereaderwillﬁndasummaryofthetheoryofdifferentialequationsaswellassomecomplements.Thenatureoftheproblemsencounteredbythecontrolengineermakesitnecessarytostudylineardifferentialequationswithconstantcoefﬁcientsunderhypotheseswhicharemoregeneralthanintheusualapproach.12.5.1.GeneralitiesClassicalnotionofasolutionLetbethedifferentialequation˙x=F(tx)(12.90)418LinearSystemswhereFisafunctionfromI×KnintoKn(withK=RorC)andwhereIisanintervalofRoftheform|t1+∞)−∞≤t1<+∞.FromaconcretepointofviewwealwayshaveK=R;howeverforcalculationsitisoftenmoreconvenienttoembedRinCthustoassumeK=C.Andthisiswhatwearegoingtodohere.Afunctionx:I(cid:20)t(cid:3)→x(t)∈Cnisasolution(intheclassicsense)ofequation(12.90)ifxisabsolutelycontinuousanditsderivativeinthesenseofdistributionssatisﬁes(12.90)almosteverywhere.TheCauchyproblemLet(t0x0)∈I×Cn.TheCauchyproblemconsistsofhavingtodetermineafunctionxwhichisasolutionof(12.90)andwhichsatisﬁestheinitialconditionx(t0)=x0.(12.91)Thisobviouslyequivalenttotheproblemofdeterminingasolutiontotheintegralequationx(t)=x0+(cid:2)tt0F(τx(τ))dτ.FortheCauchyproblemtohaveoneandonlyonesolutionontheentireintervalIfunctionFmustsatisfycertainconditionsspeciﬁedhereafter.(Formoregeneralconditionsunderwhichtheuniquenessofthesolutionisnotguaranteedsee[30]Chapter2.)Theinitialcondition(12.91)includestheinitialinstantt0andtheinitialstatex0(thisterminologyisonlyclassicwhenequation(12.90)representsasystem).TheCauchy-LipschitztheoremThefollowingcalledtheCauchy-Lipschitztheoremisfundamentalbutitsproofisoutsidethescopeofthisbook(see[111]PropositionC.3.8).SupposethatthefunctionFsatisﬁesthefollowinghypotheses:i)Thefunctiont(cid:3)→F(tη)islocallyintegrableonIforanyη∈Cn;ii)Thefunctionη(cid:3)→F(tη)iscontinuousinCnforanyt∈I.iii)Thereexistsalocallyintegrablefunctionα:I→R+suchthatforanyης∈Cnandanyt∈I(cid:23)F(tη)−F(tς)(cid:23)≤α(t)(cid:23)η−ς(cid:23).(12.92)Thenthedifferentialequation(12.90)admitsauniquesolutionon[t0+∞)whichsatisﬁestheinitialcondition(12.91).Appendix1:Analysis419AfunctionFsatisfyingconditioniii)issaidtobegloballyLipschitz(theadverb“globally”referstothefactthatin(12.92)ηandςcanvaryinthewholeCnforauniquefunctionα).Inparticularifthefunction(tη)(cid:3)→F(tη)iscontinuousanddifferentiablewithrespecttothesecondvariableandifthefunctionη(cid:3)→∂F∂η(tη)islocallyuniformlyboundedthenalltheaboveconditionshold.DifferentialequationofhigherorderConsiderthedifferentialequationoforderny(n)=G(cid:9)ty(n−1)...y(cid:10)(12.93)whereGisafunctionfromI×Cp×...×CpintoCpandwhereIisanintervalofRdeﬁnedasbefore.Inordernottomakethepresentationcumbersomeweassumeinwhatfollowsthatp=1wheneverwearedealingwithdifferentialequationsofthistypeexceptwhenotherwisestated.Wecomebacktothepreviouscasebywriting⎧⎪⎪⎪⎨⎪⎪⎪⎩x1=y(n−1)x2=y(n−2)...xn=y.(12.94)Indeedthedifferentialequation(12.93)isequivalenttothesystemofdifferentialequations(cid:20)˙x1=G(tx1...xn)˙xk+1=xk1≤k≤n−1whichisofform(12.90).Theinitialstateatinstantt0isaccordingto(12.94)x(t0)=(cid:23)y(n−1)(t0)...y(t0)(cid:24)T.(12.95)LineardifferentialequationThedifferentialequation(12.90)issaidtobelinearifthemappingη(cid:3)→F(tη)isafﬁnei.e.ifF(tη)isoftheformF(tη)=A(t)η+f(t).Thedifferentialequationcanthusbewrittenas˙x=A(t)x+f(t).(12.96)420LinearSystemsInthiscasetheconditionsoftheCauchy-Lipschitztheoremaresatisﬁedifbothfunctionst(cid:3)→A(t)andt(cid:3)→f(t)arelocallyintegrableonI.Thisisthecaseforexampleifthesefunctionsarepiecewisecontinuous*(ormoregenerallyregulated[35])*.LineardifferentialequationsofhigherorderConsideralineardifferentialequationofordernoftheform*∂n+n(cid:12)i=1∂n−iai+y=*n(cid:12)i=1∂n−ibi+u(12.97)where∂=d/dtandforanyi∈{1...n}thecoefﬁcientsaiandbiarefunctionsoftdeﬁnedontheintervalIandsatisfythefollowingproperty(P):(P)Thefunctionsaiandbiaren−itimesdifferentiableintheintervalIandtheirderivativeofordern−iispiecewisecontinuous*(ormoregenerallyregulated)*inthisinterval.OntheotherhanduisafunctionoftdeﬁnedinIandisassumedtosatisfyProperty(P’)wearegoingtospecifybelow.Insteadofproceedingasin(12.94)wedeﬁnethevariablexasfollows:⎧⎪⎪⎨⎪⎪⎩x1=y∂x1=−a1x1+b1u+x2...∂xn−1=−anx1+bnu+xn.(12.98)Equality(12.97)canbesimpliﬁedas∂xn=−anx1+bnu.(12.99)Wethusobtainthedifferentialequation˙x=⎡⎢⎢⎢⎢⎢⎢⎢⎣−a110···0−a20............0......0.........01−an00···0⎤⎥⎥⎥⎥⎥⎥⎥⎦x+⎡⎢⎢⎢⎢⎢⎢⎢⎣b1.........bn⎤⎥⎥⎥⎥⎥⎥⎥⎦u(12.100)y=(cid:25)10······0(cid:26)x.(12.101)System(12.100)isofform(12.96)withthefunctiont(cid:3)→A(t)whichisobviouslylocallyintegrableinIandthefunctiont(cid:3)→f(t)willhavethissamepropertyifthefunctiont(cid:3)→u(t)satisﬁesthefollowingproperty(P’):(P’)Thefunctionuispiecewisecontinuous*(ormoregenerallyregulated)*intheintervalI.Appendix1:Analysis42112.5.2.Lineardifferentialequations:constantcoefﬁcientsMatrixformConsiderthedifferentialequation(12.96)wherethematrixA(t)∈Cn×nisconstant.Thisequationisthuswrittenas˙x=Ax+f(t)(12.102)Byatranslationoftheoriginoftimetheproblemcomesdowntothecasewheretheinitialinstantist0=0;thuswecanassumethatI=[0+∞)andthefunctionf:I(cid:20)t(cid:3)→f(t)∈CnisassumedtobelocallyintegrableonI.HomogenousequationThehomogenousequationassociatedwith(12.102)istheoneobtainedwhenf=0.Itcanthusbewrittenas˙x=Ax.(12.103)ByreplacingAbyAtin(12.70)(seesection12.4.2)weobtaineAt=+∞(cid:12)k=0tkAkk!=In+tA+t2A22!+t3A33!...(12.104)Wecandifferentiatethispowerseriestermbytermandwegetddt(cid:15)eAt(cid:16)=A+tA2+...=A(cid:21)In+tA+t2A22!+...(cid:22)=AeAt(12.105)thereforethefunctiont(cid:3)→eAtisasolutionof(12.103).CalculationofeAtThereexistnumerouswaystocalculateeAt.OneofthemconsistsinreducingAtoitsJordanform(section13.3.4Theorem530).AccordingtotheJordantheoremthereexistsachangeofbasismatrixPsuchthatJ=P−1APisadiagonalsumofJordanblocksJλk;eachoftheseblocksappearsinthediagonalsumanumberoftimesthatisequaltothegeometricmultiplicityoftheeigenvalueλandisassociatedwiththeelementarydivisor(s−λ)k.ItisimmediatethatA=PJP−1andthataccordingto(12.104)eAt=PeJtP−1.422LinearSystemsAccordingto(12.73)weareledtocalculateeJλkt.WehaveJλk=λIk+J0kwhereIkandJ0kcommute;thusby(12.72)eJλkt=eλteJ0kt.(12.106)TheJordanblockJ0kisthematrixofdimensionk×kdeﬁnedbyJ0k=⎡⎢⎢⎢⎢⎣00···01.....................0···10⎤⎥⎥⎥⎥⎦;asaresultaccordingto(12.104)eJ0kt=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣10······0t1...0t22!t.....................0tk−1(k−1)!tk−2(k−2)!···t1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦.(12.107)OfcourseinthecasewherethematrixAisdiagonalizablethesecalculationsaresimpler.IndeedwethenhaveJ=diag(λ1...λn)wheretheλi’s1≤i≤naretheeigenvalues(notnecessarilydistinct)ofA.WethenobtaineJt=diag(cid:15)eλ1t...eλnt(cid:16).UseoftheinverseLaplacetransformDeﬁnition(12.48)oftheLaplacetransformextendswithoutdifﬁcultytothecaseofvector-valuedfunctions(i.e.fromRintoCn)ormatrix-valuedfunctions(fromRintoCn×m).WecancalculateeAtusingthisgeneralizedLaplacetransform.Wehaveindeed(12.52)withu(t)=eAtthusu(0)=Inaccordingto(12.71).Andby(12.105)wegetsˆu(s)−In=Aˆu(s)fromwhichL(cid:15)eAt1(t)(cid:16)=(sIn−A)−1.(12.108)AmethodtocalculateeAtconsistsofcalculatingallentriesofthematrix(sIn−A)−1andthendeterminingtheirinverseLaplacetransformswhicharetheentriesofeAt1(t).Appendix1:Analysis423SolutionsofthehomogenousequationIfϕisanyvectorinCnthefunctionx(t)=eAtϕ(12.109)istheuniquesolutionof(12.103)satisfyingtheinitialconditionx(0)=ϕ.Allthesolutionsof(12.103)arethusoftheform(12.109)whereϕspansCn.ThissetofsolutionsisthusaC-vectorspaceofdimensionngeneratedbythenfunctionsηi(t)=eAtεi1≤i≤nwhere(εi)1≤i≤nisthecanonicalbasisofCn.(Thesenfunctionsηi(t)1≤i≤narelinearlyindependentbecauseeAtisinvertible.)Letλ1...λqbethedistincteigenvaluesofA.Eacheigenvalueλi(1≤i≤q)correspondstooneorseveralJordanblocks;letmibethemaximalorderoftheseJordanblocksi.e.theorderofmultiplicityofλiasarootoftheminimalpolynomialofA.Accordingto(12.106)and(12.107)allsolutionsof(12.103)areoftheformx(t)=q(cid:12)i=1eλitpi(t)(12.110)wherepi(t)isapolynomialintheindeterminatetwithcoefﬁcientsinCnandofdegreemi−1.SolutionsofthecompleteequationIfzisaparticularsolutionofequation(12.102)itisimmediatethatallsolutionsof(12.102)areoftheformx=y+zwhereyspansthesetofsolutionsofthehomogenousequation(12.103).ThesesolutionsthusformanafﬁneC-spaceofdimensionn.Wedetermineaparticularsolutionof(12.102)byapplyingthemethodof“variationoftheconstant”tothesolutionsofthehomogenousequation(12.103).Wesawthatanysolutionof(12.103)isoftheform(12.109).Themethodofvariationoftheconstantconsistsinreplacingtheconstantvectorϕbyanabsolutelycontinuousfunctiont(cid:3)→ϕ(t)∈Cn.Inotherwordswelookforasolutionxof(12.102)intheformx(t)=eAtϕ(t).(12.111)Byusingthisin(12.102)wegetAeAtϕ(t)+eAt˙ϕ(t)=AeAtϕ(t)+f(t)fromwhich˙ϕ(t)=e−Atf(t)424LinearSystemsandthusϕ(t)=x0+(cid:2)t0e−Aτf(τ)dτx0∈CnAccordingto(12.111)weobtainx(t)=eAtx0+(cid:2)t0eA(t−τ)f(τ)dτ.(12.112)Thefunctionintheright-handsideof(12.112)istheuniquesolutionof(12.102)satisfyingtheinitialconditionx(0)=x0.UseoftheLaplacetransformWerecognizein(12.112)aconvolutionproduct.Thisisnotsurprisingforwecansolve(12.102)usingtheLaplacetransform.Wethenobtainfrom(12.102)and(12.52)sˆx(s)−x0=Aˆx(s)+ˆf(s)fromwhichˆx(s)=(sIn−A)−1x0+(sIn−A)−1ˆf(s)andweobtain(12.112)accordingto(12.108).LineardifferentialequationofhigherorderDecompositionoftheproblemInthecasewherethecoefﬁcientsareconstantwecanconsideralineardifferentialequationofamoregeneralformthan(12.97)thatisa(∂)y=b(∂)u(12.113)a(∂)=∂n+a1∂n−1+...+anb(∂)=bk∂n−k+...+bnbk(cid:5)=0k∈Zwhereuisgivenandyistheunknown.DEFINITION448.–Thedifferentialequation(12.113)is(i)strictlyproperifk≥1(ii)properifk≥0(iii)improperifk<0.Appendix1:Analysis425Wecanadoptthefollowingapproach[120]:thevariableuisassumedtobeoftheformu=f+TwherefisanindeﬁnitelydifferentiablefunctioninIandwhereT∈A+(see(12.53)).ByperformingtheEuclideandivisionofb(∂)bya(∂)(seesection13.1.3)wegetb(∂)=a(∂)q(∂)+r(∂)whered◦(r)<d◦(a)andwhered◦(q)=−kifk≤0andq=0ifnot.Equation(12.113)isequivalenttoa(∂)(y−q(∂)u)=r(∂)uaswellasbywritingz=y−q(∂)utoa(∂)z=r(∂)u(12.114)y=z+q(∂)u.(12.115)Puttingw=y−zweobtainw=q(∂)u.(12.116)SolutionsofthehomogenousdifferentialequationWecanproceedwiththedifferentialequation(12.114)aswehavedonewith(12.97)replacinginthislastequationybyzandb(∂)byr(∂).Wethenobtainthedifferentialequation(12.100)withthecoefﬁcientsbi(1≤i≤n)replacedbythecoefﬁcientsriofr(∂).Allcoefﬁcientsareconstant.ThematrixAisacompanionofthepolynomiala(s)=sn+a1sn−1+...+an(seesection13.4.3);a(s)isthusthecharacteristicpolynomialofAandisalsocalledthecharacteristicpolynomialofthedifferentialequation(12.113).SinceAiscyclica(s)isalsotheminimalpolynomialofthismatrix(section13.4.3Corollary568).Thereforethedistinctrootsλi(1≤i≤q)ofa(s)arethedistincteigenvaluesofA.Moreoverthemultiplicityordermiofλiasarootofthepolynomiala(s)istheintegermialreadyencounteredwhenanalyzingequation(12.110);toλiasaneigenvalueofAtherecorrespondsauniqueJordanblockwhichhasordermi.Thuswecanstatethefollowingresult:THEOREM449.–ThesolutionsofthehomogenousdifferentialequationconstituteaC-vectorspaceofdimensionn.AbasisofthisvectorspaceisformedbythenC-linearlyindependentfunctionst(cid:3)→tk−1eλit1≤k≤mi1≤i≤q.426LinearSystemsSolutionsofthecompleteequationTakingtheLaplacetransformof(12.114)andusing(12.55)weobtainanexpressionoftheformˆz(s)=r(s)a(s)ˆu(s)+˜r(s∂0)u0−˜a(s∂0)z0a(s)where˜r(s∂0)(resp.˜a(s∂0))isapolynomialwithrespecttothetwovariablessand∂0ofdegreed◦(r)−1or−∞(resp.n−1or−∞)withrespecttoeachofthesevariables;thisdegreeisdenotedasd◦(˜r)(resp.d◦(˜a))inwhatfollows.InthesamemannertakingtheLaplacetransformof(12.116)weobtainˆw(s)=q(s)ˆu(s)+˜q(s∂0)u0where˜q(s∂0)isapolynomialwithrespecttothetwovariablessand∂0ofdegreeequaltod◦(q)−1or−∞withrespecttoeachofthesevariables;thisdegreeisdenotedasd◦(˜q)inthesequel.Finallywehaveaccordingto(12.115)ˆy(s)=b(s)a(s)u(s)+˜r(s∂0)u0−˜a(s∂0)z0a(s)+˜q(s∂0)u0.(12.117)Letˆyf(s)=b(s)a(s)u(s).Thisquantityisobtainedfrom(12.117)byputtingz(i)(0−)=0(0≤i≤d◦(˜a))andu(i)(0−)=0(0≤i≤max(d◦(˜r)d◦(˜q)))(“zeroinitialconditions”).ItsinverseLaplacetransformisaccordingto(12.22)and(12.49)fort≥0yf(t)=(cid:2)t+0−g(t−τ)u(τ)dτwhereg=L−1(cid:20)b(s)a(s)&.(12.118)ThisisanelementofA+calledtheforcedresponse.Wenowwriteˆylr(s)=˜r(s∂0)u0−˜a(s∂0)z0a(s).(12.119)Thisisastrictlyproperrationalfunctionwithrespecttos;itthusadmitsaccordingtoTheorem445aninverseLaplacetransformwhichisanindeﬁnitelydifferentiablefunctionylr.Whentheinitialconditionsz(i)(0−)(0≤i≤d◦(˜a))andu(i)(0−)=0(0≤i≤d◦(˜r))varyylrspansaccordingtoTheorem449aC-vectorspaceofdimensionnhavingabasisconsistingofthenfunctionsspeciﬁedintheabove-citedAppendix1:Analysis427theorem.Thefunctionsbelongingtothisvectorspacearereferredtoastheregularfreeresponses.Lastlywewriteˆyli(s)=˜q(s∂0)u0.(12.120)If˜q(s∂0)(cid:5)=0theinverseLaplacetransformylispansastheinitialconditionsvaryaC−vectorspaceofdimensionequaltoρwhereρ=1+d◦(˜q)havingasabasisthedistributionsδ˙δ...δ(ρ−1).Thedistributionsbelongingtothisvectorspacearecalledtheirregularfreeresponses.Wecannowgathertheresultswehaveobtainedinthefollowingtheoremwhereρ=1+d◦(˜q)=d◦(q).THEOREM450.–i)Supposeuisoftheformu=f+TwherefisanindeﬁnitelydifferentiablefunctioninIandwhereT∈A+.Thenthesolutionsyofthedifferentialequation(12.113)areofthesameform.Theycanbedecomposedaccordingtoy=ylr+yli+yfwhereyfistheforcedresponseobtainedwithzeroinitialconditionsandgivenby(12.118)ylristheregularfreeresponsewhoseLaplacetransformisgivenby(12.119)andyliistheirregularfreeresponsewhoseLaplacetransformisgivenby(12.120).ii)TheregularfreeresponsesformaC-vectorspaceofdimensionnabasisofwhichisformedbythenfunctionsinTheorem449.iii)Theirregularfreeresponsesarezeroifk≥0andtheyformaC-vectorspaceofdimension−kifk<0abasisofwhichisformedbythedistributionsδ˙δ...δ(−k−1).iv)Asaresultthefreeresponsesofthedifferentialequation(12.113)formaC-vectorspaceofdimensionmax(nn−k)16andareall“regular”(thatistheyarealllocallyintegrablefunctions)ifandonlyifk≥0.REMARK451.–Theorem450becomessimplerwhenk≥0(caseofa“properdifferentialequation”)sincetheirregularfreeresponsesarethenreducedto0(inotherwordsallfreeresponsesareregular).Supposethisisthecaseandinadditionuisalocallyintegrablefunction;thenaccordingtoTheorem445andthepropertiesoftheconvolutionproduct(section12.2.2)theforcedresponseyfisalocallyintegrablefunctionifk≥0andacontinuousfunctionifk>0.16.In[120]thisquantitymax(nn−k)isreferredtoastheorderofthedifferentialequation(12.113)forobviousreasons.428LinearSystems12.6.Functionsofseveralvariables;optimizationThepresentationonfunctionsofseveralvariablesmadehereisextremelysuccinct(withonlyafewproofsandlimitedtothecaseofreal-valuedfunctions).Forageneralpresentationsee([35]ChapterVIII).12.6.1.FunctionsofclassC1LetΩbeanon-emptyopensubsetofRnandx∗∈Ω.WesaythatafunctionJ:Ω→Radmitsapartialderivative∂J∂xi(x∗)withrespecttothevariablexiatthepointx∗ifthepartialmappingJi:xi(cid:3)→J(cid:15)x∗1...x∗i−1xix∗i+1...x∗n(cid:16)isdifferentiableatx∗i;andinthiscasewehavebydeﬁnition∂J∂xi(x∗)=dJidxi(x∗i).ThefunctionJissaidtobedifferentiableatthepointx∗ifthereexistsalinearformdenotedbydJ(x∗)andcalledthederivativeofJatx∗suchthatJ(x∗+h)−J(x∗)=dJ(x∗)h+o((cid:23)h(cid:23))(12.121)whereo((cid:23)h(cid:23))isafunctiondeﬁnedintheneighborhoodof0andsuchthatlim(cid:10)h(cid:10)→0(cid:10)h(cid:10)(cid:12)=0o((cid:23)h(cid:23))(cid:23)h(cid:23)=0.InthecanonicalbasisofRndJ(x∗)isrepresentedbytherowmatrixDJ(x∗)=(cid:25)∂J∂x1(x∗)...∂J∂xn(x∗)(cid:26)calledtheJacobianmatrixofJatx∗.Ifweidentifythevectorh=(h1...hn)withthecolumnmatrix(cid:25)h1...hn(cid:26)TwehavedJ(x∗)h=DJ(x∗)hinawaythatleadsustoidentifythederivativedJ(x∗)withtheJacobianmatrixDJ(x∗).Thecolumnvector∇J(x∗)=DJ(x∗)TisthegradientofJatx∗.ThefunctionJissaidtobeofclassC1ifitisdifferentiableatanypointofΩandifitsderivativedJ:x→dJ(x)iscontinuous.OnecanshowthatJisofclassC1ifandonlyifitadmitspartialderivatives∂J∂xiwithrespecttoallitsvariablesatanypointofΩandifthesepartialderivativesarecontinuous.Appendix1:Analysis42912.6.2.FunctionsofclassC2LetJ:Ω→Rbeafunctionadmittingapartialderivative∂J∂xi(x)atanypointx∈Ωandconsiderthepartialmapping(DJ)ij:xj(cid:3)→∂J∂xi(cid:15)x∗1...x∗j−1xjx∗j+1...x∗n(cid:16).Ifitisdifferentiableatthepointx∗j∈ΩwesaythatJadmitsasecondorderpartialderivative∂2J∂xi∂xj(x∗)withrespecttothevariablesxiandxjatx∗andbydeﬁnition∂2J∂xi∂xj(x∗)=d(DJ)ijdxj(cid:15)x∗j(cid:16).ThefunctionJissaidtobetwicedifferentiableatthepointx∗ifitisofclassC1andifitsderivativedJ:Ω→(Rn)(cid:2)isdifferentiableatx∗(where(Rn)(cid:2)isthedualofRn:seesection12.1.2;(Rn)(cid:2)isidentiﬁedwiththeR-vectorspaceofrowmatriceswithnentriesbelongingtoRi.e.withR1×n).ThederivativeofdJatx∗iscalledthesecondderivativeofJatthatpointandisdenotedasd2J(x∗).ItisanelementofL(RnL(RnR))representedinthecanonicalbasesbythesquarematrixofpartialderivativesoforder2HJ(x∗)=(cid:9)∂2J∂xi∂xj(x∗)(cid:10)1≤i≤n1≤j≤ncalledtheHessianmatrixofJatx∗.Thismatrixissymmetric(∂2J∂xi∂xj=∂2J∂xj∂xi).Leth1h2betwovectorsofRn;wehavebasedontheaboveidentiﬁcationsd2J(x∗)(h1h2)=hT1HJ(x∗)h2.(12.122)ThefunctionJissaidtobeofclassC2ifitistwicedifferentiableatanypointofΩandifitssecondderivatived2J:x→d2J(x)iscontinuous.OnecanshowthatJisofclassC2ifandonlyifitadmitssecondorderpartialderivatives∂2J∂xi∂xjwithrespecttoallitsvariablesatanypointofΩandifthesepartialderivativesarecontinuous.WeleaveittothereadertodeﬁnebyinductionthepthderivativedpJ(x∗)aswellasafunctionofclassCp.12.6.3.Taylor’sformulaSeth1=handhp=(cid:15)hp−1h(cid:16)forp>1.430LinearSystemsTaylor’sformulawithYoung’sremainderLetJ:Ω→Rbeafunctionwhichisp−1timesdifferentiableinΩandptimesdifferentiableatpointx∗.ThereforeJ(x∗+h)=J(x∗)+p(cid:12)k=11k!dkJ(x∗)hk+o((cid:23)h(cid:23)p)(12.123)(Forp=1thisformulaisnothingbutthedeﬁnitionofthederivativedJ(x∗).)Taylor’sformulawithLagrange’sremainderLetJ:Ω→Rbeafunctionwhichhasaderivativeororderp−1inΩ.Supposetheclosedsegment[x∗x∗+h]iscontainedinΩandJadmitsaderivativeoforderpatanypointoftheopensegment(x∗x∗+h).17Thenthereexistsθ∈(01)suchthatJ(x∗+h)=J(x∗)+p−1(cid:12)k=11k!dkJ(x∗)hk+1p!dpJ(x∗+θh)hp(12.124)12.6.4.ConvexitycoercivityellipticityFormoredetailsonwhatfollowstheinterestedreadermayconsult[29].ConvexityAnon-emptysubsetΩofRnissaidtobeconvexifforanypointsaandbofΩthesegment[ab]isincludedinΩ.LetJ:Ω→RwhereΩisconvex.ThefunctionJissaidtobeconvex(resp.strictlyconvex)ifforanydistinctpointsaandbofΩandanyλ∈(01)J((1−λ)a+λb)≤(1−λ)J(a)+λJ(b)(resp.J((1−λ)a+λb)<(1−λ)J(a)+λJ(b)).LetusrecallthefollowingdeﬁnitionswhereΩdenotesanon-emptypartofRn:–ThefunctionJ:Ω→Radmitsaglobal(resp.strictglobal)minimumatx∗∈ΩifJ(x∗)≤J(x)foranyx∈Ω(resp.J(x∗)<J(x)foranyx∈Ωx(cid:5)=x∗);wethenwritex∗∈argminx∈ΩJ(x)(resp.x∗=argminx∈ΩJ(x)).17.Thesegment[ab](resp.(ab))isthesetofallpointsoftheform(1−λ)a+λbλ∈[01](resp.λ∈(01)).Seesection12.4.3.Appendix1:Analysis431–ThefunctionJ:Ω→Radmitsalocal(resp.strictlocal)minimumatx∗∈ΩifthereexistsaneighborhoodVofx∗inΩsuchthattherestrictionofJtoVadmitsaglobal(resp.strictglobal)minimumatx∗.Convexityplaysakeyroleinthetheoryofoptimizationforthefollowingreason:THEOREM452.–LetΩbeaconvexsubsetofRnandJ:Ω→R.IfJadmitsalocalminimumatx∗andisconvex(resp.strictlyconvex)thenthisminimumisglobal(resp.strictglobal).Notethatastrictlyconvexfunctiondoesnotnecessarilyadmitaminimum.ThisisthecaseforexampleofJ(x)=1/xinΩ=(0+∞).AnecessaryconditionforafunctiontoadmitalocalminimumistheEulerconditionanimmediateconsequenceof(12.121):PROPOSITION453.–LetΩbeanon-emptyopensubsetofRnandJ:Ω→Rbeadifferentiablefunction.ForJtoadmitalocalminimumatpointx∗theEulerconditiondJ(x∗)=0musthold.Thefollowingisaconsequenceof(12.123)and(12.124):PROPOSITION454.–LetΩbeaconvexopensubsetofRnandletJ:Ω→RbefunctionofclassC2.ThefunctionJisconvexifandonlyifHJ(x)≥0foreveryx∈Ω.IfHJ(x)>0foreveryx∈ΩthenJisstrictlyconvex.Ontheotherhandthefollowingisaclassicresult:THEOREM455.–LetΩbeaconvexopensubsetofRnandletJ:Ω→Rbeadifferentiablefunction.IfJisconvexandthereexistsasolutiontotheEulerequationdJ(x∗)=0thenJ(x∗)isaglobalminimumofJ.IfinadditionJisstrictlyconvexthesolutionx∗hereaboveisuniqueandJadmitsastrictglobalminimumatthispoint.CoercivityLetΩbeanunboundedpartofRnandJ:Ω→Rbecontinuousfunction.Thisfunctionissaidtobecoerciveiflim(cid:10)x(cid:10)→+∞x∈ΩJ(x)=+∞.THEOREM456.–LetΩbeanunboundedclosedpartofRn;everycoercivefunctionJ:Ω→Radmitsaglobalminimum.432LinearSystemsPROOF.Let˜x∈ΩandΦ={x∈Ω:J(x)≤J(˜x)}.ThesetΦisclosed(becauseJiscontinuous)andboundedthuscompact.AsaresulttherestrictionofJtoΦadmitsaminimum(seetheproofofTheorem444).ThishereisobviouslyaglobalminimumofJ.EllipticityLetJ:Rn→RbeafunctionofclassC2.Thisfunctionissaidtobeellipticifthereexistsarealδ>0suchthatforeveryx∈RnHJ(x)−δIn≥0.THEOREM457.–LetJ:Rn→Rbeanellipticfunction;thisfunctionisstrictlyconvexandcoercive.PROOF.ItisobviousthatJisstrictlyconvex.Inadditionaccordingto(12.124)foranypointsxandx∗suchthatx(cid:5)=x∗J(x)=J(x∗)+dJ(x∗)(x−x∗)+12d2J(y)(x−x∗)2forsomey∈(x∗x).Accordingto(12.122)andtheconditionofellipticityJ(x)≥J(x∗)+dJ(x∗)(x−x∗)+δ2(cid:23)x−x∗(cid:23)2thusJ(x)→+∞as(cid:23)x(cid:23)→+∞.COROLLARY458.–LetJ:Rn→Rbeanellipticfunction;thisfunctionadmitsastrictglobalminimumattainedatapointx∗whichistheuniquesolutionoftheEulerequationdJ(x∗)=0.12.6.5.OptimizationalgorithmsTheprincipaloptimizationalgorithms(forproblemswithoutconstraintsandexceptfortheconjugategradientsalgorithm)arebrieﬂydescribedbelow.GradientmethodLetΩbeanon-emptyopensubsetofRnandJ:Ω→Rbeadifferentiablefunction.Letθ∈Ωandh∈Rnbesuchthatθ+h∈Ω.Wehave(seesection12.6.3)J(θ+h)−J(θ)=dJ(θ)h+o((cid:23)h(cid:23))(12.125)anddJ(θ)=∇J(θ)T.LEMMA459.–Lethbeoftheform−ρ∇J(θ)ρ>0.ThenJ(θ−ρ∇J(θ))<J(θ)if∇J(θ)(cid:5)=0andifρ>0issufﬁcientlysmall.Appendix1:Analysis433PROOF.Accordingto(12.125)J(θ−ρ∇J(θ))−J(θ)=−ρ(cid:23)∇J(θ)(cid:23)2+o(ρ).Anincrementhasaboveisinthedirectionoppositetothatofthegradient.Lemma459showsthatthisdirectionisadirectionofdescenti.e.inthisdirectionthecostfunctionJdecreasesatleastforasmallincrement.The“gradientmethod”isaniterativeminimizationmethodofthefunctionJ.Fromapointθ(0)∈Ωwewillconstructasequence(cid:9)θ(k)(cid:10)k≥0ofpointsofΩcalledaminimizingsequenceofthefollowingmanner:θ(k+1)=θ(k)−ρk∇J(cid:9)θ(k)(cid:10)whereρk≥0while∇J(cid:9)θ(k)(cid:10)(cid:5)=0.Recallthatifthereexistsθ(k)suchthat∇J(cid:9)θ(k)(cid:10)=0andifbothΩandJareconvexthenJ(cid:9)θ(k)(cid:10)isaglobalminimumofJ(seesection12.6.4Theorem455)thusθ(k)isthevalueofthevectorofparametersθwearelookingfor.Incertaincasesthe“step”ρkischosentobeconstant(withrespecttok)toreducethecalculations:thisistheﬁxed-stepgradientmethod.Wecanalsoatthecostofmoreimportantandextensivecalculations(butforbetterefﬁciency)optimizethestepρkbyminimizingateachiterationkthefunctionofasinglevariable˜Jk(ρ)=J(cid:9)θ(k)−ρ∇J(cid:9)θ(k)(cid:10)(cid:10)(withρ≥0).Thisiswhatwecallaunidirectionalminimization(madeinthedirectiondk=−∇J(cid:9)θ(k)(cid:10)(cid:5)=0).Denoteasρ∗ktheoptimalstepifitexists.THEOREM460.–SupposethatΩ=RnandthatJisofclassC2andiselliptic(section12.6.4).Thentheunidirectionalminimizationproblemadmitsauniquesolutionandwecanwriteρ∗k=argminρ≥0˜Jk(ρ).PROOF.SincethefunctionJiscoercivesoisobviously˜Jktoo.AsaresultaccordingtoTheorem456(section12.6.4)˜Jkadmitsaglobalminimumintheunbounded434LinearSystemsclosedconvexset[0+∞).Inadditiond˜Jkdρ(ρ)=−dJ(cid:9)θ(k)−ρ∇J(cid:9)θ(k)(cid:10)(cid:10)∇J(cid:9)θ(k)(cid:10)(12.126)d2˜Jkdρ2(ρ)=d2J(cid:9)θ(k)−ρ∇J(cid:9)θ(k)(cid:10)(cid:10)(cid:9)∇J(cid:9)θ(k)(cid:10)∇J(cid:9)θ(k)(cid:10)(cid:10)=(cid:9)∇J(cid:9)θ(k)(cid:10)(cid:10)THJ(cid:9)θ(k)−ρ∇J(cid:9)θ(k)(cid:10)(cid:10)∇J(cid:9)θ(k)(cid:10)≥δ(cid:29)(cid:29)(cid:29)∇J(cid:9)θ(k)(cid:10)(cid:29)(cid:29)(cid:29)2>0therefore˜Jkisstrictlyconvex(accordingtoProposition454ofsection12.6.4).Theminimumof˜Jkin[0+∞)isthusstrictandunique(accordingtoTheorem455).REMARK461.–Theoptimalstepρ∗kbelongsnecessarilyto(0+∞).Indeedifwehadρ∗k=0wewouldhaveJ(cid:9)θ(k)−ρ∇J(cid:9)θ(k)(cid:10)(cid:10)>J(cid:9)θ(k)(cid:10)foranyρ>0whichisimpossibleaccordingtoLemma459.Asaresultρ∗kischaracterizedbytheEulerequationd˜Jkdρ(ρ∗k)=0whichyields∇J(cid:9)θ(k+1)(cid:10)T∇J(cid:9)θ(k)(cid:10)=0accordingto(12.126).Inotherwordsthedirectionsoftwosuccessivedescentsareorthogonal.Onecanshowthefollowingresult([29]Theorem8.4-3):THEOREM462.–UnderthehypothesesofTheorem460theoptimal-stepgradientmethodconverges(i.e.(cid:9)J(cid:9)θ(k)(cid:10)(cid:10)→minJ).Thepointisnowtoexplicitlydeterminetheoptimalstepρ∗k.Thisisnotpossibleinthegeneralcaseanditisthennecessarytomakeuseofasearchbydichotomies.OneexceptiontothissituationisthatwherethecostfunctionJisquadratic(andelliptic)i.e.isoftheformJ(θ)=12θTAθ+bTθwhereAisasymmetricrealpositivedeﬁnitematrix.Letwk=Aθ(k)+b=∇J(cid:9)θ(k)(cid:10).Itiseasytoverifythatρ∗k=(cid:23)wk(cid:23)2wTkAwk.Appendix1:Analysis435Newton-RaphsonmethodTheNewton-Raphsonmethod–likeallmethodsbelow–isamethodofdescent.LetJ:Rn→RbeafunctionofclassC2andassumethatJiselliptic.WehavebyapplyingtheTaylor-Youngformulaintheneighborhoodofθ(k)(section12.6.3)J(θ)=Jk(θ)+o(cid:21)(cid:29)(cid:29)(cid:29)θ−θ(k)(cid:29)(cid:29)(cid:29)2(cid:22)whereJk(θ)isthe“secondorderapproximation”ofJintheneighborhoodofθ(k)i.e.settingHk=HJ(cid:9)θ(k)(cid:10)Jk(θ)=J(cid:9)θ(k)(cid:10)+∇J(cid:9)θ(k)(cid:10)(cid:9)θ−θ(k)(cid:10)+12(cid:9)θ−θ(k)(cid:10)THk(cid:9)θ−θ(k)(cid:10).ThefunctionJkisobviouslyellipticandthusadmitsastrictglobalminimumatthepointθ(k+1)characterizedbytheEulerequation∇Jk(cid:9)θ(k+1)(cid:10)=0(section12.6.4Corollary458).Wehave∇Jk(θ)=∇J(cid:9)θ(k)(cid:10)+Hk(cid:9)θ−θ(k)(cid:10)andthereforeθ(k+1)=θ(k)−H−1k∇J(cid:9)θ(k)(cid:10).(12.127)REMARK463.–(i)ContrarytothegradientmethodtheNewton-Raphsonmethoddoesnotrequireunidirectionalminimization(neverthelesssee(iii)below).(ii)InthecasewhereJisquadratic(andalsoelliptic)theNewton-Raphsonconvergesinoneiterationwhereasingeneralthegradientmethodconvergesinaninﬁnitenumberofiterations[29].Foranon-quadraticfunctionJwhenθisclosetotheoptimumJk(θ)isagoodapproximationofJ(θ)thustheNewton-Raphsonmethodconvergesrapidly.Itishowevertobeavoidedwhenθisstillfarawayfromtheoptimumandwewouldthenpreferthegradientalgorithm.(iii)Inordertoavoidthe“blocked”situationsfarawayfromtheoptimumwecanimprovetheNewton-Raphsonmethodbymakingaunidirectionalminimizationateachiteration.Inthiscase(12.127)isreplacedby⎧⎨⎩θ(k+1)=θ(k)−ρ∗kH−1k∇J(cid:9)θ(k)(cid:10)ρ∗k=argminρ≥0J(cid:9)θ(k)−ρH−1k∇J(cid:9)θ(k)(cid:10)(cid:10)(withofcourseρ∗k=1inthecaseofaquadraticfunction).Wearriveatamethodofdescentwithadirectiongivenbydk=−H−1k∇J(cid:9)θ(k)(cid:10).436LinearSystemsNewton-GaussmethodThebigdisadvantageoftheNewton-RaphsonmethodisthelargeamountofcalculationsnecessaryforthedeterminationoftheHessianmatrixHkateachiterationk.Thisamountcanbereducedinthecaseofaquadraticcriterioni.e.J(θ)=12(cid:12)i∈Iji(θ)2whereIisaﬁnitesetofindicesandwherethefunctionsji:Rn→RareofclassC2.WegetdJ(θ)=(cid:12)i∈Iji(θ)dji(θ)HJ(θ)=(cid:12)i∈I(cid:23)∇ji(θ)∇ji(θ)T+Hji(θ)ji(θ)(cid:24).IntheaboveexpressionthecalculationoftheHji(θ)’sisdisadvantageous.TheNewton-Gaussmethodneglectsthesetermswhichanywayarezeroifthefunctionsjiarelinear.WethusobtaintheapproximationHk(cid:9)(cid:11)i∈I∇ji(cid:9)θ(k)(cid:10)(cid:9)∇ji(cid:9)θ(k)(cid:10)(cid:10)T.(12.128)Besidessimplifyingthecalculationsanotheradvantageisthatthematrixontheright-handsideof(12.128)isalwayssymmetricrealnon-negativedeﬁnitewhichisanessentialpointfordktoactuallybeadirectionofdescent(seebelow).Quasi-NewtonmethodsLetJ:Rn→RbeafunctionofclassC1.Consideriterationsoftheformθ(k+1)=θ(k)+ρkdk(12.129)ρk>0whereeachdirectiondkisoftheformdk=−Gk∇J(cid:9)θ(k)(cid:10)(12.130)Gkbeingasymmetricrealmatrix.Wehavethefollowing:THEOREM464.–AsufﬁcientconditionfordktobeadirectionofdescentisthatGkbepositivedeﬁnite.Appendix1:Analysis437PROOF.WehaveaccordingtotheTaylor-YoungformulaJ(cid:9)θ(k)+ρdk(cid:10)=J(cid:9)θ(k)(cid:10)−ρ∇J(cid:9)θ(k)(cid:10)TGk∇J(cid:9)θ(k)(cid:10)+o(ρ)thusJ(cid:9)θ(k)+ρdk(cid:10)<J(cid:9)θ(k)(cid:10)if∇J(cid:9)θ(k)(cid:10)(cid:5)=0Gk>0andifρ>0issufﬁcientlysmall.Wecallaniterativemethodoftheform(12.129)(12.130)aquasi-NewtonmethodwheneverGk>0.SupposeJisofclassC2andiselliptic.Thenwecanoptimizethestepρkbyaunidirectionalminimizationwhichleadstoa“quasi-Newtonmethodwithoptimalstep”.FollowingthesamerationaleasintheproofofTheorem460andinRemark461weindeedobtainthefollowing:THEOREM465.–Let˜Jk(ρ)=J(cid:9)θ(k)+ρdk(cid:10).Theunidirectionalminimizationproblem˜Jk(ρ)→minadmitsauniquesolutionρ∗k=argminρ≥0˜Jk(ρ)>0characterizedbytheEulerconditionwhichisequivalentto∇J(cid:9)θ(k+1)(cid:10)Tdk=0.Twoquasi-Newtonmethodshavebeendiscussedsofar:–thegradientmethodwhichcorrespondstoGk=In;–theNewton-RaphsonmethodwhichcorrespondstoGk=H−1k(amatrixwhichispositivedeﬁniteifJiselliptic).IntermediatestrategiescanbeconsideredforexampleGk=(cid:20)In1≤k<k0H−1kk≥k0wherek0issuchthatθ(k0)is“sufﬁcientlyclose”totheoptimum.Thismethodmakesitpossibletohaveafasterconvergenceintheneighborhoodoftheoptimumthanthegradientmethod.OntheotherhandforpointsfurtherawayfromtheoptimumweavoidpossibledifﬁcultiessuchasforexamplegettingasingularHessianmatrixinthecasewhereJisnotelliptic.Inordertoavoidasingularityin438LinearSystemsthecalculationsattheneighborhoodoftheoptimumwecantaketheprecautionofreplacinghereaboveHkbyHk+εkInwhereεk>0isasufﬁcientlysmallrealnumber.ByreplacingagainHkbyitsapproximation(12.128)fromtheNewton-GaussmethodweobtaintheLevenberg-Marquardtmethodwhichisveryefﬁcient.12.7.ProbabilisticnotionsThissectionconsistsofsummariesandcomplements.Formoredetailsonthetheoryofmeasureandonthatofprobabilitiesseee.g.[103]and[83]respectively.12.7.1.Probabilityspaceσ-algebrasandmeasurabilityLetΩbeanon-emptyset.Aσ-algebraFoverΩisasetofpartsofΩcontainingtheemptysetandwhichisstablebypassingintocomplementaswellasbycountableunion;thepair(ΩF)iscalledaprobabilizablespace(thisnotionissynonymoustothatofmeasurablespace).Anintersectionofσ-algebrasisagainaσ-algebra.LetEbeanon-emptysetofpartsofΩ;thesmallestσ-algebracontainingEiscalledtheσ-algebrageneratedbyE.IfΩisatopologicalspace(section12.1.1)theσ-algebrageneratedbytheopensubsetsofΩistheBorelσ-algebraofΩ(andtheelementsofthisσ-algebraarecalledtheBorelsetsofΩ).Let(ΩF)and(YT)betwoprobabilizablespaces;afunctionY:Ω→Yissaidtobe(FT)-measurable(orF-measurableifYisatopologicalspaceandTisaBorelσ-algebraofY)ifforanyB∈TY−1(B)∈F.ItisimmediatethatY−1(T)isaσ-algebraitisalsothesmallestamongtheσ-algebrasGforwhichYis(GT)-measurable.Moregenerallylet(YiTi)i∈Ibeafamilyofprobabilizablespacesand(Yi)i∈IbeafamilyoffunctionsYi:Ω→Yi.Itisalsoclearthat7i∈IY−1i(Ti)isaσ-algebraanditisthesmallestamongalltheσ-algebrasGforwhicheachoftheYi’s(i∈I)is(GT)-measurable.DEFINITION466.–WecallY−1(T)theσ-algebrageneratedbyYand7i∈IY−1i(Ti)theσ-algebrageneratedbythefamily(Yi)i∈I.Appendix1:Analysis439ProbabilityLet(ΩF)beaprobabilizablespace.Ameasureofprobability(ormoresuccinctlyaprobability)PonsuchaspaceisapositivemeasuresuchthatP{Ω}=1whichwealsowriteasP{Ω}=(cid:1)ΩdP(ω)=1.Thenthetriple(ΩFP)iscalledaprobabilityspace.REMARK467.–AsimpleandimportantcaseisthatwhentheprobabilitymeasurePisdeﬁnedbyadensityp;inthiscaseΩ=RnFistheBorelσ-algebraBnofRndP(x)=p(x)dxwherep(x)≥0dxistheLebesguemeasureonRnand(cid:2)Rnp(x)dx=1.12.7.2.RandomvariableGeneralnotionsLet(ΩFP)beaprobabilityspace;arandomvariablewithvaluesinaprobabilizablespace(YT)isan(FT)-measurablemappingY:Ω→Y.(ArandomvariableisthusafunctionΩ(cid:20)ω(cid:3)→Y(ω)∈Y;thedependenceofYwithrespecttoωexpressesthe“randomdraws”thatwecanperformfromYandforﬁxedωa“realization”ofYisavalueY(ω).)Arandomvariableissaidtobereal(resp.complex)iftheprobabilizablespace(YT)isthesetofreal(resp.complexnumbers)numbersequippedwithitsBorelσ-algebra.Aneventis“almostsure”ifitsprobabilityisequalto1;inparticulartworandomvariablesXandYwithvaluesinthesameprobabilizablespace(YT)arealmostsurelyequalifP{X=Y}=1.DEFINITION468.–(i)LetF1andF2betwosub-σ-algebrasofF;thesesub-σ-algebrasaresaidtobeindependentifforanyA1∈F1andanyA2∈F2P{A1∩A2}=P{A1}P{A2}.(ii)LetYbearandomvariableandGbeasub-σ-algebraofF.WesaythatYisindependentofGifthelatterandtheσ-algebrageneratedbyYareindependent.(iii)LetY1andY2betworandomvariables;thesearesaidtobeindependentiftheσ-algebrastheygenerateareindependent.TheprobabilitylawofarandomvariableYwithvaluesin(YT)istheimageprobabilityofPbyYi.e.theprobabilityνYdeﬁnedon(YT)byνY(B)=P(cid:31)Y−1(B) B∈T.440LinearSystemsSupposeY1andY2aretworandomvariableswithvaluesintheprobabilizablespaces(Y1T1)and(Y2T2)respectively.ForanyB1∈T1andanyB2∈T2ν(Y1Y2)(B1×B2)=P{Y1∈B1Y2∈B2}=P(cid:31)Y−11(B1)∩Y−12(B2) .TheserandomvariablesareindependentifandonlyifforallsetsB1andB2asaboveP(cid:31)Y−11(B1)∩Y−12(B2) =P(cid:31)Y−11(B1) P(cid:31)Y−12(B2) .Wededucethefollowing:PROPOSITION469.–TheaboverandomvariablesY1andY2areindependentifandonlyifν(Y1Y2)(B1×B2)=νY1(B1)νY2(B2)∀B1∈T1∀B2∈T2.RandomvariablesoftheﬁrstorderLetXbearandomvariablewithvaluesinthemeasurablespace(KnBn)whereBnistheBorelσ-algebraofKn(K=RorC);abusingthelanguagewewillsaythatXhasitsvaluesinKn.18ThisrandomvariableissaidtobeoftheﬁrstorderifE[(cid:23)X(cid:23)]=(cid:2)Ω(cid:23)X(ω)(cid:23)dP(ω)<+∞where(cid:23).(cid:23)isthestandardEuclideanorHermitiannorm(section12.1.2).Byidentifyingtwoalmostsurelyequalrandomvariables(whichwewilldointhesequel19)thesetofrandomvariablesoftheﬁrstorderwithvaluesinKnisdenotedasL1(ΩFP;Kn)orL1(ΩFP)ifn=1andK=R;thisisaBanachspaceequippedwiththenorm(cid:23)X(cid:23)1(cid:1)E[(cid:23)X(cid:23)].ThequantityE[X]=(cid:1)ΩX(ω)dP(ω)iscalledtheexpectationofX.IfK=RE[X]isexpressedasafunctionoftheprobabilitylawνXofXaccordingtoE[X]=(cid:2)RnxdνX.(12.131)18.WecanalwayscomebacktothecasewhereK=RbyidentifyingCwithR2thusCnwithR2nequippedwithitsBorelσ-algebra.19.Thisissimilartowhatwasdoneinsection12.2.2.ThistimehoweverthemeasureconsideredistheprobabilityPinsteadoftheLebesguemeasure.Appendix1:Analysis441RandomvariablesofthesecondorderTherandomvariableXissaidtobeofthesecondorderif(cid:23)X(cid:23)2isarandomvariableoftheﬁrstorder.ThesetofrandomvariablesofthesecondorderwithvaluesinKnisdenotedasL2(ΩFP;Kn)orL2(ΩFP)ifn=1andK=R;thisisaHilbertspaceequippedwiththescalarproduct XY!=E[X∗Y](where(.)∗designatestheconjugate-transpose:seesection13.5.3);theassociatedHilbertnormisdenotedas(cid:23)X(cid:23)2.WehaveL2(ΩFP;Kn)⊂L1(ΩFP;Kn).Ifn=1|E[X]|2≤E(cid:23)|X|2(cid:24);thequantityσ2X=E(cid:23)|X|2(cid:24)−|E[X]|2isthevarianceofXandσXisitsstandarddeviation;ifn≥1thematrixQX=E(cid:25)(X−E[X])(X−E[X])∗(cid:26)iscalledthecovariancematrixofX.LetXandYbetworealrandomvariablesofthesecondorder.ThenXYisarandomvariableoftheﬁrstorderandaccordingto(12.131)E[XY]=(cid:2)R2xydν(XY)(xy).PROPOSITION470.–(i)IftherandomvariablesXandYareindependentthenE[XY]=E[X]E[Y].(ii)IfinadditiononeofthemiscenteredthenE(cid:23)|X+Y|2(cid:24)=E(cid:23)|X|2(cid:24)+E(cid:23)|Y|2(cid:24).PROOF.(i)AccordingtoProposition469dν(XY)(xy)=dνX(x)dνY(y)thusE[XY]=(cid:2)RxdνX(x)(cid:2)RdνY(y)=E[X]E[Y].WehaveE(cid:23)|X+Y|2(cid:24)=E(cid:23)|X|2(cid:24)+E(cid:23)|Y|2(cid:24)+2E[XY];asaresult(ii)isanimmediateconsequenceof(i).DistributionfunctionandprobabilitydensityLetX=(X1...Xn)bearandomvariablewithvaluesintheprobabilizablespace(RnBn)whereBnistheBorelσ-algebraofRn.ThedistributionfunctionofXdenotedasFXisdeﬁnedonRnbyFX(x1...xn)=P{X1<x1...Xn<xn}.442LinearSystemsThedistributionfunctionofXdeterminesitsprobabilitylaw.SupposethattheprobabilitylawofXadmitsadensityδXcalledtheprobabilitydensityofXthatistosaythatthemeasureνXisabsolutelycontinuouswithrespecttotheLebesguemeasuredxi.e.dνX=δXdx(12.132)(Remark467section12.7.1).WethenhaveFX(x1...xn)=(cid:2)X(x)δX(ξ)dξ(12.133)wherex=(x1..xn)X(x)=(1≤i≤n(−∞xi)andξ=(ξ1...ξn);ontheotherhandifXisoftheﬁrstorderwethenhaveaccordingto(12.131)(12.132)E[X]=(cid:1)RnxδX(x)dx.(12.134)ThefollowingresultisadirectconsequenceofProposition469andof(12.133):PROPOSITION471.–LetXandYbetwoindependentvariableswithvaluesinRnandRmrespectively.(i)TheyareindependentifandonlyifthedistributionfunctionF(XY)ofthepair(XY)satisﬁesforany(xy)∈Rn×RmtheequalityF(XY)(xy)=FX(x)FY(y).(ii)If(XY)admitsaprobabilitydensityδ(XY)thenXandYadmitprobabilitydensitiesδXandδYrespectivelytheformergivenbyδX(x)=(cid:2)Rmδ(XY)(xy)dyandthelatterbyasimilarexpression.InadditionXandYareindependentifandonlyifforany(xy)∈Rn×Rmδ(XY)(xy)=δX(x)δY(y).GaussianrandomvariableTheaboverandomvariableissaidtobeGaussianifitsprobabilitydensityisGaussian.ThatisδX(x)=1√(2π)ndetQXexp(cid:7)−(1/2)(x−¯x)TQ−1X(x−¯x)(cid:8)Appendix1:Analysis443where¯x=E[X]andQXisthecovariancematrix(assumedinvertible)ofX.If¯x=0andQX=InthenthecorrespondingprobabilitylawiscalledthereducedGaussian(ornormal)law.Gaussianrandomvariablesareparticularlyimportantduetothecentrallimittheorem:THEOREM472.–Let(Xn)beasequenceofindependentrealrandomvariablesofthesecondorderwithexpectation¯xstandarddeviationσandthesameprobabilitylaw.Thenasn→+∞therandomvariableYn=1σ√nn(cid:12)k=1(Xk−n¯x)convergesinlawtowardsthereducednormallaw(i.e.theprobabilitylawofYnweakly*convergestothereducednormallaw–see[83]formoredetails).IfinadditioneachXnhasaprobabilitydensitythentheprobabilitydensityofYnconvergesuniformlytothatofthereducednormallaw.12.7.3.ConditionalexpectationCaseofrandomvariablesofthesecondorderTheconditionalexpectationcanbedeﬁnedforrandomvariablesoftheﬁrstorderbywayofaslightlydifﬁcultapproachandisnotnecessaryforthisbook.Wethuswillonlyconsiderthecaseofrandomvariablesofthesecondorder.Let(ΩFP)beaprobabilityspaceletX∈L2(ΩFP;Kn)andletGbeasub-σ-algebraofF.LEMMA473.–(i)ThereexistsauniquerandomvariableˆX∈L2(ΩGP;Kn)suchthat(cid:29)(cid:29)(cid:29)X−ˆX(cid:29)(cid:29)(cid:29)2=minY∈L2(ΩGP;Kn)(cid:23)X−Y(cid:23)2.(ii)ThisrandomvariableischaracterizedbytherelationE[Y∗X]=E(cid:23)Y∗ˆX(cid:24)∀Y∈L2(ΩGP;Kn).(iii)ThemappingX(cid:3)→ˆXisK-linearfromL2(ΩFP;Kn)ontoL2(ΩGP;Kn)andhasanorm1.PROOF.AsalreadyseenL2(ΩFP;Kn)andL2(ΩGP;Kn)areHilbertspacesandL2(ΩGP;Kn)⊂L2(ΩFP;Kn).Itsufﬁcesthereforetoapplytheorthogonalprojectiontheorem(Theorem429section12.1.2).444LinearSystemsDEFINITION474.–(i)TheaboverandomvariableˆXiscalledtheconditionalexpectationofXwithrespecttotheσ-algebraGandisdenotedasE[X|G].(ii)LetYbearandomvariablewithvaluesintheprobabilizablespace(YT)andconsidertheσ-algebraG=Y−1(T)generatedbyY;E[X|G]isalsocalledtheconditionalexpectationofXwithrespecttotherandomvariableYandisdenotedasE[X|Y].(iii)BygivingtoYavaluey∈YE[X|Y]becomestheelementofKndenotedasE[X|Y=y].Letusnowestablishseveralpropertiesoftheconditionalexpectation.THEOREM475.–(i)ThemappingX(cid:3)→E[X|G]isK-linear.(ii)IfXisG-measurablethenE[X|G]=X.(iii)E[E[X|G]]=E[X].(iv)Giventwoσ-algebrasHandGsuchthatH⊂G⊂FwehaveE[E[X|G]|H]=E[X|H]=E[E[X|H]|G].PROOF.(i)isareformulationofLemma473(iii).(ii)IfXisG-measurablethenX∈L2(ΩGP;Kn)thusE[X|G]=X.(iii)LetY=ejthejthvectorofthecanonicalbasisofKn.ItisclearthatY∈L2(ΩGP;Kn)thusaccordingtoLemma473(ii)E[Xj]=E(cid:23)ˆXj(cid:24).Proceedingthiswayforanyj∈{1...n}weobtainE[X]=E(cid:23)ˆX(cid:24).(iv):WecanobtaintheorthogonalprojectionE[X|H]ofXontoL2(ΩHP;Kn)byﬁrsttakingitsorthogonalprojectionE[X|G]ontoL2(ΩGP;Kn)andthentheorthogonalprojectionofE[X|G]ontoL2(ΩHP;Kn)⊂L2(ΩGP;Kn)whichprovestheﬁrstequality.Thesecondimmediatelyfollowsfrom(ii)sinceE[X|H]isG-measurable.SupposenowthatXandYaretworandomvariablesofthesecondorderassumedtoberealforthesakeofsimplicityandletGbeasub-σ-algebraofF.PROPOSITION476.–(i)IfYisG-measurableandessentiallybounded(withrespecttotheprobabilitymeasureP)thenE[XY|G]=YE[X|G].(ii)IfXisindependentofGthenE[X|G]=E[X].PROOF.(i)WehaveYE[X|G]∈L2(ΩGP)andforanyZ∈L2(ΩGP)E[YE[X|G]Z]=E[XYZ]accordingtoLemma473(ii)whichprovestheresult.(ii)XisindependentofanyY∈L2(ΩGP)thusE[XY]=E[X]E[Y]=E[E[X]Y]andtheresultfollowsfromLemma473(ii).CasewhereaprobabilitydensityexistsSupposenowthatK=Randthat(XY)isarandomvariablewithvaluesinRn×Rmwhichadmitsaprobabilitydensity(xy)(cid:3)→δ(XY)(xy)(section12.7.2).Appendix1:Analysis445WecallthefunctiondeﬁnedbyδY=yX(x)=δ(XY)(xy)δY(y)theconditionaldensityofXwithrespecttoY=y;accordingtoProposition471(ii)δY(y)=(cid:1)Rnδ(XY)(xy)dx.AccordingtothesamepropositiontherandomvariablesXandYarethusindependentifandonlyifδY=yX(x)=δX(x)forany(xy)∈Rn×Rm.Inthegeneralcasethisisofcoursenottrue;insteadE[X|Y=y]=(cid:1)RnxδY=yX(x)dxwhichissimilarto(12.134).Chapter13Appendix2:Algebra13.1.Commutativeringsandﬁelds13.1.1.GeneralitiesThenotionofringAringRisasetequippedwithanaddition+suchthat(R+)isanadditivegroupandwithamultiplication×whichisdistributiverelativetotheadditionandsatisﬁestheusualproperties;wesupposethatthereexistsaunitelementdenotedby1suchthat1a=a1=aforanya∈R.Recallthatanyelementahasanopposite−a(suchthata+(−a)=0)sinceRisanadditivegroup.TheinvertibleelementsofR(i.e.theelementsuforwhichthereexistsaninversedenotedbyu−1suchthatuu−1=u−1u=1)arecalledtheunitsofthatring.Twoelementsaandbaresaidtobeassociates(orassociated)ifthereexistunitsυandυ(cid:2)suchthata=υbυ(cid:2).ForexampleintheringZofrationalintegerswheretheunitsare−1and1nand−nareassociates.Letusreviewafewexamplesofringsandthenotionsthatgowiththem.RingsofmatricesWedenotethesetofsquarematricesofordernwithrealentriesasRn×n;thisisaring.Itsunitsarethematriceswhosedeterminantisnon-zero(itsunitelementistheidentitymatrix).NotethatfortwomatricesAandBwehaveingeneralAB(cid:5)=BA;themultiplicationofmatricesisthusnon-commutative.OntheotherhandtheproductABcanbezerowhileneitherAnorBiszero;AandBarethencalledzerodivisors(AontheleftBontheright).DEFINITION477.–Aringissaidtobecommutativewhenitsmultiplicationiscommutative;aringthathasnozerodivisorsissaidtobeintegralortobeadomain.Thereforeacommutativeintegralringiscalledacommutativedomain.447Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.448LinearSystemsFromtheaboveitfollowsthatRn×nisanon-commutativeandnon-integralring.Inwhatfollowsallringsarecommutativedomainsexceptwhenotherwisestated.PolynomialringsAtypicalexampleofringisthesetK[s]ofallpolynomialsintheindeterminateswithcoefﬁcientsbelongingtotheﬁeldK(K=RorC).Theunitsarethepolynomialsofdegreezeroi.e.theconstantnon-zeropolynomials(byconventionthedegreeofthezeropolynomialis−∞).Ifg(s)andh(s)aretwopolynomialswehavetheequalityg(s)h(s)=h(s)g(s):theringK[s]isthuscommutative.Inadditionifg(s)h(s)=0wemusthaveg(s)=0orh(s)=0sothatK[s]isintegral.Apolynomialissaidtobemonicifthecoefﬁcientofitshighestdegreetermis1.Itisclearthattwomonicpolynomialswhichareassociatedarenecessarilyequal.Apolynomialp(s)∈K[s]ofdegreen≥0hasnrootsinC.Forexamplethepolynomialp(s)=(cid:15)s2+1(cid:16)(s−r)k(k≥1)hask+2rootsinCformingtheset{i−ir...r}whererisrepeatedktimes.Thesetofthedistinctrootsofthispolynomialis{i−ir}buttherootrhasanorderofmultiplicity(orforshortamultiplicity)equaltok.RingsofformalpowerseriesTakeathirdexample:thesetK[[s]]offormalpowerseriesinswithcoefﬁcientsbelongingtoKwhichiscomposedofallelementsoftheforma(s)=(cid:12)n≥0ansn.(13.1)Thispowerseriesisformalinthesensethatwedonotconcernourselveswiththequestionofconvergence:this“formalpowerseries”isnothingbutthesequenceofcoefﬁcients(an)exceptthatwedeﬁneonformalseriesanadditionandamultiplicationinthefollowingmanner:(cid:12)n≥0ansn+(cid:12)n≥0bnsn=(cid:12)n≥0(an+bn)sn(cid:12)n≥0ansn.(cid:12)n≥0bnsn=(cid:12)n≥0cnsnwherethesequence(cn)istheconvolutionproductof(an)and(bn)(seesection12.2.1);sincethetwolastsequencesareonlydeﬁnedinthesetofnaturalnumbersN(butcanbeextendedtothesetofrationalintegersZbypositivelysupportedsequences)thesequence(cn)iswell-deﬁned.Appendix2:Algebra449TheringK[[s]]iscommutativeandonecanshowthatitisintegral[25].AunitofK[[s]]isaformalpowerseriesoftheform(13.1)suchthata0(cid:5)=0.Itsinverseisthusoftheform(cid:11)n≥0bnsnwithb0=1a0.RingsofholomorphicfunctionsLetΩbeanon-emptyopensubsetofC.ThesetO(Ω)ofallholomorphicfunctionsinΩ(section12.4.1)isacommutativedomain.FieldsﬁeldoffractionsLetRbearing.Ifnon-zeroelementsofRareinvertibleRiscalledadivisionring.Aﬁeldisacommutativedivisionring.ConsideracommutativedomainR.Thesetofallelementsoftheformbawhereb∈Randabelongstothecomplementof{0}inR(denotedbyR(cid:2){0}orbyR×)isaﬁelddenotedasQ(R)andcalledtheﬁeldoffractionsofR.TheﬁeldoffractionsofZisQi.e.theﬁeldofrationalnumbers;32−53etc.areelementsofQ.TheﬁeldoffractionsofK[s]istheﬁeldK(s)ofrationalfunctionsinswithcoefﬁcientsintheﬁeldK(formoredetailsaboutrationalfunctionsseesection13.6.1).TheﬁeldoffractionsofK[[s]]istheﬁeldK((s))ofLaurentserieswithcoefﬁcientsintheﬁeldKandtheelementsofwhichareoftheform(cid:11)n≥νansnwhereν∈Zandan∈K.TheﬁeldoffractionsofO(Ω)(seeabove)istheﬁeldM(Ω)ofallmeromorphicfunctionsinΩ(section12.4.1).ApolynomialcanbeconsideredasaspecialmeromorphicfunctioninC.Thenarootofthispolynomialisnothingbutazeroofthismeromorphicfunction.LikewisearationalfunctioncanbeconsideredasaspecialmeromorphicfunctionsinC.*DifferentialﬁeldAdifferentialﬁeldisaﬁeldKequippedwithaderivationδ:a→aδsatisfyingLeibniz’srule:(ab)δ=abδ+aδb[31].ForexampletheﬁeldC(t)ofrationalfunctionsinthevariabletandwithcomplexcoefﬁcientsequippedwiththeusualderivativewithrespecttotisadifferentialﬁeld.AconstantofKisanelementa∈Kwhosederivativeaδiszero.ThesetofconstantsofKisaﬁeldk⊂KcalledtheﬁeldofconstantsofK.Thederivativeaδisoftendenotedas˙a.VectorspaceLetKbeaﬁeld1calledtheﬁeldofscalars(forexampleRorC).AvectorspaceoverK(alsocalledaK-vectorspace)isasetEequippedwithanadditionfromE×E1.Oradivisionring.450LinearSystemsintoEsatisfyingtheusualproperties(insuchawaythatEisanadditivegroup)aswellaswithamultiplicationfromK×EintoE.WecanaddorsubtracttwovectorsthatistwoelementsofEandwecanmultiplyavectorbyascalar.ThemostclassicexampleofK-vectorspaceisKnthesetofallelementsoftheform(x1...xn)xi∈Ki∈{1...n}.Thisvectorspaceisdetailedinsection13.3.1.ThesetsK[s]K[[s]]K(s)andK((s))arealsoK-vectorspaces(ofinﬁnitedimensioncontrarytoKn).AlgebraLetKbeaﬁeldandletEbeasetsatisfyingthefollowingconditions:(i)Eisaring(resp.acommutativering);(ii)EisK-vectorspace;(iii)themultiplicationoftheringEiscompatiblewiththemultiplicationfromK×EintoEinthesensewhereλ(ab)=(λa)b=a(λb)foranya∈Eb∈Eλ∈K.SuchasetEiscalledanunitaryK-algebra(resp.acommutativeunitaryK-algebra).Theadjective“unitary”meansthatthereexistsaunitelementinthisalgebra2.ForexamplethesetsK[s]K[[s]]K(s)andK((s))areunitarycommutativeK-algebras.ThesetKn×nisanon-commutativeunitaryK-algebra.*ThisdeﬁnitionofaK-algebraextendstothecasewhereKisacommutativeringreplacingcondition(ii)by(ii’):EisaK-module.*13.1.2.DivisibilityIdealAnidealina(commutative)ringRisasubgroupIoftheadditivegroupRsuchthatforanyr∈Randanya∈Ira∈I.SuchanidealIissaidtobegeneratedbyasetJ⊂RifanyelementofIisoftheform(cid:12)λrλaλaλ∈Jwhere(rλ)isaﬁnitelysupportedsequenceofelementsofR.WesaythatJisageneratingsetofIandthisidealissaidtobeﬁnitelygeneratedifJisﬁnite.AprincipalidealinRisanidealgeneratedbyauniqueelementaandisdenotedas(a)orRa.Itisclearthat(1)=R.2.Onecandeﬁnenon-unitaryalgebraswhichdonotadmitaunitelement(andthusarenotrings).Oneexampleofanon-unitaryalgebraisgiveninsection12.2.2(algebraK+).Appendix2:Algebra451MultipleanddivisorAmultipleofanelementaofRisanelementoftheformbab∈R.Leta(cid:5)=0andbbetwoelementsofR;wesaythataisadivisorofbwhichwedenoteasa|bifbisamultipleofawhichisequivalentto(b)⊂(a).Thenthereexistsauniqueelementcsuchthatb=caandthatwewritec=ba.ForexampleintheringZ3=62.lcmLetaandbbetwonon-zeroelementsofaringR.Thesetofmultiplesofaistheideal(a).Thesetofallcommonmultiplesofaandbisthus(a)∩(b).Supposethisidealisprincipali.e.oftheform(m).Thiselementmisaleastcommonmultiple(lcm)ofaandbsinceeverycommonmultipleofaandbisamultipleofm.Anyotherlcmofaandbisassociatedwithm.Wedenotelcm(ab)thesetofalllcm’sofaandb.Thisextendstoanynumberofnon-zeroelementsaii∈{1...n}:anlcmoftheseelements(ifitexists)isanelementmsuchthat(m)=∩ni=1(ai).IfR=K[s]lcm(ab)designatestheuniquelcmofaandbwhichisamonicpolynomial.gcdandGCDdomainLetaandbbetwonon-zeroelementsofaringR.Anelementdiscalledagreatestcommondivisor(gcd)ofaandbifdisadivisorofaandbandifanydivisorofaandbdividesd.Everyothergcdofaandbisassociatedwithd;thesetconsistingofalltheseelementsisdenotedasgcd(ab).IfR=K[s]gcd(ab)designatestheuniquegcdofaandbwhichisamonicpolynomial.Onecanshowthefollowing([31]section3.1Exercises7&8):LEMMA478.–(i)Ifnon-zeroelementsaandbofaringRadmitanlcmmtheyadmitagcddsuchthatab=dm(theexistenceofagcddoesnotingeneralimplythatofanlcm).(ii)InaringRallpairsofnon-zeroelementsadmitalcmifandonlyifallpairsofnon-zeroelementsadmitagcd.Theabovelemmaleadstothefollowing:DEFINITION479.–AGCDdomainisa(commutative)domainRsuchthatanytwonon-zeroelementsofthisringadmitagcd.452LinearSystemsCoprimenessTwonon-zeroelementsaandbofaringRaresaidtobecoprime(orrelativelyprime)iftheyhavenocommondivisorsexceptunitsinotherwordsifoneoftheirgcd’sis1.AtomsandprimesAnatominaringRisanon-zeroelementwhichisonlydivisiblebytheunitsofRandbyitself.AprimeelementinR(oraprimeforshort)isanon-zeroelementpwhichisanonunitandcannotdividetheproductoftwoelementsofRwithoutdividingoneofthem(*inotherwordsitisanelementpsuchthattheprincipalideal(p)isprime*).EveryprimeinRisanatomandtheconverseholdstrueifRisaGCDdomain([10]sectionVI.13RemarkafterProposition14).IntheringZtheprimesaretheprimenumbers235711etc.andtheirassociates−2−3−5−7−11etc.InC[s]theprimesarethepolynomialsoftheﬁrstdegree.InR[s]theprimesarethepolynomialsoftheﬁrstdegreeandthosepolynomialsoftheseconddegreewhichdonothaverealroots.ArepresentativesystemofprimesinadomainRisafamily(pi)ofprimesinRsuchthateveryprimeinR(ifany)isassociatedwithoneandonlyonepi.IfR=K[s](K=RorC)thechosenrepresentativesystemalwaysconsistsofmonicpolynomials.UniquefactorizationintoprimesUFD’sLetRbearinglet(pi)bearepresentativesystemofprimesinRandleta∈R×.Ifacanbeuniquelywrittenintheforma=υ’ipαii(13.2)whereυisaunitofRandthenon-negativeintegersαi’sarezeroexceptforaﬁnitenumberofthemwesaythataadmitsuniquefactorizationintoprimesgivenby(13.2);thequantityl(a)=(cid:12)iαiisthencalledthelengthofa.DEFINITION480.–AcommutativedomainRiscalledauniquefactorizationdomain(UFD)ifeverynon-zeroelementofRadmitsuniquefactorizationintoprimes.Onecanprovethefollowing([91]sectionX.1):iftheringRisaUFDthenR[X](whereXdesignatesanindeterminate)isagainaUFD.WededucebyinductionthatK[X1...Xn]isaUFD.PROPOSITION481.–AUFDisaGCDdomain.Appendix2:Algebra453PROOF.InaUFDletabegivenby(13.2)andletb=υ(cid:2)(ipβii;thend=’ipmin(αiβi)im=’ipmax(αiβi)iareagcdandanlcmofaandbrespectively.BézoutdomainsDEFINITION482.–A(commutative)Bézoutdomainisacommutativedomaininwhichanyﬁnitelygeneratedidealisprincipal.SupposeRisaBézoutdomainandletabbetwonon-zeroelementsofR;(a)+(b)isthesmallestidealcontaining(a)and(b).Thisidealisgeneratedbyaandbthusitisprincipalandthereexistsdsuchthat(a)+(b)=(d).Itisclearthatdisagcdofaandb.Wehavethusproventhefollowing:PROPOSITION483.–ABézoutdomainisaGCDdomain.Leta1...anbenon-zeroelementsofaBézoutdomainR;anelementdisagcdoftheseelementsifandonlyif(d)=(cid:11)1≤i≤n(ai).Thefollowingisprovedin([103]section15.15):PROPOSITION484.–LetΩbeanopenconnectedsubsetofC.TheringO(Ω)isaBézoutdomain.InparticulartheringO(C)ofallentirefunctions(section12.4.1)isaBézoutdomain.13.1.3.PrincipalidealdomainsDEFINITION485.–(i)Acommutativedomainiscalleda(commutative)principalidealdomainifeveryidealinthisringisprincipal.(ii)ANoetherianringisaringinwhicheveryidealisﬁnitelygenerated.THEOREM486.–Thefollowingconditionsareequivalent:(i)Risaprincipalidealdomain;(ii)RisaNoetherianringwhichisaBézoutdomain;(iii)RisbothaUFDandaBézoutdomain.PROOF.Theequivalencebetween(i)and(ii)isobvious.Fortheequivalencebetween(i)and(iii)see([11]sectionVII.2Exercise17andsectionVII.3Exercise10).AﬁeldFisatrivialexampleofaprincipalidealdomainforitsonlyidealsare{0}=(0)andF=(1).454LinearSystemsEuclideandomainDEFINITION487.–(A)AnEuclidean(commutative)domainisacommutativedomainRinwhichwehavedeﬁnedanEuclideanfunction.ThatistosayamappingθfromRintoN∪{−∞}(whereNisthesetofnon-negativeintegers)suchthat:(i)θ(0)=−∞;(ii)foranyelementsaandbofR×θ(ab)≥θ(a);(iii)foranyainRandbinR×thereexistelementsqandrofRsuchthata=bq+randθ(r)<θ(b)(Euclideandivisionofabyb:qisthequotientandrtheremainder).(B)AcommutativedomainissaidtobestronglyEuclideanifcondition(ii)isreplacedbythefollowingstrongercondition(ii’):foranyelementsaandbofRθ(a−b)≤max{θ(a)θ(b)}andforanyelementaandbofR×θ(ab)=θ(a)+θ(b);inthiscasetheEuclideanfunctionθiscalledadegree.REMARK488.–(i)ThequotientandremainderoftheEuclideandivisioninthecommutativedomainRareuniqueifandonlyiftheringisstronglyEuclidean.(ii)AnelementυofaEuclideanringisaunitifandonlyifθ(υ)=θ(1);ifa(cid:5)=0isnotaunitθ(a)>θ(1).(iii)TheringZofrationalintegersisEuclidean(withθ(n)=|n|ifn(cid:5)=0)butnotstronglyEuclidean.TheringK[s]isthemosttypicalexampleofastronglyEuclideandomain(θisthenthedegreeofapolynomialintheusualsense:θ=d◦).THEOREM489.–AnEuclideandomainisaprincipalidealdomain.PROOF.LetIbeanon-zeroidealinaEuclideandomainR.Amongthenon-zeroelementsofIconsideroneoftheelementsbofminimumdegree.LetxbeanyelementofIandperformtheEuclideandivisionofxbyb;wehavex=bq+rwithθ(r)<θ(b)thusθ(r)=−∞inotherwordsr=0andx∈(b).WethushaveI=(b).Non-EuclideanprincipalidealdomainsThereexistprincipalidealdomainsthatarenotEuclideandomains.AtypicalexampleisK[[s]].Wecannotdeﬁnethedegreeofaformalpowerseries.Ontheotherhandwecandeﬁneitsorder.Theorderω(a)oftheformalpowerseries(13.1)assumedtobenon-zeroisthesmallestintegerksuchthatak(cid:5)=0.Weputω(0)=+∞.AunitofK[[s]]isaformalpowerseriesofzeroorder.Aformalpowerseriesa(s)oforderkisoftheformskυ(s)whereυ(s)isaunitinK[[s]].Itisclearthata(s)|b(s)ifandonlyifω(a)≤ω(b).AllidealsinK[[s]]areoftheform(cid:15)sk(cid:16)anditfollowsthatK[[s]]isaprincipalidealdomain.FurthermoreK[[s]]admits(amongtheproperidealsi.e.thosethataredifferentfromK[[s]])auniquemaximalidealwhichis(s);aringthatadmitsauniquemaximalidealissaidtobelocal.TheringK[[s]]isthusaprincipalidealdomainwhichislocal.Appendix2:Algebra45513.1.4.MatricesovercommutativeringsMatrixcalculationsThesetofmatriceshavingprowsandmcolumns(saidtobeofsizep×m)withentriesinacommutativeringRisdenotedasRp×m.Asquarematrixofsizen×nissaidtobeofordern.Thebasicoperationsonmatrices(additionandmultiplicationoftwomatricesmultiplicationofamatrixbyascalar)areassumedtobeknown.TheyaredeﬁnedinthesamemanneroveracommutativeringRasoverRorC.TheidentitymatrixofordernisdenotedasIn.TheelementoftheithrowandjthcolumnofamatrixAisdenotedasaij.LetA∈Rp×m;thematrixAT∈Rm×ptheelementoftheithrowandthejthcolumnofwhichisajiiscalledthetransposeofA.DiagonalsumoftwomatricesLetA1∈Rp1×m1andA2∈Rp2×m2.ThematrixA=(cid:27)A100A2(cid:28)wherethezerosdesignateblocksofzerosofappropriatesizesisdenotedbyA=A1⊕A2andiscalledthediagonalsumofA1andA2.WecandeﬁneA1⊕A2⊕A3=(A1⊕A2)⊕A3andsoon.DeterminantLetusgooverafewpropertiesofdeterminantsagain:OnRn×nthedeterminantistheuniquen-linearalternatingformwithrespecttothecolumnsofthematricesofthissetandsuchthatdetIn=1;thisremainstrueifwereplacecolumnsbyrows.AsaconsequenceifwedenotebyA•j1≤j≤nthecolumnsofthematrixAdetA=0ifandonlyifthecolumnsofAareR-linearlydependentinotherwordsifthereexistelementscj∈R1≤j≤nnotallzerossuchthat(cid:11)nj=1cjA•j=0;thisisalsotruefortherowsAi•.Anotherconsequence(obtainedbyexchangingrowsandcolumns)isthatdetA=detAT.IfAandBbothbelongtoRn×ndet(AB)=det(BA)=detAdetB.ConsiderthematrixofordernA=(cid:27)A10A3A2(cid:28)(13.3)456LinearSystemswhereA1∈Rn1×n1A2∈Rn2×n2A3∈Rn2×n1withn1+n2=n.ThendetA=detA1detA2.Inparticulardet(A1⊕A2)=detA1detA2.FinallyifRisaﬁeldandA=(cid:27)XYZT(cid:28)(13.4)whereXandTaresquarematricesandXisnon-singulardetA=detXdet(cid:15)T−ZX−1Y(cid:16).(13.5)MinorsandcofactorsLetA∈Rp×m.AminorofordernofA(wheren≤min{pm})isthedeterminantofasquaresubmatrixofAofordern(obtainedbysuppressingrowsandcolumnsofA).Aprincipalminorisaminorformedbysuppressingrowsandcolumnswithsameindices.NowletA∈Rn×n;denotebymijtheminorofordern−1obtainedbysuppressingtheithrowandjthcolumnofA.Thecofactorofaijisαij=(−1)i+jmij.Thematrixαwithentriesαij1≤i≤n1≤j≤niscalledthecofactormatrixofA.WehavedetA=n(cid:12)j=1aijαij=n(cid:12)i=1aijαij(developmentsofthedeterminantalongtheithrowandthenthejthcolumnwhichareparticularcasesoftheLaplaceexpansionofA:see[10]sectionIII.8.6).Theaboveequalitiesmakeitpossibletocalculatethedeterminantofasquarematrixofordernfromthedeterminantsofsquarematricesofordern−1andthusbyrepeatingthisproceduretocalculatethedeterminantofanysquarematrixbecausethedeterminantofasquarematrixoforder1(i.e.ofanelementofR)isequaltothematrixitself.RankTherankrofamatrixA∈Rp×misthemaximalorderofthenon-zerominorsofA.ThisrankisequaltothenumberofrowsandthenumberofcolumnsofAthatarelinearlyindependent.Ofcourser≤min{pm}.Appendix2:Algebra457Ifr=mAissaidtoberight-regularorfullcolumnrank;ifr=pAissaidtobeleft-regularorfullrowrank;ifAiseitherright-orleft-regularitissaidtobesemiregular.FinallyasquarematrixAthatisbothleft-andright-regularissaidtoberegular(ornon-singular).AnelementofacommutativedomainRcanbeconsideredasanelementoftheﬁeldoffractionsF=Q(R)ofthisring(wethensaythatwehave“embeddedRinF”).ThismakesA∈Rp×manelementofFp×m.WethuscanaprioridistinguishtherankofAoverR(thatiswhenAisconsideredasanelementofRp×m)andtherankofAoverF(whenAisconsideredasanelementofFp×m);infactonecanshowthatthesetwonotionscoincide.ForthecalculationoftherankitismoreefﬁcienttoworkovertheﬁeldFsincethenotionofrankoveraﬁeldismorefamiliarandconvenientthanthatofrankoveraring.Wewillseelaterefﬁcientmethodsofcalculationoftherankofamatrixoveraﬁeld(Corollary501ofsections13.2.3and13.5.7).AmatrixisregularoverRifandonlyifitisinvertibleoverF.EXAMPLE490.–ConsiderthecasewhereR=K[s].AmatrixA(s)∈K[s]p×miscalledapolynomialmatrix.AccordingtotheabovetherankofAoverK[s]anditsrankoverK(s)coincide.AverydifferentnotionistherankofA(s)overKwhenthevariablestakesonaparticularvalue.Considerthefollowingmatrix:A(s)=(cid:27)s101(cid:28).(13.6)WehavedetA(s)=sthustherankofA(s)overR(denotedbyrkRA(s))isequalto2becausedetA(s)isnotthezeropolynomial.OntheotherhandoverK=RorCtherankofA(s)isequalto2ifs(cid:5)=0andto1ifs=0.Thevalues=0isarootofdetA(s).SylvesterinequalityandSylvesterdomainLetA∈Rp×nandB∈Rn×m;ifRisaﬁeldonecanshowthefollowingcalledtheSylvesterinequality(seeforexample[26])rk(A)+rk(B)−n≤rk(AB)≤min{rk(A)rk(B)};(13.7)inadditiontheabovetwoinequalitiesbecomeequalitiesifp=nandAisinvertible.IfRisaringcertainprecautionshavetobetaken.DEFINITION491.–ASylvesterdomainisacommutativedomainRoverwhichwhenevertwomatricesA∈Rp×nandB∈Rn×maresuchthatAB=0thenrk(A)+rk(B)≤n.Wenowcanstatethefollowing([31]Chapter5):458LinearSystemsTHEOREM492.–(i)Theinequalityontherightsideof(13.7)isvalidoveranycommutativedomainRandtheoneontheleftsideisvalid(foranymatricesA∈Rp×nandB∈Rn×m)ifandonlyifRisaSylvesterdomain.(ii)ABézoutdomainandaﬁeldareSylvesterdomains.InverseofasquarematrixConsiderthecofactormatrixαofamatrixA∈Rn×n.ItstransposeαTiscalledtheclassicaladjointmatrixortheadjugatematrixofA.Onecanshowthat:αTA=AαT=detA.InAnecessaryandsufﬁcientconditionforAtobeinvertibleoverF(thatisasanelementofFn×nandthustohaveaninverseinthisset)isdetA(cid:5)=0sincethenA−1=1detAαT.(13.8)InotherwordsanecessaryandsufﬁcientconditionforAtobeinvertibleoverFisthatAbenon-singular.IfthesubmatricesA1andA2ofthematrixAdeﬁnedby(13.3)areinvertibleoverFthenAisinvertibleoverFandA−1=(cid:27)A−110−A−12A3A−11A−12(cid:28).(13.9)Finallywestatethe“Inversionlemma”:LEMMA493.–LetABCDbematriceswithentriesinaﬁeldFofsizen×nn×mm×mandm×nrespectivelyandsuchthatAandCareinvertibleoverF.Then(A+BCD)−1=A−1−A−1B(cid:15)DA−1B+C−1(cid:16)−1DA−1.InvertiblematricesoveraringLetRbeacommutativedomain.Accordingto(13.8)amatrixA∈Rn×nisinvertibleoverR(ormoresuccinctlyinvertible)ifandonlyifitsdeterminantisaunitinR.ThesetofinvertiblematricesinRn×nisdenotedasGLn(R);itisamultiplicativegroupcalledthegenerallineargroupofsquarematricesofordernoverR.ThesubgroupofGLn(R)formedofmatricesofdeterminantequalto1(calledunimodularmatrices)isthespeciallineargroupofthesquarematricesofordernoverRandisdenotedasSLn(R)([10]n◦III.8.3).Appendix2:Algebra459ItisimportanttomakeadifferencebetweenaninvertiblematrixoverFandaninvertiblematrixoverR.Forexamplethematrix(13.6)isinvertibleoverF=K(s)anditsinverseisA−1(s)=(cid:27)1s−1s01(cid:28).ItisclearthatA−1(s)∈K(s)2×2butthatA−1(s)/∈K[s]2×2.ThematrixA(s)isthusnotinvertibleoverR.EquivalenceofmatricesTwomatricesA∈Rp×mandB∈Rp×maresaidtobeleft-equivalent(resp.right-equivalent)whichwedenoteasAl∼B(resp.Ar∼B)ifthereexistsamatrixU∈GLp(R)(resp.V∈GLm(R))suchthatB=U−1A(resp.B=AV).ThematricesAandBarethenofthesamerank.TwomatricesA∈Rp×mandB∈Rp×maresaidtobeequivalentwhichwedenoteasA∼BifthereexisttwomatricesU∈GLp(R)andV∈GLm(R)suchthatB=U−1AV.ElementaryandsecondaryoperationsElementaryoperationsThe3matricesU1U2andU3belowareinvertiblematricesofR2×2:U1=(cid:27)0110(cid:28)U2=(cid:27)1α01(cid:28)U3=(cid:27)100υ(cid:28)whereα∈RandυisaunitofR.LetA∈R2×m;itisimmediatethat:U1AisobtainedfromAbypermutingrows1and2ofthismatrix;U2AisobtainedbyaddingtotheﬁrstrowofAthesecondrowmultipliedbyα;U3AisobtainedbymultiplyingthesecondrowofAbyυ.Inamoregeneralmannerwedeﬁne3typesofelementaryoperationsontherowsofamatrixA∈Rp×m:(i)permutingtworowsofA;(ii)addingtotheithrowofAthejthrow(j(cid:5)=i)multipliedbyanelementofR;(iii)multiplyingarowofAbyaunitinR.SecondaryoperationsAsecondaryoperationontherowsisdeﬁnedinthefollowingmanner:(iv)Left-multiplytworowsofAbyamatrixU4∈GL2(R).Eachoftheelementaryorsecondaryoperationsontherowscorrespondstotheleft-multiplicationbyaninvertiblematrixoverR.460LinearSystemsWedeﬁneinthesamemanner3typesofelementaryoperationsonthecolumnsandonetypeofsecondaryoperations;eachofthemcorrespondstoaright-multiplicationbyaparticularinvertiblematrix.Theimportanceofelementaryandsecondaryoperationswillbecomeobviousinsection13.2.13.1.5.BézoutequationGeneralcaseLetabandcbe3elementsofaBézoutdomainRwhereaandbarenotbothzero.WecallBézoutequationanequationoftheformax+by=c(13.10)withunknownsxandy.Thisequationplaysaveryimportantroleincontroltheory.THEOREM494.–(i)TheBézoutequation(13.10)admitsasolutionifandonlyifc∈(d)whered∈gcd(ab).Suppose(x0y0)isasolutionofthisequation.Alltheothersolutionsareoftheformx=x0+qbdy=y0−qadwhereqspansR(parameterizationofthesolutionsbyq).(ii)IfRisastronglyEuclideandomainthereexistsauniquesolution(xy)suchthatd◦(y)<d◦(cid:15)ad(cid:16).PROOF.(i)ItisclearthattheBézoutequation(13.10)hasnosolutionifd(cid:4)c.Supposenowthatd|c;letαβandγbesuchthata=αd(thatisα=ad)b=βdandc=γd.Then(13.10)isequivalenttoαx+βy=γ(13.11)whereαandβarecoprime.ThusthereexistelementszandwofRsuchthatαz+βw=1andthereforeα(γz)+β(γw)=γ.TheBézoutequationthusadmitsasolution(γzγw).Ifnow(x0y0)isanysolutionof(13.10)orinanequivalentmannerof(13.11)everysolution(xy)satisﬁesα(x−x0)+β(y−y0)=0.Takingintoaccountthecoprimenessofαandβα|(y−y0)andβ|(x−x0).ThusthereexistsanelementqofRsuchthatx−x0=qβandy−y0=−qαwhichinturnprovidestheparameterizationofthesolutions.(ii)Wehavefromtheabovey0=qα+y.IfthedomainRisstronglyEuclideanweobtainthisexpressionbyperformingtheEuclideandivisionofy0byαqdesignatingthequotientandytheremainder;thusthereexistsauniquesolution(xy)suchthatd◦(y)<d◦(α).Appendix2:Algebra461BézoutequationoverK[s]SylvestertheoremLetbetwopolynomialsa(s)=a0sn+a1sn−1+...+anandb(s)=b0sm+b1sm−1+...+bmwherea0(cid:5)=0andb0(cid:5)=0.TheSylvestermatrixΣ(ab)ofthesetwopolynomialsisthesquarematrixofordern+mdeﬁnedbyΣ(ab)=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣a00b00...a1...b1b00...a2...a0...b1...0......a1bm......b0an...a20bm...b10.........0......···0an0···0bm⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦orthetransposeofthismatrix.Therearemcolumnscomprisingcoefﬁcientsaiandncolumnscomprisingcoefﬁcientsbi.ThedeterminantofthismatrixiscalledtheSylvesterresultant.TheSylvestertheoremstatesthatitisnon-zero(inotherwordstheSylvestermatrixisinvertible)ifandonlyifthepolynomialsa(s)andb(s)arecoprime([10]n◦IV.6.6).SylvestersystemassociatedwithaBézoutequationConsidertheBézoutequation(13.10).AccordingtoTheorem494eitherthisequationdoesnotadmitasolutionoritcomesdowntotheBézoutequation(13.11)wherethepolynomialsα(s)andβ(s)arecoprime.Inthesecondcasewhichwillhenceforthbetheonlyoneofinteresttouswearethusledbacktoresolvinganequationoftheform(13.10)wherethepolynomialsa(s)andb(s)arecoprime.Write⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩a(s)=a0sα+a1sα−1+...+aαb(s)=b0sβ+b1sβ−1+...+bβc(s)=c0sγ+c1sγ−1+...+cγx(s)=x0sξ+x1sξ−1+...+xξy(s)=y0sυ+y1sυ−1+...+yυwherea0andc0arenon-zero.Theunknownsarex0...xξy0...yυandarethusofthenumberξ+υ+2.Supposewithoutlossofgeneralitythatα+ξ≥β+υ.Theequationsareobtained462LinearSystemsbyequatingthecoefﬁcientsoftermsofthesamedegreein(13.10).Wenecessarilyhaveγ+1=α+ξ+1andthusγ=α+ξ.Asaresultthereareasmanyequationsasthereareunknownsifandonlyifξ+υ+2=α+ξ+1thatisυ=α−1.(13.12)AccordingtoTheorem494withd=1weknowthereexistsauniquesolution(x(s)y(s))suchthatυ≥d◦(y)satisﬁescondition(13.12).Itisthissolutionthatwewilldeterminebelow.Byequatingthecoefﬁcientsofthesamedegreein(13.10)weobtainthelinearsystem(13.13)belowwhichwewillcallthe“Sylvestersystem”associatedwiththeBézoutequationunderconsideration.⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣a00···············0a1a00b0......a1......b1...............a0......b00aα......a1bβ...b1b00aα......0......b1..................bβ...0···0aα0···0bβ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣x0x1...xξy0y1...yυ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣c0c1...cαcα+1......cα+ξ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(13.13)ThesquarematrixMofthissystemisoforderξ+2+υ.Thenumberofcolumnsthatcontainthecoefﬁcientsai(resp.bi)isξ+1(resp.υ+1).ThematrixMisoftheformM=(cid:27)A0(cid:27)Σ(ab)(cid:28)whereAisasquarelowertriangularmatrixoforder(α−β)andwhosealldiagonalentriesareequaltoa0.ThissubmatrixAisthusinvertibleorempty(α−βisthenumberofcoefﬁcientsabovetheﬁrstcoefﬁcientb0thatareequalto0whenonelooksatthematrixMfromlefttoright).AsaresulttheSylvestersystem(13.13)admitsauniquesolutionsinceΣ(ab)isinvertible(accordingtotheSylvestertheorem)thepolynomialsa(s)andb(s)beingcoprimebyassumption.Thesystem(13.13)canbesimpliﬁedwhenthematrixAisnon-emptythatiswhenα>β(whichisoftenthecase).Thenthepolynomialsa(s)c(s)andx(s)canbeassumedtobemonic;theﬁrstequationoftheSylvestersystembecomestrivialandAppendix2:Algebra463wecanremovethetermx0(equalto1)fromtheunknownsbysuppressingtheﬁrstrowandtheﬁrstcolumnofthematrixMaswellastheﬁrstelementofthevectorofunknownsandofthevectorintheright-handmemberwheretheci’sarereplacedbyci−aifor1≤i≤α.Wethusobtaintheformasindicatedinsection6.4.5.13.2.MatricesoverprincipalidealdomainsInthissectionunlessotherwisestatedRisacommutativeprincipalidealdomain.Howeverwewilluseﬁnerhypothesesonacase-by-casebasis.13.2.1.InvertiblematricesoverprincipalidealdomainsOnecanshowthatanyleft-multiplicationbyaninvertiblematrixoverRisacompositionofaﬁnitenumberofelementaryandsecondaryoperationsontherows(eachoftheseoperationscanofcoursebeusedmorethanonce):see([91]sectionX.7Prop.16).InthemoreparticularcasewhereRisanEuclideandomainonecanshowthattheuseofsecondaryoperationsisnotnecessary([91]sectionX.7Theorem17);itcanneverthelessprovidesimpliﬁcations.13.2.2.HermiteformColumnHermiteformTHEOREM495.–LetRbeacommutativeBézoutdomain.(i)AmatrixA∈Rp×misleft-equivalenttoanuppertriangularmatrixcalledacolumnHermiteformofA:(cid:27)T∗00(cid:28)(13.14)whereTisasquareuppertriangularmatrix.(ii)ThereexistsapermutationmatrixP∈GLm(R)suchthatthecolumnHermiteformofAPis(13.14)whereTisregularoforderequaltor=rkRA.PROOF.ThisproofisimportantbecauseitisconstructivethatisinconcretecaseswedeterminethecolumnHermiteformofamatrixbyrepeatingtherationalebelow.WewilllimitourselvestothecasewhereR=K[s](forthecasewhereRisacommutativeBézoutdomainsee[67]or[22]).1)Supposetheﬁrstcolumnisnon-zero.Bypermutingrowsweareledtothecasewherethecoefﬁcienta11issuchthat−∞<d◦(a11)≤d◦(aj1)foranyindexj∈{1...p}.Multiplytheﬁrstrowbytheinverseoftheleadingcoefﬁcientofa11464LinearSystemsandthenperformtheEuclideandivisionoftheaj1’s(j(cid:5)=1)bya11i.e.writeaj1=a11qj1+¯aj1whered◦(¯aj1)<d◦(a11).Fromthejthrow(j(cid:5)=1)subtracttheﬁrstonemultipliedbyqj1thenwrite¯a11=a11.Theﬁrstcolumnofthematrixthenobtainediscomposedofthe¯aj1andd◦(¯aj1)<d◦(¯a11)ifj(cid:5)=1.Wecancontinuetheprocessuntiltheonlynon-zerotermoftheﬁrstcolumnistheﬁrstone(whichiswithindex(11)inthematrix).2)Havingdonethatifthesecondcolumnisnon-zeroproceedinthesamemannerforthesubmatrixobtainedbysuppressingtheﬁrstrowandtheﬁrstcolumn.Theﬁrstcolumnobtainedforthissubmatrixeventuallyhasthesamepropertyastheﬁrstcolumnofthewholematrix.Itsﬁrstcoefﬁcientiswithindex(22)inthelatter.3)WenowperformtheEuclideandivisionofthecoefﬁcientwithindex(12)(inthewholematrix)bythatwithindex(22)thenwesubtractfromtheﬁrstrowthesecondrowmultipliedbythequotientofthisdivisionandweobtaind◦(¯a12)<d◦(¯a22).4)Continuingthiswayweobtaintheformdesiredforpart(i).5)(ii)isnowobvious.RowHermiteformByexchangingtherolesoftherowsandthecolumnsweobtainthefollowing:THEOREM496.–LetRbeacommutativeBézoutdomain.(i)AmatrixA∈Rp×misright-equivalenttoalowertriangularmatrixcalledarowHermiteformofA:(cid:27)T0∗0(cid:28)(13.15)whereTisasquarelowertriangularmatrix.(ii)ThereexistsapermutationmatrixP∈GLp(R)suchthattherowHermiteformofPAis(13.15)whereTisregularoforderequaltor=rkRA.13.2.3.SmithformTHEOREM497.–AmatrixA∈Rp×mofrankrisequivalenttoamatrixoftheformΣ=diag(α1...αr0...0)Appendix2:Algebra465wheretheαi1≤i≤rareallnon-zeroandaresuchthatαi|αi+11≤i≤r−1.3Thesediagonalelementsαiareuniquelydetermineduptoassociates.DEFINITION498.–ThematrixΣistheSmithformofAandtheelementsαi1≤i≤rareitsinvariantfactors.TwomatricesA∈Rp×mandB∈Rp×mareequivalentifandonlyiftheyhavethesameSmithform(theSmithformofamatrixisthusalsocalleditsSmithnormalformorforshortitsnormalform).Letusdeﬁneanelementarydivisorring(inthecasewheresucharingisacommutativedomain).DEFINITION499.–AnelementarydivisorringisaBézoutdomainRinwhichwhenever(a)+(b)+(c)=Rthereexistpandqsuchthat(pa)+(pb+qc)=R.REMARK500.–ThestatementofTheorem497remainsvalidwhenRisanelementarydivisorring.Allprincipalidealdomainsdomainsareelementarydivisorringsbutherearetwoexampleselementarydivisorringswhicharenotprincipalidealdomains:(i)theringO(C)ofentirefunctions([67]section5);(ii)theringE=R(s)[e−s]∩O(C)(theelementsofEarecalledpseudo-polynomials;Ewasintroducedin[54]andusedin[86][55]forthestudyofsystemswithcommensuratedelays).ProofofthetheoremWewillonlygiveheretheconstructivepartoftheproof.ForaproofofuniquenessoftheSmithform(guaranteedundertheconditionthateachinvariantfactorcanbereplacedbyanassociate)seeforexample([10]ChapterVII).WethuswillshowhowonecanconstructtheSmithformofamatrixA.WelimitourselvestothecasewhereRisanEuclideandomain.IfA=0thereisnothingtoprove.ObtainingadiagonalformIfA(cid:5)=0byapermutationofcolumnsweareledtothecasewheretheﬁrstcolumnofAisnon-zero.ProceedingasinTheorem495wearriveusingelementaryorsecondaryoperationsonrowstothecasewherethisﬁrstcolumnhasauniquenon-zeroelementa11.Nowdothesamefortheﬁrstrow:theEuclideandivisionofa1j(j>1)bya11iswrittenasa1j=a11q1j+¯a1jwhered◦(¯a1j)<d◦(a11).Usingelementaryorsecondaryoperationsoncolumnswethusobtain(a11¯a12...¯a1m)astheﬁrstrow.3.Abusingthelanguagediag(α1...αr0...0)representsamatrix(squareornotdependingonthecontext)theelementsoftheprincipaldiagonalofwhichareα1...αr0...0andalltheotherentriesarezero.466LinearSystemsByapermutationofcolumnswebringto(11)positionamongtheseelementsoneofthosethatarenon-zeroandofminimumdegree.Wecannowre-iteratetheabovewholeprocess(byﬁrstoperatingontherowsandthenonthecolumns);theseiterationswillnecessarilystopbecauseeachtimethedegreeofthetermwithindex(11)decreases.Wethusﬁnishbyobtainingamatrix¯AequivalenttoAandoftheform¯A=β1⊕¯A1.Dothesamewiththesubmatrix¯A1andsoon.Weﬁnallyobtaintheformdiag(β1...βr0...0)wheretheβi’s1≤i≤rareallnon-zero.Nonethelesstheyingeneraldonotsatisfytherequireddivisibilityproperty.ObtainingthedivisibilitypropertyConsiderthesubmatrixB=diag(β1β2).Letγ1∈gcd(β1β2)insuchawaythat(accordingtotheBézouttheorem)thereexistxandyinRsuchthatxβ1+yβ2=γ1.Thereexists¯β1inRsuchthatβ1=γ1¯β1.Weobtain(cid:27)1y01(cid:28)(cid:27)β100β2(cid:28)(cid:27)x110(cid:28)=(cid:27)γ1β1β20(cid:28)∼(cid:27)γ100¯β1β2(cid:28)thuswecanlowerthedegreeofβ1exceptifβ1|β2forinthiscaseγ1=β1.Bycontinuingthisprocedureweﬁnallyobtaindiag(β1...βr0...0)∼diag(α1...αr0...0)whereαi|αi+11≤i≤r−1.Theconstructivepartofthetheoremisthusproven.EquivalenceoveraﬁeldCOROLLARY501.–AmatrixA∈Fp×m(whereFisaﬁeld)ofrankrisequivalenttothematrixΣ=(cid:27)Ir000(cid:28).Indeedwecanapplythepreviousresultbutasdivisibilityistrivialinaﬁeldtheinvariantfactorscanbetakenas1.Weobtainthisformbyusingonlytheﬁrstpartoftheaboveprocedurewhichitselfalsobecomesconsiderablysimpler.Thismethodisanefﬁcientwayofcalculatingtherankofamatrixoveraﬁeld.Wewillseeanevenmoreefﬁcientmethod–fromanumericalpointofview–later(section13.5.7)whenF=CorR.Appendix2:Algebra467DirectcalculationofinvariantfactorsTheinvariantfactorsofamatrixA∈Rp×mofrankrsatisfythefollowingpropertywhichcanbeemployedfortheirdirectcalculation(whichincertaincasescanbeadvantageousespeciallywhenAistriangular):PROPOSITION502.–Letα1...αrbetheinvariantfactorsofA.Thenα1isagcdoftheelementsofAandforanyintegerq∈{1...r}(qi=1αiisagcdofthenon-zerominorsoforderqofA.(InparticularifA∈Rn×nisnon-singular(ni=1αi=detA.)ExampleLetR=K[s]andA=diag(β1β2β3)whereβ1=s+2β2=(s+1)(s+2)β3=(s+1)(s+3).WegetfromProposition502α1=gcd(β1β2β3)=1α2=(s+1)(s+2)α3=(s+1)(s+2)(s+3).MarkingoutelementaryoperationsTheadvantageofusingrowandcolumnelementaryandsecondaryoperationsisthatitallowsonetoeasilydeterminetheinvertiblematricesUandVsuchthatU−1AV=ΣwhereΣistheSmithformofA.IndeedletM=(cid:27)AIpIm0(cid:28).ByoperatingontheﬁrstprowsandtheﬁrstmcolumnsofMweobtain(cid:27)U−100Im(cid:28)M(cid:27)V00Ip(cid:28)=(cid:27)ΣU−1V0(cid:28).Thismethodcanalsobeappliedofcoursetodetermineaninvertiblematrixthatwillallowonetotransform(byleft-orright-equivalence)amatrixAintooneofitsHermiteforms.468LinearSystemsExampleOverR=K[s]letA=(cid:27)sss2ss2s4(cid:28);thenM=⎡⎢⎢⎢⎢⎣sss210ss2s401100000100000100⎤⎥⎥⎥⎥⎦∼⎡⎢⎢⎢⎢⎣s00100s(s−1)0−111−1s20001−s(s+1)0000100⎤⎥⎥⎥⎥⎦;fromwhichΣ=(cid:27)s000s(s−1)0(cid:28)U−1=(cid:27)10−11(cid:28)V=⎡⎣1−1s201−s(s+1)001⎤⎦.13.2.4.ElementarydivisorsLetαj+1...αrbethenon-invertibleinvariantfactorsofamatrixAdeﬁnedoveraprincipalidealdomainR.Ifwefactorizeeachoftheseinvariantfactorsintoprimestheytaketheformofproductsofpairwisecoprimetermsoftheformpnkwherethepk’sareprimesinR.DEFINITION503.–Thesepnk’saretheelementarydivisorsofA.Thenumberoftimesweencounteroneoftheseelementarydivisorsinalltheabovedecompositionsintoprimesisitsmultiplicity(oritsorderofmultiplicity).ExampleOverR=K[s]letA∼diag(α1α2α30...0)withα1(s)=(s−1)2(s−2)α2(s)=(s−1)2(s−2)2α3(s)=(s−1)2(s−2)2(s−3)Theelementarydivisorsare:δ1(s)=(s−1)2(oforder3)δ2(s)=s−2(oforder1)Appendix2:Algebra469δ3(s)=(s−2)2(oforder2)δ4(s)=s−3(oforder1).CalculationofinvariantfactorsfromelementarydivisorsAsseenintheaboveexample(andasaresultofthedeﬁnition)thecalculationoftheelementarydivisorsfromtheinvariantfactorsisimmediate.Converselyknowingtheelementarydivisorsofamatrixwecancalculateitsinvariantfactors.Itsufﬁcestoconstructatableinwhicheachrowiscomposedofthepowersofthesameprimerepeatedanumberoftimesequalitsorderofmultiplicitythesepowersbeingindecreasingorder.Theproductofthecolumnsthusprovidesinreverseordertheinvariantfactors.Usingthepreviousexample:(s−1)2(s−1)2(s−1)2(s−2)2(s−2)2s−2s−311α3(s)α2(s)α1(s)13.2.5.SmithzerosConsiderthecasewhereR=C[s].LetA∈C[s]n×mandletα1(s)...αr(s)beitsinvariantfactors.Letα(s)=(ri=1αi(s).Therootsofα(s)arecalledtheSmithzerosofA.IntheaboveexampletheSmithzerosofAformtheset{111111222223}.IndeedthenumberoftimesaSmithzerozisrepeatedinthissetisequaltoitsorderofmultiplicitywhenzisconsideredasarootofthepolynomialα(s).LetzbeaSmithzeroofAandlet(s−z)ν1...(s−z)νk(1≤ν1≤...≤νk)betheelementarydivisorsofAthataremultiplesofs−zeachoftheseelementarydivisorsrepeatedanumberoftimesequaltoitsorderofmultiplicity.Weadoptthefollowingdeﬁnition[19]:DEFINITION504.–Theintegersν1...νkarethestructuralindicesoftheSmithzeroz(orofthematrixAatz)ρ=νkistheorderofthiszeroand(cid:11)ki=1νiisitsdegree.470LinearSystemsNoticethatifA∈C[s](thatisifthispolynomialmatrixisonlyapolynomial)theSmithzerosofthismatrixaretherootsofthispolynomial.LetzbeoneoftheseSmithzeros;ithasonlyonestructuralindexequaltoitsordertoitsdegreeandtotheorderofmultiplicityoftherootz.Letuscontinuewiththeexampleofsection13.2.4:theSmithzero1hasstructuralindices{222}itsorderis2itsdegreeis6.TheSmithzero2hasstructuralindices{122}itsorderis2itsdegreeis5.TheSmithzero3hasauniquestructuralindex1;itsorderanditsdegreearebothequalto1.13.2.6.DivisibilityofmatricesLeft-divisorLetRbearingletE∈Rn×kE(cid:5)=0andletL∈Rn×n.WesaythatLisaleft-divisorofEifthereexistsamatrixX∈Rn×msuchthatE=LX(whichimpliesrkL≥rkEaccordingtotheSylvesterinequality:seeTheorem492section13.1.4).NowletA∈Rn×n(cid:1)andB∈Rn×m.WesaythatL∈Rn×nisacommonleft-divisorofAandBifLisaleft-divisorofthematrixE(cid:1)(cid:25)AB(cid:26).Greatestcommonleft-divisorLetABandEbeasabove;L∈Rn×nisagreatestcommonleft-divisor(gcld)ofAandBifLisaleft-divisorofEandifanyleft-divisorofEisaleft-divisorofL.THEOREM505.–SupposethatRisaBézoutdomainandletr(cid:1)rk(cid:25)AB(cid:26).(i)ThereexistsamatrixL∈Rn×nofrankrsuchthat(cid:25)AB(cid:26)r∼(cid:25)L0(cid:26)andLisagcldofAandB.(ii)Thesetofallgcld’sofAandBisthesetofallmatriceswhichareright-equivalenttoL.PROOF.AccordingtoTheorem495(section13.2.2)weknowthat(cid:25)AB(cid:26)r∼(cid:25)L0(cid:26)whereL∈Rn×nisobviouslyofrankr.WeeasilydeducethatLisagcldofAandB.Foradetailedproofof(ii)see([114]section4.1Lemma2).Left-primenessandcoprimenessDEFINITION506.–(i)LetE∈Rn×kbeamatrixofrankn;Eissaidtobeleft-primeifEisright-invertibleoverR.(ii)LetA∈Rn×n(cid:1)andB∈Rn×mbesuchthatE(cid:1)(cid:25)AB(cid:26)isofrankn;thematricesAandBaresaidtobeleft-coprimeifEisleft-prime.REMARK507.–IfRisnotaBézoutdomainthereexistotherweakerformsofleft-primenessofamatrix(seeforexample[121]).Appendix2:Algebra471Right-divisibilityWhathasjustbeensaidaboutleft-divisibilitytransposesinanobviouswaytoright-divisibility.InparticularEisright-primeifandonlyifETisleft-prime;thematricesAandCareright-coprimeifandonlyifATandCTareleft-coprime.LikewiseTheorem505transposesinanobviouswaytothecaseofright-coprimeness(thedetailsarelefttothereader).13.2.7.CoprimefactorizationsLetRbeacommutativedomainletF=Q(R)beitsﬁeldoffractions(seesection13.1.1)andletG∈Fp×m.DEFINITION508.–Apairofmatrices(DlNl)∈Rp×p×Rp×misaleft-coprimefactorizationofGoverRif:DlisinvertibleoverFG=D−1lNlandthematrices{DlNl}areleft-coprime.Inthesamemannerwedeﬁnearight-coprimefactorization(NrDr)ofG=NrD−1r.Left-coprimefactorizationshavethepropertybelow(thecorrespondingstatementforright-coprimefactorizationsislefttothereader).LEMMA509.–Supposethataleft-coprimefactorization(DlNl)ofGoverRexists;then(D(cid:2)lN(cid:2)l)isaleft-coprimefactorizationofGoverRifandonlyifthereexistsaninvertiblematrixU∈Rp×psuchthat(cid:25)DlNl(cid:26)=U(cid:25)D(cid:2)lN(cid:2)l(cid:26).PROOF.If(D(cid:2)lN(cid:2)l)isaleft-coprimefactorizationofGoverR(cid:25)D(cid:2)lN(cid:2)l(cid:26)isright-invertibleandG=D(cid:2)−1lN(cid:2)l.ThusthereexistmatricesX(cid:2)andY(cid:2)overRsuchthatD(cid:2)lX(cid:2)+N(cid:2)lY(cid:2)=Iandsoleft-multiplyingthisrelationbyDlD(cid:2)−1lDlX(cid:2)+Y(cid:2)=DlD(cid:2)−1lthereforeDlD(cid:2)−1l∈Rp×p.BysymmetryD(cid:2)lD−1l∈Rp×p.AsaresultDlD(cid:2)−1l=Uisinvertibleand(cid:25)DlNl(cid:26)=U(cid:25)D(cid:2)lN(cid:2)l(cid:26).Theconverseisobvious.DEFINITION510.–AmatrixG∈Fp×madmitsadoubly-coprimefactorizationoverRifwiththeabovenotation(cid:27)∗∗DlNl(cid:28)(cid:27)−Nr∗Dr∗(cid:28)=IwheretheasterisksdenotesubmatriceswithentriesinR.472LinearSystemsThefollowingisrelatedtothedoubly-coprimefactorization:DEFINITION511.–LetE∈Rn×kbeamatrixofrankn;Eissaidtobecompletableifthereexistsamatrix∗suchthat(cid:27)E∗(cid:28)isinvertibleoverR.LetRbeacommutativedomainletF=Q(R)beitsﬁeldoffractionsandletM(F)bethesetofallmatricesofﬁnitesizewithentriesinF.Theresultbelowcanbeeasilyproven([114]section8.1)andjustiﬁesDeﬁnition510(ii):LEMMA512.–SupposethatG∈M(F)admitsaleft-coprimefactorization(DlNl)overR;thenGadmitsaright-coprimefactorizationoverRifandonlyifE(cid:1)(cid:25)DlNl(cid:26)iscompletable(inotherwordsifGadmitsadoubly-coprimefactorizationoverR).DEFINITION513.–AcommutativedomainRhavingthefollowingpropertyiscalledanHermitering:anyrowA=(cid:25)a1···an(cid:26)suchthat(cid:12)1≤i≤n(ai)=Riscompletable.AllSylvesterdomains(Deﬁnition491section13.1.4)areHermiteringsbuttheconversedoesnothold([31]section5.5).Onecanprovethefollowing([114]section8.1):THEOREM514.–(A)Thefollowingconditionsareequivalent:(i)RisaBézoutdomain;(ii)everymatrixG∈M(F)admitsbothaleft-coprimefactorizationandaright-coprimefactorizationoverR;(iii)everymatrixG∈M(F)admitsadoublycoprimefactorizationoverR.(B)Thefollowingconditionsareequivalent:(i)RisaHermitering;(ii)ifG∈M(F)admitseitheraleft-oraright-coprimefactorizationoverRthenitadmitsadoubly-coprimefactorizationoverR.13.2.8.BézoutmatrixequationsLeftBézoutequationLetRbearingletA∈Rn×n(cid:1)andB∈Rn×mbetwomatricessuchthatrk(cid:25)AB(cid:26)=nandletC∈Rn×n.ThefollowingequationiscalledaleftBézoutmatrixequation:AX+BY=C(13.16)wherethematricesX∈Rn(cid:1)×nandY∈Rm×naretheunknowns.Appendix2:Algebra473THEOREM515.–SupposeRisaBézoutdomainandletLbeagcldofAandB.Thenequation(13.16)admitsasolutionifandonlyifCisaright-multipleofL.PROOF.ThereexistmatricesA(cid:2)∈Rn×n(cid:1)andB(cid:2)∈Rn×msuchthatA=LA(cid:2)andB=LB(cid:2)thusforanymatricesX∈Rn(cid:1)×nandY∈Rm×nAX+BY=L(A(cid:2)X+B(cid:2)Y);theconditionisthereforenecessary.ConverselysupposethatC∈Rn×nisaright-multipleofLandthusoftheformC=LM.WeknowthatthereexistsaninvertiblematrixVsuchthat(cid:25)AB(cid:26)V=(cid:25)L0(cid:26).BydecomposingVasV=(cid:27)V11V12V21V22(cid:28)withthepropersizesweobtain(cid:20)AV11+BV21=LAV12+BV22=0.AsaresultAV11M+BV21M=LM=Cand(V11MV21M)isasolutionof(13.16)whichcompletestheproof.RightBézoutequationWeleaveittothereadertotransposeTheorem515tothecaseofa“rightBézoutmatrixequation”XA+YB=C.13.3.Homomorphismsofvectorspaces13.3.1.VectorspacesThenotionofK-vectorspaceisclassicwhenKisaﬁeld4whichthereadercanassumetobeRorC(forfurtherdetailsseesection13.4.1).BasisHereweareonlyconcernedwithﬁnite-dimensionalK-vectorspaces.AK-vectorspaceEissaidtobeofdimensionnwhenanybasisofEhasnelements.Tomakethisnotionclear:aﬁnitesequence(ei)1≤i≤nofelementsofEisabasisofEifanyvectorxofEcanbeexpressedinauniquemannerasaK-linearcombinationofvectorsofthisbasisthatisx=n(cid:12)i=1xiei.(13.17)4.Avectorspacecanbedeﬁnedoveradivisionring.474LinearSystemsThexi∈Kin(13.17)arethecomponentsofxinthebasisunderconsideration.Thecolumn-matrixX=[x1...xn]Tistherepresentativematrix(ortherepresentativeforshort)ofxinthisbasis.WhatwehaveseensofarshowsthatthechoiceofabasismakesitpossibletorepresentanyvectorofEbyanelementofKn.UsingatermspeciﬁedlaterEis“isomorphic”toKn.ThereforeKnisthe“model”ofallK-vectorspacesofdimensionn.The“canonicalbasis”(γi)1≤i≤nofKnisdeﬁnedinthefollowingmanner:γi=(0...010...0)i∈{1...n}wherethe1islocatedattheithposition.Inthisbasistherepresentativeof(x1...xn)isthecolumnmatrix[x1...xn]Twhichwewillalsocalla“columnvector”.QuotientspaceLetE1beasubspaceofE.WecanconsiderthefollowingequivalencerelationR:xRy⇔x−y∈E1.ThesetconsistingoftheequivalenceclassesisagainaK-vectorspacecalledthequotientspaceE/E1.DirectsumLetE1E2betwosubspacesofthesamevectorspaceE.WedenoteasE1+E2thesubspaceofEconsistingofallelementsx=x1+x2(x1∈E1x2∈E2);E1+E2iscalledthesumofE1andE2.IfE1∩E2=0(where0denotesthesubspaceofEconsistingofzeroalone)theaboveexpressionofxisuniqueandthesumofE1andE2issaidtobe“direct”;itisthendenotedasE1⊕E2.LetE1E2betwovectorspaces.TheproductE1×E2isthesetofallpairs(x1x2)wherex1∈E1andx2∈E2;obviouslyE1×E2hasastructureofvectorspace.Byidentifyingx1andx2with(x10)and(0x2)respectivelytheproductE=E1×E2(sometimescalledtheexternaldirectsumofE1andE2)canbeidentiﬁedwithE1⊕E2whereE1andE2areconsideredassubspacesofE.LetbethedirectsumE=E1⊕E2;(13.18)E2iscalledasupplementofE1.(Suchasupplementisnotunique.)Thenletx=x1+x2wherex1∈E1andx2∈E2;x1andx2areuniquelydeterminedinfunctionAppendix2:Algebra475ofx.Thevectorx1iscalledthecomponentofxinE1andthemapp1:x(cid:3)→x1iscalledtheprojectionofEontoE1paralleltoE2.LetB1=(εi)1≤i≤n1beabasisofE1andB2=(ηi)1≤i≤n2abasisofE2;then(cid:15)ε1...εn1η1...ηn2(cid:16)isabasisBofE=E1⊕E2;wedenotethisbasisbyB18B2andwecallittheconcatenationofbasesB1andB2(thisisnotasimpleunionsincetheorderofthebasisvectorsisimportantasspeciﬁedabove).Asaresult:dim(E1⊕E2)=dimE1+dimE2.(13.19)Thefollowingisclassicandimportant:THEOREM516.–LetEbeaK-vectorspace;everysubspaceofEadmitsasupplement.COROLLARY517.–(Theoremoftheincompletebasis).LetEbeaK-vectorspaceofdimensionnandlet(xi)1≤i≤ρbeasequenceofρlinearlyindependentvectors(ρ<n).Thereexistsasequence(xi)ρ+1≤i≤nofvectorsofEsuchthat(xi)1≤i≤nisabasisofE.PROOF.LetE0bethesubspaceofEgeneratedbythevectorsxi(1≤i≤ρ)andlet˜E0beasupplementofE0.Letxρ+1beanon-zerovectorbelongingto˜E0.Thevectorsxi(1≤i≤ρ+1)areK-linearlyindependent.ContinuethisconstructionandsupposewehavedeterminedkK-linearlyindependentvectorsxρ+1...xρ+k(withρ+k<n).LetEkbethesubspaceofEgeneratedbythevectorsxi(1≤i≤ρ+k)andlet˜EkbeasupplementofEk.Letxρ+k+1beanon-zerovectorbelongingto˜Ek.Thevectorsxi(1≤i≤ρ+k+1)areK-linearlyindependent.Thisconstructioniscompletewhenρ+k+1=n.13.3.2.HomomorphismsandmatricesHomomorphismsLetEandFbetwoK-vectorspacessuchthatdimE=nanddimF=m.AK-linearmappingufromEintoFisalsocalledahomomorphism(ofvectorspaces)fromEintoF.Foranyx∈Ethevectoru(x)∈Fisoftendenotedasu.xorux.ThekernelofudenotedaskeruisthesubspaceofEdeﬁnedbykeru={x∈E:u.x=0}.476LinearSystemsTheimageofudenotedbyimuisthevectorsubspaceofFdeﬁnedbyimu={u.x;x∈E}.Thehomomorphismuifitisinjective(thatisifkeru=0)iscalledamonomorphism.Ifitissurjective(i.e.ifimu=F)itiscalledanepimorphism(fromEontoF).Ifitisbothinjectiveandsurjectivei.e.bijectiveitiscalledanisomorphism(fromEontoF).InthiscasethevectorspacesEandFaresaidtobeisomorphic(whichwedenoteasE∼=F).Twosuchvectorspaceshaveexactlythesameproperties:makingcalculationsineitheroneamountstodoingintheotheroneby“transporting”thecalculationswiththeisomorphismu(ortheinverseisomorphismu−1);thisiswhythesespacesareoftenidentiﬁed.CanonicalepimorphismLetE1beasubspaceofE.ThemappingφthatmapsavectorxofEtoitsclassinE/E1isanepimorphism:itiscalledthecanonicalepimorphismfromEontoE/E1.Obviouslyφ.x=0ifandonlyifx∈E1;inotherwordskerφ=E1.InclusionConsiderthemappingifromE1intoEwhichtoavectorxinE1associatesthissameelementxnowconsideredtobeinE;iisamonomorphismandiscalledtheinclusion(orthecanonicalinjection)fromE1intoE.CanonicaldecompositionofahomomorphismThediagraminFigure13.1belowiscommutative;itrepresentsthecanonicaldecompositionofahomomorphismufromEintoF.Inthisdiagram˜uistheepimorphismfromEontoimusuchthat˜u.x=u.x∀x∈E;φisthecanonicalepimorphismfromEontoE/keru;iistheinclusionfromimuintoF;∼=designatesanisomorphism.ApplicationtoadirectsumConsiderthedirectsum(13.18)andtheprojectionp1fromEintoE1paralleltoE2.ThecanonicaldecompositionofthisepimorphismisrepresentedbythediagraminFigure13.2below.WehaveE/E2∼=E1andthusaccordingto(13.19)(seesection13.3.1):dim(E/E2)+dimE2=dimE.Appendix2:Algebra477uiu~EIm uFE/(Keru)Figure13.1.CanonicaldecompositionofahomomorphismREMARK518.–LetubeahomomorphismfromEintoFandletE1andF1besubspacesofEandFrespectivelysuchthatu(E1)⊂F1.Ifx1andx2havethesamecanonicalimageinE/E1thenux1andux2havethesamecanonicalimageinF/F1.Thereforethereexistsahomomorphism¯u:E/E1→F/F1calledthehomomorphisminducedbyumakingthediagrambelowcommutativewhereφandψdenotethecanonicalepimorphismfromEontoE/E1andfromFontoF/F1respectively:Eu−→F↓φ↓ψE/E1¯u−→F/F1NotethatifF1={0}thenF/F1=Fandtheconditionu(E1)⊂F1(whichisnecessaryandsufﬁcientfor¯utoexist)isequivalenttoE1⊂keru.RankofahomomorphismTherankofahomomorphismu:E→Fisdeﬁnedby:rku=dim(imu).21EEE p1E1E/E2Figure13.2.Decompositionofaprojection478LinearSystemsSinceimu∼=E/keruwehaverku+dim(keru)=dimE.(13.20)MatrixrepresentationMatrixofahomomorphismLetu:E→Fbeahomomorphism(ei)1≤i≤nbeabasisofEand(εi)1≤i≤mbeabasisofF.InthesebasesweassociateamatrixA∈Km×nwithuinthefollowingmanner:theelementaijoftheithrowandjthcolumnofAistheithcomponentofu.ejinthebasis(εi)1≤i≤m.5Letx∈Eandy=u.x∈F;letX=[x1...xn]Tbetherepresentativeofxinthebasis(ei)1≤i≤nandletY=[y1...ym]Tbetherepresentativeofyinthebasis(εi)1≤i≤m.Wehave:y=u.x=u.m(cid:12)j=1xjej=m(cid:12)j=1xju.ej;asaresultyi=m(cid:12)j=1aijxjsothatY=AX.(13.21)ChangeofbasisConsideranewbasis(e(cid:2)i)1≤i≤nofE.ThechangeofbasismatrixPfrom(ei)1≤i≤nto(e(cid:2)i)1≤i≤nisthematrixoftheidentityofE(denotedbyIE)fromEequippedwiththebasis(e(cid:2)i)1≤i≤nontoEequippedwiththebasis(ei)1≤i≤n.TheentrypijofthismatrixP∈Kn×nisthustheithcomponentinthebasis(ei)1≤i≤nofe(cid:2)j(inotherwordsthejthcolumnofPconsistsofthecomponentsofe(cid:2)jinthebasis(ei)1≤i≤n).IfXandX(cid:2)aretherepresentativesofasamevectorx∈Einthebases(ei)1≤i≤nand(e(cid:2)i)1≤i≤nrespectivelywehaveaccordingto(13.21)X=PX(cid:2).(13.22)Everychangeofbasismatrixisinvertibleandconverselyeveryinvertiblematrixcanbeconsideredasachangeofbasismatrix.5.Adifferentconventionisusedinsection13.4.Appendix2:Algebra479ChangeofbasisandequivalenceLetusdothesameinF:let(ε(cid:2)i)1≤i≤mbeanewbasisofFandletQ∈Km×mbethechangeofbasismatrix.LastletA(cid:2)bethematrixrepresentingthehomomorphismuinthenewbasesandletY(cid:2)betherepresentativeofy=u.xinthebasis(ε(cid:2)i)1≤i≤m.WehaveY(cid:2)=A(cid:2)X(cid:2)andthereforeaccordingto(13.21)and(13.22)A(cid:2)=Q−1AP.(13.23)Thustwomatricesrepresentthesamehomomorphismifandonlyiftheyareequivalent(section13.1.4).Inparticularthesimplestpossibleformofamatrixrepresentingahomomorphism(ofvectorspaces)isfurnishedbyCorollary501(section13.2.3).Thereadercanverifythatthenotionofrankasdeﬁnedinthepresentparagraphisidenticaltothatdeﬁnedinsection13.1.4(inthecasewheretheringconsideredisaﬁeld).TranspositionLetu:E→F(wheretheK-vectorspacesEandFareﬁnite-dimensional)andletE∗andF∗bethedualspacesofEandFrespectively(section12.1.2).Thereexistsauniquehomomorphismtu:F∗→E∗suchthatforanyx∈Eandanyy∗∈F∗ tuy∗x!= y∗ux!andtuiscalledthetransposeofu.Let(ei)1≤i≤nbeabasisofE.Thelinearformse∗ideﬁnedby e∗iej!=δij(1≤i≤n1≤j≤n)whereδijistheKroneckerindex(deﬁnedbyδij=0ifi(cid:5)=jδii=1)formabasisofE∗and(e∗i)1≤i≤niscalledthedualbasisof(ei)1≤i≤n.Let(ei)1≤i≤nand(εi)1≤i≤mbebasesofEandFrespectively.InthesebasesuisrepresentedbyamatrixA.Wecaneasilyverifythatinthedualbases(ε∗i)1≤i≤mand(e∗i)1≤i≤ntuisrepresentedbythematrixATi.e.thetransposeofmatrixA(section13.1.4).13.3.3.EndomorphismsofvectorspacesEndomorphismsandautomorphismsLetEbeaK-vectorspaceofdimensionn.AnendomorphismuofEisahomomorphismfromEintoitself.Thisendomorphismisinjectiveifandonlyifdim(keru)=0whichaccordingto(13.20)isthesameassayingthatrku=northatuissurjective.OnesuchendomorphismiscalledanautomorphismofE.480LinearSystemsMatrixrepresentationOnceabasisofEischosenanendomorphismuisrepresentedbyamatrixA∈Kn×n.ThismatrixAisinvertibleifandonlyifuisanautomorphism.LetE1beasubspaceofEofdimensionn1.WesaythatE1isinvariantunderuifu.E1⊂E1(whereu.E1={u.x1:x1∈E1}).ThenletE2beasupplementofE1andletbeabasisofEthatistheconcatenationofabasisofE1andabasisofE2.InthisnewbasisthematrixArepresentingutakestheformA=(cid:27)A1∗0∗(cid:28)(13.24)whereA1∈Kn1×n1.ThismatrixA1representsinthebasisofE1consideredtherestrictionu1ofutoE1(thinkingofthisrestrictionasanendomorphismofE1).DiagonalsumofendomorphismsLetE1...EqbesubspacesofaK-vectorspaceEsuchthatE=E1⊕...⊕Eq.Inadditionforanyi∈{1...q}letuibeanendomorphismofEi.OntheotherhandletxbeavectorofEandletxibeitscomponentinEi(i∈{1...q}).WecandeﬁnetheendomorphismuofEwhichassociatesui.xiwitheachxiforanyi∈{1...q}.Thisendomorphismiscalledthediagonalsumoftheendomorphismsuiandisdenotedasu1⊕...⊕uq(byanalogywithadiagonalsumofmatrices).ConverselyletubeanendomorphismofE=E1⊕...⊕Eqsuchthatforeveryi∈{1...q}Eiisinvariantunderu.ThendenotingtherestrictionofutoEiasui(thinkingofthisrestrictionasanendomorphismofEi)wehaveu=u1⊕...⊕uq.ConsiderabasisBofEoftheform81≤i≤qBiwhereforeachi∈{1...q}BiisabasisofEi.EachendomorphismuiisrepresentedinthebasisBibyasquarematrixAi;andaccordingto(13.24)theendomorphismu=u1⊕...⊕uqisrepresentedinthebasisBbythematrixA1⊕...⊕Aq.ChangeofbasisandsimilarityLetubeanendomorphismofaK-vectorspaceEofdimensionnBbeabasisofEandA∈Kn×nbethematrixrepresentinguinthisbasis.LetB(cid:2)beanotherbasisofEPbethechangeofbasismatrixfromBtoB(cid:2)andA(cid:2)∈Kn×nbethematrixrepresentinguinthebasisB(cid:2).Accordingto(13.23)wehaveA(cid:2)=P−1AP.(13.25)Appendix2:Algebra481TwomatricesAandA(cid:2)ofKn×nsatisfyingsucharelationaresaidtobesimilar[10]orconjugate[31](whichwedenoteasA≈A(cid:2)).Accordingto(13.25)detA(cid:2)=detP−1detAdetP=detA(13.26)andthusthedeterminantofasquarematrixdependsonlyontheendomorphismuthatthismatrixrepresents.Wecancallitthedeterminantofthisendomorphismanddenoteitasdetu.EigenvaluesandeigenvectorsAnelementλ∈Kiscalledaneigenvalueoftheendomorphismuifthereexistsanon-zerovectorx∈Esuchthatu.x=λxinotherwords(λIE−u).x=0.(13.27)Inthiscasexiscalledaneigenvectorassociatedwiththeeigenvalueλ.Itisclearthatthereexistvectorsx(cid:5)=0satisfying(13.27)ifandonlyifker(λIE−u)(cid:5)=0thusifdet(λIE−u)=0.(13.28)Wecallthepolynomialpu(s)=det(sIE−u).thecharacteristicpolynomialofu.Asaresultλ∈Kisaneigenvalueoftheendomorphismuifandonlyifλisarootofitscharacteristicpolynomial.Onecanprovethefollowing([10]n◦III.8.1):LEMMA519.–Thecharacteristicpolynomialpu(s)canbewrittenaspu(s)=sn+n(cid:12)k=1(−1)k∆ksn−kwhere∆1=Tru(thetraceofui.e.thesumofalleigenvaluesofu)and∆n=detu.LetAbethematrixrepresentinguinanybasisofE;then∆kisthesumoftheprincipalminorsoforderkofA.WeassumeinwhatfollowsthatthecharacteristicpolynomialoftheendomorphismconsideredhasitsrootsinK(whichisobviouslyensuredifK=CormoregenerallyifKisanalgebraicallyclosedﬁeld).ThesetofalleigenvaluesofuiscalleditsspectrumandisdenotedasSp(u).482LinearSystemsAusefullemmaLetA∈Kn×mandB∈Km×nbetwomatrices.LEMMA520.–(i)Thenon-zeroeigenvaluesofABandthoseofBAcoincide.(ii)In+ABisinvertibleifandonlyifIm+BAisinvertibleandinthatcaseB(In+AB)−1=(Im+BA)−1B.PROOF.(i)λ∈KisaneigenvalueofABifandonlyifthereexistsanon-zerovectorx∈Knsuchthat(λIn−AB)x=0.Supposeλ(cid:5)=0andwritey=Bx.Theny(cid:5)=0and(λIm−BA)y=0whichshowsthatλisaneigenvalueofBA.Bysymmetrytheconverseholdstoo.(ii)ThematrixIn+ABisinvertibleifandonlyif−1isnotaneigenvalueofABandthisholdsifandonlyifIm+BAisinvertibleaccordingto(i).Letthenv∈Knbeanyvectorandy=B(In+AB)−1v=Buwithu=(In+AB)−1v.Wethushaveu+Ay=vfromwhichBAy=Bv−Bu=Bv−yandﬁnallyy=(Im+BA)−1Bv.SinceB(In+AB)−1v=(Im+BA)−1Bvforanyv∈Knwegetthedesiredidentity.MultiplicitiesofaneigenvalueAlgebraicmultiplicityLetλ∈Kbearootofpu(s)andletσbetheorderofmultiplicityofthisrootsothatpu(s)=(s−λ)σπλ(s)whereπλ(s)isapolynomialwhichisnotdivisiblebys−λ.Wecallσthealgebraicmultiplicityoftheeigenvalueλ.EigenspaceandgeometricmultiplicityTheeigenspaceassociatedwiththeeigenvalueλisEλ=ker(λIE−u).Thegeometricmultiplicityoftheeigenvalueλisρ=dimEλ.LEMMA521.–Theinequalityρ≤σisalwayssatisﬁed.PROOF.Let(εi)1≤i≤nbeabasisofEtheﬁrstρelementsofwhichformabasisofEλ;inthisbasisuisrepresentedaccordingto(13.24)byamatrixoftheformA=(cid:27)Λ∗0B(cid:28)whereΛ=diag(λ...λ)λrepeatedρtimes.Asaresultdet(sIn−A)=(s−λ)ρdet(sIn−ρ−B)thus(s−λ)ρdividespu(s)whichprovesthatρ≤σ.Appendix2:Algebra483DiagonalizableendomorphismsAnendomorphismuofEissaidtobediagonalizableifthereexistsabasisofEinwhichuisrepresentedbyadiagonalmatrixA.ItisimmediatethatinthiscasetheeigenvaluesofuarefoundonthediagonalofAandthattheimageofanyvectorεofthisbasisbyuisoftheformu.ε=λεthusthebasisinquestionisconstitutedofeigenvectors.AsaresultanecessaryconditionforutobediagonalizableisthatuhasnK-linearlyindependenteigenvectors.Itisclearthatthisisalsoasufﬁcientcondition.Letuslookatthisinmoredetail.Letλ1...λkbethedistincteigenvaluesofuσ1....σkbetheiralgebraicmultiplicitiesandρ1....ρkbetheirgeometricmultiplicities.Firstwehave(cid:11)ki=1σi=d◦(pu)=n.OntheotherhandEλi∩Eλj=0(i(cid:5)=j).(13.29)Indeedifx∈Eλi∩Eλji(cid:5)=jwehaveux=λix=λjxthus(λi−λj)x=0fromwhichx=0becauseλi(cid:5)=λj.Foranyi∈{1...k}EλiisinvariantunderuandtherestrictionuλiofutoEλiisrepresented(whateverthebasisofEλiconsidered)bythediagonalmatrixλiIρi.Asaresult:Ifρi=σi∀i∈{1...k}thenE=Eλ1⊕...⊕Eλku=uλ1⊕...⊕uλkandthisendomorphismisrepresentedbythediagonalmatrixA=(cid:15)λ1Iρ1(cid:16)⊕...⊕(cid:15)λkIρk(cid:16)inabasisconstitutedofeigenvectors.Ifthereexistsanindexj∈{1...k}suchthatρj<σjtherewillnotbenK-linearlyindependenteigenvectorsanduisnotdiagonalizable.Wethushaveobtainedthefollowing:THEOREM522.–AnendomorphismuofEisdiagonalizableifandonlyifthegeometricmultiplicityofeachofitseigenvaluesisequaltoitsalgebraicmultiplicity.ThisholdsifandonlyifthereexistsabasisofEthatconsistsofeigenvectorsofuandthenuisrepresentedinthisbasisbyadiagonalmatrix.484LinearSystemsPolynomialsofendomorphismsLetp(s)=sm+p1sm−1+···+pmbeapolynomialofK[s].OntheotherhandletubeanendomorphismoftheK-vectorspaceE.Writep(u)=um+p1um−1+···+pmIEwhereuiistheithiterationofudeﬁnedbyinductionaccordingtou0=IEandu.ui=ui+1i≥0;p(u)isanendomorphismofE.MinimalpolynomialTHEOREM523.–Thereexistsanon-emptysubsetAofK[s]consistingofthosepolynomialsp(s)thatannihilateui.e.whicharesuchthatp(u)=0.ThissetAisanidealinK[s].Thereexistsauniquepolynomialqu(s)inAwhichismonicandofminimaldegree.PROOF.Let{e1...en}beabasisofEandconsidertheithelementeiofthisbasis.Then+1elementseiu.ei...un.eiareK-linearlydependentsincedimE=n.Thereforethereexistn+1coefﬁcientsγijofKnotallzerosuchthat(cid:11)n+1j=1γijuj−1.ei=0;thusputtingγi(s)=n+1(cid:12)j=1γijsj−1weobtainγi(u).ei=0.Thereforeγ(s)=(ni=1γi(s)isapolynomialsuchthatforanyi∈{1...n}γ(u).ei=0andthusγ(u).x=0∀x∈E.Asaresultγ(u)=0.ThusthesetAisnon-emptysinceγ(s)∈A.ItisclearthatAisanadditivegroup.Ifω(s)∈Aandifϕ(s)∈K[s]isamultipleofω(s)i.e.thereexistsψ(s)∈K[s]suchthatϕ(s)=ω(s)ψ(s)thenϕ(u)=ω(u)ψ(u)=0andsoϕ(s)∈A.ThereforeAisanidealinK[s];andsincethisringisaprincipalidealdomaintheidealAisprincipalandthereexistsauniquemonicpolynomialqu(s)suchthatA=(qu).DEFINITION524.–Thepolynomialqu(s)iscalledtheminimalpolynomialofu.LEMMA525.–Therootsoftheminimalpolynomialaretheeigenvaluesofu.PROOF.LetustemporarilyaddthehypothesisthatKcontainstheeigenvaluesofuandtherootsofqu.1)Ifµ∈Kisarootofquandisnotaneigenvalueofuwecanwritequ(s)=(s−µ)q(cid:2)u(s)whereq(cid:2)u(s)∈K[s].Itisclearthatu−µIEisanautomorphismthusqu(u)=0impliesq(cid:2)u(u)=0whichisimpossiblesinceAppendix2:Algebra485d◦(q(cid:2)u)<d◦(qu).2)ConverselyforanyeigenvalueλofutheassociatedeigenspaceEλisonlyannihilatedbythemultiplesofs−λthuss−λmustbeadivisorofqu(s).ThefollowingpropositionisanobviousconsequenceofLemma525:PROPOSITION526.–Thefactorizationofqu(s)intoprimesisoftheformqu(s)=k’i=1(s−λi)βiwhereβi≥1i∈{1...k}.13.3.4.*JordanformThissectionprovidesaconstructiveproofofthereductiontoJordanform6.GeneralizedeigenspacesConsidertheexpressionoftheminimalpolynomialqu(s)ofanendomorphismugivenbyProposition526.THEOREM527.–Let˜Eλi=ker(λiIE−u)βi.Thesespaces˜Eλihavethefollowingproperties:(i)˜Eλiisinvariantunderu;(ii)˜Eλi∩˜Eλj=0ifi(cid:5)=j;(iii)E=˜Eλ1⊕...⊕˜Eλk;(iv)˜Eλi⊃Eλiand˜Eλi=Eλiifandonlyifβi=1.PROOF.Wehaves(λi−s)βi=(λi−s)βisthusu(λiIE−u)βi=(λiIE−u)βiu.ThereforeifxisavectorofEsuchthat(λiIE−u)βix=0then(λiIE−u)βiux=0whichmeansthat˜Eλiisinvariantunderu.(ii)Supposethatx∈˜Eλi∩˜Eλji(cid:5)=j.Sincethepolynomials(λi−s)βiand(λj−s)βjarecoprimethereexistaccordingtotheBézouttheorempolynomialsv(s)andw(s)inK[s]suchthatv(s)(λi−s)βi+w(s)(λj−s)βj=1.Asaresultv(u)(λiIE−u)βi.x+w(u)(λjIE−u)βj.x=x6.Theapproachtakenhereisthemosttraditional.Anothermoreabstractapproachispresentedinsection13.4.2.486LinearSystemsthusx=0.(iii)Let˜E=˜Eλ1⊕...⊕˜Eλk;if˜E(cid:5)=Ethissubspace˜EwilladmitasupplementinE(section13.3.1Theorem516).Nowsupposethatx(cid:5)=0isavectorbelongingtothissupplement:itisclearthatwewouldhavequ(u).x(cid:5)=0whichisimpossible.(iv)isobvious.DEFINITION528.–Ifβi>1˜Eλiiscalledthegeneralizedeigenspaceassociatedwiththeeigenvalueλi.AccordingtoTheorem527wehaveu=˜uλ1⊕...⊕˜uλkwhere˜uλiistherestrictionofuto˜Eλi.Tofurtherstudytheendomorphismuwearenowledtostudyanyoneofitsrestrictions˜uλi.Theproblemcomesdowntothecasewhereλi=0bywriting˜vλi=˜uλi−λiI˜Eλi.Theminimalpolynomialof˜vλiisobviouslysβi.Thisstudyiscarriedoutbelowwithasimpliﬁednotation.NilpotentendomorphismsSoletvbeanendomorphismofaK-vectorspaceEofdimensionntheminimalpolynomialofwhichisqv(s)=smm≤n.Wethushavevm=0:thisendomorphismissaidtobenilpotent.Ontheotherhandvr(cid:5)=0∀r∈{0...m−1}.CyclicsubspaceSincevm−1(cid:5)=0thereexistsavectore1ofEsuchthatvm−1e1(cid:5)=0.Thevectorse1v.e1...vm−1.e1areK-linearlyindependent.Indeedsupposethereexistscalarsξ1...ξmnotallzerosuchthatm(cid:12)i=1ξivm−ie1=0(13.30)andletξjbetheﬁrstnon-zeroscalarofthelist(ξi)1≤i≤m.Byapplyingtheoperatorvj−1to(13.30)weobtainξjvm−1e1=0whichisimpossible.LetE1bethesubspacegeneratedbythevectorse1v.e1...vm−1.e1;thissubspaceadmitsabasis(cid:31)e1v.e1...vm−1.e1 andisobviouslyinvariantunderv.Asubspacehavingthisdoubleproperty(i.e.asubspacethatisinvariantundervandadmitsabasishavingtheaboveparticularform)issaidtobev-cyclicandweAppendix2:Algebra487call(cid:31)e1v.e1...vm−1.e1 av-cyclicbasisofE1.Wecalle1av-cyclicgeneratorofthisbasisaswellasofE1.Therestrictionv1ofvtoE1isrepresentedinthecyclicbasisconsideredbythesquarematrixofordermJ0m=⎡⎢⎢⎢⎢⎣00···01.....................0···10⎤⎥⎥⎥⎥⎦alltheelementsofwhicharezeroexceptthoseonthesubdiagonalwhichare1.ThematrixJλm=J0m+λImiscalledtheJordanblockofordermrelativetoλ.DecompositionintocyclicsubspacesIfn=mtheentirevectorspaceEisv-cyclicandthereisnothingmoretosayaboutit.Supposem<n.Itisclearthatimvr⊂imvr(cid:1)ifr≥r(cid:2)andimvm=0thusthereexistsanintegerm2∈{1...m}suchthatimvm2⊂E1andimvm2−1∩E1(cid:5)=0.Thusletε2beanon-zerovectorofEsuchthatvm2−1.ε2/∈E1andletµ0...µm−1bethecomponentsofvm2.ε2inthebasis(cid:31)e1v.e1...vm−1.e1 ofE1.Writingthatvm−m2vm2ε2=0weimmediatelyshow(usingthefactthatthevectorsvm−m2e1...vm−1.e1areK-linearlyindependent)thatµi=0(1≤i≤m2−1)thusvm2.ε2=µm2vm2e1+...+µm−1vm−1e1.Nowlete2=ε2−(cid:15)µm2e1+...+µm−1vm−m2−1e1(cid:16).(13.31)Thenvm2−1.e2/∈E1andvm2.e2=0.Thus(cid:31)e2ve2...vm2−1.e2 isav-cyclicbasisofav-cyclicsubspaceE2anditiseasilyshownthatE1∩E2=0.Continuingthiswayweconstructaﬁnitenumberνofv-cyclicsubspacesEisuchthatdimEi+1≤dimEiandE=E1⊕...⊕Eν.TherestrictionviofvtoEiisrepresentedinav-cyclicbasisofEibythelowertriangularJordanblockJ0miandwehavev=v1⊕...⊕vν.(13.32)488LinearSystemsThisendomorphismisthusrepresentedinabasisofEthatconsistsoftheconcatenationofthev-cyclicbasesofthev-cyclicsubspacesEi1≤i≤νbythelowertriangularmatrixJ0m1⊕...⊕J0mν.(13.33)REMARK529.–(i)Inthebasis(cid:31)fi1=vmi−1.ei...fimi=ei ofEiviisrepresentedbytheuppertriangularJordanblockJT0mi.Wehaveindeedtherecurrencerelationfij=v.fij+11≤j≤mi−1.(13.34)ThereforeJ0mi≈JT0mi.(ii)Itisclearthatfi1∈kerv;butwecannotgeneratestartingfromanyelementofkervav-cyclicsubspaceEibytherecursiveprocedure(13.34):thisprocedureneedstobefollowedintheotherdirectionfromanon-zeroelementofkervmi−1.SimilarityinvariantsandelementarydivisorsLetubeanendomorphismofaK-vectorspaceEofdimensionnandletA∈Kn×nbeamatrixrepresentinguinabasisofE.TheinvariantfactorsofsIn−Aareunchangedunderchangeofbasisthustheyonlydependontheendomorphismu.Amongthemthosewhicharenotinvertible(thatisnotequalto1)arecalledthesimilarityinvariantsofu(orofA).LikewisetheelementarydivisorsofsIn−Adependonlyontheendomorphismuandarealsocalledtheelementarydivisorsofthisendomorphism(orofA).IfKcontainsallrootsofpu(s)theseelementarydivisorsareoftheform(s−λ)lwhereλisaneigenvalueofu.ItisclearthattheJordanblockJ0mihasauniquesimilarityinvariantwhichissmi.Thereforethesimilarityinvariantsof(13.32)aresmν...sm1andtheycoincidewiththeelementarydivisorsofthisnilpotentendomorphism.JordantheoremLetubeanendomorphismofaﬁnite-dimensionalK-vectorspaceEsuchthatallrootsofitscharacteristicpolynomialpu(s)belongtoK.TheJordantheorembelowisadirectconsequenceoftheabove:THEOREM530.–ThevectorspaceEcanbeexpressedasadirectsumofu-cyclicsubspacesEλlwhereEλlisassociatedwiththeelementarydivisor(s−λ)lofu(λ∈Sp(u)).Choosingau-cyclicbasisineachofthesesubspacesuisrepresentedbyadiagonalsumofJordanblocksJλleachoftheseblocksappearinginthisdiagonalsumanumberoftimesequaltothemultiplicityorderoftheelementarydivisor(s−λ)l.ThenumberofblocksJλlappearinginthisdiagonalsumfortheAppendix2:Algebra489sameeigenvalueλbutdifferentvaluesoflisequaltothegeometricmultiplicityoftheeigenvalueλ.ThematrixobtainedinthiswayislowertriangularandiscalledtheJordanformofu.TwomatriceshavingthesameJordanformaresimilar.DEFINITION531.–LetustakeabasisofEinwhichtheendomorphismuisrepresentedbyitsJordanform.Thevectorsofthisbasisarecalledthegeneralizedeigenvectorsofuandageneralizedeigenvectorxforwhichthereexistsλ∈Sp(u)andk≥1suchthatx∈ker(λIE−u)kissaidtobeassociatedwiththeeigen-valueλ.COROLLARY532.–AnendomorphismisdiagonalizableifandonlyifallitsJordanblocksareoforder1thatisiftherootsofitsminimalpolynomialareallsimple.COROLLARY533.–Letubeanendomorphism;detuistheproductoftheeigenvaluesofu(repeatingeacheigenvalueinthisproductanumberoftimesequaltoitsalgebraicmultiplicity).FromTheorem530andRemark529(i)wededucethefollowingresult:PROPOSITION534.–EverymatrixA∈Kn×nissimilartoitstranspose.Theproofofthefollowingcorollaryisnoweasy:COROLLARY535.–LetEbeaﬁnite-dimensionalK-vectorspaceubeanendomorphismofEandtubethatendomorphismofE∗whichisthetransposeofu(section13.3.2).Theendomorphismsuandtuhavethesamesimilarityinvariantsandinparticulareacheigenvalueofuisaneigenvalueoftu.Letλandµbeeigenvaluesofuletxbeageneralizedeigenvectorofuassociatedwithλ(Deﬁnition531)andlety∗beageneralizedeigenvectoroftuassociatedwithµ;ifλ(cid:5)=µthenwehave y∗x!=0.Cayley-HamiltontheoremLetαj+1(s)...αn(s)bethesimilarityinvariantsofu(wherejisthenumberofinvariantfactorsofsIn−Athatareequalto1).AccordingtoTheorem530thefactthatαi|αi+1(j+1≤i≤n−1)andequality(13.26)(section13.3.3)wehavethefollowingresult:PROPOSITION536.–Theminimalpolynomialqu(s)andthecharacteristicpolynomialpu(s)ofanendomorphismu:E→E(wheredimE=n)canbeexpressedasafunctionofitssimilarityinvariantsαi(s)(j+1≤i≤n)inthe490LinearSystemsfollowingmanner:qu(s)=αn(s)(13.35)pu(s)=n’i=j+1αi(s)(13.36)Thereforetheminimalpolynomialqu(s)dividesthecharacteristicpolynomialpu(s)(i.e.pu(s)∈A:seeTheorem523insection13.3.3).WethusobtainthefollowingresultcalledtheCayley-Hamiltontheorem:THEOREM537.–Foranyendomorphismuofaﬁnite-dimensionalK-vectorspaceEtheequalitypu(u)=0issatisﬁed.ExampleofreductiontoJordanformLetA=⎡⎢⎢⎢⎢⎣0000110011−110−100000−100000⎤⎥⎥⎥⎥⎦.WehavepA(s)=det(sI5−A)=s5;thusthematrixAhasauniqueeigenvalueλ=0whichisofalgebraicmultiplicity5.NeverthelesswecaneasilyverifythatdimkerA=2thustheeigenvalueλ=0isofgeometricmultiplicity2.ThusAisnotdiagonalizableaccordingtoTheorem522(section13.3.3)andissimilartothediagonalsumoftwoJordanblocksaccordingtoTheorem530.Apriorithesemaybeeitheroforder1and4oroforder2and3.ThereadercanasanexerciseverifythattheSmithformofsI5−Aisdiag(cid:15)111s2s3(cid:16)thusthesimilarityinvariantsofAares2ands3;itfollowsthattheelementarydivisorsofAares2ands3thereforeA≈J03⊕J02.TheminimalpolynomialisqA(s)=s3.InadditionE=E1⊕E2whereE1andE2arecyclicsubspacesofdimension2and3respectively.Appendix2:Algebra491InordertodetermineE1letuschooseageneratore1suchthatA2e1(cid:5)=0.7WehaveA2=⎡⎢⎢⎢⎢⎣0000000000100110000000000⎤⎥⎥⎥⎥⎦.TheﬁrstcolumnofA2isnon-zerothuswecantakee1=[10...0]T;Ae1andA2e1arethentheﬁrstcolumnofAandA2respectivelyi.e.[01−100]Tand[00100]T.Therefore(cid:31)e1Ae1A2e1 isacyclicbasisofE1.Letusnowchooseavectorε2suchthatAε2doesnotbelongtoE1.Thevectorε2=[00001]TsatisﬁesthisconditionsinceAε2=[110−10]T(ﬁfthcolumnofA).OntheotherhandwehaveA2ε2=A2e1;letustakeaccordingto(13.31)e2=ε2−e1=[−10001]T.ThechangeofbasismatrixthenobtainedisP=(cid:25)e1Ae1A2e1e2Ae2(cid:26)=⎡⎢⎢⎢⎢⎣100−11010000−11010000−100010⎤⎥⎥⎥⎥⎦ThereadercanverifythatP−1AP=J03⊕J02.NotethatthedeterminationofPismucheasierbythepreviousdeterminationofthestructureoftheJordanformofAi.e.bythecalculationofitselementarydivisors.13.4.*Thelanguageofmodules13.4.1.GeneralnotionsDeﬁnitionThenotionofmoduleisaverypracticalwayofefﬁcientlyintroducingthekeyconceptsofsystemstheory[42].Amoduleisanalogoustoavectorspacebutinsteadofthescalarsbeingelementsofaﬁeldtheseareelementsofaring.7.HerebythesameabuseoflanguageasusualweidentifyavectorofR5withitsrepresentativeinthecanonicalbasisofthisspace.492LinearSystemsLetEbeavectorspacedeﬁnedoveraﬁeldK(Kcanbeforexampletheﬁeldofrealorcomplexnumbers)letvbeanelementofEandletλ(cid:5)=0beanelementofK.Ifλv=0thenv=0since(1/λ)λv=0.ThisrationaleisnolongervalidifλhasnoinversewhichcouldbethecaseifKwerearingnotaﬁeld.NowletAbeanyring.AnA-moduleMisasetequippedwithanaddition+:M×M→M(whichiscommutativeandmakesManabeliangroup)andwithamultiplicationA×M→M:(λm)(cid:3)→λm(foraleftmodulei.e.whenthescalarsmultiplywiththeelementsofthismodulefromtheleft)suchthat–λ(m1+m2)=λm1+λm2–λ1(λ2m)=(λ1λ2)m–(λ1+λ2)m=λ1m+λ2m–1m=mforanyλλ1λ2∈Aandanymm1m2∈M.IfAisaﬁeldoradivisionringKthenMisaK-vectorspace.ElementarynotionsLetMbeanA-module.AsubmoduleNofMisanA-moduleincludedinM.WecandeﬁneaquotientmoduleM/N(whereNisasubmoduleofM)asumM1+M2andadirectsum(orexternaldirectsum)M1⊕M2oftwoA-modulesM1andM2exactlyaswedidinsection13.3.1forvectorspaces.AhomomorphismofA-module(alsocalledanA-homomorphism)isanA-linearmappingf:M→N(whereMandNareA-modules).WelikewisedeﬁneamonomorphismanepimorphismandanisomorphismofA-modules(thelatterisagaindenotedas∼=).Onecanshowthefollowing(see[91]sectionIII.10and[102]Chapter2):THEOREM538.–(i)Thecanonicaldecompositionofahomomorphism(seethecommutativediagraminFigure13.1)thenotionofinducedhomomorphism(seeRemark518)andthenotionofprojectionontoasubmoduleparalleltoasupplementarysubmodule(seethecommutativediagraminFigure13.2)arestillvalideveninthecaseofahomomorphismofA-modules.(ii)LetM(cid:2)andM(cid:2)(cid:2)besubmodulesofanA-moduleM.ThenM(cid:2)M(cid:2)∩M(cid:2)(cid:2)∼=M(cid:2)+M(cid:2)(cid:2)M(cid:2)(cid:2).(iii)LetMM(cid:2)andM(cid:2)(cid:2)beA-modulessuchthatM(cid:2)(cid:2)⊂M(cid:2)⊂M.ThenM/M(cid:2)∼=M/M(cid:2)(cid:2)M(cid:2)/M(cid:2)(cid:2).Appendix2:Algebra493(iv)LetMbeanA-moduleandM(cid:2)beasubmoduleofM.Thereexistsaone-to-onecorrespondencebetweensubmodulesSofM/M(cid:2)andtheintermediatesubmodulesΣbetweenM(cid:2)andM(thatissuchthatM(cid:2)⊂Σ⊂M)givenbyS(cid:3)→Σ(cid:1)ϕ−1(S)whereϕ:M→M/M(cid:2)isthecanonicalepimorphism.(v)LetMM(cid:2)M(cid:2)(cid:2)N(cid:2)andN(cid:2)(cid:2)beA-modulessuchthatM=M(cid:2)⊕M(cid:2)(cid:2)N(cid:2)⊂M(cid:2)andN(cid:2)(cid:2)⊂M(cid:2)(cid:2).ThenwehavethecanonicalisomorphismM(cid:2)⊕M(cid:2)(cid:2)N(cid:2)⊕N(cid:2)(cid:2)∼=M(cid:2)N(cid:2)⊕M(cid:2)(cid:2)N(cid:2)(cid:2).FinitelygeneratedmodulesAnA-moduleMissaidtobeofﬁnitelygeneratedifitisgeneratedbyaﬁnitenumberofelementsm1...mki.e.foranyx∈Mthereexistλ1...λk∈Asuchthatx=(cid:11)1≤i≤kλimi(thisexpressionisingeneralnon-unique).WethenwriteM=(cid:23)(mi)1≤i≤k(cid:24)Aorsimply[m]Awheremistheﬁnitesequence(mi)1≤i≤k(whichcanbeidentiﬁedwiththecolumnmatrix[m1...mk]T).FreemodulesAbasisofanA-moduleMisalsodeﬁnedasinsection13.3.1.Butwhereaseveryvectorspaceadmitsabasisthosemodulesthatadmitabasisarequiteparticularandarecalledfreemodules.AﬁnitelygeneratedfreemoduleLisisomorphictoadirectsumk9i=1Ai=AkwhereeachAiisamoduleisomorphictoA(whenthelatterisconsideredasamoduleoveritself).TheintegerkiscalledtherankofthefreemoduleL.PresentationofamoduleLEMMA539.–LetMbeanA-modulegeneratedbykelementsmi1≤i≤k.ThenMisisomorphictothequotientofafreeA-moduleofrankk.PROOF.Let(ci)1≤i≤kbethecanonicalbasisofAkandletϕ:Ak→MtheA-homomorphismdeﬁnedbyϕ(ci)=mi1≤i≤k.ItisclearthatϕisanepimorphismthusM∼=Ak/kerϕaccordingtoTheorem538(i).THEOREM540.–LetM=[m]AbeaﬁnitelygeneratedA-modulewherem=[m1...mk]T.Thefollowingconditionsareequivalent:(i)M∼=Ak/kerϕandkerϕisﬁnitelygenerated.(ii)ThereexistsanA-homomorphismf:Aq→Aksuchthat494LinearSystemsM∼=Ak/imf(cid:1)cokerf.8(iii)ThereexistsamatrixR∈Aq×ksuchthatthegeneratorsmi1≤i≤kareonlyrelatedbytheequalityRm=0.(13.37)PROOF.1)(i)⇒(ii):IfMisﬁnitelygeneratedthenM∼=Ak/kerϕaccordingtoLemma539.Supposekerϕisgeneratedbyqelements.ApplyingLemma539tothemodulekerϕ⊂Akthereexistsanepimorphismg:Aq→kerϕ.Letf:Aq→AkbetheA-homomorphismdeﬁnedbyf(x)=g(x)x∈Aq.Thenimf=kerϕ.2)(ii)⇒(iii):Let(ai)1≤i≤qand(ci)1≤i≤kbethecanonicalbasesofAqandAkrespectivelyandwritef(ai)=k(cid:12)j=1rijcj1≤i≤q;(13.38)letRbethematrix(rij)(1≤i≤q1≤j≤k).Letmj=ϕ(cj)1≤j≤kwhereϕ:Ak→Ak/imfisthecanonicalepimorphism.ThenthegeneratorsmiofAk/imfarerelatedbytheonlyequality(13.37).3)(iii)⇒(i):SeeRemark542.DEFINITION541.–AnA-moduleMsatisfyingoneofthethreeequivalentconditionsinTheorem540issaidtobeﬁnitelypresented.Therelation(13.37)iscalledapresentationofMandthematrixRappearinginthisrelationiscalledamatrixofdeﬁnition(ormatrixofpresentation)ofM.REMARK542.–IfwerepresenttheelementsofAqandAkbyrowsinthecanonicalbasesthenRisthematrixoffinthesebasesandfisidentiﬁedwiththeright-multiplicationbyR(writtenf=•R)9.Accordingto(13.38)f(Aai)isthesubmoduleofAkgeneratedbytheithrowofR.ThemoduleM=[m]AisrelatedtothedeﬁnitionmatrixRbytheisomorphismM∼=coker(•R)=Ak/kerϕwhereϕ:Ak→Ak/kerϕisthecanonicalepimorphism.Adeﬁnitionmatrixofaﬁnitelypresentedmoduleisnon-unique.Wehaveindeedthefollowingresult([31]section0.6):THEOREM543.–LetMbeaﬁnitelypresentedA-modulewithdeﬁnitionmatrixR∈Aq×k;thenR(cid:2)∈Aq(cid:1)×k(cid:1)isagainadeﬁnitionmatrixofMifandonlyifcoker(•R(cid:2))∼=coker(•R).ThematricesRandR(cid:2)arethensaidtobeleft-similarandthisimpliesk−q=k(cid:2)−q(cid:2)(thisintegeriscalledthecharacteristicofM).IfR(cid:2)∼RthenRandR(cid:2)areleft-similarbuttheconversedoesnothold.8.Ak/imf(cid:1)cokerfiscalledthecokerneloff.9.Thisconventionisadoptedthroughoutsection13.4butonlyinthatsection.Therearegoodreasonsforthis(see[15][22]).Appendix2:Algebra495SupposefromhereonthattheringAisacommutativedomain.Onecanprovethefollowingresult([11]ChapterI):THEOREM544.–IfAisNoetherianthenanyﬁnitelygeneratedA-moduleisﬁnitelypresented.TheNoetherianconditionistoorestrictiveforcertainapplications(especiallyaBézoutdomainisnotNoetherianunlessitisaprincipalidealdomain)andthisiswhythefollowingisuseful:DEFINITION545.–AringAiscoherentifeveryﬁnitelygeneratedidealinAisﬁnitelypresented.BézoutdomainsNoetherianringsareallcoherent.Wehavethefollowingresult([31]TheoremA.9p.555):THEOREM546.–IftheringAiscoherentthenthecategoryofﬁnitelypresentedA-modulesofisclosedundertakingﬁnitelygeneratedsubmodulesﬁnitedirectsumskernelsandcokernels*andthusitisanabeliansubcategoryofthecategoryofallA-modules.*Onecaneasilyprovethefollowing:THEOREM547.–LetMbeanA-modulewithdeﬁnitionmatrixR∈Aq×k.ThemoduleisMfreeifandonlyifRiscompletable.TorsionLetλ(cid:5)=0beanelementofAandletmbeanelementofM.Asalreadysaidtheequalityλm=0doesnotimplym=0unlessλisaninvertibleelementofA.AnelementofMsatisfyingsuchanequalityiscalledatorsionelementandanelementthatisnotoftorsioniscalledfree.Theonlytorsionelementofavectorspaceis0.ThesetoftorsionelementsofanA-moduleMisasubmoduleofMandisreferredtoasthetorsionsubmoduleofMdenotedbyT(M).AmoduleMsuchthatT(M)=0(thatisT(M)isreducedtotheelement0orinotherwordsMonlycontainsfreeelements)issaidtobetorsion-free.ForanyA-moduleMthequotientM/T(M)istorsion-free.AllfreeA-modulesaretorsion-freewhiletheconversedoesnotholdingeneral.Wehavethefollowing([10]n◦II.7.10&ChapterVII)([31]section0.3):THEOREM548.–(A)LetMbeaﬁnitelypresentedA-moduleandR∈Aq×kbeapresentationmatrixofM.TheA-moduleMisoftorsionifandonlyifRis496LinearSystemsleft-invertibleoverF=Q(A).(B)IfAisaprincipalidealdomain(resp.aBézoutdomain)thenanysubmodule(resp.anyﬁnitelygeneratedsubmodule)ofafreeA-moduleisfree.CyclicmodulesAcyclicA-moduleisamoduleΓgeneratedbyauniqueelementmi.e.Γ=[m]A=Am.Wecalltheannihilatorofm(andwedenoteasAnn(m))thesubsetaofAsuchthatam=0thatisλm=0∀λ∈a.Itisclearthataisanideal.Letϕ:A→Γbetheepimorphismdeﬁnedbyϕ(λ)=λm;wehavekerϕ=athusΓ∼=A/a.ConverselyletabeanidealinAandletψ:A→A/abethecanonicalepimorphism.ThemoduleA/aiscyclicgeneratedbyψ(1).IndecomposablemodulesDEFINITION549.–AnA-moduleMissaidtobedecomposableifitisthedirectsumoftwosubmodulesdifferentfrom0andMandissaidtobeindecomposableotherwise.Wehavethefollowing([10]ChapterI):LEMMA550.–LetabeanidealinA.ThecyclicmoduleA/aisdecomposableifandonlyifthereexisttwoidealsbandcdifferentfrom0andAsuchthatA=b+canda=b∩c;thenA/a∼=A/b+A/c.13.4.2.ModulesoverprincipalidealdomainsExceptwhenotherwisestatedRisaprincipalidealdomaininthesequel.PrimarydecompositionofacyclicmoduleSinceanyidealinRisprincipalitisgeneratedbyauniqueelementaandisthusoftheformRa=(a).LetΓ∼=R/(a)beacyclicR-module.(i)Ifa=0thenΓ∼=R(0)=Risafreemoduleofrank1.(ii)Ifa(cid:5)=0thenthecyclicmoduleΓistorsionandΓ=0ifandonlyifaisaunitofR.Inwhatfollowswewillusethefollowingresult(validwhenRisaBézoutdomain):PROPOSITION551.–(i)Letbc∈R×betwocoprimeelements.ThenR(bc)∼=R(b)⊕R(c).(ii)LetpbeaprimeinRandnbeapositiveinteger;thenthemoduleR(pn)isindecomposable.Appendix2:Algebra497PROOF.(i)Leta=bc.Sincebandcarecoprimewehave(b)+(c)=Rand(b)∩(c)=(a).ThusAssertion(i)isaconsequenceofLemma550.(ii)SupposeR(pn)isdecomposable.AccordingtoLemma550thereexistelementsbc∈Rthatarecoprimeandsuchthatpn∈lcm(bc).Thisisobviouslyimpossible.GivenanonunitaofRletusconsidertheuniquefactorizationofthiselementintoprimes(section13.1.2)nowwrittenintheforma=υ’ipkiiki≥0.AccordingtoProposition551itisclearthatR(a)∼=iR(cid:9)pkii(cid:10)(13.39)andthatthemodulesR(cid:9)pkii(cid:10)(ki(cid:5)=0)areindecomposable.Thetermspkiiwhicharenotequalto1arecalledtheelementarydivisorsofthecyclicmoduleM∼=R(a).Amodulethatisadirectsumj∈JR(pji)(Jﬁnite)issaidtobeprimary(wewillencountersuchmodulesintheexplicitdecompositioninTheorem556below).CanonicaldecompositionofamoduleLetMbeaﬁnitelygeneratedmoduleoveraprincipalidealdomainR.AccordingtoProposition544(section13.4.1)MisﬁnitelypresentedthusM=[m]Aisdeﬁnedbyarelationsuchthat(13.37)whereR∈Rq×k.ThereexistmatricesU∈GLq(R)andV∈GLk(R)suchthatU−1RV=ΣwhereΣ=diag(α1...αr0...0)istheSmithformofR(Theorem497section13.2.3).Thepolynomialsαi1≤i≤raretheinvariantfactorsofR.Theyarenon-zeroandsuchthatαi|αi+11≤i≤r−1.Writem=Vvwherev=[v1...vk]T.ThenM=[v]Randequation(13.37)ofsection13.4.1isequivalenttodiag(α1...αr0...0)⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣v1...vrvr+1...vk⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦=0.(13.40)498LinearSystemsIfq>r(13.40)(andthus(13.37))includesq−rtrivialequations0=0whichcanbesuppressed.Wethuscanassumeq=r;thenequation(13.40)reducesto:αivi=01≤i≤r.(13.41)For1≤i≤rviisatorsionelementwhichgeneratesthecyclictorsionmodule[vi]R∼=R(αi)andthismoduleiszeroifandonlyifαiisinvertible.Supposethatthenumberofinvertibleelementsinthelist(αi)1≤i≤risequaltoj;thenweonlyneedtoconsidertheαi’sj+1≤i≤r.Forr+1≤i≤kvigeneratesthefreecyclicmodule[vi]R∼=R(0)=R.WehaveM=[v]R=j+1≤i≤k[vi]R.HereaboveT(M)=j+1≤i≤r[vi]RandΦ=r+1≤i≤k[vi]Risafreemoduleofrankk−rsince(vi)r+1≤i≤kisabasisofΦ(settingΦ=0ifr=k).WehavethusobtainedthedecompositionM=T(M)⊕Φ(13.42)whereΦisafreesubmoduleofMsuchthatΦ∼=Rk−r.InadditionT(M)∼=j+1≤i≤rR(αi).(13.43)DEFINITION552.–Theelementsαij+1≤i≤r(uniquelydetermineduptoassociates)arecalledtheinvariantfactorsofthemoduleM.Theintegerk−riscalledtherankofthemoduleM.REMARK553.–Theterminologyof([10]ChapterVII)isslightlydifferentandisjustiﬁedbythefollowingrationale:(i)Ratherthantheelementsαij+1≤i≤rwecaninamore“intrinsic”mannerconsidertheidealsgeneratedbytheseelementsi.e.theak−i+1=(αi)j+1≤i≤r.(iii)InadditionΦ∼=r+1≤i≤kR(0);thek−rzeroidealsai(1≤i≤k−r)inthissumcanbetakenintoaccountinthesamewayastheabovenon-zeroidealsai(k−r+1≤i≤k−j).TheinvariantfactorsofMinthesenseof([10]ChapterVII)areintheendtheprincipalidealsa1...ak−j(someofwhichmaybezero);thesearesuchthata1⊂a2...⊂ak−j(cid:5)=RandM∼=1≤i≤k−jR/ai(13.44)(theprincipalidealsaiareuniquelydeterminedbytheseconditions).Appendix2:Algebra499Wethushavethefollowing:THEOREM554.–LetRbeaprincipalidealdomainandMbeaﬁnitelygeneratedRmodule(ormoregenerallyletRbeanelementarydivisorringandMbeaﬁnitelypresentedR-module).ThenMisthedirectsumofitstorsionsubmoduleT(M)andofafreesubmodule(ofthesamerankasM).InadditionT(M)decomposesaccordingto(13.43)asafunctionofitsnon-zeroinvariantfactorsαior(αi)(j+1≤i≤r)andMdecomposesaccordingto(13.44)asafunctionofallitsinvariantfactorsai(1≤i≤k−j)(“canonicaldecompositionofthemoduleM”).PROPOSITION555.–LetRbeaBézoutdomainandMabeaﬁnitelygeneratedR-module.(i)Mdecomposesaccordingto(13.42)(whereΦisafreemodule).(ii)ThemoduleMisfreeifandonlyifitistorsion-free.(iii)SupposenowthatMisﬁnitelypresented;Mhasaleft-regulardeﬁnitionmatrixRMisfreeifandonlyifRisequivalentto(cid:25)Ir0(cid:26)andMistorsionifandonlyifRisregular.PROOF.(i):See([31]section5.1Theorem1.3).(ii)isanobviousconsequenceof(i).(iii):AccordingtoTheorem495(section13.2.2)theproblemcomesdowntothecasewherethedeﬁnitionmatrixR∈Rq×kofMisincolumnHermiteformandthentothecaseq=r(r=rkB)bysuppressingtheq−rtrivialequations0=0.ThemoduleMisfreeifandonlyifRiscompletablethusequivalentto(cid:25)Ir0(cid:26).BesidesaccordingtoTheorem548(i)MistorsionifandonlyifRnowofrankrisleft-invertibleoverQ(R)thusinvertibleoverthatﬁeldandﬁnallyregularoverR.TheoryofelementarydivisorsThecompletetheoryofelementarydivisorsofaﬁnitelygeneratedmoduleMoveraprincipalidealdomainRcanimmediatelybededucedfromTheorem554andfromtheprimarydecompositionofacyclicmodule.Theresultmaybestatedasfollows:THEOREM556.–Let(pi)bearepresentativesystemofprimesinR(section13.1.2).Thereexistintegersµ(0)≥0ni≥0andµ(πi)≥0whereπi=pniiwiththefollowingproperties:theseintegersareuniquelydeterminedandarezeroexceptforaﬁnitenumberofthemandinthedecomposition(13.42)T(M)∼=9i(cid:9)R(πi)(cid:10)µ(πi)Φ∼=Rµ(0).DEFINITION557.–Theπi=pniii∈I(ortheideals(πi)theygenerate)arecalledtheelementarydivisorsofmoduleMandtheintegersµ(πi)aretheirmultiplicities.Ifµ(0)>0thisinteger(whichistherankofM)iscalledthemultiplicityoftheelementarydivisor0.500LinearSystemsREMARK558.–JustasTheorem554isareformulationofTheorem497(section13.2.3)Theorem556isareformulation(evenmoreprecise)ofwhathasbeenstatedinbeginningofsection13.2.4.Thisiswhythetheoryofmodulespresentedinthissectioncanbeconsideredasalanguage–whichprovestobeextremelyuseful.SmithzerosofamoduleLetR∈Rq×kwhereR=C[∂].TheSmithzerosofRandtheirmultiplicitieshavebeendeﬁnedinsection13.2.5.ThesequantitiesonlydependontheelementarydivisorsofRwhichthemselvesonlydependonT(M)whereM∼=coker(•R).ThesezeroscanthusbecalledinamoreintrinsicmannertheSmithzerosofthemoduleT(M)[19].Deﬁnition504(section13.2.5)remainsvalidmutatismutandis.ForanyﬁnitelygeneratedtorsionR-moduleTZ(T)denotesthesetofitsSmithzeros(eachofthemrepeatedanumberoftimesequaltoitsdegree)andε(T)denotesthesetofitselementarydivisors(eachofthemrepeatedanumberoftimesequaltoitsmultiplicity).Thefollowingisprovenin[19]:LEMMA559.–LetT1andT2betwoﬁnitelygeneratedtorsionR-modules.(i)IfT2⊂T1thenZ(T1)=Z(T2)˙∪Z(T1/T2)where˙∪designatesthe“disjointunion”10.(ii)IfT1∩T2=0thenZ(T1⊕T2)=Z(T1)˙∪Z(T2)andε(T1⊕T2)=ε(T1)˙∪ε(T2).ComplementonquotientmodulesLetRbeaprincipalidealdomainMbeaﬁnitelygeneratedR-moduleandFbeafreesubmoduleofM.LEMMA560.–(i)ThereexistsamaximalfreesubmoduleΦFofMwhichcontainsF.(ii)WehaveM=T(M)⊕ΦF.PROOF.*(i):LetLFbethesetofallfreesubmodulesofMthatcontainsForderedbyinclusion.Thissetisinductive;indeedletCbeachainofLFi.e.atotallyorderedsubsetandF1=:L∈LFL;thenthesubmoduleF1ofMistorsion-freeanditisfreeaccordingtoProposition555(ii);itcontainsFthusitbelongstoCandisitslargestelement.Statement(i)thereforederivesfromtheZornLemma.(ii):SupposeT(M)⊕ΦF(cid:5)=Mandletm∈Mm/∈T(M)⊕ΦF;thenΦF(cid:5)=[m]R+ΦFandmisafreeelementthusthemodule[m]R+ΦFistorsion-freeandsoisfree;thismodulethusbelongstoLFwhichcontradictsthefactthatΦFisamaximalelementofthisset.*LetNbeasubmoduleofM;accordingtoProposition555(i)thereexistsafreesubmoduleFofNsuchthatN=T(N)⊕F.WehaveT(N)⊂T(M)and10.Forexample{xy}˙∪{xz}={xxyz}([91]sectionI.8).Appendix2:Algebra501accordingtoLemma560thereexistsafreesubmoduleΦFofMsuchthatM=T(M)⊕ΦFandF⊂ΦF.WritingΦN=ΦFweobtainthefollowing:PROPOSITION561.–ThereexistsafreesubmoduleΦNofMsuchthatM=T(M)⊕ΦNandN=T(N)⊕(ΦN∩N).IfΦ(cid:2)NisanotherfreesubmoduleofMsuchthatM=T(M)⊕Φ(cid:2)NandN=T(N)⊕(Φ(cid:2)N∩N)thenΦ(cid:2)N∼=ΦNandΦ(cid:2)N/(Φ(cid:2)N∩N)∼=ΦN/(ΦN∩N).PROOF.WehaveF=ΦN∩F=ΦN∩N.AssumeΦ(cid:2)NisfreeM=T(M)⊕Φ(cid:2)NandN=T(N)⊕(Φ(cid:2)N∩N).ThenΦ(cid:2)N∼=M/T(M)∼=ΦN;inadditionT(N)⊆T(M)andΦN∩N⊆NΦ(cid:2)N∩N⊆NthusbyTheorem538bothmodulesbelowareisomorphictoM/N:T(M)/T(N)⊕ΦN/(ΦN∩N)∼=T(M)/T(N)⊕Φ(cid:2)N/(Φ(cid:2)N∩N)henceΦN/(ΦN∩N)∼=Φ(cid:2)N/(Φ(cid:2)N∩N)by([31]section8.2Corollary2.5).13.4.3.StructureofendomorphismsInthisparagraphRistheprincipalidealdomainK[∂]whereK=RorC;∂istheindeterminateandthusinprincipleplaysapurelyformalrolebutitisconvenienttothinkof∂astheusualderivatived/dt.CompanionmatricesassociatedwithacyclicmoduleSinceRisaprincipalidealdomaineveryidealainRisprincipalandthusisoftheformRa=(a)a∈R.AsaresultifΓ=[w]RisacyclicR-modulethereexistsapolynomiala=a(∂)∈R(chosentobemonic)suchthatΓ∼=R(a)anda(∂)issuchthata(∂)w=0.(13.45)Wesaythatequation(13.45)isanon-trivialhomogenousdifferentialequationif0(cid:5)=a(∂)isanon-invertiblepolynomial(i.e.ofdegree≥1).LetΓbethecyclicmodulewithgeneratorwdeﬁnedby(13.45)wherea(∂)(cid:5)=0isamonicpolynomialofdegreen≥1thatisa(∂)=∂n+a1∂n−1+...+anai∈K.502LinearSystemsWritexi=∂n−iw(1≤i≤n)andx=(cid:25)x1···xn(cid:26)T.Weget∂x=⎡⎢⎢⎢⎢⎢⎢⎣−a1−a2······−an1000.....................00···010⎤⎥⎥⎥⎥⎥⎥⎦x(13.46)∂xT=xT⎡⎢⎢⎢⎢⎢⎢⎢⎣−a110···0−a20.....................0............1−an···000⎤⎥⎥⎥⎥⎥⎥⎥⎦.(13.47)ThematricesC1(a)andC2(a)intheright-handmemberof(13.46)and(13.47)arecalledthecompanionmatricesofthepolynomiala(∂).ThereexisttwoothercompanionmatricesC3(a)andC4(a)obtainedbywritingtheequations∂η=C3(a)ηand∂ηT=ηTC4(a)withη=(cid:25)xn···x1(cid:26)T.TheexplicitexpressionsofC3(a)andC4(a)arelefttothereader.PROPOSITION562.–(i)ThecharacteristicpolynomialofthecompanionmatricesCi(a)i∈{1...4}isa=a(∂).(ii)ThecyclicmoduleΓ∼=R(a)consideredasaK-vectorspace(denotedasEa)isofdimensionn=d◦(a).PROOF.(i)Weobtaindet(∂In−C1(a))=a(∂)bydevelopingthisdeterminantwithrespecttotheﬁrstrow.Theothercompanionmatricesofa(∂)aresimilartoC1(a)(sinceaccordingtoProposition534ofsection13.3.4amatrixissimilartoitstranspose)andthushavethesamecharacteristicpolynomialasthismatrix.(ii)Accordingto(13.46)theelementsx1...xnareK-linearlyindependentand(xi)1≤i≤nisabasisofEa.REMARK563.–(i)Themapping∂:M→MisK-linearthusisanendomorphismAaofEa.Since(cid:31)xn∂xn...∂n−1xn isabasisofEathisvectorspaceisAa-cyclicandtheabovebasisisAa-cyclic(section13.3.4).ThevectorxnisanAa-cyclicgeneratorofEa(andoftheabovebasis).AnymatrixrepresentingtheendomorphismAaofEaissimilartoacompanionmatrixofa(∂)andissaidtobecyclic.(ii)Lety=(cid:11)1≤i≤nYixi∈EaandletY=[Y1...Yn]betherowrepresentingyinthebasisx=(xi)1≤i≤n=[x1...xn]T(seeRemark542section13.4.1).Sincey=Yxtheendomorphism∂=AaisthemapY(cid:3)→YC1(a)inthebasisx.Appendix2:Algebra503REMARK564.–InwhatfollowsC(a)designatesoneofthecompanionformsCm(a)wherem∈{1...4}ischosenonceandforall.RationalcanonicalformofanendomorphismConsiderthedifferentialequation∂x=Ax(13.48)whereA∈Kn×nrepresentsinthebasisx=(xi)1≤i≤nofE∼=Knanendomorphismu(adoptingtheconventioninRemark542ofsection13.4.1).Thedifferentialequation(13.48)isalsowrittenasR(∂)x=0whereR(∂)=∂In−A.AccordingtoCorollary555(ii)(section13.4.2)itdeﬁnesaﬁnitelygeneratedtorsionR-moduleM.Themap∂:M→Misidentiﬁedwithu.11Theseobservationscanbecompletedbythefollowing:THEOREM565.–LetMbeaﬁnitelygeneratedR-module.ThemoduleMistorsionifandonlyifitisaﬁnite-dimensionalK-vectorspace.PROOF.(i)IfMistorsionthenM=T(M)satisﬁestherelation(13.43)ofsection13.4.2andeachcyclictorsionmoduleR(αi)isaﬁnite-dimensionalK-vectorspaceaccordingtoProposition562.(ii)ConverselyletMbeanR-modulewhichisaﬁnite-dimensionalK-vectorspaceandlet(xi)1≤i≤nbeabasisofM.Foranyi∈{1...n}∂xiisaK-linearcombinationofthexk1≤k≤nthusthereexistsamatrixA∈Kn×nsuchthattherelation(13.48)issatisﬁed.ThemoduleMisthustorsion.ThetorsionmoduleM=T(M)deﬁnedby(13.48)admitsdecomposition(13.43).LetEαibethecyclicmoduleR(αi)(j+1≤i≤r)consideredasaK-vectorspace.AccordingtoRemark563∂:R(αi)→R(αi)isanendomorphismAαiofEαiandthelatterspaceisAαi-cyclic.InanAαi-cyclicbasisAαiisthusrepresentedbyacompanionmatrixC(αi).Wehavethereforethefollowing:THEOREM566.–LetEbeaﬁnite-dimensionalK-vectorspaceandletubeanendomorphismofE.(i)Ecanbeexpressedasadirectsumofu-cyclicsubspacesE=j+1≤i≤rEαi(13.49)suchthattherestrictionAαiofutoEαiisrepresentedinau-cyclicbasisbyacompanionmatrixC(αi).(ii)Thepolynomialsαi(∂)(j+1≤i≤r)arethe11.Herewetakeadvantageofthefactthattheindeterminate∂isidentiﬁedwithd/dt.Anequivalentbutmoreabstractapproachconsistsinusingtheformalismof([10]sectionVII.5).504LinearSystemssimilarityinvariantsofu(section13.3.4);iftheseareorderedinsuchawaythatαi|αi+1(j+1≤i≤r−1)thedecomposition(13.49)isunique.(iii)Intheconcatenationoftheaboveu-cyclicbasestheendomorphismuisrepresentedbythediagonalsum9j+1≤i≤rC(αi)(“rationalcanonicalformoftheendomorphismu”).DEFINITION567.–Thenumberoftermsofthesum(13.39)(i.e.r−j)iscalledthecyclicindexofu.Theendomorphismuissaidtobecyclicifitscyclicindexisequalto1(thatisifuisrepresentablebyacyclicmatrixinanybasisandbyacompanionmatrixinau-cyclicbasis:seeRemark563).Recallthattheminimalpolynomialofuisαr(∂)andthatitscharacteristicpolynomialis(j+1≤i≤rαi(∂)(Proposition536section13.3.4).WededucefromTheorem566thefollowing:COROLLARY568.–Anendomorphismiscyclicifandonlyifitsminimalpolynomialisequaltoitscharacteristicpolynomial.JordanformLetK=C;thenaprimepolynomialp(∂)isoftheform∂−rr∈Candanelementarydivisorisoftheformπ(∂)=(∂−r)k.AnindecomposablemoduleR(π)isaC-vectorspaceEπ.LetvbeageneratorofR(π)suchthat(∂−r)kv=0.PutXk−i=(∂−r)iv1≤i≤kandX=(cid:25)X1···Xk(cid:26)T.Weobtain∂X=JrkXwhereJrkistheJordanblockoforderkrelativetorasdeﬁnedinsection13.3.4;Jrkrepresentstheendomorphism∂:R(π)→R(π)inthebasisXofEπ.Theorem530(section13.3.4)isnowadirectconsequenceofTheorem556(section13.4.2)byconcatenatingthebasesconstructedhereaboveforeachC-vectorspaceEπasπspansthesetofelementarydivisorsoftheendomorphismu(andtakingintoaccounttheirmultiplicities).13.5.OrthogonalityandsymmetryInwhatfollowsEisaHilbertspaceofﬁnitedimensionnoverKwhereK=RorC.Thisspaceisequippedwithascalarproduct ..!(seesection12.1.2).Wewillbecarefulnottoconfusethelatterwithadualitybracket.Appendix2:Algebra50513.5.1.OrthonormalbasisLetB=(ei)1≤i≤nbeabasisofE.Thisbasisissaidtobeorthonormalif eiej!=δijwhereδijistheKroneckerindex(seesection13.3.2).Oneshowsusingthe“Gram-Schmidtorthogonalizationprinciple”(seee.g.[91]aswellastheproofofTheorem579atsection13.5.7)thatwecanalwayschooseanorthonormalbasisinE.LetxandybetwovectorsofEwithcomponentsx1...xnandy1...ynrespectivelyinthisbasis.Wehave xy!=;n(cid:12)i=1xiein(cid:12)j=1yjej<=n(cid:12)i=1n(cid:12)j=1¯xiyj eiej!=n(cid:12)i=1¯xiyi=X∗Y(13.50)whereX=[x1...xn]TY=[y1...yn]TandwhereX∗=[¯x1...¯xn]T(conjugatetransposeofX).Thequantityintheright-handsideof(13.50)isthe“standardscalarproduct”onKn(thevector(x1...xn)ofthisspaceisidentiﬁedwiththecolumnmatrixX=[x1...xn]Twhichrepresentsitinthecanonicalbasis)and(13.50)makesitpossibletoidentifythespaceEwithKnequippedwiththestandardscalarproduct.13.5.2.OrthogonalityTwovectorsxandyaresaidtobeorthogonal(x⊥y)if xy!=0.IfFisasubspaceofEthesetofvectorsofEwhichareorthogonaltoallvectorsofFisasubspaceofEcalledtheorthogonalofF(anddenotedasF⊥).ItisimmediatethatF∩F⊥=0.OnecanalsoshowthatF+F⊥=E.ThusFandF⊥aresupplementary.WecallF⊥theorthogonalsupplementofFandEthe“orthogonaldirectsum”ofFandF⊥whichwewriteasE=F⊥⊕F⊥.OnecanalsoshowthatF⊥⊥=F.NotethatFdoesnothaveauniquesupplement(asidefromthetrivialcasewhereF=0orF=E)whileFhasbydeﬁnitionauniqueorthogonalsupplement.506LinearSystems13.5.3.AdjointendomorphismTHEOREM569.–LetubeanendomorphismofE.Thereexistsauniqueendomorphismu∗ofEcalledtheadjointofusuchthatforanyxy∈E xuy!= u∗xy!.(13.51)MorespeciﬁcallyletA=(aij)bethematrixrepresentinguinanorthonormalbasisB;u∗istheendomorphismrepresentedinthissamebasisbyA∗(cid:1)¯AT=(¯aji)(conjugatetransposeofA).PROOF.Letz=uy.Accordingto(13.21)therepresentativeofzinthebasisBisZ=AY.Thusby(13.50) xuy!=X∗AY=(A∗X)∗Yandthetheoremisproven.PropertiesoftheadjointTHEOREM570.–(i)WehaveE=keru⊥⊕imu∗.(13.52)(ii)Themappingu→u∗isantilinear(section12.1.2)and(u∗)∗=u.(iii)Ifλisaneigenvalueofuthen¯λisaneigenvalueofu∗anddimker(λIE−u)=dimker(cid:15)¯λIE−u∗(cid:16).(iv)detu∗=detu.PROOF.(i)derivesfromthefollowingobservation:anelementofkeruisavectoryofEsuchthatuyisorthogonaltoallvectorsxofEandby(13.51)thisinturnmeansthatyisorthogonaltoimu∗.Theproofof(ii)iseasy.(iii)canbeprovedinthefollowingmanner:accordingto(13.52)wehaveforanyλ∈CE=ker(λIE−u)⊥⊕im(cid:15)¯λIE−u∗(cid:16)thusrk(cid:15)¯λIE−u∗(cid:16)=n−dimker(λIE−u).Nowdimker(cid:15)¯λIE−u∗(cid:16)=n−rk(cid:15)¯λIE−u∗(cid:16)accordingto(13.20)(section13.3.2)andthusdimker(cid:15)¯λIE−u∗(cid:16)=dimker(λIE−u).(iv)isprovedasfollows:iftheendomorphismuisrepresentedinanorthonormalbasisbythematrixAwehavedetu=detA=det¯A=det¯AT(seesection13.1.4orProposition534)fromwhichwegetdetu=detu∗.Appendix2:Algebra50713.5.4.UnitaryendomorphismAnendomorphismuissaidtobeunitaryifitpreservesthescalarproductthatisifforanyxy∈E uxuy!= xy!.Thisisequivalentto u∗uxy!= xy!thusalsotou∗=u−1.(13.53)Anendomorphismisunitaryifandonlyifittransformsanorthonormalbasisintoanotherorthonormalbasis.LetAbethematrixrepresentinganendomorphismuinanorthonormalbasis;itisclearthatuisunitaryifandonlyifA∗=A−1.(13.54)Amatrixsatisfying(13.54)issaidtobeunitaryandorthogonalifK=R(inthiscaseA∗=AT).Accordingtotheproperty(iv)inTheorem570and(13.53)iftheendomorphismuisaunitarywehavedetu=±1.13.5.5.NormalendomorphismDEFINITION571.–AnendomorphismuofEissaidtobenormalifitcommuteswithitsadjoint:uu∗=u∗u.THEOREM572.–Letubeanormalendomorphism.(i)Foranyeigenvalueλofuker(λIE−u)=ker(cid:15)¯λIE−u∗(cid:16).(ii)Theendomorphismuisdiagonalizableinanorthonormalbasis.(iii)Converselyadiagonalizableendomorphismuinanorthonormalbasisisnormal.PROOF.(i)Letλbeaneigenvalueofuandwritev=u−λIE.Itisclearthatavectorxisaneigenvectorofuassociatedwiththeeigenvalueλifandonlyifxisaneigenvectorofvassociatedwiththeeigenvalue0.Onesuchvectorcanbetakentobeunitary.Inadditionvisanormalendomorphism.Thenlety=v∗x.Wehave(cid:23)y(cid:23)2= v∗xv∗x!= vv∗xx!= v∗vxx!=0.Asaresulty=0andxisaneigenvectorofv∗associatedwiththeeigenvalue0.Converselychangingvtov∗itisclearthatanyeigenvectorofv∗associatedwiththe508LinearSystemseigenvalue0isaneigenvectorofvassociatedwiththissameeigenvalue.Itfollowsthatkerv=kerv∗.(ii)Wethushave(kerv)⊥=(kerv∗)⊥=imvaccordingto(13.52)(section13.5.3).Thereforetherestrictionofvtoimvisinjective.Theonlyelementarydivisorsofvthataremultiplesofsarethereforeequaltosbecauseifthereexistsanintegerm>1andavectorxsuchthatvmx=0wehavev(cid:15)vm−1x(cid:16)=0thusvm−1x=0;byinductionweobtainvx=0.Therootsoftheminimalpolynomialqu(s)arethussimplewhichprovesthatuisdiagonalizable(section13.3.4Corollary532).Supposethatvwhichhasaneigenvalue0alsohasanon-zeroeigenvalue˜µ.Letusshowthatkerv⊥ker(v−˜µIE).Letx∈kerv=kerv∗andy∈ker(v−˜µIE).Fromthefactthatv∗x=0weget˜µ xy!= x˜µy!= xvy!= v∗xy!=0thus xy!=0.Consideragaintheendomorphismu:wehavejustshownthatifλandµaretwodistincteigenvaluesofuthenker(λIE−u)⊥ker(µIE−u).AccordingtotheGram-Schmidtorthogonalizationprinciplewecanchooseanorthonormalbasisineacheigenspaceofu.TheconcatenationofthesebasesisanorthonormalbasisofEinwhichudiagonalizes.(iii)LetubeanendomorphismrepresentedinanorthonormalbasisbyadiagonalmatrixΛ=diag(λ1...λn).WehaveΛ∗=diag(cid:15)¯λ1...¯λn(cid:16)thusΛΛ∗=Λ∗Λ=diag(cid:9)|λ1|2...|λn|2(cid:10).13.5.6.Self-adjointendomorphismGeneralcaseDEFINITION573.–AnendomorphismuofEissaidtobeself-adjointifu∗=u.LetubeanendomorphismrepresentedinanorthonormalbasisBbyamatrixA;itisclearthatuisself-adjointifandonlyifA∗=A.OnesuchmatrixAissaidtobeHermitian(andsymmetricifK=R;inthiscaseA∗=AT).THEOREM574.–Aself-adjointendomorphismisdiagonalizableinanorthonormalbasisandallitseigenvaluesarereal.PROOF.Aself-adjointendomorphismuisnormalthusisdiagonalizableinanorthonormalbasis.InadditionbyTheorem572(i)(section13.5.5)foranyeigenvalueλofuker(λIE−u)=ker(cid:15)¯λIE−u(cid:16)thusλ=¯λ.Non-negativeorpositiveself-adjointendomorphismsDEFINITION575.–Aself-adjointendomorphismuissaidtobenon-negative(resp.positive)12whichwecanwriteasu≥0(resp.u>0)ifallitseigenvaluesare12.Suchanendomorphismisalsocallednon-negativedeﬁnite(resp.positivedeﬁnite).Appendix2:Algebra509non-negative(resp.positive);amatrixArepresentinguinanorthonormalbasisissaidtobeHermitian(symmetricifK=R)non-negative(resp.positive)deﬁnitewhichwewriteasA≥0(resp.A>0).Itisclearthatapositiveself-adjointendomorphismisinvertibleandisthusanautomorphism(seesection13.3.3).Letubeaself-adjointendomorphismandx=n(cid:12)k=1xkekbeavectorofEwhere{e1...ek}isanorthonormalbasisofEformedbytheeigenvectorsofuekbeingassociatedwiththeeigenvalueλk.Wehave xux!=;n(cid:12)k=1xkekn(cid:12)k=1xkλkek<=n(cid:12)k=1λk|xk|2.(13.55)Asaresult–uisnon-negativeifandonlyif xux!≥0foranyx∈E.–uispositiveifandonlyif xux!>0foranyvectorx(cid:5)=0ofE.By(13.55)ifuisanon-negativeself-adjointendomorphismwehavetheinequality xux!≤λmax(u)(cid:23)x(cid:23)2whereλmax(u)isthelargesteigenvalueofu.Inadditionthetwosidesofthisinequalitybecomeequalwhenxisaneigenvectorassociatedwiththeeigenvalueλmax(u).Thereforemax(cid:10)x(cid:10)=1% xux!=%λmax(u).(13.56)Theproofofthefollowingisstraightforward:PROPOSITION576.–LetbetheHermitianmatrixA=(cid:27)A11A12A∗12A22(cid:28)whereA11>0.ThenA=(cid:27)I0A∗12A−111I(cid:28)(cid:27)A1100C(cid:28)(cid:27)I0A∗12A−111I(cid:28)∗C=A22−A∗12A−111A12andA≥0(resp.A>0)ifandonlyifC≥0(resp.C>0).510LinearSystemsSquarerootsofaHermitianmatrixWewilldeﬁnetheHermitiansquarerootofanendomorphismu≥0(orinanequivalentmanneroftheHermitianmatrixQ≥0whichrepresentsitinanorthonormalbasis).OnesuchmatrixQdiagonalizesinanorthonormalbasisthusthereexistsanunitarymatrixUsuchthatQ=U∗ΛUwhereΛ=diag{λ1...λn}andtheλi’saretheeigenvaluesofQ.SinceQ≥0theλiarerealnumbers≥0.Write√Λ=diag(cid:31)√λ1...√λn .TheHermitiansquarerootofQisdeﬁnedby%Q=U∗√ΛU.Thisterminologyisjustiﬁedbythefollowingtwoproperties:(i)√Q≥0;(ii)√Q√Q=Q.MoregenerallyasquarematrixQofordernisHermitian(resp.symmetricreal)non-negativedeﬁniteifandonlyifthereexistsamatrixEwithcomplex(resp.real)entriessuchthatQ=E∗E(resp.Q=ETE)andthismatrixEcanbechosentobeleft-regularthatishavingrrowsandncolumnswherer=rkQ.Theabovelatterassertioncanbeprovedasfollows:letQbeaHermitiannon-negativedeﬁnitematrixofordernandofrankrletλ1...λrbethenon-zeroeigenvaluesofQandletΛ0=diag{λ1...λr}.ThereexistsaunitarymatrixUsuchthatQ=U∗(cid:15)Λ0⊕0(cid:16)U=E∗EwhereE=(cid:25)Λ00(cid:26).AnymatrixEsuchthatQ=E∗EiscalledasquarerootofQ.Forexample(cid:25)√20(cid:26)and(cid:25)−√20(cid:26)aretwoleft-regularsquarerootsofthematrix(cid:27)2000(cid:28)anditsuniquesymmetricsquarerootis(cid:27)√2000(cid:28).AsacomplementseeCorollary586(section13.5.7).13.5.7.SingularvaluesNormofoperatorsinHilbertspacesLetEandFbetwoﬁnite-dimensionalHilbertspacesandletubeahomomorphismfromEintoF.Accordingtosection12.1.3thenormofu(inducedbythenormsofEandF)isdeﬁnedby13(cid:23)u(cid:23)(cid:1)max(cid:10)x(cid:10)=1(cid:23)ux(cid:23).(13.57)13.Thefunctionx→(cid:1)ux(cid:1)iscontinuousinthecompactset(cid:1)x(cid:1)=1andthusadmitsamaximuminthisset.Wethuscanreplacethesupbymax.Appendix2:Algebra511THEOREM577.–Foreveryhomomorphismu:E→Fwehavetheequality(cid:23)u(cid:23)=(cid:23)u∗(cid:23)=%λmax(u∗u)(cid:1)¯σ(u).PROOF.Wehave(cid:23)ux(cid:23)2= uxux!= u∗uxx!andu∗uisclearlyanon-negativeself-adjointendomorphismofE.Wehavethusforanyx(cid:5)=0(cid:23)ux(cid:23)=% u∗uxx!.Inadditionaccordingto(13.56)(section13.5.6)themaximum(13.57)isattainedbyputtingxasaunitaryeigenvectorofu∗uassociatedwiththelargesteigenvalueofthisendomorphism.Fortheequality(cid:23)u(cid:23)=(cid:23)u∗(cid:23)see([35](11.5.2)).SupposethattheabovespacesEandFareofdimensionnandmrespectivelyandthatorthonormalbasesarechoseninthesetwospaces.InthesebasesthehomomorphismuisrepresentedbyamatrixA∈Km×n.The“operatornorm”ofthismatrixisdeﬁnedby(cid:23)A(cid:23)=(cid:23)u(cid:23)andthisquantity¯σ(u)isequallydenotedas¯σ(A).Thisnormisclearlymultiplicative(seesection12.1.3).Notethataunitaryendomorphismhasanoperatornormequalto1;thisalsoholdsforaunitary(ororthogonal)matrix.Onecaneasilyshowthefollowingresult(seeforexample[35](11.1.3)):PROPOSITION578.–LetA∈Kn×n;everyeigenvalueλofAsatisﬁes|λ|≤¯σ(A).SingularvaluedecompositionTHEOREM579.–LetA∈Km×n.Thereexistunitary(ororthogonalifK=R)matricesU∈Km×mandV∈Kn×nsuchthatU∗AV=diag(σ1...σp)∈Km×np=min(mn)(13.58)whereσ1≥...≥σp≥0.Thesequantitiesσiareuniquelydetermined.Wehaveσ1=¯σ(A)andthelargestintegerrsuchthatσr(cid:5)=0istherankofA.PROOF.Accordingtotheabovethereexistsaunitarycolumnvectoru1∈Knsuchthat(cid:23)Au1(cid:23)=σ1whereσ1=¯σ(A).Therethusexistsaunitarycolumnvectorv1inKmsuchthatAu1=σ1v1.TheGram-SchmidtorthogonalizationprincipleappliedtoKncanbeexpressedasfollows:thereexistsamatrixV1∈Kn×(n−1)suchthat512LinearSystemsV=(cid:25)v1V1(cid:26)isaunitarymatrix.SimilarlythereexistsamatrixU1∈Km×(m−1)suchthatU=(cid:25)u1U1(cid:26)isaunitarymatrix.OneeasilyshowsthatU∗AVhasthefollowingstructure:U∗AV=(cid:27)σ1w∗0B(cid:28)(cid:1)A1.Wethushave(cid:23)A1(cid:23)≤(cid:23)U∗(cid:23)(cid:23)A(cid:23)(cid:23)V(cid:23)=(cid:23)A(cid:23).OntheotherhandA=UA1V∗thus(cid:23)A(cid:23)≤(cid:23)U(cid:23)(cid:23)A1(cid:23)(cid:23)V∗(cid:23)=(cid:23)A1(cid:23).Consequently(cid:23)A(cid:23)=(cid:23)A1(cid:23).Thecolumnvectorz=(cid:25)σ1wT(cid:26)Tissuchthat(cid:23)A1z(cid:23)2≥(cid:15)σ21+w∗w(cid:16)2=(cid:15)σ21+w∗w(cid:16)(cid:23)z(cid:23)2whichimpliesthat(cid:23)A1(cid:23)2≥σ21+w∗w.Butsince(cid:23)A1(cid:23)=(cid:23)A(cid:23)=σ1thisimpliesw=0fromwhichA1=σ1⊕Bwith(cid:23)B(cid:23)≤σ1.Continuingthiswayweobtaintheresultbyinduction.DEFINITION580.–Thequantitiesσi(1≤i≤p)arecalledthesingularvaluesofA.Theexpression(13.58)iscalledthesingularvaluedecompositionofA.14PROPOSITION581.–(i)LetA∈Km×nbeamatrixofrankrandσi(A)1≤i≤rbeitsnon-zerosingularvalues.Wehaveσi(A)=%λi(AA∗)=%λi(A∗A)1≤i≤rwhereλi(.)istheitheigenvalueofthematrixinparentheses(theseeigenvaluesarrangedindecreasingorder).(ii)AandA∗havethesamesingularvalues.(iii)LetA∈Kn×nbeaninvertiblematrix.Thenσi(cid:15)A−1(cid:16)=1σn−i+1(A).Inparticular¯σ(cid:15)A−1(cid:16)=1σ(A)where¯σandσdenotethelargestandsmallestsingularvaluerespectively.PROOF.(i)LetΣ=diag(σ1...σr).Accordingto(13.58)wehaveU∗AV=(cid:27)Σ000(cid:28)thusU∗AA∗U=U∗AV(U∗AV)∗=(cid:27)Σ2000(cid:28);similarlyV∗A∗AV=(U∗AV)∗U∗AVthus(i)isproven.(ii)isanimmediateconsequenceof(i).(iii)Letσi1≤i≤nbethesingularvaluesofAarrangedindecreasingorder(theyareallnon-zero).Accordingto(13.58)thereexistunitarymatricesUandVsuchthatU∗AV=diag{σi}thusV∗A−1U=diag(cid:31)σ−1i (sinceU−1=U∗andV−1=V∗).ThesingularvaluesofA−1arrangedindecreasingorderarethusσ−1n...σ−11whichproves(iii).14.Onecandeﬁnethesingularvaluedecompositionofahomomorphismbutthepresentationisfacilitatedbychoosingbasesandbyreasoningonthematrixrepresentingthishomomorphisminthechosenbasesaswhatwehavedonehere.Appendix2:Algebra513Pseudo-inverseGeneralcaseLetA∈Km×nbeamatrixofrankr≤min(nm).LetΣ=diag(σ1...σr)whereσ1≥...≥σrarethenon-zerosingularvaluesofA.Thesingularvaluedecompositiontheorem(Theorem579)showsthatthereexisttwounitarymatricesU∈Km×mandV∈Kn×nsuchthatA=U(Σ⊕0)V∗.DEFINITION582.–Wecallapseudo-inverseofAamatrixA†∈Kn×msuchthatA†=V(cid:15)Σ−1⊕0(cid:16)U∗.Left-inversePROPOSITION583.–Supposer=n(whichimpliesm≥n).(i)ThematrixAisleft-invertibleandA†isaleft-inverseofAthatisA†A=In;aleft-inverseisnon-uniqueifm>n.(ii)WehaveA†=(A∗A)−1A∗.PROOF.(i)WecanwriteAintheformA=U(cid:27)Σ0(cid:28)V∗thereforeA†=V(cid:25)Σ−10(cid:26)U∗.WededucethatA†A=V(cid:25)Σ−10(cid:26)U∗U(cid:27)Σ0(cid:28)V∗=In.Inadditionthereisnon-uniquenessinthecasem>nbecauseinthatcasethereexistseveralsolutionsLtotheequationLA=InsincetherearenmunknownsinLforn2equations.Thisproves(i).(ii)OntheotherhandA∗A=V[Σ0]U∗U(cid:27)Σ0(cid:28)V∗=VΣ2V∗thus(A∗A)−1A∗=VΣ−2V∗V[Σ0]U∗=V(cid:25)Σ−10(cid:26)U∗=A†whichproves(ii).514LinearSystemsRight-inverseInterchangingtherolesofAandA∗weobtainthefollowing:PROPOSITION584.–Supposethatr=m(whichimpliesn≥m).(i)ThematrixAisright-invertibleandA†isaright-inverseofAthatisAA†=Im;aright-inverseisnon-uniqueifn>m.(ii)WehaveA†=A∗(AA∗)−1.ConditionnumberofamatrixDEFINITION585.–LetA∈Kn×nbeaninvertiblematrix.TheconditionnumberofthematrixAisthenumberκ(A)deﬁnedby:κ(A)=¯σ(A)σ(A)=(cid:23)A(cid:23)(cid:29)(cid:29)A−1(cid:29)(cid:29).Whenκ(A)isverylargethematrixAissaidtobeill-conditioned.Suchamatrixposesdifﬁcultiesindigitalcomputation:see[56].RelationsbetweensquarerootsofamatrixLetA∈Kr×nandB∈Kr×nbetwosquarerootsofthesamematrixQ∈Kn×nQ≥0andforthesakeofsimplicitysupposethatAandBareleft-regularthatisr=rkQ.ThefollowingresultisaconsequenceofTheorem579:COROLLARY586.–(i)ThereexistsaunitarymatrixU∈Kr×rsuchthatB=UA.(ii)ConverselyifA∈Kr×nisasquarerootofQ≥0andifU∈Kr×risunitarythenB=UAisagainasquarerootofQ.PROOF.(i)AccordingtoTheorem579thereexisttwounitarymatrices˜UandVsuchthatA=˜U∗(cid:25)Σ0(cid:26)VwhereΣ=diag(σ1...σr)isthediagonalmatrixformedbythenon-zerosingularvaluesofA.WethushaveA∗A=V∗(cid:27)Σ∗0(cid:28)˜U˜U∗(cid:25)Σ0(cid:26)V=V∗(cid:27)Σ2000(cid:28)V.SinceA∗A=B∗BthereexistsaunitarymatrixWsuchthatB=W∗(cid:25)Σ0(cid:26)V=W∗˜UA=UAwhereU(cid:1)W∗˜U∈Kr×risunitary.(ii)isobvious.Appendix2:Algebra51513.6.Fractionsandspecialrings13.6.1.RationalfunctionsSetK(s)p×mArationalfunctionf(s)withcoefﬁcientsintheﬁeldK=RorCisthequotientoftwopolynomialswithcoefﬁcientsinK:f(s)=N(s)D(s)(13.59)whereD(s)isnotthezeropolynomial.ThesetoftheserationalfunctionsisdenotedbyK(s).ThissetisbothaﬁeldandaK-vectorspacethusaK-algebra.ThesetofmatricesofrationalfractionswithprowsandmcolumnsisdenotedbyK(s)p×m.ThissetisbothaK-vectorspaceandaK(s)-vectorspace.InadditionwecanformtheproductofanytwoelementsofK(s)n×nandthisproductbelongstothissetwhichisthusaring(thisringisnon-commutativeifn>1)andthereforeaK(s)-algebra;thisalgebraisunitarywithunitelementIn.IrreduciblerationalfunctionsTherationalfunction(13.59)isirreducibleifN(s)andD(s)havenocommonfactor.Wecancomebacktothatcasebycancelingthecommonfactors(ifany).Thereforeallrationalfunctionsareassumedtobeirreducibleinthesequel.PoleszerosandresiduesArationalfunctionisameromorphicfunctionofaspecialkindthusazeroapoleandtheirmultiplicitiesaswellastheresidueatapolearedeﬁnedforarationalfunctionasforameromorphicfunction(section12.4.1).Morespeciﬁcallythezerosoff(s)aretherootsz1...zmofN(s)inC(withm=d◦(N))whilethepolesofthisrationalfunctionaretherootsp1...pnofD(s)inC(withn=d◦(D)).LetZ(resp.P)bethesetofdistinctzeros(resp.poles)off(s).Therelativedegreeoff(s)istherationalintegerδ(f)=n−mwhile−δ(f)issometimescalledthedegreeoff(see[10]ChapterIV)generalizingthenotionofdegreeofapolynomial.Iffandgaretworationalfunctionsthenδ(fg)=δ(f)+δ(g).BehavioratinﬁnityArationalfractionf(s)issaidtobeproper(resp.strictlyproperbiproper)ifδ(f)≥0(resp.δ(f)>0δ(f)=0).Arationalfunctionthatisnotproperissaidtobeimproper.Itisclearthat516LinearSystemsf(s)isproperifandonlyiflim|s|→+∞|f(s)|<+∞;f(s)isstrictlyproperifandonlyiflim|s|→+∞|f(s)|=0;f(s)isbiproperifandonlyiflim|s|→+∞|f(s)|∈R(cid:2){0}.MoregenerallyamatrixG(s)belongingtoR(s)p×missaidtobeproper(resp.strictlyproper)ifallitselementsareproper(resp.strictlyproper).Itissaidtobebiproperifp=mG(s)isinvertibleandproperandG−1(s)isproper.DecompositionintosimpleelementsinC(s)PerformingtheEuclideandivisionofN(s)byD(s)(seesection13.1.3)weobtainN(s)=D(s)Q(s)+R(s)wherethequotientQ(s)isapolynomialandwheretheremainderR(s)isapolynomialsuchthatd◦(R)<d◦(D).Asaresultf(s)=Q(s)+R(s)D(s)=Q(s)+g(s)(13.60)whereg(s)=R(s)D(s)isastrictlyproperrationalfunction.Thisdecompositionisunique.Ontheotherhandg(s)=(cid:12)p∈P(cid:12)1≤j≤nPαpj(s−p)j(13.61)whereαpj∈C.Recallthatthecomplexnumberαp1istheresidueRes(f;p)offatthepolep(section12.4.1).13.6.2.Algebra(cid:1)H∞DeﬁnitionWedenoteby(cid:1)H∞thesetofrationalfunctionswithrealcoefﬁcientswhichi)areproperii)havenopolesintheclosedrighthalf-plane¯C+={s:Re(s)≥0}.Thering(cid:1)H∞Thesumoftwoelementsof(cid:1)H∞belongsto(cid:1)H∞.Theelementf(s)∈(cid:1)H∞hasanopposite−f(s).Theproductoftwoelementsof(cid:1)H∞belongsto(cid:1)H∞.Asaresult(cid:1)H∞isaring.Thisringisacommutativedomain.Letfbeanon-zeroelementof(cid:1)H∞letm+(f)bethenumberofitszerosin¯C+andletδ(f)beitsrelativedegree.Writeθ(f)=m+(f)+δ(f).Onecanprovethefollowing([114]section2.1):Appendix2:Algebra517THEOREM587.–Thefunctionθisadegreeonthering(cid:1)H∞.Asaresult(cid:1)H∞isastronglyEuclideandomain(andthusaprincipalidealdomain).Examples1)Letf(s)=(s−1)s(s+1)3.Wehavem+(f)=2δ(f)=1thusθ(f)=3.2)Letg(s)=(s+2)(s+3)(s+1)2Wehaveθ(g)=0thusgisaunitof(cid:1)H∞(seesection13.1.3).Thealgebra(cid:1)H∞Ontheotherhand(cid:1)H∞isanR-vectorspaceandthusisacommutativeR-algebra.Thespaceofoperators(cid:1)H∞Wewillnowleavepurealgebraandaddsomeanalyticalconsiderations.THEOREM588.–Themapping(cid:23).(cid:23)∞from(cid:1)H∞intoR+deﬁnedby(cid:23)f(cid:23)∞=supRe(s)≥0|f(s)|(13.62)isanormonthevectorspace(cid:1)H∞.Thisnormismultiplicative(seesection12.1.3)whichmakes(cid:1)H∞a“normedalgebra”.Lastwehavetheequality(cid:23)f(cid:23)∞=supω≥0|f(iω)|.(13.63)PROOF.Letf∈(cid:1)H∞.Sincefhasnopolesintheclosedrighthalf-planeandisaproperrationalfunctionthequantity(13.62)isﬁnitethus(cid:23)f(cid:23)∞∈R+.Ifα∈Ritisclearthat(cid:23)αf(cid:23)∞=|α|(cid:23)f(cid:23)∞.Ifg∈(cid:1)H∞thetriangleinequality(cid:23)f+g(cid:23)∞≤(cid:23)f(cid:23)∞+(cid:23)g(cid:23)∞obviouslyholds.Supposethat(cid:23)f(cid:23)∞=0.Letf∞=lim|s|→+∞f(s)insuchawaythath(s)=f(s)−f∞isastrictlyproperrationalfunction.Wehavef(s)=f∞+h(s)=0∀s.Inadditionlim|s|→+∞h(s)=0thereforef∞=0andf=h.Accordingtosection12.4.4fistheLaplacetransformofafunctionu∈L1.TheFouriertransformofuisthefunctionω(cid:3)→f(iω)=0thusu=0andﬁnally518LinearSystemsf=0.Inadditionitisclearthat(cid:23)fg(cid:23)∞≤(cid:23)f(cid:23)∞(cid:23)g(cid:23)∞.Theexpression(13.63)isaneasyconsequenceofthemaximummodulusprinciple(Theorem444).Let˜Σbetheconvolutionoperatoru(cid:3)→g∗uwhereg∈S(cid:2)+isadistributionwhoseLaplacetransformˆg(s)isaproperrationalfunction.Onecanprovethefollowing[34][115]:THEOREM589.–Thefollowingﬁveconditionsareequivalent:(i)ˆg(s)∈(cid:1)H∞;(ii)thekernelgisoftheformg=g0δ+hwhereg0∈Randh∈L1;(iii)foranyp∈[1+∞]˜ΣisacontinuouslinearoperatorfromLpintoLp;(iv)˜ΣisacontinuouslinearoperatorfromL2intoL2ofwhichthenorm(inducedbythatofL2anddenotedasγ2(cid:9)˜Σ(cid:10))isequalto(cid:23)ˆg(cid:23)∞;(v)˜ΣisacontinuouslinearoperatorfromL∞intoL∞whosenorm(inducedbythatofL∞anddenotedasγ∞(cid:9)˜Σ(cid:10))isequalto|g0|+(cid:23)h(cid:23)1.Theset(cid:1)Hp×m∞isthatofallmatricesofsizep×mtheentriesofwhichbelongto(cid:1)H∞.ItisanR-vectorspaceanditisanon-commutativeR-algebraifp=m>1.Onecaneasilyverifythatthemapping(cid:23).(cid:23)∞from(cid:1)Hp×m∞intoR+deﬁnedby(cid:23)G(cid:23)∞=supRe(s)≥0¯σ(G(s))=supω≥0¯σ(G(iω))(13.64)isanormon(cid:1)Hp×m∞.AnelementG(s)∈(cid:1)Hp×m∞isthetransfermatrixofaconvolutionoperator˜ΣfromLm2intoLp2(withtheconventionofsection2.5.1).Onecanshow[122]thatthenormofthisoperator(inducedbythenormsofLm2andLp2anddenotedasγ2(cid:9)˜Σ(cid:10))isγ2(cid:9)˜Σ(cid:10)=(cid:23)G(cid:23)∞.(13.65)13.6.3.*AlgebraH∞Thenormedalgebra(cid:1)H∞isasubalgebraofaBanachalgebra(i.e.acompletenormedalgebra)denotedasH∞and(cid:1)H∞=H∞∩R(s).ThespaceH∞iscalledaHardyspace.Afunctionofthecomplexvariablef(s)belongstoH∞ifandonlyif(i)fisanalyticintherighthalf-planeand(ii)(cid:23)f(cid:23)∞(cid:1)supRe(s)>0|f(s)|<+∞.Wethenhave(cid:23)f(cid:23)∞=ess.supω≥0|f(iω)|([53]Theorem2.3.1).OntheotherhandH∞isaGCDdomain[109]whichisneitheraUFDnoraBézoutdomainbuthasthefollowingpropertyasshownin[98]:Appendix2:Algebra519THEOREM590.–H∞isacoherentSylvesterdomain.WedenotebyHp×m∞theR-vectorspace(whichisanR-algebraifp=m)consistingofmatricesofsizep×mandwhoseallentriesbelongtoH∞.Thenormdeﬁnedonthisspaceis:(cid:23)G(cid:23)∞=supRe(s)>0¯σ(G(s)).ItisimmediatelyclearthatHp×m∞isaBanachspaceandaBanachalgebraifp=m.13.6.4.*ClassiﬁcationofringsWehavethefollowingclassiﬁcationoftheringsencounteredsofar:UFD→GCD(↑*ED→PID→EDR→BD→CSD→SD→HR+↓NR→CRwhere→signiﬁes“ismorerestrictivethan”andwhereEDPIDEDRBDCSDSDHRNRCRUFDandGCDrespectivelymeansEuclideandomainprincipalidealdomainelementarydivisorringBézoutdomaincoherentSylvesterdomainSylvesterdomainHermiteringNoetherianringcoherentringuniquefactorizationdomainandGCDdomain.13.6.5.*ChangeofringsRingoffractionsLetRbearingandSbeamultiplicativepartofR(thatistosayifs1s2∈Sthens1s2∈S)notcontainingzero.WedenotebyS−1Rthesetofelementsoftheformr/sr∈Rs∈S;oneeasilyshowsthatthissetisaringcalledtheringoffractionsofRwithdenominatorinS.Thefollowingtheoremsummarizestheresultsprovenin([11]ChapterVIIsection1-3)([102]Chapter4)[60]([31]Theorem5.11)([79]Theorem3.8).THEOREM591.–IftheringRisaprincipalidealdomain(resp.aBézoutdomainauniquefactorizationdomainaNoetherianringacoherentringaSylvestercoherentdomainanelementarydivisorring)thensoisS−1R.520LinearSystemsExtensionoftheringofscalarsLetMbeanR-moduleandAbeanR-algebra(seesection13.1.1).ThenthetensorproductA⊗RMisthesetofallﬁnitelinearcombinations(cid:11)ai⊗miai∈Ami∈M;A⊗RMisanA-module.ThefunctorA⊗R−fromthecategoryofR-modulesintothatofA-modulesisexactcovariant15andiscalledthefunctor“extensionoftheringofscalars”.TheR-homomorphismM(cid:20)m→1⊗m∈A⊗RMissaidtobecanonical(thisisnotanepimorphism).RestrictionoftheringofscalarsLetAbeanR-algebraandMbeanA-module.ThesetMhasacanonicalstructureofR-module;thismoduleisdenotedasM[R].ThefunctorM(cid:3)→M[R]isexactcovariantandiscalledthefunctor“restrictionoftheringofscalars”.Thisrelationbetweentheextensionandtherestrictionoftheringofscalarsisdetailedin([10]n◦II.5.2).Thereadershouldbeawarethattheseoperationsarenotinverseofoneanother.Forexamplesince2isaunitofQQ⊗Z(Z/2Z)=0whoserestrictiontoZ/2Zremainszero.ModulesoffractionsTheringoffractionsA=S−1RisanR-algebraandwhenAisdeﬁnedinthatwayA⊗RMisdenotedasS−1M.Thetensorproducta⊗m(a∈Am∈M)canbedenotedasam([11]sectionII.2)thecanonicalhomomorphismθ:M→S−1Miswrittenasm→11m(where11istheunitelementofA)andaccordingto([102]Theorem3.71)kerθ={m∈M:∃s∈Ssm=0}.(13.66)LetA=KtheﬁeldoffractionsofR(seesection13.1.1);thetensorproductK⊗RMisaK-vectorspacedenotedbyM(K).SinceS=R(cid:2){0}wehaveaccordingto(13.66)kerθ=T(M).(13.67)InparticularifMistorsion-freeitiscanonicallyisomorphicto(andidentiﬁedwith)θ(M)⊂M(K)accordingtoTheorem538(ii).Thereforeconsideringelementsm1...mkofMtheseareR-linearlyindependentifandonlyifθ(m1)...θ(mk)areK-linearlyindependent.OntheotherhandifM(cid:2)⊂MM(cid:2)(K)isidentiﬁedwithasubspaceofM(K)and(M/M(cid:2))(K)isidentiﬁedwithM(K)/M(cid:2)(K)([10]n◦II.7.10).15.Thenotionsofcategoryandfunctoraredetailedin[102](asfarascategoriesofmodulesareconcerned);seealso[91].Asuccinctaccountisgivenin[22].Chapter14SolutionsofExercises14.1.ExercisesofChapter1SolutionofExercise1Applyingthemeshruleandthenodalruleweﬁnd:CdVdt=R1+R2R1R2i+(R2C+LR1)didt+LCd2idt2.SolutionofExercise2ThesimplestapproachistoapplytheLagrangeequationswhichyield:(M+2m)¨y+(cid:11)2i=1(mlicosθi¨θi−mlisinθi˙θ2i)−f=0li¨θi−gsinθi+¨ycosθi=0i=12.SolutionofExercise3Volumeisconserved:Sdhdt=Q1+Q2−σ√2gh.Massisconserved:ddt(csSh)=c1Q1+c2Q2−csσ√2ghwhichtogetherwiththepreviousequationyields:Shdcsdt+cs(Q1+Q2)=c1Q1+c2Q2.521Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.522LinearSystemsSolutionofExercise4Volumeisconserved:Sdhdt=Q1+Q2−σ√2gh.Energyisconserved:ShdTsdt+Q1(Ts−T1)+Q2(Ts−T2)=0.Wenotethattheequationsarethesameasthoseinthepreviousexercisetheconcentrationsbeingreplacedbythetemperatures.14.2.ExercisesofChapter2SolutionofExercise53Wehavefrom(1.19)assumingzeroinitialconditionsandwritingu=f:(cid:15)m1s2+k(cid:16)ˆz1−kˆz2=ˆu(cid:15)m2s2+k(cid:16)ˆz2−kˆz1=0.Thereforeputtingy=z2:G(s)=ˆyˆu=km1m2s2(cid:9)s2+km1+m2m1m2(cid:10).Thesystemthushasnotransmissionzeroswhilethetransmissionpolesare{00iω0−iω0}withω0=(cid:30)km1+m2m1m2.SolutionofExercise54Theoutputthistimeisωandwithzeroinitialconditionsˆω(s)=sˆθ(s).ThetransferfunctionofthenewcontrolsystemconsideredisthereforeH(s)=sG(s)whereG(s)isgivenby(2.33)i.e.H(s)=ks2+2ςω0s+ω20.Thusthesystemhasnotransmissionzerosandhastransmissionpoles{p1p2}wherep1andp2aretherootsofthepolynomials2+2ςω0s+ω20.SolutionofExercise55Case(a):wehaveG(s)=1(s−1)3(s+1)(cid:27)(s+1)2(s−1)(s−1)20(s+1)3(cid:28)∼1(s−1)3(s+1)(cid:27)100(s−1)(s+1)5(cid:28)=#1(s−1)3(s+1)00(s+1)4(s−1)2$.SolutionsofExercises523ThedistinctpolesofG(s)arethuss=−1(simplepole)ands=1;thelatterhasstructuralindices{23}thusitsorderis3anditsdegreeis5.Thetransmissionorderofthesystemis5+1=6.OntheotherhandG(s)hasauniquezeros=−1;thisoneherehasauniquestructuralindexof4(thusitsorderanditsdegreearebothequalto4).Case(b)and(c):theSmith–MacMillanformofG(s)is(b)(cid:27)1(s+1)2(s+2)200s+2(cid:28);(c)(cid:27)1(s+1)2(s+2)00s(cid:28)andwecometothesameconclusionasabove.SolutionofExercise56Equilibriumpositionsoftheinverteddoublependulum:Theﬁrstequationyieldsf∗=0.Thesecondequationyieldssinθ∗i=0i=12fromwhichwegetθ∗i=0orπ(mod.2π).Linearizationofthesystematy∗=0θ∗1=θ∗2=0:(cid:19)(M+2m)d2ydt2+ml1d2θ1dt2+ml2d2θ2dt2−f=0lid2θidt2−gθi+d2ydt2=0i=12.SolutionofExercise57Equilibriumofvolume:Q∗1+Q∗2−σ√2gh∗=0.Equilibriumofconcentrationcs:c∗s(Q∗1+Q∗2)=c1Q∗1+c2Q∗2.Usingtheﬁrstequationweobtainc1Q∗1+c2Q∗2=c∗sσ√2gh∗.Notethatexceptifc1=c2thevaluesofthedifferentvariablesatequilibriumareentirelydeterminedbyh∗andc∗s.Linearizationoftheequations:(cid:20)ddt∆h+σS%g2h∗∆h=1S(∆Q1+∆Q2)ddt∆cs+2σS%g2h∗∆cs=c1−c∗sSh∗∆Q1+c2−c∗sSh∗∆Q2.524LinearSystemsSolutionofExercise58ThelinearizedequationsoftheinvertedpendulumcanbeputintheformD(∂)y=N(∂)uwithD(∂)=(cid:27)(M+m)∂2ml∂2∂2l∂2−g(cid:28)N(∂)=(cid:27)10(cid:28)andy=[zθ]Tu=f.WehavedetD(∂)(cid:5)=0thustherankofthesystemisequalto1.Thereisthusoneinputwhichisthecontrolandisnaturallytheforcef.Inthisrepresentationwhichisaleftformthereisnolatentvariables.Wecanintroduceasintermediatevariablesvelocitiesv=dzdtandq=dθdtwhicharethenlatentvariables.SolutionofExercise59Thereareclearlytwoinputswhichwewillpickas∆Q1and∆Q2.Itisnecessarytoregulate∆hinsuchawaythatthetankdoesnotoverﬂownorbecomeempty.Theaimofthemixeristomakeitpossibletoregulatetheconcentration.Thetwocontrolledvariablesarethus∆hand∆cs.SolutionofExercise60(i)Theequilibriumconditionsaref(x∗u∗)=0y∗=g(x∗u∗).(ii)Write∆x=x−x∗∆u=u−u∗∆y=y−y∗.Weobtain∂∆x=A∆x+B∆u∆y=C∆x+D∆uwithA=∂f∂x(x∗u∗)B=∂f∂u(x∗u∗)C=∂g∂x(x∗u∗)D=∂g∂u(x∗u∗).14.3.ExercisesofChapter3SolutionofExercise70ThestaticgainofthesystemisG(0)=0.WriteK=|G(iω)|2k2and=(cid:9)ωω0(cid:10)2.WeobtainK=(−1)2+4ς2.WeeasilyverifythatdKdbecomeszerowhen=1thuswhenωr=ω0.Themaximumof|G(iω)|is|G(iωr)|=k2ς.SolutionsofExercises525SolutionofExercise71(a)ThestepresponseisthatofaﬁrstordersystemthetransferfunctionofwhichisG(s)=k1+τs.Thestaticgainisk=0.1.Thetimeconstantτistheabscissaatwhichtheintersectionofthetangentattheoriginandthehorizontalaxisislocatedandtheordinateofwhichisthestaticgain.Wehavethusτ(cid:9)0.1s.(b)ThestepresponseisthatofasecondordersystemthathasnozeroandthetransferfunctionofwhichisthusoftheformG(s)=kω20s2+2ςω0s+ω20.Thestaticgainisk=10.Theovershootisoftheorderof37%whichcorrespondstoadampingcoefﬁcientς(cid:9)0.3.Ontheotherhandthemaximumofthecurveisattainedatt=1sthusπωp=1fromwhichwealsohaveω0=π√1−ς2(cid:9)3.3rad/s.(c)Thissystemhasnoresonance.Theslopeoftheamplitudegoesfrom0to−40dB/decintheneighborhoodof0.1rad/s.Theargumentgoesfrom0◦to−180◦andis−90◦forω=0.1rad/sapproximately.Itisthereforeastablesecondordersystemwithundampednaturalangularfrequencyω0=0.1rad/sandwithadampingcoefﬁcientςbetween0.7and1.Itisdifﬁculttobemorespeciﬁcregardingς.Thestaticgainisequalto10(i.e.20dB).SolutionofExercise72(a)WehaveG(s)=λ(n−rk=1(zk−s)(nk=1(pk−s).(b)Thesystemiswith“negativestart”ifandonlyifρ<0.(c)Wehaveρ=y(r)(0+)G(0)=(n−rk=1(cid:9)−1zk(cid:10)(nk=1(cid:9)−1pk(cid:10)sinceaccordingtotheInitialvaluetheorem(section12.3.4)y(r)(cid:15)0+(cid:16)=lims∈Rs→+∞srG(s).526LinearSystems(d)Considerthedenominator(nk=1(cid:9)−1pk(cid:10).Ifapolepkisrealwehave−1pk>0sincethispolebelongstothelefthalf-plane(forthesystemisstable).Ifapolepkiscomplextheconjugateterm−1¯pkalsoappearsintheproduct.Wehave(cid:9)−1pk(cid:10)(cid:9)−1¯pk(cid:10)=1|pk|2>0.Asaresultthedenominatorispositive.Thisreasoningalsoappliestothetermsofthenumeratorexceptthosethatcorrespondtonon-negativerealzeros.(e)Accordingto(b)and(d)thesystemΣiswith“negativestart”ifandonlyifthenumberofnon-negativerealzerosisodd.(ThisresultisduetoVidyasagar[113];weshouldtakecarenottoconfuseasystemwith“negativestart”withanon-minimalphasesystem:thereexistnon-minimumphasesystemswhosestepresponseismonotonicincreasingforexamplethesystemwithtransferfunctionG(s)=s2+2ςω0s+ω20ω20(s+1)2whereω0=10andς=−0.1.)(f)ThisresultiscoherentwiththestepresponsesinFigures3.14and3.15:thesystemswithtransferfunctionsen(s)andn(s)arewith“negativestart”forn=1andn=3andwith“positivestart”forn=2.Theintegernisalsothenumberofnon-negativerealzerosofthesetransferfunctions(aseasilyveriﬁed).14.4.ExercisesofChapter4SolutionofExercise100(i)–(iii)BodeNyquistandBlackplots:Figure14.1.Bodeplot–Exercise100SolutionsofExercises527Figure14.2.Nyquistplot–Exercise100Figure14.3.Blackplot–Exercise100528LinearSystems(iv)Thenecessarygaintoobtainaphasemarginof60◦is−6.3dBwhichisabout0.48.Thedelaymarginisthereforeof2.4t.u.(t.u.=timeunit).Thegainmarginis(−20.3dB+∞).SolutionofExercise101Questions(i)–(iv)areeasy:thefeedbacksystemisunstable.(v)Thisexampleshowsthatthe“Nyquistcriterion”obtainedwithaBromwichcontourthatencirclestheimaginarypolesontherightisfalse.SolutionofExercise102Question(i)iseasy:thefeedbacksystemisunstable.(ii)Thisexampleshowsthattheproposedstatementisincorrect.SolutionofExercise105WehaveLiγ(s)=Loγ(s)=Pγ(s)=(cid:27)1s+1−γs+10s+2s+1(cid:28)gγ(s)=|I2+Loγ(s)|−1=2s+3(s+1)2.SincetheNyquistplotofgγ(s)(thatistheMIMONyquistplotofLoγ(s))isentirelylocatedinsidetherighthalf-planethedistancefromthisplotto−1isequalto1whateverthevalueofγis.(ii)SupposeA0isreplacedbyAεthecontrolvariablebeingu=−y.Thefeedbacksystemequationbecomes˙y=(cid:27)−2γε−2(cid:28)y=Fywhere|sI2−F|=s2+4s+4−γε.ThefeedbacksystemisstableonconditionthatthetwoeigenvaluesofFlieinthelefthalf-planethatisγε<4.Thisfeedbacksystemisthusunstablewheneverγ≥4ε.(iii)WehaveMmiγ=Mmoγ=infω≥0σ(I2+Loγ(iω))andthereadercanverifyafterafewcalculationsthatthisquantitytendsto0asγ→+∞.(iv)AsaresultMmiγ=Mmoγisagoodindicatorofthelackofrobustnessoftheclosed-loopsystemforlargevaluesofγwhereasthislackofrobustnessisabsolutelynotreﬂectedbytheMIMONyquistplot.14.5.ExercisesofChapter5SolutionofExercise107(i)YessincePisanintegratorsystem(seesection5.1.2).(ii)Weusealead.Weneedaphaseleadϕd=29◦atω0=1rad/sthusα=2.9andτ=0.59s.WededucefromtherethatKPD(s)=0.861+1.70s1+0.59s.(iii)No(asforthecontrollerdeterminedat(ii)).(iv)ThePIDcontrollerissuchthatϕd−ϕI=29◦.ChoosingS=ϕd+ϕI=90◦weobtainϕd=59.5◦andϕI=30.5◦.ThecharacteristicsofthePIDcontrollerareﬁnallyasfollows:k=1;TI=5.1s;Td=0.95s;N=3.5.SolutionsofExercises52914.6.ExercisesofChapter6SolutionofExercise121(i)Ifδ0>0theSylvestersystemiswrittenas⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣10······00······0a11..................a1.........b0......an......0b1...00......1......0.........a1bnb1b0.........0......b1...an......bn...0·········00···0bn⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣σ1σ2......σn+δ0−1r0r1...rn⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦=⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣c1−a1...cn−ancn+1............c2n+δ0⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦.Theorderofthematrixontheleftisstill2n+δ0andthenumberofzerosbelowthelastan(resp.abovetheﬁrstb0)isequalto1(resp.δ0−1).(ii)Ifδ0=0itisnecessarytogobackto(13.13)insection13.1.5.WehaveAcl(∂)=c0∂2n+c1∂2n−1+...+c2nwherec0(cid:5)=1ifb0(cid:5)=0.SolutionofExercise1231)R(∂)=21∂2+31∂+10;S(∂)=∂3+14∂2+24∂;T(∂)=(∂+1)(∂+10).2)Seesection6.3.5.3)δ(cid:15)RS(cid:16)=1.SolutionofExercise124(i)R(∂)=(∂+1)2;S(∂)=∂(∂+2);T(∂)=∂+1.(ii)So(s)=ss+1;Mm=1.(iii)ThetransferfunctionbetweenrandyisG=BTAcl=1(s+1)2.ThestepresponseistheinverseLaplacetransformof1sG(s)=1s(s+1)2=1s−1s+1−1(s+1)2whichis(1−te−t−e−t)1(t)(seesection12.4.4).Thisresponsethushasnoovershoot(andofcoursewithnostaticerror).SolutionofExercise125(i)Areasonablevalueofαisoftheorderof10(butsincethesystempresentsnoparticulardifﬁculties–foritisopen-loopstableandminimumphase–wecanchooseasmallervalueofα).(ii)Withα=5wetakeAs(∂)=(∂+1)(∂+5)andAcl(∂)=As(∂)(∂+1)2fromtherewehaveR(∂)=5∂2+10∂+5S(∂)=∂(cid:15)∂2+8∂+12(cid:16)T(∂)=(∂+1)(∂+5).(iii)TheadvantageoftheRSTcontrollerdeterminedinExercise124isitssimplicitybutitsdisadvantagecompared530LinearSystemsFigure14.4.BodeplotofE(s)–Exercise126withtheonedeterminedinthisexerciseisthatisdoesnotﬁlterthemeasurementnoiseandthatitgeneratesalessrapidroll-offoftheopen-looptransferfunctioninhighfrequencies;thiscouldmeanlessrobustnessagainstneglecteddynamicsatthesefrequencies.SolutionofExercise126(i)TheresultiscoherentsincethetransferfunctionL(s)resemblesthatofapureintegrator(cid:15)1s(cid:16)inthelowfrequenciesasthisisthecaseforExercise124withaslopewhichsteepensaround5rad/s(whichcorrespondstotheabsolutevalueofthepoleadded).(ii)TheBodeplotofE(s)isshowninFigure14.4forω0=30rad/s.Themaximumisattainedattheresonantangularfrequencyωr=ω0√1−2ς2(cid:9)ω0andwithavalueof12ς√1−ς2(cid:9)12ς=100i.e.40dB.(iii)WiththeRSTcontrollerdeterminedinExercise125thestabilityispreservedinpresenceofthemodelingerrorbecauseforω≥30rad/s|L(iω)|islessthan−40dB;thus|P(iω)E(iω)|remainslessthan0dBatthesefrequencies(andtheNyquistcriterioncanbeusedtoconcludestability).ThisisnotthecasewiththeRSTcontrollerdeterminedinExercise124(whereL(s)=1s).SolutionofExercise127(i)S(∂)=∂(∂+3)R(∂)=4∂2+7∂+1T(∂)=(∂+1)2.(ii)S(∂)=∂(cid:15)∂2+4∂+7(cid:16)R(∂)=7∂2+12∂+1T(∂)=(∂+1)3.(iii)ForQuestion(i)SolutionsofExercises531“tooslow”apoleischosen:itwouldbepreferabletoplaceitat−10forexample;forQuestion(ii)two“faster”polesoftheclosed-loopsystemmustbechosen.14.7.ExercisesofChapter7SolutionofExercise208(i)Let{ABC}beastaterepresentationoftheDCmotorwherexisthespeciﬁedstatevector.WeobtainA=⎡⎣0100−λJKJ0−KL−RL⎤⎦B=⎡⎣001L⎤⎦C=(cid:25)100(cid:26).(ii)ControllabilityandobservabilityareveriﬁedusingtheKalmancriterion(Theorems141and150).WehaveindeedΓ(AB)=⎡⎣00KJL0KJL∗1L∗∗⎤⎦whichisofrank3sinceK(cid:5)=0(seesection1.3).OntheotherhandΩ(CA)=⎡⎣10001000KJ⎤⎦isofrank3forthesamereason.SolutionofExercise209(i)Thestateequationis˙x=Ax+BuwithA=⎡⎢⎢⎣0−εσ20000100(1+ε)σ2001000⎤⎥⎥⎦B=⎡⎢⎢⎣10−10⎤⎥⎥⎦.(ii)ThecharacteristicpolynomialofAispA(s)=s2(cid:15)s2+(ε+1)σ2(cid:16)thusthesystempolesare(cid:31)00±√1+εσ .Thesystemisthereforeunstable.(iii)WeeasilyverifyusingtheKalmancriterionthatthesystemiscontrollable.(iv)IfC=(cid:25)∗∗∗0(cid:26)thenthesystemisunobservable;itdoeshaveastructuresuchas(7.13)withA¯o=0.ThesystemisnotdetectableaccordingtoProposition171(section7.2.3)andDeﬁnition185(section7.3).(v)IfC=(cid:25)0001(cid:26)the532LinearSystemsKalmancriterionshowsthatthesystemisobservable.(vi)ThetransmissionzerosareequaltotheinvariantzerosaccordingtoTheorem179(i)(section7.2.6)andarethusthevaluess∈CforwhichtheRosenbrockmatrix(7.20)“loosesitsrank”.Weobtain{i.z.}={±σ}.Whenε(cid:16)1twosystempolesapproachthetwoi.z.’sandthesystemtendstowardsunobservability.SolutionofExercise210(i)Thestateequationis˙x=Ax+BuwithA=⎡⎢⎢⎣00−λλ00−λρλρ10000100⎤⎥⎥⎦B=⎡⎢⎢⎣1000⎤⎥⎥⎦.(ii)ThecharacteristicpolynomialofAispA(s)=s2(cid:15)s2+λ(ρ+1)(cid:16).Sinceλ(ρ+1)=km1+m2m1m2thesystempolesare(cid:7)00±i(cid:30)km1+m2m1m2(cid:8);thesevaluesaretheSmithzerosofthepolynomialmatrixD(s)accordingto(2.13)whichisclearlycoherentaccordingtoDeﬁnition17(section2.3.7).(iii)ItiseasytoverifyusingtheKalmancriterionthatthesystemiscontrollable.(iv)IfC=(cid:25)0100(cid:26)theKalmancriterionshowsthatthesystemisunobservable.Theinterpretationisimmediate:bymeasuringonlythevelocity˙z2wecannotdeterminethepositionz2.(v)Sincethesystemhasnoi.d.z.’swehave{h.m.}={o.d.z.}accordingtoCorollary176(section7.2.5).Theo.d.z.’saredeterminedusingProposition170(section7.2.3).Theo.d.z’sareindeedtheSmithzerosofthematrix(cid:27)sIn−AC(cid:28).Inthiscaseweeasilyshowthat{o.d.z.}={0}.Thus{h.m.}={0}.(vi)WithC=(cid:25)0001(cid:26)weshowusingtheKalmancriterionthatthesystemisobservable.(vii)Withthischoicethesystemisminimalanditst.z.’sarethusidenticaltoitsi.z.’si.e.tothetheSmithzerosoftheRosenbrockmatrix.SincethelatterissquareitsSmithzerosarerootsofitsdeterminant.Weseethatthisisaconstantandsothesystemhasnotransmissionzeros.AnotherfasterwayofgettingtothisconclusionistousetheresultobtainedinExercise53.SolutionofExercise211(i)Thecontrollablecanonicalformis{ABC}withA=(cid:27)−RL−1LC10(cid:28)B=(cid:27)10(cid:28)C=(cid:25)1L0(cid:26).SolutionsofExercises533(ii)TheobservabilitymatrixisΩ=(cid:27)1L0−RL2−1L2C(cid:28).Ifthecapacitytendstoward+∞thematrixΩbecomessingularandthesystembecomesunobservable(thechargeqofthecapacitorisnolongerobservable).SolutionofExercise212(i)Wecaneliminated2ydt2oftheequationsofmotionoftheinverteddoublependulumandweobtainthestaterepresentation˙x=Ax+BuwhereA=⎡⎢⎢⎣00100001a1a200a3a400⎤⎥⎥⎦B=⎡⎢⎢⎣001/l11/l2⎤⎥⎥⎦witha1=(M+m)gMl1a2=mgMl1a3=mgMl2a4=(M+m)gMl2.(ii)WestudythecontrollabilityusingtheKalmancriterion.SincethematrixΓ(AB)issquareitsufﬁcestoexamineonwhatconditionsitsdeterminantisnon-zero.Thisconditionisl1(cid:5)=l2(thereaderisrequestedtowonderwhy).(iii)TheKalmancriterionshowsthatthesystemisobservablewhentheoutputisx1.SolutionofExercise216(i){o.d.z.}={1}and{i.z.}=∅.(ii){i.d.z.}={1}and{i.z.}=∅.(iii)TheseresultsarecoherentwithTheorem179andshowthatwecannotexpecttoimprovethelatter.SolutionofExercise217(i)iseasy.For(ii)weproceedasinsection7.4.3andweobtainAc=⎡⎢⎢⎣−4−300100000−6−80010⎤⎥⎥⎦Bc=⎡⎢⎢⎣10000100⎤⎥⎥⎦Cc=(cid:27)13141112(cid:28).Thisformisnotunique:ifxdesignatesthestateoftheabovecanonicalformx(cid:2)isequallythestateofthecontrollablecanonicalformwithx(cid:2)=(cid:25)x3x4x1x2(cid:26)T.Thisisduetothefactthattwoofthecontrollabilityindicesareequal;indeed{µ1µ2}={22}.534LinearSystemsSolutionofExercise218ThewholeexerciseconsistsinusingthedualityrelationsofCorollary153(section7.1.4)andtheobservationsthatfollowthiscorollary.Asaresult:(i)Theobservabilityindicesof(CA)arethecontrollabilityindicesof(cid:15)ATCT(cid:16).(ii)ThematricesBooandBohavenoparticularform.Thecanonicalformstobedeterminedarethusthoseof(CA).Wehave(CooAoo)=(cid:15)ATccCTcc(cid:16)and(CoAo)=(cid:15)ATcCTc(cid:16)where(cid:15)ATccCTcc(cid:16)(resp.(cid:15)ATcCTc(cid:16))isthecanonicalformofcontrollability(resp.thecontrollablecanonicalform)of(cid:15)ATCT(cid:16).SolutionofExercise219(i)WeeasilyverifythatG(s)=D−1(s)N(s)andthatthematrices{DN}areleft-coprime.(ii)LetH(s)=GT(s).WehaveH(s)=NT(s)D−T(s)where(cid:31)NTDT areright-coprime.WedeterminearealizationofH(s)incontrollablecanonicalformbyproceedingasinsection7.4.3andwededucefromtherearealizationofG(s)inobservablecanonicalformbyusingtheresultsofExercise218.Thisrealization{AoBoCo}isobtainedbychoosingastheoutputy(cid:2)=(y2y1)andisgivenbyAo=⎡⎢⎢⎢⎢⎣−41000−4000000−41010−50120−200⎤⎥⎥⎥⎥⎦Bo=⎡⎢⎢⎢⎢⎣1−1000−10000⎤⎥⎥⎥⎥⎦Co=(cid:27)1000000100(cid:28).Theobservabilityindicesare{ω1ω2}={23}.Sincethesearealldifferentthiscanonicalformofobservabilityisunique.SolutionofExercise220(i)ThecontrollabilitymatrixofΣisΓ=⎡⎣0001−390−48⎤⎦.SincerkΓ=2Σisuncontrollableandhasonei.d.z.(for1=3−2).TheobservabilitymatrixofΣisΩ=⎡⎣1−1−121−5010−309000000⎤⎦T.SolutionsofExercises535AsaresultrkΩ=2Σisunobservableandhasoneo.d.z.(for1=3−2).(ii)ThesystempolesaretheeigenvaluesofAwhichislowertriangular;thesearethusthediagonalelementsofAwhichare{−1−31}.(iii)Theﬁrstequationofthesystemis∂x1=−x1.Thus−1isani.d.z.anditistheonlyoneaccordingto(i).Ontheotherhand{ABC}isdirectlydecomposedaccordingtoobservabilityandhasauniqueo.d.z.whichisequalto1.AccordingtoProposition173(section7.2.4){i.o.d.z.}=∅.AsaresultaccordingtoCorollary176(section7.2.5){h.m.}={−11}.AtlastaccordingtoTheorem179(i)(section7.2.6){t.p.}={s.p.}\{h.m.}={−3}.(iv)ThesystemΣisthusstabilisable(foralli.d.z.’sbelongstothelefthalf-plane)butnotdetectable(theo.d.z.belongstotherighthalf-plane).(v)TheRosenbrockmatrixisR(s)=⎡⎢⎢⎢⎢⎣s+1000−1s+30−1−14s−101000−1100⎤⎥⎥⎥⎥⎦∼⎡⎢⎢⎢⎢⎣0000000100s−1010000100⎤⎥⎥⎥⎥⎦.Itsrankis4fors(cid:5)=1andis3fors=1thus{i.z.}={1}.SolutionofExercise2211)(i)Thesystemisneithercontrollablenorobservable.(ii)Itspolesare{−110}.(iii)(a)ThetransferfunctionisG(s)=C(sI3−A)B=1s.(b)Toﬁndthei.d.z.’sweformthematrix(cid:25)sI3−AB(cid:26)=⎡⎣s+1000−1s−10000s1⎤⎦∼⎡⎣1000010000(s−1)(s+1)1⎤⎦.536LinearSystemsAsaresult{i.d.z.}={1−1}(seeProposition166section7.2.2).(c)Toﬁnditso.d.z.’sweformthematrix(cid:27)sI3−AC(cid:28)=⎡⎢⎢⎣s+100−1s−1000s−101⎤⎥⎥⎦∼⎡⎢⎢⎣10001000s−1000⎤⎥⎥⎦.Asaresult{o.d.z.}={1}(seeProposition170section7.2.3).(d)Accordingto(a){t.p.}={0}thus{h.m.}={s.p.}\{t.p.}={−11}.Consequently{i.o.d.z.}={i.d.z.}˙∪{o.d.z}\{h.m.}={1}.(iv)Thesystemisneitherstabilizablenordetectable.(v)ToﬁndtheinvariantzerosweformtheRosenbrockmatrixR(s)=(cid:27)sI3−A−BC0(cid:28)=⎡⎢⎢⎣s+1000−1s−10000s−1−1010⎤⎥⎥⎦∼⎡⎢⎢⎣100001000010000(s−1)(s+1)⎤⎥⎥⎦;asaresult{i.z.}={1−1}.ThisresultcanequallybeobtainedmoresimplybyapplyingTheorem179(iv)(section7.2.6)knowingthat{t.z.}=∅.SolutionofExercise222(i)ApplythePopov–Belevitch–Hautustest(Corollary139section7.1.3).(ii)Usingthecontrollability↔observabilityduality(Corollary153section7.1.4)weobtainthefollowingcriterion:Σisobservableifandonlyifthereexistsno(right)eigenvectorofAthatwillannihilateCthatisnocolumnvectorv(cid:5)=0suchthatAv=λvandCv=0.SolutionofExercise223AccordingtotheTaylor’sformulaP(t)=2n+1(cid:12)j=0pj(t−t0)jSolutionsofExercises537withpj=P(j)(t0)/j!.Itisimmediatethatp0=y0andpj=0for1≤j≤n.Consequentlyfor0≤β≤nP(β)(t1)=2n+1−β(cid:12)j=n+1j!(j−β)!∆j−βpj.Then+1coefﬁcientspj(n+1≤j≤2n+1)arethusobtainedbysolvingthesystemofn+1linearequations2n+1(cid:12)j=n+1∆jpj=y1−y02n+1−β(cid:12)j=n+1j!(j−β)!∆j−βpj=01≤β≤n.SolutionofExercise224(i)Wehavez1=z2+(m2/k)¨z2andf=(m1+m2)¨z2+(m1m2/k)z(4)2.Asaresultz2isaﬂatoutputofthesystem.Thisﬂatoutputisthemostnaturalpossible.(ii)Theequilibriumconditionsarez2(t0)=z20z2(t1)=z21z(β)2(ti)=01≤β≤4i∈{01}.14.8.ExercisesofChapter8SolutionofExercise263Letbethesystem˙x=Ax+Buy=Cx+Duandthestatefeedbackcontrolu=v−Kx.TheRosenbrockmatrixofthefeedbacksystemis(cid:27)sIn−A+BK−BC−DKD(cid:28)∼(cid:27)sIn−A−BCD(cid:28);theequivalenceisobtainedbysubtracting(cid:27)−BD(cid:28)Kfrom(cid:27)sIn−A+BKC−DK(cid:28)usingcolumnselementaryoperations.SolutionofExercise264(i)WecaneasilyshowthatΣiscontrollableandobservableusingtheKalmancriterion.(ii)G(s)=C(sI2−A)−1B=−5s2+s−2.SinceΣhasnohiddenmodesitsinvariantzeroscoincidewithitstransmissionzeros–therearenone–anditspolescoincidewithitstransmissionpoleswhichare{1−2}(Theorem179(i)section7.2.6).(iii)TheanswerisyessinceConditions(i)and(ii)inProposition250hold.538LinearSystems(iv)System(8.33)isthuscontrollableandwecanplacethepolesoftheclosed-loopsystemusingthecontrol(8.34).WritingK=(cid:25)k1k2k3(cid:26)andidentifyingdet(sI3−F+GK)termbytermwith(s+2)(s+1)2=s3+4s2+5s+2weobtaindet(sI3−F+GK)=s3+(k1−2k2+1)s2+(−3k1+k2−2)s−5k3whichyields⎧⎨⎩k1−2k2=33k1−k2=−75k3=−2andﬁnallywegetk1=−17/5=−3.4k2=−16/5=−3.2k3=−2/5=−0.4.ThepolesofLi(s)coincidewiththeeigenvaluesofFwhichare{−201}.Thecontrolisfoundtobeu=−Kpx−Ki(cid:2)e(t)dt+const.withKp=(cid:25)k1k2(cid:26)andKi=k3.(v)Sincetheclosed-loopsystemisstabletheNyquistplot(takingintoaccountthe“half-circleatinﬁnity”)enclosesinthedirectsensepoint−1anumberoftimeswhichisequaltothenumberofpolesofLi(s)belongingto¯C+i.e.2times.SinceRule111(section6.3.5)isbeingappliedthemodulusmarginisequalto1.WecandeducefromtheseconsiderationsthattheNyquistplotofLi(s)hastheshapeshowninFigure14.5.Figure14.5.NyquistplotofLi(s)–Exercise264SolutionsofExercises539Figure14.6.NyquistplotofLi(s)–Exercise265SolutionofExercise265(i)WehaveshowninExercise209thatΣiscontrollableandobservable(withC=(cid:25)0001(cid:26)).(ii)ThepolesofΣare(cid:31)00±√11 itsinvariantzerosare(cid:31)±√10 .(iii)SameanswerasinExercise264forthesamereasons.(iv)K=(cid:25)−65.2−270.4−85.2−104.4−72(cid:26).(v)TheNyquistplotofLi(s)hastheshapeshowninFigure14.6sinceMmi=1.SolutionofExercise266(i)Thestateequationsdirectlyexpressthedischargeequations(seesection1.4)andC=(cid:25)0α3(cid:26).(ii)UsingtheKalmancriterionweimmediatelyshowthatthesystemiscontrollableandobservable.(iii)Thesystempolesare{−α2−α3}theyarelocatedinthelefthalf-planeandthusthesystemisstable.(iv)FormingtheRosenbrockmatrixweshowthatthesystemhasnoﬁnitezeros.ThestaticgainisG(0)withG(s)=C(sI2−A)−1BandsoG(0)=−CA−1B=0.01.(v)Thecontrollawwearelookingforisoftheform(8.35)withKp=(cid:25)1230.5(cid:26)andKi=0.625.ThechoiceofthispoleplacementisjustiﬁedbyRule111(section6.3.5).540LinearSystemsSolutionofExercise267(i)Takingasastatevectorx=(cid:25)θvh(cid:26)Tweobtainasystem{ABC}withA=⎡⎣−1τ100σ−1τ20010⎤⎦B=⎡⎣1001τ200⎤⎦C=(cid:25)001(cid:26).(ii)Systempoles:(cid:7)−1τ1−1τ20(cid:8).Thesystemisunstable(andmoreprecisely“marginallystable”:seeDeﬁnition183section7.3).(iii)Sinceu=0thematrixBisreplacedbyitssecondcolumnb2.Thesystemisobservablebutnotcontrollable(and{i.d.z.}=(cid:7)−1τ1(cid:8)).Sincetheﬁrstequationisτ1˙θ=−θθconvergesinevitablyto0.(iv)(a)ThistimethematrixBisreplacedbyitsﬁrstcolumnb1.Thishasnoinﬂuenceontheobservabilitybutthesystemisnowcontrollable(thedeterminantofthecontrollabilitymatrixisσ2).(b)Weobtainacontrollawoftheform(8.35)withKp=(cid:25)19/101(cid:26)andKi=1.SolutionofExercise269TheonlymodiﬁcationneededisthatmatrixGofSystem(8.33)isnowwrittenas:G=(cid:27)BD(cid:28).14.9.ExercisesofChapter9SolutionofExercise297(i)Let˜K=(cid:27)˜k1˜k2(cid:28)betheobservergain.Wehavedet(cid:9)sI2−A+˜KC(cid:10)=))))s+2˜k1˜k1−22˜k2−1s+˜k2+1))))=s2+s(cid:9)2˜k1+˜k2+1(cid:10)+3˜k1+4˜k2−2.SolutionsofExercises541Identifyingthispolynomialtermbytermwith(s+5)2=s2+10s+25weobtaintheequations(cid:20)2˜k1+˜k2=93˜k1+4˜k2=27hence˜k1=9/5and˜k2=27/5.(ii)TheobserverpoleslieontherealaxistheyarenegativeandfastcomparedtothesystempolesthustheycomplywiththeruleexpressedbyTheorem112(sinceallsystemzerosareatinﬁnity).SolutionofExercise298(i)Theobservergainis˜K=(cid:27)3217(cid:28).(ii)SamereasonasisinExercise297.SolutionofExercise299(i)Σisinacanonicalcontrollableformthusitscharacteristicpolynomialcanbe“read”directlyonthematrixA.Itspolesare{1−2}andΣisunstable.(ii)Transferfunction:G(s)=s+β(s−1)(s+2).Ifβ/∈{−12}thetransmissionpolesare{1−2}andthesetofitstransmissionzerosis{−β}.Ifβ=−1(resp.β=2)Σhasauniquetransmissionpole−2(resp.1)andhasnoﬁnitetransmissionzero.(iii)Σisclearlycontrollable(see(i))andisobservableforβ/∈{1−2}.(iv)UsingtheRosenbrockmatrixweﬁndthatΣhasoneinvariantzero−β(regardlessofthevalueofβ).(v)Writingthatdet(sI2−A+BK)=s2+3s+2weﬁndthatK=(cid:25)24(cid:26).Theequilibriumstateisx∗=−(A−BK)−1Bk0andthecorrespondingoutputisy∗=Cx∗.Wethusneedtohave−C(A−BK)−1Bk0=1hencek0=2/β.(vi)(a)NeitherΣnorthecontrollerisanintegratorsystemthusthereisanon-zerostaticerrorinthepresenceofanon-zerooutputdisturbance.(b)Theanswerisyessincethenumberofoutputsisequaltothenumberofinputsands=0isnotaninvariantzeroofΣ.(c)WeobtainKp=(cid:25)35(cid:26)andKi=2.(d)Rule111isbeingcompliedwith.(vii)(a):See(9.14).(b)Identifyingdet(cid:9)sI2−A+˜KC(cid:10)termbytermwiths2+11s+10weobtain˜K=(cid:25)91(cid:26)T.SolutionofExercise300(i)TheKalmancriterionshowsthatΣisnotcontrollablebutobservable.ThepoleswhicharetheeigenvaluesofAare{−11}.(ii)ThestabilizabilitycanbestudiedusingthePopov-Belevitch-Hautustest(Proposition186).Theuniquei.d.z.is542LinearSystems−1thusΣisstabilizable.(iii)Letu=−(cid:25)k1k2(cid:26)x.Weobtaindet(sI2−A+BK)=s2+s(k1−k2)+k1−k2−1=(s+1)(s+k1−k2−1).Wethushavedet(sI2−A+BK)=(s+1)(s−λ)ifandonlyifk2+1−k1=λ.Asaresultasolutionexistsbutisnotunique.Forλ=−1aparameterizationofthesolutionsisk2=kk1=k+1−λwherekisanarbitraryparameter.(iv)Thefull-orderobserverisgivenby(9.3)whereD=0andwhere˜K=(cid:25)˜k1˜k2(cid:26)Tissuchthatdet(cid:9)sI2−A+˜KC(cid:10)=(s+1)(s+10).Afteratermbytermcomparisonweobtain˜k1=11and˜k2=−11.(v)Inordertouse(9.34)putx(cid:2)=(cid:25)x2x1(cid:26)T.Weobtain˙x(cid:2)=A(cid:2)x(cid:2)+B(cid:2)uy=C(cid:2)x(cid:2)withA(cid:2)=(cid:27)−1/2−3/2−1/21/2(cid:28)B(cid:2)=(cid:27)−11(cid:28)C(cid:2)=(cid:25)01(cid:26).Theonlystatetobereconstructedisx(cid:2)1=x2.WehaveA(cid:2)r=−1/2andC(cid:2)r=−1/2.Take=−10asanobserverpole.Thegain˜KrofthisobserverhastobechosensuchthatA(cid:2)r−˜KrC(cid:2)r=thus˜Kr=(A(cid:2)r−)/C(cid:2)r=−19.Accordingto(9.38)theminimalobserverisgivenby(cid:20)˙z=−10z+198y+18uˆx2=z+19y.(vi)Intheparameterizationof(iv)withk=0weobtainu=−k1x1=−2yandsoanobserverisnotrequired.SolutionofExercise301(i)Σisdeﬁnedbytheleftform(cid:15)∂2+∂−1(cid:16)y=u.Incase(a)S(∂)=∂2+3∂R(∂)=4∂2+7∂+1T(∂)=∂+1.Incase(b)S(∂)=∂3+4∂2+7∂R(∂)=7∂2+12∂+1T(∂)=∂2+2∂+1.(ii)WehaveJ=0andF=(cid:27)−1110(cid:28)G=(cid:27)01(cid:28)H=(cid:25)10(cid:26).(a)u=−Kpx−Ki(cid:1)e(t)dt+const.withKp=(cid:25)22(cid:26)Ki=1.(b)Theminimalobserveris˙z=−z+y+uˆx1=yˆx2=z+y.Thefull-orderobserveris∂ˆx=Fˆx+Gu+L(y−Hˆx)L=(cid:27)11(cid:28).(c)Thetwostatefeedback/observersyntheseswithintegralactionareoftheform(9.30)andareidenticaltotheRSTcontrollerinQuestion(i)(a)inthecaseofaSolutionsofExercises543minimalobserverandtothatofQuestion(i)(b)inthecaseofafull-orderobserver.(iii)Thedrawbackofthesecontrollersisthattheydonotplacetheclosed-looppolesaccordingtotheconditionsinTheorem112whichcanleadtoalackofrobustness.Tocorrectthisﬂawwecanchooseobserversthathavefasterpoles(at−10forexampleinthecaseoftheminimalobserverandat{−10−10}inthecaseofthefull-orderobserver)sinceΣhasallitszerosatinﬁnity.SolutionofExercise302Letbethesystem⎧⎨⎩∂x=Ax+Bu+d1y=Cx+Du+d2e=Ey−rwithsizesasspeciﬁedinsection9.2.1.Wegetz=E(Cx+Du+d2)−r=Hx+Ju+d3−rwhereH=ECandJ=ED.Ifthestatexisavailableforthecontrolthesolutiondevelopedinsection8.3.2remainsvalid(changingthenotation)undertheconditionthatthefollowinghypothesesaresatisﬁed:–(AB)iscontrollable;–thetransfermatrixE(sIn−A)−1B+JissemiregularoverR(s);–m≥pandthesystem{ABHJ}hasnoinvariantzerothatisarootofϕ(s).Inthepresentcasethecontrol(8.38)hastobereplacedbyυ=−(cid:25)KpKε(cid:26)(cid:27)ˆηηε(cid:28)whereˆηisobtainedbyreconstructingthevariableη=ϕ(∂)xbymeansoftheobserver∂ˆη=Aˆη+Bυ+˜K(ϕ(∂)y−Cˆη−Dυ)whereυ=ϕ(∂)uandwhere˜KisagainmatrixsuchthatA−˜KChasallitseigenvaluesappropriatelychoseninthelefthalf-plane.Onesuchgaincanbechosenundertheconditionthatthefollowinghypothesisissatisﬁed:–(CA)isobservable.Nowletˆxbeavariablesuchthatϕ(∂)ˆx=ˆη.Thecontrollawsolutionoftheproblemisﬁnallygivenbytherelation(8.39)ofsection8.3.2andbyu=−Kpˆx−Kεxεwhichreplaces(8.40).14.10.ExercisesofChapter10SolutionofExercise355Weobtainxd=0.Thestrictinequalityisthusessential.544LinearSystemsSolutionofExercise356(i)ThepolesofΣare{−2−2}.(ii)WehaveA=−2I2+J02(withthenotationofsection13.3.4).WethushaveAd=exp(AT)=exp(−2T)exp(J02T)andexp(J02t)=I2+J02T=(cid:27)10t1(cid:28)because(J02)2=0.AsaresultAd=(cid:27)e−0.400.2e−0.4e−0.4(cid:28)=(cid:27)0.670300.13410.6703(cid:28).OntheotherhandBd=(cid:1)T0eAtdtB=(cid:1)T0eAtBdtwitheAtB=(cid:27)e−2tte−2t(cid:28).Wehave(cid:1)T0e−2tdt=12(cid:15)1−e−2T(cid:16)=0.1648.Furthermore(cid:1)te−2tdtisoftheforme−2t(at+b).Differentiatingthislastexpressionandidentifyingtheresultwithte−2twegeta=−1/2andb=−1/4.Therefore(cid:2)T0te−2tdt=14(cid:25)1−(2T+1)e−2T(cid:26)=0.0154andweobtainBd=(cid:27)0.16480.0154(cid:28).OfcourseCd=C.(iii)WehaveGd(z)=C(zI2−Ad)−1Bd=0.0154z+0.0118z2−1.3406z+0.4493.ThepolesofΣdarethediagonalelementsofAd(sincethismatrixislowertriangular)i.e.{0.67030.6703}=(cid:31)epTepT withp=−2.SolutionofExercise357(i)A=(cid:27)0100(cid:28)B=(cid:27)01(cid:28)C=(cid:25)10(cid:26).(ii)Ad=(cid:27)1T01(cid:28)Bd=(cid:27)T2/2T(cid:28)SolutionsofExercises545thus{AdBdC}isnotinobservablecanonicalform.(iii)ThetransferfunctionofΣdisGd(z)=T2(z+1)2(z−1)2(seeTable(10.14))thusΣdisdescribedbytheleftformD(q)yd=N(q)udwithD(q)=(q−1)2N(q)=T22(q+1).(iv)Takingintoaccountadelayτ=nTtheleftformofΣdbecomes(q−1)2qnyd=T22(q+1)ud.(14.1)(v)Thecorrespondingobservablecanonicalformisforn=3Ad=⎡⎢⎢⎢⎢⎣21000−10100000100000100000⎤⎥⎥⎥⎥⎦Bd=⎡⎢⎢⎢⎢⎣000T2/2T2/2⎤⎥⎥⎥⎥⎦Cd=(cid:25)10000(cid:26).(vi)Thereaderisaskedtocarefullydrawthesuggestedblockdiagramwhichwillmakeapparenta“delayline”.(vii)Thegeneralizationtoanynisthenevident.(viii)Equation(14.1)isaclassicleftformofadiscrete-timesystem.AsaresultanRSTcontrollercanbeeasilydesignedusingtheinvokedmethodsanditisthenaveryeffectivepredictivecontrol.SolutionofExercise358ThefollowingformulasarecorrectifλisnotaneigenvalueofˇA:Ad=(cid:15)λI−ˇA(cid:16)−1(cid:15)λI+ˇA(cid:16)Bd=λT(cid:15)λI−ˇA(cid:16)−1ˇBC=2TˇC(cid:15)λI−ˇA(cid:16)−1D=ˇC(cid:15)λI−ˇA(cid:16)−1ˇB+ˇD.IngeneralD(cid:5)=0.SolutionofExercise359(i)isclearaccordingto(10.37).(ii)Wehave˜Ad=(1/T)(Ad−In)and˜Bd=(1/T)Bd.(iii)Accordingto(10.37)˜Ad→Aand˜Bd→B.ForsmallsamplingperiodsthequantizationrisksdestroyingalltheinformationcontainedinmatricesAdandBdaccordingto(i)whichisnotthecaseifweusethedeltaformalismaccordingtotheabove.(iv)Letbeforexampleacontrollerrepresentedbyastate-spacesystemusingtheoperatordeltai.e.δηd=Fηd+Gzdud=Hηd+Jzdwhereηdis546LinearSystemsthestatezdisthecontrollerinputconsistingofdiscretizedmeasurementsandofthereferencesignalandwhereudisthediscrete-timecontrolcalculated.(iv)Usingthedeltaformalismthecalculationsareorganizedasfollows:(1)AtinstantkT(a)weacquiredsignalzd(b)wecalculateud=µd+Jzdwhereµd=Hηdaquantitywhichcanbecalculatedbeforehand(c)weapplythecontrol.(2)BetweeninstantskTand(k+1)T(a’)wecalculateνd(cid:1)Fηd+Gzd(b’)wecalculateηd+1=Tνd+ηd(c’)wecalculateµd+1=Hηd+1.Atinstant(k+1)Twearereadytore-iteratethecalculationswithkchangingtok+1.Thecalculationsarethussimpleandfastwhichisinnowaythecasewiththeoperator∆;thisoperatorisveryusefulforthesynthesisofadiscrete-timecontrollerbutnotatallforitsimplementation.SolutionofExercise360(i)Gd(z)=1−az−a=Bd(z)Ad(z)wherea=exp(−T)andˇG(w)=Gd(cid:21)1+(T/2)w1−(T/2)w(cid:22)=−(cid:9)1−a1+a(cid:10)w+2T1−a1+aw+T21−a1+a.(ii)ForT→0+1+a→21−a∼TthusˇG(w)∼11+w=G(w)whereG(s)isthetransferfunctionofΣ.ThisisconsistentwithRemark343(ii).(iii)Itnowremainstoapplythemethoddiscussedinsection6.3.4andtotakeintoaccounttheresultsofExercise121(section6.5).ThuswetakeAcl(∆)=(∆+10/3)3andthepolynomialsˇS(∆)ˇR(∆)oftheformˇS(∆)=∆2+ˇσ1∆ˇR(∆)=ˇr1∆+ˇr2.WithT=0.2wegetˇG(w)=−0.05w+0.9992w2+0.9992w.SolvingtheSylvestersystemweobtainˇσ1=11.56ˇr1=25.61ˇr2=37.16.OntheotherhandˇT(∆)=κ(∆+10/3)2whereκissuchthatˇT(0)=ˇR(0)=ˇr2whichyieldsˇT(∆)=3.34∆2+22.30∆+37.16.(iv)Thecorrespondingdiscrete-timeRSTcontrollerisobtainedbywriting(cid:25)Rd(z)Td(z)(cid:26)Sd(z)=(cid:25)ˇR(w)ˇT(w)(cid:26)ˇS(z)|w=2Tz−1z+1SolutionsofExercises547whichyieldsRd(q)=1.3603q2+0.3448q−1.0155Sd(q)=q2−0.9278q−0.0722Td(q)=2.7583q2−2.7583q+0.6896.WehaveSd(1)=0(integratorsystem)Rd(1)=Td(1)(nostaticerror)andRd(−1)(zerogainatNyquistfrequency).(v)WeagainusethemethodofExample352(ii).Thetransferfunctionofthepseudo-continuoussystemwhichwebeginthecomputationwithisgivenbyˇG(w)=1−(T/2)w1+(T/2)wGd(cid:21)1+(T/2)w1−(T/2)w(cid:22).Thispseudo-continuoussystemisdeﬁnedbytheleftformwithpolynomialsˇA(∆)=∆2+10.9967∆+9.9668ˇB(w)=0.0997∆2−1.9934∆+9.9668.Thecharacteristicpolynomialofthepseudo-continuousclosed-loopis(seesection6.3.5)Acl(∆)=Ac(∆)(∆+10/3)(∆+2/T)Ac(∆)=(∆+10/3)(∆+2/T).SolvingtheSylvestersystemwegetˇS(∆)=∆3+16.24∆2+168.70∆ˇR(∆)=27.72∆2+314.38∆+371.60ˇT(∆)=3.34∆3+55.74∆2+260.12∆+371.60hence(aftersimpliﬁcationoftherootq=0commontoallthepolynomials)weﬁnallygetSd(q)=q3−0.6813q2−0.0722q−0.2466Rd(q)=1.4585q2+0.3448q−1.1137Td(q)=2.7583q2−2.7583q+0.6896.ForinformationonlythebehaviorofthecontrolledsystemisshowninFigure14.7whenthefollowingeventsoccur:(a)unitstepcommandatt=0;(b)stepdisturbance548LinearSystemsFigure14.7.Time-domainresponses–Exercise360addingattheoutputfromt=5.Wecanseetheresponseswiththeﬁrstcontroller(-)andwiththesecondone(--).Theabsolutevalueofthesensitivityfunction))))))11+Bd(z)Ad(z)Rd(z)Sd(z)))))))z=eiωTexpressedindBisalsoshownasafunctionofωinFigure14.8.Thisshowsthatwiththetwocontrollersthemodulusmarginiscorrect(withofcourseanadvantagefortheﬁrstcontrollerbecausetheintroductionofapuredelaycanonlybedisadvantageous).SolutionofExercise361Itsufﬁcestoreplacetheangularfrequencyω0intheexpressionofD1(∆)by˜ω0=(2/Te)tan(Teω0/2).SolutionofExercise362(i)LetΣdbethesystemobtainedbydiscretizationofSystemΣwithsamplingperiodT>0whereΣisdeﬁnedby˙x=Ax+Bu.ThenzisapoleofΣdifandonlyifz=esTwheresisapoleofΣaccordingto(10.15)(section10.3.4)andProposition439(section12.4.2).Asaresultzcannotbelongto(−∞0).(ii)IfΣisadelaysystemΣdhaspolesat0(seeExercise357).SolutionsofExercises549Figure14.8.Sensitivityfunction–Exercise360SolutionofExercise363(i)ThesystemΣdassociatedwiththeR-moduleMhasani.d.z.whichis−1itisthereforenotstabilizable.(ii)TheequationoftheB-moduleQ−1M=˘Misq˘yd=˘udandbyrestrictionoftheringofscalarsfromBtoCweobtain(1+∆/λ)˘yd=(1−∆/λ)˘udwhichdeﬁnesthepseudo-continuoussystem˘Σ(seeExample347).ThissystemiscontrollablewhichdoesnotcontradictTheorem350norProposition346since−1isapoleofΣd.(iii)ThesystemΣdwhichhasapole−1couldnothavebeenobtainedbydiscretizationofacontinuous-timesystemaccordingtoExercise362.14.11.ExercisesofChapter11SolutionofExercise421(i)Wehave(cid:27)ryy(0)−ryu(0)−ryu(0)ruu(0)(cid:28)(cid:27)ˆaˆb(cid:28)=(cid:27)−ryy(1)ryu(−1)(cid:28).(ii)Wecalculater(cid:2)yy(1)r(cid:2)yu(0)andr(cid:2)yu(−1)usingProposition367.Thesequantitiesallhaveanabsolutevalueoflessthan1andtheresultsareconsistent.(iii)Weobtainˆa=−0.6091andˆb=0.9636.(iv)Thepoleoftheidentiﬁedsystemisz=0.6091.Itisinsidetheopenunitcirclethustheidentiﬁedsystemisstable.Itsstaticgainisˆbz−11+ˆaz−1forz=1whichis2.4651.550LinearSystemsSolutionofExercise424(i)LetθCbethevectorwhosecomponentsarethecoefﬁcientsofthepolynomialCoftheARMAXmodel.Withthismodelthetransferfunctionsoftheexpression(11.32)(section11.2.3)areoftheformG(cid:15)q−1θ(cid:16)=B(cid:15)q−1θs(cid:16)A(q−1θs)H(cid:15)q−1θ(cid:16)=C(cid:15)q−1θC(cid:16)A(q−1θs)andsothemodelcannotbeputintheform(11.35)ofsection11.2.5:thehypothesesofTheorem408(i)arenotinforce.(ii)Suppose˘G(cid:15)q−1(cid:16)=˘B(cid:15)q−1(cid:16)˘A(q−1)=G(cid:9)q−1˘θs(cid:10)isthedeterministicpartofthe“true”systemandn(t)=˘C(cid:15)q−1(cid:16)˘D(q−1)w(t)isthe“actual”coloredmeasurementnoisesothaty(t)=˘B(cid:15)q−1(cid:16)˘A(q−1)u(t)+˘C(cid:15)q−1(cid:16)˘D(q−1)w(t).InthequantityQ2oftheproofofTheorem408theratio˜H=˘H/Hisoftheform˜H(cid:15)q−1(cid:16)=˘C(cid:15)q−1(cid:16)˘D(q−1)A(cid:15)q−1θs(cid:16)C(q−1θC).TominimizeQ2wehavetomakethisratioequalto1andthustoobtaintheequalityC(cid:15)q−1θC(cid:16)=˘C(cid:15)q−1(cid:16)˘D(q−1)A(cid:15)q−1θs(cid:16).Inordertohaveatthesametimeθs=˘θsitisnecessarythatC(cid:15)q−1θC(cid:16)=˘C(cid:15)q−1(cid:16)˘D(q−1)˘A(cid:15)q−1(cid:16)whichispossiblewithapowerseriesC(cid:15)q−1θC(cid:16)=1++∞(cid:12)τ=1c(τ)q−τbutnotwithapolynomial.Bytruncatingtheabovepowerseriesweobtainagoodapproximationifasufﬁcientnumberoftermsarekept.WethuscanobtainacorrectestimationofthedeterministicpartbyusingapolynomialC(cid:15)q−1θC(cid:16)whosedegreeislargeenough.SolutionsofExercises551SolutionofExercise425SameprincipleasusedinExercise424.SolutionofExercise426Wehavey(t)=Gi(cid:15)q−1θ(cid:16)˜u(t)+Hc(cid:15)q−1(cid:16)Hi(cid:15)q−1θ(cid:16)w(t).with˜u(t)=Gc(cid:15)q−1(cid:16)u(t)whichistheinputtobeconsidered.WedealwiththefactorHc(cid:15)q−1(cid:16)asinsection11.2.6.SolutionofExercise427(i)YesaccordingtoTheorem417.(ii)UsingH(cid:15)q−1θβ(cid:16)=1in(11.56)wealsoobtainabicausalbistablesecondordertransferfunctionH(cid:15)q−1θsθβ(cid:16)thusnc=2istheminimalnumberofcoefﬁcientswhichwewilluseforpolynomialC(cid:15)q−1(cid:16)oftheARMAXmodel(i.e.C(cid:15)q−1(cid:16)=1+c1q−1+c2q−2).Bibliography[1]B.D.OAndersonJ.B.MooreOptimalFilteringPrentice-HallEnglewoodCliffsNJ1979.[2]B.D.OAndersonJ.B.MooreOptimalControl–LinearQuadraticMethodsPrentice-HallEnglewoodCliffsNJ1989.[3]K.J.AströmB.WittenmarkAdaptiveControlAddison-Wesley1989.[4]K.J.AströmB.WittenmarkComputer-ControlledSystemsTheoryandDesign2nded.Prentice-HallEnglewoodCliffsNJ1990.[5]J.BassFonctionsdecorrélationfonctionspseudo-aléatoiresetapplicationsMasson1984.[6]M.BenidirB.Picinbono“ExtendedtableforeliminatingthesingularitiesinRouth’sarray”IEEETrans.Automat.Control35(2)218–2221990.[7]S.P.BhattacharyyaH.ChapellatL.H.KeelRobustControl:TheParametricApproachPrentice-HallEnglewoodCliffsNJ1995.[8]A.Blanc-LapierreB.PicinbonoFonctionsaléatoiresMasson1981.[9]H.W.BodeNetworkAnalysisandFeedbackAmpliﬁerDesignVanNostrand1945.[10]N.BourbakiAlgebraIandIISpringer1989–1990.[11]N.BourbakiCommutativeAlgebraChapters1–7Springer1989.[12]N.BourbakiFunctionsofaRealVariable–ElementaryTheorySpringer2004.[13]H.Bourlès“Surlarobustessedesrégulateurslinéairesquadratiquesmultivariablesoptimauxpourunefonctionnelledecoûtquadratique”C.RAcad.Sci.SerieI252971–9741981.[14]H.Bourlès“Semi-cancellablefractionsinsystemtheory”IEEETrans.Automat.Control39(10)2148–21531994.[15]H.Bourlès“Structuralpropertiesofdiscreteandcontinuouslineartime-varyingsystems:auniﬁedapproach”AdvancedTopicsinControlSystemsTheory–LectureNotesfromFAP2004(F.Lamnabhi-LagarrigueA.LoriaandE.Panteleyeds)chap.6pp.225–280LectureNotesinControlandInformationSciencesvol.311Springer553Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.554LinearSystems2005.“Structuralpropertiesoflinearsystems–PartII:Structureatinﬁnity”AdvancedTopicsinControlSystemsTheory–LectureNotesfromFAP2005(A.LoriaF.Lamnabhi-LagarrigueandE.Panteleyeds)chap.7pp.259–284LectureNotesinControlandInformationSciencesvol.328Springer2006.[16]H.Bourlès“Impulsivesystemsandbehaviorsinthetheoryoflineardynamicalsystems”ForumMathematicum17(5)781–8082005.[17]H.BourlèsF.Aïoun“ApprocheH∞etµ-synthèse”LaRobustesse–analyseetsynthèsedecommandesrobustesA.Oustaloup(co-ordinator)chap.3pp.163–235HermèsParis1994.[18]H.BourlèsF.Colledani“W-StabilityandLocalInput-OutputStabilityResults”IEEETrans.onAutom.Control40(6)pp.1102–11081995.H.Bourlès“Addendumto‘W-StabilityandLocalInput-OutputStabilityResults’”IEEETrans.onAutomat.Control45(6)1220–12212000.[19]H.BourlèsM.Fliess“Finitepolesandzerosoflinearsystems:anintrinsicapproach”Int.J.Control68(4)897–9221997.[20]H.BourlèsE.Irving“LaméthodeLQG/LTR:uneapprochepolynômialetempscontinu/tempsdiscret”RAIROAPII25545–5921991.[21]H.BourlèsB.Marinescu“Polesandzerosatinﬁnityoflineartime-varyingsystems”IEEETrans.Automat.Control441981–19851999.[22]H.BourlèsB.MarinescuLinearTime-VaryingSystems:AnAlgebraic-AnalyticApproachSpringer-Verlag(forthcoming).[23]H.BourlèsU.Oberst“Dualityfordifferential-differencesystemsoverLiegroups”SIAMJ.ControlOptim.482051–20842009.[24]J.-P.CaronJ.-P.HautierModélisationetcommandedelamachineasynchroneEditionsTechnip1995.[25]H.CartanElementaryTheoryofAnalyticFunctionsofOneorSeveralVariablesDoverPublicationsInc.1995.[26]C.-T.ChenLinearSystemTheoryandDesignHoltRinehartandWinston1984.[27]J.Chen“Multivariablegain-phaseandsensitivityintegralrelationsanddesigntradeoffs”IEEETrans.Automat.Control43(3)373–3851998.[28]P.ChevrelH.Bourlès“ReducedorderH2andH∞observers”32ndIEEECDC13–17DecemberSanAntonio(Texas)pp.2915–29161993.[29]P.G.CiarletIntroductionàl’analysenumériqueetàl’optimisationMasson1990.(Englishtranslation:IntroductiontoNumericalLinearAlgebraandOptimisationCambridgeUniversityPress1989.)[30]E.A.CoddingtonandN.LevinsonTheoryofOrdinaryDifferentialEquationsMcGraw-Hill1955.[31]P.CohnFreeRingsandtheirRelationsAcademicPress1985.[32]P.CohnFurtherAlgebraandApplicationsSpringer2003.Bibliography555[33]R.F.CurtainH.J.ZwartAnIntroductiontoInﬁniteDimensionalLinearSystemsTheorySpringer1995.[34]C.A.DesoerM.VidyasagarFeedbackSystems:Input-OutputPropertiesAcademicPress1975.[35]J.DieudonnéElémentsd’AnalyseVol.ItoVIGauthier-Villars1969–1975.(Englishtranslation:TreatiseonAnalysisAcademicPress1969–1978.)[36]J.DieudonnéInﬁnitesimalCalculusKershawPublishing1973.[37]J.L.DoobStochasticProcessesJohnWiley1953.[38]J.C.DoyleG.Stein“Robustnesswithobservers”IEEETrans.Automat.Control24607–6111979.[39]J.C.DoyleB.A.FrancisA.R.TannenbaumFeedbackControlTheoryMacMillan1992.[40]J.A.FarrellM.M.PolycarpouAdaptiveApproximationBasedControl-UnifyingNeuralFuzzyandTraditionalAdaptiveApproximationApproachesWiley2006.[41]A.FeintuchR.SaeksSystemTheory–AHilbertSpaceApproachAcademicPress1982.[42]M.Fliess“Somestructuralpropertiesofgeneralisedlinearsystems”SystemsControlLett.15391–3961990.[43]M.Fliess“RemarkonWillems’trajectorycharacterizationoflinearcontrollability”SystemsControlLett.1943–451992.[44]M.Fliess“SomeremarksontheBrunovskycanonicalform”Kybernetika29(5)417–4221993.[45]M.Fliess“UneinterprétationalgébriquedelatransformationdeLaplaceetdesmatricesdetransfert”LinearAlgebraAppl.203–204429–4421994.[46]M.FliessH.Bourlès“Discussingsomeexamplesoflinearsysteminterconnections”SystemsControlLett.271–71996.[47]M.FliessS.T.Glad“Analgebraicapproachtolinearandnonlinearcontrol”EssaysonControl:PerspectivesintheTheoryanditsApplications(H.L.TrentelmanandJ.C.Willemseds)Birkhäuserchap.8223–2671993.[48]M.FliessJ.LévinePh.MartinP.Rouchon“Flatnessanddefectofnon-linearsystems:introducingtheoryandexamples”Int.J.Control61(6)1327–13611995.[49]A.L.D.FrancoH.BourlèsE.R.dePieriH.Guillard“RobustnonlinearcontrolassociatingrobustfeedbacklinearizationandH∞control”IEEETrans.Automat.Control51(7)1200–12072006.[50]J.S.FreudenbergD.P.Looze“Righthalfplanepolesandzerosanddesigntradeoffsinfeedbacksystems”IEEETrans.Automat.Control30(6)555–5651985.[51]J.S.FreudenbergD.P.LoozeFrequencyDomainPropertiesofScalarandMultivariableFeedbackSystemsSpringer1988.[52]F.R.GantmacherTheTheoryofMatricesvol.1Chelsea1959.556LinearSystems[53]J.B.GarnettBoundedAnalyticFunctionsAcademicPress1981.[54]H.Glüsing-Lüerssen“Abehavioralapproachtodelaydifferentialequations”SIAMJ.ControlOpt.35480–4991997.[55]H.Glüsing-LüerssenLinearDelay-DifferentialSystemswithCommensurateDelays:AnAlgebraicApproachSpringer2001.[56]G.H.GolubC.F.VanLoanMatrixComputations2nded.TheJohnsHopkinsUniversityPress1989.[57]G.C.GoodwinK.S.SinAdaptiveFilteringPredictionandControlPrentice-Hall1984.[58]H.GoreckiS.FuksaP.GrabowskiA.KorytowskiAnalysisandSynthesisofTimeDelaySystemsWiley1989.[59]J.K.HaleS.M.VerduynLunelIntroductiontoFunctionalDifferentialEquationsSpringer1993.[60]M.E.Harris“Someresultsoncoherentrings”Proc.Amer.Math.Soc.17474–4791966.[61]P.IoannouG.Tao“Frequencydomainconditionsforstrictlypositiverealfunctions”IEEETrans.Automat.Control32(1)53–541987.[62]A.IsidoriNonlinearControlSystems(3rded.)Springer1995.[63]T.KaczorekLinearControlSystems–Volume1JohnWiley1992.[64]T.KailathLinearSystemsPrentice-Hall1980.[65]R.E.Kalman“Mathematicaldescriptionoflineardynamicalsystems”SIAMJ.ControlSer.A1(2)152–1921963.[66]I.KaminerA.M.PascoalP.P.KhargonekarE.Coleman“Avelocityalgorithmfortheimplementationofgain-scheduledcontrollers”Automatica31(8)1185–11911995.[67]I.Kaplansky“Elementarydivisorsandmodules”Trans.Amer.Soc.66464–4911949.[68]V.KrishnanNonlinearFilteringandSmoothing:AnIntroductiontoMartingalesStochasticIntegralsandEstimationWiley1984.[69]V.Kucera“Acontributiontomatrixquadraticequations”IEEETrans.Automat.Control17(3)344–3471972.[70]P.KundurPowerSystemStabilityandControlMcGraw-Hill1994.[71]H.Kwakernaak“Optimallow-sensitivitylinearfeedbacksystems”Automatica52791969.[72]H.KwakernaakR.SivanLinearOptimalControlSystemsWiley1972.[73]T.Y.LamLecturesonModulesandRingsSpringer1999.[74]I.D.LandauCommandedessystèmes–ConceptionidentiﬁcationetmiseenœuvreHermès2002.[75]I.D.LandauA.Karimi“ARecursiveAlgorithmforARMAXModelIdentiﬁcationinClosedLoop”IEEETrans.onAutomat.Control44(4)840–8431999.Bibliography557[76]L.D.LandauE.M.LifshitzMechanics(3rded.)PergamonPress1976.[77]P.deLarminatY.ThomasAutomatiquedessystèmeslinéaires-vol.1-3Flammarion1975–1977.[78]P.deLarminatAutomatique–commandedessystèmeslinéaires(2èmeed.)Hermès-science1996.[79]M.D.LarsenJ.LewisT.S.Shores“Elementarydivisorringsandﬁnitelypresentedmodules”Trans.Amer.Math.Soc.187(1)231–2481974.[80]D.J.LeithW.E.Leithead“Surveyofgain-schedulinganalysisanddesign”Int.J.Control73(11)1001–10252000.[81]L.LjungSystemIdentiﬁcation–TheoryfortheUserPrentice-Hall1987.[82]L.LjungU.Forssell“Analternativemotivationfortheindirectapproachtoclosed-loopidentiﬁcation”IEEETrans.Automat.Control44(11)2206–22091999.[83]M.LoèveProbabilityTheory(3rded.)VanNostrandCo.1963.[84]H.Logemann“OntheNyquistcriterionandrobuststabilizationofinﬁnite-dimensionalsystems”RobustControlofLinearSystemsandNonlinearControl(M.AKaashoekJ.H.vanSchuppenandA.C.M.Raneds)vol.IIBirkhäuser627–6341990.[85]H.Logemann“Stabilizationandregulationofinﬁnite-dimensionalsystemsusingcoprimefactorizations”LNCIS185Springer102–1391993.[86]J.J.Loiseau“Invariantfactorassignmentforaclassoftime-delaysystems”Kybernetika37(3)265–2752001.[87]D.G.Luenberger“Observingthestateofalinearsystem”IEEETrans.Mil.Electron.874–801964.[88]D.G.Luenberger“Anintroductiontoobservers”IEEETrans.Automat.Control16596–6021971.[89]D.C.MacFarlaneK.GloverRobustControllerDesignUsingCoprimeFactorPlantDescriptionsSpringer1990.[90]J.M.MaciejowskiMultivariableFeedbackDesignAddison-Wesley1989.[91]S.MacLaneG.BirkhoffAlgebra(2nded.)MacMillan1979.[92]B.MarinescuH.Bourlès“Robustpredictivecontrolwithseparationproperty:areduced-statedesignforcontrolsystemswithnon-equaltimedelays”Automatica36555–5622000.[93]C.L.MatsonP.S.Maybeck“OnanassumedconvergenceresultintheLQG/LTRtechnique”IEEETrans.Automat.Control36123–1251991.[94]U.Oberst“Multidimensionalconstantlinearsystems”ActaAppl.Math.201–1751990.[95]R.PalludelaBarrièreCoursd’automatiquethéoriqueDunod1966.(Englishtranslation:OptimalControlTheory:ACourseinAutomaticControlTheoryDoverPublications1980.)558LinearSystems[96]J.W.PoldermanJ.C.WillemsIntroductiontoMathematicalSystemTheorySpringer1998.[97]V.M.Popov“Invariantdescriptionoflineartime-invariantcontrollablesystemsSIAMJ.Control10252–2641972.[98]A.Quadrat“Thefractionalrepresentationapproachtosynthesisproblems:analgebraicanalysisviewpoint.PartI:(weakly)doublycoprimefactorizations.PartII:internalstabilization”SIAMJ.ControlOptim.42266–299300–3202003.[99]R.RabahB.Bergeon“Onstatespacerepresentationforlineardiscrete-timesystemsinHilbertspaces”KharkovUniversityVestnik514(50)53–622001.[100]H.H.RosenbrockState-SpaceandMultivariableTheoryNelson1970.[101]H.H.Rosenbrock“Correctionto‘thezerosofasystem”’Int.J.Control20(3)525–5271974.[102]J.J.RotmanAnIntroductiontoHomologicalAlgebraAcademicPress1979.[103]W.RudinRealandComplexAnalysis(3rded.)McGraw-Hill1987.[104]W.J.RuthJ.S.Shamma“Researchongainscheduling”Automatica361401–14252000.[105]C.E.Shannon“CommunicationsinthePresenceofNoise”Proc.IRE37pp.10–211949.[106]L.SchwartzThéoriedesdistributionsHermann1966.[107]S.SkogestadandI.PostlethwaiteMultivariableFeedbackControl–AnalysisandDesignWiley2001.[108]J.-J.SlotineW.LiAppliedNonlinearControlPrentice-Hall1991.[109]M.C.Smith“Onstabilizationandtheexistenceofcoprimefactorizations”IEEETrans.Automat.Control34(9)1005–10071989.[110]T.SöderströmP.StoicaSystemIdentiﬁcationPrentice-Hall1989.[111]E.D.SontagMathematicalControlTheorySpringer1990.[112]V.delToroBasicElectricMachinesPrentice-Hall1990.[113]M.Vidyasagar“Onundershootandnonminimumphasezeros”IEEETrans.Automat.Control314401986.[114]M.VidyasagarControlSystemSynthesis–AFactorizationApproachMITPress1987.[115]M.VidyasagarNonlinearSystemsAnalysis(2nded.)Prentice-Hall1993.[116]J.C.WillemsTheAnalysisofFeedbackSystemsMITPress1971.[117]J.C.Willems“Paradigmsandpuzzlesinthetheoryofdynamicalsystems”IEEETrans.Automat.Control36259–2941991.[118]W.M.Wonham“Randomdifferentialequationsincontroltheory”ProbabilisticMethodsinAppliedMathematicsvol.2(A.T.Bharucha-Reided.)131–220AcademicPress1970.Bibliography559[119]W.M.WonhamLinearMultivariableControl–AGeometricApproachSpringer1985.[120]L.A.ZadehC.A.DesoerLinearSystemTheoryMcGraw-Hill1963.[121]E.Zerz“Primenessofmultivariatepolynomialmatrices”SystemsControlLett.29139–1451996.[122]K.ZhouJ.C.DoyleK.GloverRobustandOptimalControlPrentice-Hall1996.IndexAactuator25adjointclassical-458advance386algebra450Banach-518convolution-379385393normed-517sigma-438Borel-438unitary-450almost-everywhere377-surely439annihilator496approximationEuler313Padé-89associated-elements447associates447atom452automorphism387479Bballclosed-371open-371unit-372basis-ofavectorspace473canonical-474cyclic-487dual-479orthonormal-505behaviorfree-33bracketduality-372Butterworthconﬁguration233Ccategory520abelian-495Cauchyproblem418characteristic-ofamodule494nonlinear-22chartHall-109Nichols-110classLebesgue-377closure370coefﬁcientdamping-71Fourier-390cofactor456cokernel494561Linear Systems        Henri Bourlès© 2010 ISTE Ltd.  Published 2010 by ISTE Ltd.562LinearSystemscomoment3component475connected-370concatenation-oftwobases475conditionEuler-431Shannon-285conjugatetranspose-ofamatrix505constanttime-65controllinear-quadratic-xxLQ-xx229LQG-xxpredictive-319545controller1-DOF-1292-DOF-1453-DOF-145PD-134PI-135PID-139RST-144convergence372abscissaof-395annulusof-404bandof-400radiusof-401strong-375weak*-372conversionADC-290DAC-290coprime-elements452criterionKalman-controllability180297observability184301Kharitonov-79Nyquist-99MIMO-103quadratic-230Routh-79Ddecade68decibel67decompositioncanonical--ofahomomophism476Kalman-controllability182general187observability185primary--ofamodule497defect-ofatransfermatrix48degree454-ofaSmithzero469-ofarationalfunction515MacMillan-48relative--ofarationalfunction515delay386densityconditional-445cross-spectral-325probability-442Gaussian-442spectral-325derivative428-inthesenseofdistributions382logarithmic-416partial-428second-429descentdirectionof-433determinant-ofamatrix455-ofanendomorphism481diagramstandard-122system-56diffeomorphism307Diraccomb386distance370distribution381compactlysupported-400Dirac-383Index563singular-381tempered-381distributivity(leftright)-60disturbance25-rejection113divisionEuclidean-454divisor451elementary-488-ofamatrix468-ofamodule499greatestcommon-451left-470zero-447domain447Bézout-453460commutative-447Euclidean-454strongly-454GCD-451principalideal-453Sylvester-457UFD452uniquefactorization-452dualalgebraic-372topological-372Eeigenspace482generalized-486eigenvalue481eigenvector481generalized-489elementfree-495lengthofan-452torsion-495endomorphism479adjoint-506diagonalizable-483nilpotent-486non-negative-508normal-507positive-508self-adjoint-508unitary-507energy-ofasignal378kinetic-6potential-12epimorphism476canonical-476equalityKalman-231equationalgebraicRiccati-226230Bézout-460matrix-472Lyapunov-227normal-333reduced-25errormodeling-multiplicative-111observation-251prediction-335reconstruction-251estimatorconsistent-337non-biased-336asymptotically336expansionLaplace-456Laurent--ofameromorphicfunction.406expectation440conditional-444exponential-ofamatrix407extension-oftheringofscalars520Ffactorinvariant-465498factorizationbicausalspectral-329coprimedoubly-471564LinearSystemsfactorization(Continued)coprime-left-471unique-intoprimes452ﬁeld449-ofconstants449-offractions449differential-449ﬁlteranti-aliasing-290digital-326Kalman-xx258ﬂat-output210-system210formcanonical--ofcontrollability202206Brunovski-222controllable-200209observability214observable-199214Hermite-463Jordan-489left31normal-465pseudo-continuous-309rationalcanonical-ofanendomorphism503right-32Schur-226Smith-464Smith-MacMillan-42213-atinﬁnity47formulaBass-Gura-220Bromwich-399Cauchy’sintegral-411interference-326Plancherel-Parseval-388393Poissonsummation-283391reciprocity-387Shannoninterpolation-286Taylor’s-withLagrange’sremainder430withYoung’sremainder430Torricelli’s-15frequencynatural-72undamped-71normalizedangular-284Nyquist-284unitygain-106function-ofamatrix406absolutelycontinuous-383analytic-405antilinear-373characteristic-377coercive-431continuous-370convex-430correlation-322normalized323cross-correlation-322normalized323differentiable-428distribution-441elliptic-432entire-406453essentiallybounded-378Euclidean-454generalized-381holomorphic-404449integrable-377Lebesgue-measurable-377Lipschitz-419locallybounded-379locallyintegrable-379measurable-438meromorphic-406449rapidlydecaying-380rational-515biproper-516improper-516irreducible-515proper-516strictlyproper-516sensitivity-97sinecardinal-285slowlyincreasing-380spectral-324squareintegrable-378test-380Index565transfer-37open-loop-97PRQSPR-128stable-63functionsequalalmosteverywhere-377functor520Laplace-213Ggaincritical-100static-64gcd451gcld470generatorcyclic-487generators-ofamodule494-ofanideal450gradient428Hhiddenmode192297hold288ﬁrst-order-288sample-and-289zero-order-288homomorphism475canonical-520induced477Iideal450ﬁnitelygenerated-450principal-450proper-454identiﬁabilitystructural-344identiﬁcationclosed-loop357direct357indirect357modiﬁeddirect363parametric-321imaginaryaxisindented-100inclusion476independent-randomvariables439-sigma-algebras439index-ofapointw.r.t.aclosedpath409controllability-205cyclic-504Kronecker-479observability-214structural-3543-ofaSmithzero469inequalitySchwarz-373Sylvester-457triangle-370Young--forfunctions379-forsequences376inertia-matrix5-tensor5initialcondition418injectioncanonical-476integral-alongapath409interpolationHermite’s-215invariantsimilarity-488isomorphism-oftopologicalvectorspace371-ofvectorspace476JKJordanblock487kernel-ofaconvolutionoperator-ondistributions385-onfunctions379-onsequences376kineticmoment4566LinearSystemsLlag386Lagrangian12lawFaraday’s-14Hooke’s-9Newton’s-7probability-439reducednormal-443temporal-330lcm451lead386-compensator131lemma-inversion458LFT123lineargroupgeneral-458special-458logarithm-ofamatrix408branchofthe-principal-408loopshaping115Mmargindelay-106MIMO-122gain-106reduction106modulus-107input-120output-120phase-lag106lead106matrices(leftright)equivalent-459conjugate-481left-similar-494similar-481matrix(leftright)regular-457-ofdeﬁnition-ofamodule494adjugate-458companion-502completable-472495conditionnumberofa-514controllability-180297covariance-441cyclic-502directterm-177Hamiltonian-226230Hermitian-508Hessian-429input-177invertible-458Jacobian-428left-invertible-513non-singular-457non-negativedeﬁnite-509observability-184orderofasquare-455orthogonal-507output-177polynomial-457positivedeﬁnite-509prime-470pseudo-inverse-513regular457right-invertible-514Rosenbrock-190scalar-231semiregular-457sizeofa-455stability-196305state-177Sylvester-461symmetric-508system-190Toeplitz-201transfer-37213open-loop-97unimodular-458unitary-507measureabsolutelycontinuous-442method-ofvariationoftheconstant423gradient-433Index567leastsquares-332generalized368recursive334Levenberg-Marquardt-438LQR-229LTR-260extended268Newton-Gauss-436Newton-Raphson-435quasi-Newton-437minimizationunidirectional-433minimumglobal-430strict-430local-430strict-430minor456principal-456modelARMAX-340ARX-334BJ-341OE-340PEM-341module491(in)decomposable-496cyclic-496ﬁnitelygenerated-493ﬁnitelypresented-494free-493primary-497torsion495torsion-free-495moment-oforder1322-oforder2322monomorphism476motionplanning209multiple451leastcommon-451multiplicity-ofanelementarydivisor468499algebraic482geometric-482Nneglecteddynamics111neighborhood370noisecolored-330pseudo-colored-329pseudo-white-327standarddeviation327variance327white-330norm372Euclidean-374standard-374Hermitian-374standard-374Hilbert-373multiplicative-375operator-374511pre-Hilbert-373Oobserverfull-order-251minimal-277reduced-order-275octave68operationelementary-459secondary-459operatoradvance-401convolution--ondistributions385-onfunctions379-onsequences376delta-320input-output-49shift-forward-401order-ofaSmithzero469-ofapole406-ofazero406-ofapowerseries454-ofasystem33transmission-45orthogonal505-supplement505568LinearSystemsPpath408closed-408phase68plotBlack-67Bode-67Nyquist-6799MIMO-103pointcritical-99equilibrium-23pole-atinﬁnity46-ofameromorphicfunction406-placement146degreeofa-35MacMillan-43non-controllable-190296-atinﬁnity54non-observable-191297orderofa-35stable-80system-33296transmission-39296polynomialcharacteristic-98425481Hurwitz-78minimal-484monic-448PRBS328predictionoptimal-334prewarping308prime452principle-ofactionandreaction9-ofsuperposition21argument-416Gram-Schmidt-(orthogonalization)505511internalmodel-163243maximummodulus-411separation-254267probability439-law439productBlaschke-117convolution--ofdistributions385-offunctions379-ofsequences376448scalar-372standard-505tensor-520projection475pseudo-polynomial465pseudometric371QRquantization281radiusspectral-406rank-ofafreemodule493-ofahomomorphism477-ofamatrix456-ofamodule498-ofasystem22reachability297realization197-ofarandomvariable439minimal-197regression-vector332linear-335regulatorproportional-100relationBayard-Bode-84Bode-117representationpseudo-continuous-309Rosenbrock-31state-space-33nonlinear-62residue-ofameromorphicfunction406-ofarationalfunction516resonance75-factor75-frequency75Index569response-ofasystem49forced-426free-33427frequency-55impulse-51step-51restriction-oftheringofscalars520ring447-offractions519coherent-495commutative-447division-449elementarydivisor-465Hermite-472integral-447local-454Noetherian-453roll-off148root-ofapolynomial448multiplicityofa-448ruleLeibniz’s-449Mason-61mesh-1nodal-2Ssampling-frequency281-period281seminorm371sensor27sequenceCauchy-371delayed-402minimizing-433positivelysupported-376slowlyincreasing-389seriesFourier-390Laurent-406449normallyconvergent-391power-405formal-448setBorel-438bounded-372closed-370convex-410430dense-370generating--ofanideal450Lebesgue-mesurable-377open-369simplyconnected-411shift386signal-ofpositivetype323centered-322digital-281discrete-time-282discretized-281meanofa-322pseudo-random-322persistentlyexciting328rational328random-321ergodic-330Gaussian-330sampled-282sampled-and-held-288T-discretizable-283signalsuncorrelated-323singularvalue512-balancing126spaceBanach-372compact-370complete-371conﬁguration-11connected-370Euclidean-374ﬁnite-dimensional-373473Fréchet-400Hardy-518Hermitian-374Hilbert-373Lebesgue-377locallycompact-374metric-370570LinearSystemsspace(Continued)Montel-381normedvector-371pre-Hilbert372probability-439probabilizable-438quotient-474topological-369topologicalvector-371vector-449spectrum-aliasing286-ofamatrix406-ofasignal388-ofanendomorphism481ray-388392squarerootHermitian-510stabilityclosed-loop-97open-loop-63standarddeviation441state33dimensionofa33partial-31pseudo-31stepoptimal-433unit-378403submoduletorsion-495subspacecyclic-486invariant-480sum-ofvectorspaces474diagonal--ofendomorphisms480-ofmatrices455direct-474external-474supplement474support-ofadistribution381-ofafunction378-ofasequence376Sylvesterresultant461synthesisstatefeedback/observer-253systemxv418-withnegativestart92augmented-122bicausal-294biproper-54bistable-306causal-294strictly-294control-28controllable-1461792970-297derivator-43detectable-196determined-22discretized-290feedback-standard-95holonomic-11integrator-43linear-21linearapproximationofa-24linearizable-24minimal-45minimumphase-83MISO-31nonlinear-21observable-1833010-301proper-52strictly-53pseudo-continuous-311quotient-controllable-182non-observable-185sampled-290SIMO-31SISO-30stabilizable-146196stable-63195304marginally-196304state-space-33Sylvester-462time-delay-85465time-invariant-21time-varying-21Index571underdetermined-22well-deﬁned-feedback-59well-formed-205well-posed-96TtestPBH-0-controllability2990-observability303controllability179299detectability196observability184303stabilizability196theorem-oftheincompletebasis475Bochner324Bromwich-Schwartz-398Cauchy-Lipschitz-418Cayley-Hamilton-490centrallimit-443exchange-389394396400402404ﬁnalvalue-399403Fisher-Riesz-378Goursat’s-405implicitmapping-11initialvalue-331399403Jordan-488orthogonalprojection-373Paley-Wiener-Schwartz-400residue-410Riesz-373robuststability-111122sampling-284secondexchange-389singularvaluedecomposition-511smallgain-103spectralfactorization-328Sylvester-461topology369induced-370strong-372weak*-372torsor3force-6kinematic-3kinetic-4momentofa-3trace-ofanendomorphism481transformbilinear-306delta-320Fourier--ofdistributions387-ofsequences393Laplace-36bilateral-400inverse-412unilateral-394Tustin308z-bilateral-403inverse-415unilateral-401transpose-ofahomomorphism479-ofamatrix455UVZuniondisjoint-500unit-element447-ofaring447-oftime25variable-ofasystem20control-25controlled-28independent-25input-25latent-28output-28random-439Gaussian442reduced-25variance441572LinearSystemsvectorcharacteristic-3column-474parameter-321unitary-372zero-atinﬁnity46-ofameromorphicfunction406blocking-40297-atinﬁnity48input-decoupling-190296-atinﬁnity54input-outputdecoupling-192297invariant-189296MacMillan-43output-decoupling-191297Smith--ofamodule500-ofapolynomialmatrix469stable-80system-194297transmission-39296
1-s2.0-S0025326X14008273- Marine Pollution Bulletin 91 (2015) 73–81Contents lists available at ScienceDirectMarine Pollution Bulletinj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / m a r p o l b u lEvaluation of beach cleanup effects using linear system analysisTomoya Kataoka a⇑ Hirofumi Hinata ba Coastal Zone Systems Division Coastal Marine and Disaster Prevention Department National Institute for Land and Infrastructure Management 3-1-1 Nagase YokosukaKanagawa 239-0826 Japanb Department of Civil and Environmental Engineering Faculty of Engineering Ehime University 3 Bunkyo-cho Matsuyama Ehime 790-8577 Japana r t i c l ei n f oa b s t r a c tArticle history:Available online 7 January 2015Keywords:Beach cleanup effectLinear system analysisAverage residence timeMarine plasticsWe established a method for evaluating beach cleanup effects (BCEs) based on a linear system analysisand investigated factors determining BCEs. Here we focus on two BCEs: decreasing the total mass of toxicmetals that could leach into a beach from marine plastics and preventing the fragmentation of marineplastics on the beach. Both BCEs depend strongly on the average residence time of marine plastics onthe beach (sr) and the period of temporal variability of the input ﬂux of marine plastics (T). Cleanupson the beach where sr is longer than T are more effective than those where sr is shorter than T. In additionboth BCEs are the highest near the time when the remnants of plastics reach the local maximum (peaktime). Therefore it is crucial to understand the following three factors for effective cleanups: the averageresidence time the plastic input period and the peak time.Ó 2014 Elsevier Ltd. All rights reserved.1. IntroductionBeach cleanup is a key approach to reducing threats on marineand coastal environments such as beach pollution from toxicmetals contained in marine plastics (Nakashima et al. 2012) andfragmentation of marine plastics (Andrady 2011). Ocean Conser-vancyfor instance organizes international coastal cleanups(Ocean Conservancy 2013) as well as waterway and ocean clean-ups every year involving many volunteers. However at present itis impossible to identify how best to organize them where andwhen to carry them out because the effects of these beach cleanupsare not quantitatively well understood. Quantifying the effects ofbeach cleanups would enable us to strategically plan beach clean-ups and to more effectively reduce the environmental risks causedby marine plastics.Nakashima et al. (2012) recently suggested that toxic metalswhich are widely used in additive agents during plastic productionleach into the beach via the water surrounding marine plastics(e.g. rainwater). They found high concentrations of lead stearate(Pb(C18H35O2)2) which is toxic to biota (Nordic Council ofMinisters 2003) in plastic ﬁshery ﬂoats made from polyvinyl chlo-ride (PVC) polymer (PVC ﬂoats). They computed the ﬂux leachinginto the surrounding water and the resultant concentration of Pb⇑ Corresponding author at: Coastal Zone Systems Division Coastal and MarineDepartment National Institute for Land and Infrastructure Management 3-1-1Nagase Yokosuka Kanagawa 239-0826 Japan. Tel.: +81 46 844 5025; fax: +81 46844 1145.E-mail address: kataoka-t852a@ysk.nilim.go.jp (T. Kataoka).http://dx.doi.org/10.1016/j.marpolbul.2014.12.0260025-326X/Ó 2014 Elsevier Ltd. All rights reserved.in the beach and concluded that Pb leaching from PVC ﬂoats is apotential future risk of marine plastics on beaches.When exposed to solar ultraviolet (UV) radiation marineplastics undergo photo-oxidative degradation and gradual frag-mentation. Marine plastics degrade much more quickly when lyingon the beach compared to ﬂoating on the sea because of relativelyhigher temperatures and the higher oxygen concentration in airenvironments (Andrady 2011). Thus degradation on beaches isthe predominant process in the fragmentation of marine plastics.On the other hand fragments of marine plastic referred to asmicroplastics pick up persistent organic pollutants (POPs) in thesea and develop high concentrations of POPs (Andrady 2011;Cole et al. 2011). Microplastics are deﬁned as particle fragmentssmaller than 5 mm (Barnes et al. 2009) and are easily ingestedby ﬁsh and seabirds in the ocean (e.g. Shaw and Day 1994;Derraik 2002; Boerger et al. 2010). Thus plastic fragments maybe important agents in the transport of toxic chemicals and con-taminants affecting the ocean food web (e.g. Mato et al. 2001;Thompson et al. 2004; Andrady 2011).These environmental risks would depend on the averageresidence time from washing ashore to backwashing offshore;thus the average residence time is crucial for evaluating the beachcleanup effects (BCEs). Recently Kataoka et al. (2013) found bymark-recapture experiments on Wadahama Beach Niijima IslandJapan thatremnants of plastic ﬁshing ﬂoats exponentiallydecreased and they successfully measured the average residencetime of the ﬂoats. The exponential decay enables us to understandthe beach characteristics as a linear input/output system for74T. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–81Table 1Normalized ampliﬁcation factor (A) and phase lag (h) in the case of n = 101 100 and101.Dimensionlessresidence time n101100101Normalizedampliﬁcation factor A1.59  1021.57  1018.47  101Phase lag h[degrees]89.0980.9632.14marine plastics i.e. amplitude and phase characteristics (Kataokaet al. 2013). Here we establish a method for evaluating BCEs basedon linear system analysis and investigate factors determiningBCEs. In this study we describe the quantiﬁcation of two BCEs:(1) decreasing the total mass of toxic metals that could leach intothe beach from marine plastics and (2) preventing the fragmenta-tion of marine plastics.2. Materials and methodsexpressed as a function of the residence time normalized by T (i.e.n = sr/T):AðnÞ ¼ HðnÞjjsrqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃÞ21 þ 2pnð3Þ1ð¼;hðnÞ ¼ tan1 Im HðnÞRe HðnÞððÞÞ ¼ tan1 2pnÞ;ðð4Þwhere A(n) and h(n) mean the ampliﬁcation factor normalized by srand the phase lag of the output with respect to the input respec-tively. Note that h is a negative value (i.e. h < 0).In principle the system characteristics of UIR deﬁned as anexponential function depend on n. If T = 365 days the system char-acteristics of a beach with one month one year and ten years of srare determined by 101 100 and 101 of n respectively. Table 1shows A and h (degrees) for three values of n. The ampliﬁcation fac-tor A (absolute value of phase lag h) of n = 101 is smaller (larger)than that of n = 101 (Table 1).2.1. Time-invariant linear input/output system for marine plastic2.2. Beach cleanup effect 1 (BCE1): decrease in total mass of toxicmetalsWe assume that the beach is a time-invariant linear input/out-put system for marine plastic and the cohort population of rem-nants of marine plastics exponentially decreases as follows:hðtÞ ¼ expðt=srÞ;0;for t P 0for t < 0ð1Þwhere t and sr are the elapsed time and the average residence timerespectively (Kataoka et al. 2013). h(t) denotes the unit impulseresponse (UIR) in the linear system analysis. The Fourier transfor-mation of UIR represents the system function (H(x)):HðxÞ ¼srð1 þ xsrðÞ2 1  ixsrÞ;ð2Þwhere x (=2p/T) is the angular frequency and T is the period of tem-poral variability of the input ﬂux of marine plastics (hereafter ‘‘plas-tic input period’’). Based on Eq. (2) the system characteristics can beFig. 1 shows a schematic image of the beach cleanup effect inrelation to toxic metals that could leach into the beach from mar-ine plastics (BCE1). Based on Nakashima et al. (2012) we considerthat toxic metals leach into the beach via surrounding water (e.g.rainwater). The beach cleanup can reduce total mass of toxic met-als leached into the beach. BCE1 is evaluated based on the differ-ence between the total mass with and without conducting beachcleanup.Based on the time-invariant linear system the leaching ﬂux(ym(t)) of toxic metals from marine plastics is evaluated as follows:ymðtÞ ¼ð5Þvðt  sÞxðsÞhðt  sÞds;tZ0where t–s denotes the age from when marine plastics washedashore (s) to time (t). v(t) is the leaching rate of toxic metals perunit time from one plastic object. And x(t) and h(t) are the inputﬂux of marine plastics (hereafter ‘‘plastic input ﬂux’’) and UIR (i.e. hcaeb a otni latem cixot fo )my( xulf gnihcaeLscitsalp enirammorf BCE1TimeBeach cleanupLeaching rate v(t)Cross section of marine plasticsSurrounding waterNo beach cleanupLeach into a beach Leach into a beachInfiltration water with toxic metalsBeach cleanupFig. 1. Schematic image of a beach cleanup effect in relation to toxic metals that could leach into the beach from marine plastics (BCE1).T. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–8175Eq. (1)) respectively. The total mass (Ym(t)) of toxic metals leachedinto the beach is evaluated by the integral of Eq. (5):ZZ ZY mðtÞ ¼ymðt0Þdt0 ¼t0t00t0vðt0  sÞxðsÞhðt0  sÞdsdt0:ð6Þ!2.3. Beach cleanup effect 2 (BCE2): prevention of plastic fragmentgenerationFig. 2 shows a schematic image of the beach cleanup effect inrelation to fragmentation of marine plastics (BCE2). Marine plasticscould eventually break down due to exposure to solar UV radiationand beach temperature (Andrady 2011). Portions of the plasticsurface could exfoliate in the breakdown process as the result ofa chemical change that drastically reduces the average molecularweight of the polymer (Andrady 2011). We assume the generationof plastic fragments by the breakdown and exfoliation of marineplastics.Based on the time-invariant linear system the generating rate(yf(t)) of plastic fragments is evaluated as follows:yfðtÞ ¼t0pðt  sÞxðsÞhðt  sÞds;ð7Þwhere p(t) is the probability that plastic fragments are generatedfrom the remnant per unit time (hereafter ‘‘generating probabil-ity’’). The amount of plastic fragments on a beach (Yf (t)) is evalu-ated by the integral of Eq. (7):!Z ZY fðtÞ ¼yfðt0Þdt0 ¼t0t00t0pðt0  sÞxðsÞhðt0  sÞdsdt0:ð8ÞZZ2.4. Simple models for plastic input ﬂux leaching rate andfragmentation probabilityThe plastic input ﬂux often shows seasonal variation due to oce-anic and/or weather conditions (e.g. Bowman et al. 1998; Kakoet al. 2010; Kataoka et al. 2013). For example immigration (i.e.plastic input to a beach) has local maximums on Wadahama BeachNiijima Island in early summer when the Kuroshio Currentapproaches the island (Kataoka et al. 2013). Accordingly the ide-alized plastic input ﬂux (x(t)) is expressed as the combination ofconstant input and sinusoidal input:ð9ÞxðtÞ ¼ x0 þ a sin 2pt=Twhere x0 T and a are the constant ﬂux period and amplitude of thesinusoidal input ﬂux respectively. Basically x(t) has three cases ofx0 and a (i.e. x0 > a; x0 = a; x0 < a).Þ;ðIn this study to simply investigate the dependence of BCEs onthe average residence time we assume that x0 is equal to a (i.e.x0 = a) that the leaching rate of toxic metals from one plastic objectis a constant value (i.e. v(t) = v0) and that the generating probabil-ity p(t) is proportional to the age of the remnants of marine plastics(t–s) (i.e. p(t) = p0(t–s)). Assuming a constant leaching rate andproportional generating probability ym(t) and yf(t) could beexpressed using yr(t) and ya(t) respectively:(ymðtÞ ¼ v0yrðtÞyfðtÞ ¼ p0yaðtÞð10Þwhere yr(t) and ya(t) denote the remnant and the total age of marineplastics on beaches respectively:yrðtÞ ¼xðsÞhðt  sÞds:ð11ÞtZZ0BCE2 is evaluated based on the difference between the amount ofplastic fragments with and without conducting beach cleanup.yaðtÞ ¼t0ðt  sÞxðsÞhðt  sÞds:ð12Þscitsalp eniram fo stnemgarf  fo )fy( etar gnitareneGBeach cleanupUV radiationFragments of marine plasticsHeatExfoliation Exposure to UV radiation and heatNo beach cleanupExposure to UV radiation and heatBCE2TimeBreakdown of marine plastics Surface of marine plasticsSurface of marine plastics exposed to UV radiation and beach temperatureBeachcleanupFig. 2. Schematic image of a beach cleanup effect in relation to fragmentation of marine plastics (BCE2).Exfoliation of marine plastics 76T. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–81ZZ0ttZZt0 Z Z0t0t00And also Ym(t) and Yf(t) are obtained by the integral of Eqs. (11) and(12) respectively. Here the integrals of yr(t) and ya(t) are deﬁned asthe cumulative remnant Yr(t) and age Ya(t) respectively:Y rðtÞ ¼yrðt0Þdt0 ¼xðsÞhðt0  sÞds!dt0:!dt0:ð13Þð14ÞY aðtÞ ¼0yaðt0Þdt0 ¼t0ðt0  sÞxðsÞhðt0  sÞdsTherefore BCE1 is evaluated based on the difference between thecumulative remnant with and without conducting beach cleanupand BCE2 is evaluated based on the difference between the cumula-tive age with and without beach cleanup.3. Results3.1. Dependence of temporal evolution of remnant and total age on theaverage residence timerr;ðsr ¼ y0constr) is expressed as:Substituting UIR (h(t): Eq. (1)) and the idealized input ﬂux (x(t):Eq. (9)) into the remnant (yr(t): Eq. (11)) and then normalizing yr(t)by x0sr the dimensionless remnant (y0(rðf; nÞ ¼ yrþ y0siny0x0Þ¼ 1  exp f=ny0constrA sin 2pf þ hr ¼ aÞ  sin h exp f=n½y0sinwhere y0r derived from the constantand sinusoidal components of x(t) respectively. f denotes theelapsed time normalized by the plastic input period (T) (i.e. f = t/T) and A and h of Eq. (15) are deﬁned by Eqs. (3) and (4) respec-tively. a/x0 is determined to be 1 by assuming x0 = a (seeSection 2.4).ðsin are the terms of y0rðx0const and y0rð15ÞÞAs the same way with y0r the dimensionless total age (y0a) isalso expressed as a function of f and n:(aðf;nÞ¼ ya¼ y0consty0x0s2r¼ 1 1þ f=nðy0constaa ¼ ay0sinþ y0sinÞexp f=nðÞÞ sin2hexp f=n ax0A2 sin 2pfþ 2hðÞðx0½aa;ðAsinh f=nðÞexp f=nÞð16Þa derived from the constanta is nor-const and y0awhere y0and sinusoidal components of x(t) respectively. Note that y02.malized by x0srsin are the terms of y0aðÞ andr and y0r and y0const and y0ra ﬁnally approach:A sin 2pf þ hFig. 3c and d shows the dependence of y0a on nrespectively. Both y0a completely depend on n. If sr is shorterthan T (e.g. n = 101) both y0const quickly increase (grayadash-dotted line in Fig. 3a and b). Conversely if sr is longer thanT (e.g. n = 101) both y0const and y0const slowly increase (gray solidarline in Fig. 3a and b). If f ? 1 y0r and y0r ! 1 þ ay0x0a ! 1 þ ay0x0as predicted by Eqs. (15) and (16) respectively. y0a) ﬂuctuateswith the amplitude A (A2) at around y0a = 1 and has a phasedelay of h (2h) compared with the plastic input ﬂux. The relaxationtime until y02 are respectivelydetermined by y0const = 0.9 which are 2.3n anda3.9n. Thus the relaxation time is proportional to n.a reach 90% of x0sr and x0srA2 sin 2pf þ 2hconst = 0.9 and y0rr and y0ð17Þð18Þr = y0r (y0y0const and y0rr and y0of y0const (y0asin and y0ra when n > 100 (n < 100). The contribution of y0sin) dominate the temporal evolutionasin andrðÞy0sin to the temporal evolution depends on the normalized ampliﬁ-acation factor (A). For instance y0sin contribute more largelyato the temporal evolution when n = 101 compared with that whenn = 101 because A when n = 101 is much larger than thatwhenn = 101 (Table 1).sin and y0r3.2. Dependence of temporal evolution of cumulative remnant and ageon the average residence timeThe dimensionless cumulative remnant (Y0r) is expressed bysubstituting h(t) (Eq. (1)) and x(t) (Eq. (9)) into Yr(t) (Eq. (13))and normalizing Yr(t) by x0sr(rðf; nÞ ¼ Y rþ Y0sinY0;x0s2rÞ¼ f=n  1 þ exp f=nðY0constÞ þ 12pn A cos 2pf þ hr ¼ aY0sinA sin h exp f=n2 as follows:¼ Y0constð19Þ þ ax0ððÞx0½1rrraaa½½½ððððÞ 1sin (Y0rAnd substituting h(t) (Eq. (1)) and x(t) (Eq. (9)) into Ya(t) (Eq. (14))3 the dimensionless cumulative ageand normalizing Ya(t) by x0sr(Y0a) is expressed as:8>><aðf;nÞ¼ Y aþ Y0sinY0x0¼ f=n 2þ f=nþ 2Y0const>>:a ¼ a2pn cos 2pfþ 2hY0sinA2x0A2 sin2h exp f=nþ ax0;ÞÞexp f=nÞþ cos2hþ ax0Þ 1r ¼ Y0consts3ððAsinh f=nþ 1ð20Þwhere Y0r (Y0a)derived from the constant and sinusoidal components of x(t)respectively.sin) are the terms of Y0aconst and Y0aconst and Y0rÞexp f=nconst and Y0rFig. 4c and d shows the dependence of Y0a completely depend on n namely Y0a on n respec-tively. Both Y0r and Y0abecome much smaller as n becomes larger. If sr is shorter than T(e.g. n = 101) both Y0const quickly increase correspond-aing to the temporal variation of y0a (gray dash-dottedline in Fig. 4a and b). Conversely if sr is longer than T (e.g.n = 101) Y0const and Y0const slowly increase (gray solid line inarFig. 4a and b). If f ? 1 Y0a are proportional to f as predictedby Eqs. (19) and (20) respectively. Overall Y0constadominate the temporal evolution of Y0and Y0sin and Y0rbecomes longer because the amplitude of Y0Eqs. (19) and (20)) is smaller (Table 1).sinrsin decreases as nasin and Y0sin (A and A2 inrasin. In addition the contribution of Y0aconst and Y0ra compared with Y0r and y0r and Y0r and Y0r and Y0r and Y03.3. Effects of yearly and biyearly cleanupsKataoka et al. (2013) revealed that the average residence time ofplastic ﬁshing ﬂoats on Wadahama Beach was 209 days and immi-gration of ﬁshing ﬂoats varied seasonally. Assuming the plasticinput period (T) on Wadahama Beach is 365 days the dimension-less residence time (n) becomes 0.57 (i.e. 209/365 = 0.57). Beachcleanup is usually conducted yearly such as in the InternationalCoastal Cleanups (Ocean Conservancy 2013). Here we show anexample of the beach cleanup effects (hereafter ‘‘BCEs’’) onWadahama Beach in a case where beach cleanup is conductedyearly or biyearly.The temporal evolution of the cumulative remnant (Yr) and age(Ya) after a beach cleanup can be represented as follows:Y rðtÞ ¼ Y rðtcÞ þt!ZZtc Zyrðt0Þdt0t0xðsÞhðt0  sÞdsdt0;ð21Þ¼ Y rðtcÞ þttctcT. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–8177Fig. 3. Dependence of the temporal evolution of dimensionless remnant (y0evolution of y0line denotes y0(d) denotes y0const. Panels (c) and (d) show the temporal evolution of y0ar and y0const and y0rr and y0a respectively. The gradation scale is shown on the right side of (c) and (d).r) and total age (y0r and y0a of n = 101 (solid line) 100 (broken line) and 101 (dash-dotted line) respectively. In panels (a) and (b) the black line denotes y0a) on the dimensionless residence time (n). Panels (a) and (b) show the temporala and the graya in the range from 102 to 102 of n respectively. The white-black gradation of (c) andr and y0Fig. 4. Dependence of the temporal evolution of the dimensionless cumulative remnant (Y0of (a) and (b) and the gradation scale of (c) and (d) are logarithmic scales. The format is the same as that of Fig. 3.r) and age (Y0a) on the dimensionless residence time (n). Note that the vertical axisZZtttc Zyaðt0Þdt0t0tctcY aðtÞ ¼ Y aðtcÞ þ¼ Y aðtcÞ þ!t0  sðÞxðsÞhðt0  sÞdsdt0;ð22Þwhere tc means when a beach cleanup is conducted which isreferred as to ‘‘cleaning time’’. Thus the BCEs are evaluated bydividing the difference between the cumulative responses (i.e. Yrand Ya) with and without conducting beach cleanup by the cumula-tive responses without beach cleanup.Fig. 5a and b shows the temporal evolution of y0r in acase where beach cleanups are conducted yearly when the phaser and Y0a and Y0of the sinusoidal component of input ﬂux (hereafter ‘‘cleaningphase hc’’) is equal to 2p respectively (bold solid line: no cleanup;bold broken line: regular cleanup). Fig. 6a and b shows the tempo-ral evolution of y0a for the cleanup at the same cleaningphase (i.e. hc = 2p).If the cleanups are conducted yearly athc = 2p (Figs. 5a and 6a) the cumulative remnant (Fig. 5b) andage (Fig. 6b) can be reduced by 30% and 60% respectively com-pared with those without beach cleanups at f = 5 (Table 2). Onthe other hand regular cleanups conducted at hc = p (Figs. 5c and6c) can greatly reduce the cumulative remnant (Fig. 5d) and age(Fig. 6d). The BCEs calculated from the cumulative remnant andage in this case are 54% and 82% at f = 5 respectively (Table 2).Therefore the BCEs depend on the timing of the regular cleanups78T. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–81Fig. 5. Temporal evolution of the dimensionless remnant (y0residence time (n). Panels (a) (c) and (e) show the temporal evolution of y0respectively. Panels (b) (d) and (f) show the temporal evolution of Y0r) and dimensionless cumulative remnant (Y0r) during the ﬁve input periods in the case of 0.57 of dimensionlessr in the case of yearly cleanup at hc = 2p yearly cleanup at hc = p and biyearly cleanup at hc = pr in these three patterns of regular cleanups. The meaning of lines is shown in the upper box.(i.e. cleaning phase hc). Regular cleanups at hc = p are more effec-tive than those at hc = 2p. On Wadahama Beach (or beaches havingthe characteristics of n = 0.57) the remnant at hc = p mostly corre-sponds to the local maximum of yr(t) (hereafter ‘‘remnant peak’’)because the phase lag h is 81° (Table 1) (i.e. p/2h  p). Thisdemonstrates that beach cleanup at the time of the remnant peak(hereafter ‘‘peak time’’) is the most effective.Generally beach cleanup involves high labor costs. If it is con-ducted every two years it should be timed to coincide with theremnant peak. Even if beach cleanup is conducted only once everytwo years it is as effective as yearly cleanup at hc = 2p (Figs. 5f 6fand Table 2). This indicates that the cleaning phase (hc) is animportant factor for conducting effective beach cleanups.3.4. Dependence of the effect of beach cleanup on the averageresidence timeFigs. 7a and 8a describe the dependence of BCEs for the remnantand total age by yearly cleanups during the ﬁve input periods (i.e.f = 0–5) on both the dimensionless residence time (n) and ther) and age (Y0cleaning phase (hc) respectively. The horizontal axes of Figs. 7aand 8a denote the phase difference (dh) between the phase of theinput ﬂux at the peak time determined by Eq. (4) (e.g. p/2hbecause the phase at the peak time is deﬁned by 2pf + h = p/2;see Eq. (15)) and hc: dh = (p/2h)hc. Note that h is a negative value(Eq. (4)). The vertical axes of Figs. 7a and 8a denote n. The white-black gradation of Figs. 7a and 8a represents the BCEs calculatedfrom the cumulative remnant (Y0a) respectively (seeSection 3.3). The BCEs clearly increase as n becomes longer. Con-versely regular cleanup on a beach with shorter n has little effecton the beach environment. Therefore regular cleanups on a beachwith longer n are more effective than those with shorter n.Figs. 7b and 8b show the dependence of BCEs of n = 101 100and 101 on the phase difference (dh). At all levels of n the BCEsfor the remnant are fully maximized at dh = 0 (Fig. 7a and b). Thusthe most effective time for BCE1 matches the peak time (Fig. 7a andb). Furthermore regular cleanups at around the peak time can alsoeffectively reduce the cumulative age although the maximum forBCE2 occurs shortly after the peak time (Fig. 8a and b). Thereforebeach cleanups conducted at around the peak time would becomeFig. 6. Temporal evolution of the dimensionless total age (y0residence time (n). The format is the same as that of Fig. 5.a) and dimensionless cumulative age (Y0a) during the ﬁve input periods in the case of 0.57 of dimensionlessT. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–8179Table 2Beach cleanup effects (BCEs) calculated from the dimensionless cumulative remnant (Y0(Y0a) after the ﬁve input periods in the case of 0.57 of dimensionless residence time (n).r) and agerYearly cleanup(case: hc = 2p)Dimensionless cumulative remnant Y0No cleanupCleanupBCE (%)Dimensionless cumulative age Y0No cleanupCleanupBCE (%)8.035.65307.072.8060aFig. 7. (a) Dependence of the beach cleanup effect (BCE) for the remnant on thedimensionless residence time (n) and the cleaning phase (hc). The horizontal axisrepresents the phase difference dh between the phase of input ﬂux at the remnantpeak and hc. The contour line and white-black gradation of (a) denotes the BCE; itsscale is shown under panel (a). (b) Dependence of the BCE of n = 101 (solid line) 100(broken line) and 101 (dash-dotted line) on hc. (c) Dependence of the effectdifference (i.e. difference between maximum and minimum BCE) on n.the most effective considering BCE1 (see Section 2.2) and BCE2 (seeSection 2.3).Figs. 7c and 8c show the effect difference in BCE1 and BCE2 cal-culated from the difference between the maximum and minimumBCEs. The effect difference also depends on n (Figs. 7c and 8c). If n issmaller than 101 the effect difference is small because the BCEsare very low for the all dh (Figs. 7a and 8a). The effect differencewhen n > 100 is smaller than that when 101 < n < 100 becausethe amplitude of Y0sin becomes smaller as n becomes longera(see Section 3.2). The smaller effect difference means that the BCEsare dependent more weakly on the timing of beach cleanup.sin and Y0r4. Discussion4.1. Efﬁcacy of the time-invariant linear system in plastic pollutionYearly cleanup(case: hc = p)Biyearly cleanup(case: hc = p)7.513.47546.481.15827.515.25306.483.0353Fig. 8. (a) Dependence of the beach cleanup effect (BCE) for total age on thedimensionless residence time (n) and the cleaning phase (hc). The format is thesame as that of Fig. 7.(v(t)) of toxic metals from marine plastics is a constant and thegenerating probability (p(t)) is proportional to the remnant age(ts). In actuality v(t) and p(t) would depend on the local precip-itation and UV radiation (and/or temperature). Thus these func-tions should be determined according to the local site. In anycase if these reasonable functions can be obtained we could eval-uate the total mass of toxic metals and the amount of plastic frag-ments by applying the concept of time-invariant linear system(e.g. Eqs. (6) and (8)).For example Nakashima et al. (2012) estimated the leachingrate of lead stearate (Pb(C18H35O2)2) from PVC ﬂoats by laboratoryexperiment. The Fickian diffusion process has been applied to theestimation of the leaching rate. If v(t) follows the diffusion processas applied in Nakashima et al. (2012) it can be approximated as anexponential function:Þ:vðtÞ ¼ v 0 exp t=smð23ÞðTo simply investigate the dependence of beach cleanup effects(BCEs) on the average residence time several assumptions areemployed for evaluating the total mass of toxic metals leached intothe beach and the amount of plastic fragments: the leaching ratewhere v0 and sm denote the initial leaching rate and average time oftoxic metal untilleaching into the beach (hereafter ‘‘leachingtime’’). Substituting Eqs. (1) (9) and (23) into Eq. (5) the leachingﬂux (ym(t)) can be expressed as:T. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–8180ymðtÞ ¼ yconstm ¼ v 0x0b 1  exp t=bÞyconstÞ  sin h0 exp t=bm ¼ v 0abA0 sin 2pt=T þ h0ysinm þ ysinm ;½½ðððð24ÞÞ:ð25Þwhere b denotes the coefﬁcient determined by the average resi-dence time (sr) and the leaching time (sm):b ¼ srsmsr þ smAlso A0 and h0 denote the normalized ampliﬁcation factor andphase lag replacing sr of Eqs. (3) and (4) with b. Thus the systemcharacteristics of the leaching ﬂux ym(t) (i.e. A0 and h0) fully dependon the ratio of b to the plastic input period (T). Therefore based onthe exponential type of leaching rate BCE1 is determined by theratio of b to T corresponding to the dimensionless residence time(n). In actuality in addition to the leaching time v(t) could varyas exfoliation and/or cracks develop on the surface of marine plas-tics. If we can determine v(t) considering these factors the BCEscould be evaluated relatively easily by the linear system analysisshown in this study.4.2. Suggestion for effective beach cleanupConsidering a beach as the time-invariant linear system we candevise a plan for effective beach cleanup. We demonstrated thatbeach cleanup is more effective at beaches with the average resi-dence time longer than the plastic input period (i.e. n > 100) fortwo reasons: (1) the BCEs are very high and (2) the BCEs areweakly dependent on the cleaning phase (hc) (Figs. 7 and 8). Herewe discuss effective beach cleanup strategies based on the depen-dence of BCEs on the average residence time.Our suggestion is to conduct cleanups on beaches with n > 100because the BCEs are relatively high. For example the amount ofbeach litter shows seasonal variation (e.g. at six beaches alongthe Mediterranean Sea in Israel: Bowman et al. (1998); New Jerseybeach in the US: Ribic (1998); Ookushi Beach in Japan: Kako et al.(2010); Wadahama Beach in Japan: Kataoka et al. (2013)). The sea-sonal variation indicates that the predominant input period atthese beaches is less than one year. If the amount of beach littervaries seasonally then beach cleanup where the average residencetime is longer than one year will be more effective.Moreover cleanup on these beaches (n > 100) has an advantagein terms of the weak dependence of the BCEs on the time for con-ducting beach cleanups. Generally beach cleanup is most effectivewhen conducted at the time of the remnant peak (hereafter ‘‘peaktime’’) (Figs. 7 and 8). The time for conducting beach cleanupswould be determined by various factors such as weather condi-tions availability of labor and cost. In particular the weather con-ditions are important for the safety of beach cleanups. If theremnant peak is during the stormy season beach cleanup will bedifﬁcult. However cleanup on beaches with longer dimensionlessresidence time will likely have a sufﬁcient effect even if it cannotbe conducted at the peak time.To plan effective beach cleanup it is necessary to understand theaverage residence time input period and peak time. The averageresidence time can be measured by the mark-recapture experi-ments described in Kataoka et al. (2013). In addition the plasticinput period and the peak time can be understood by applyingsequential monitoring for the remnant of beach litter using web-cam (Kako et al. 2010; Kataoka et al. 2012). If the beach can be con-sidered as the time-invariant linear system the plastic input periodis consistent with the remnant period (e.g. Eq. (15)). The predomi-nant input period for instance can be calculated by applying Fou-rier transformation to the temporal variability of the remnant.Furthermore the peak time can be statistically obtained as the timeof the remnant peak through long-term webcam monitoring.5. ConclusionsTo strategically plan beach cleanups and to more effectivelyreduce the environmental risks caused by marine plastics weestablished a method for evaluating beach cleanup effects (BCEs)based on a linear system analysis and investigated factors deter-mining BCEs. This study focused on two BCEs: decreasing the totalmass of toxic metals that could leach into a beach from marineplastics (BCE1) and preventing the generation of fragments of mar-ine plastics (BCE2).In the time-invariant linear system the exponential decay ofthe remnant corresponds to the unit impulse response (UIR) whichenables us to understand the system characteristics. The systemcharacteristics can be described as functions of the dimensionlessresidence time (n) which is deﬁned as the ratio of the average res-idence time (sr) to the periods of the input ﬂux of marine plastics(T). BCE1 and BCE2 can be evaluated based on the remnant and thetotal age of marine plastics on the beach by making some assump-tions: the leaching rate of toxic metals from marine plastics is aconstant and the probability of generation is proportional to theremnant age (ts) respectively.When conducting regular beach cleanups the BCEs depend onthe dimensionless residence time (n) and the phase of the plasticinput ﬂux (cleaning phase; hc). Overall the BCEs increase clearlyas n becomes longer. Considering the two BCEs beach cleanupsaround the time of the remnant peak (peak time) are most effectiveregardless of n. Therefore to plan effective beach cleanups weneed to understand the average residence time (sr) the plasticinput period (T) and the peak time. Nevertheless two BCEs on bea-ches of n > 100 are weakly dependent on the time at which beachcleanups are conducted. Our suggestion is to conduct cleanupson beaches of n > 100 (i.e. sr > T) because high BCEs can beexpected regardless of whether the cleanup is conducted at thepeak time.AcknowledgmentsWe sincerely thank Prof. Hideshige Takada of Tokyo Univer-sity of Agriculture and Technology Prof. Atsuhiko Isobe of Kyu-shu University Prof. Shin’ichi Aoki of Osaka University and Prof.Takanobu Inoue and Assoc. Prof. Shigeru Kato of Toyohashi Uni-versity of Technology for their valuable comments. This researchwas supported by the Environment Research and TechnologyDevelopment Fund (B-1007) of the Ministry of the EnvironmentJapan and by JSPS KAKENHI Grant Number 23656309 and25820234.ReferencesAndrady A.L. 2011. Microplastics in the marine environment. Mar. Pollut. Bull. 62(8) 1596–1605.Barnes D.K.A. Galgani F. Thompson R.C. Barlaz M. 2009. Accumulation andfragmentation of plastic debris in global environments. Phil. Trans. R. Soc. B 3641985–1998.Boerger C.M. Lattin G.L. Moore S.L. Moore C.J. 2010. Plastic ingestion by plank-tivorous ﬁshes in the North Paciﬁc Central Gyre. Mar. Pollut. Bull. 60 (12)2275–2278.Bowman D. Manor-Samsonov N. Golik A. 1998. Dynamics of litter pollution onIsraeli Mediterranean beaches: a budgetary litter ﬂux approach. J. Coast. Res. 14(2) 418–432.Cole M. Lindeque P. Halsband C. Galloway T.S. 2011. Microplastics ascontaminants in the marine environment: a review. Mar. Pollut. Bull. 62 (12)2588–2597.Derraik J.G.B. 2002. The pollution of the marine environmental by plastic debris: areview. Mar. Pollut. Bull. 44 (9) 842–852.Kako S. Isobe A. Magome S. 2010. Sequential monitoring of beach litter usingwebcams. Mar. Pollut. Bull. 60 (5) 775–779.Kataoka T. Hinata H. Kako S. 2012. A new technique for detecting colored macroplastic debris on beaches using webcam images and CIELUV. Mar. Pollut. Bull.64 (9) 1829–1836.T. Kataoka H. Hinata / Marine Pollution Bulletin 91 (2015) 73–8181Kataoka T. Hinata H. Kato S. 2013. Analysis of a beach as a time-invariant linearinput/output system of marine litter. Mar. Pollut. Bull. 77 (1–2) 266–273.Mato Y. Isobe T. Takada H. Kanehiro H. Ohtake C. Kaminuma T. 2001. Plasticresin pellets as a transport medium for toxic chemicals in the marineenvironment. Environ. Sci. Technol. 35 (2) 318–324.Nakashima E. Isobe A. Kako S.I. Itai T. Takahashi S. 2012. Quantiﬁcation of toxicmetals derived from macroplastic litter on Ookushi Beach Japan. Environ. Sci.Technol. 46 (18) 10099–10105.Nordic Council of Ministers 2003. Lead Review. <http://www.who.int/ifcs/documents/forums/forum5/nmr_lead.pdf>.Ocean Conservancy 2013. International Coastal Cleanup 2013 report: Working forclean beaches and clean water. <http://www.oceanconservancy.org/our-work/international-coastal-cleanup/>.Ribic C.A. 1998. Use of indicator items to monitor marine debris on a New Jerseybeach from 1991 to 1996. Mar. Pollut. Bull. 36 (11) 887–891.Shaw D.G. Day R.H. 1994. Colour- and form-dependent loss of plastic microdebrisfrom the North Paciﬁc Ocean. Mar. Pollut. Bull. 28 (1) 39–43.Thompson R.C. Olsen Y. Mitchell R.P. Davis A. Rowland S.J.John A.W.McGonigle D. Russell A.E. 2004. Lost at sea: where is all the plastic? Science304 (5672) 838.
0218127417501371 August 28 201713:51 WSPC/S0218-12741750137International Journal of Bifurcation and Chaos Vol. 27 No. 9 (2017) 1750137 (16 pages)c(cid:1) World Scientiﬁc Publishing CompanyDOI: 10.1142/S0218127417501371Noise-Induced Chaos in a Piecewise Linear SystemChen Kong∗ and Xian-Bin Liu†State Key Laboratory of Mechanics and Control of Mechanical StructuresNanjing University of Aeronautics and Astronautics29 Yudao Street Nanjing Jiangsu Province P. R. China∗kongchen bill@126.com†xbliu@nuaa.edu.cnReceived December 15 2016; Revised April 18 2017In the present paper the phenomenon of noise-induced chaos in a piecewise linear system thatis excited by Gaussian white noise is investigated. Firstly the global dynamical behaviors ofthe deterministic piecewise linear system are investigated numerically in advance by using thegeneralized cell-mapping digraph (GCMD) method. Then based on these global propertiesthe system that is excited by Gaussian white noise is introduced. Then it is simpliﬁed by thestochastic averaging method through which a four-dimensional averaged Itˆo system is ﬁnallyobtained. In order to reveal the phenomenon of noise-induced chaos quantitatively MFPT (themean ﬁrst-passage time) is selected as the measure. The expression for MFPT is formulatedby using the singular perturbation method and then a rather simple representation is obtainedvia the Laplace approximation and within which the concept of quasi-potential is introduced.Furthermore with the rays method the MFPT under a certain set of parameters is estimated.However within the process of analysis the authors had to face a diﬃcult problem concerningthe ill-conditioned matrix which is the obstacle for the estimation of MFPT which was thensolved by applying one more approximation. Finally the result is compared with the numericalone that is obtained by the Monte Carlo simulation.Keywords: Piecewise linear system;MFPT.intermittent switching behavior; noise-induced chaos;1. IntroductionIn recent years there has been considerable inter-est in the research on the phenomena of noise-induced dynamical behaviors in the ﬁeld of randomdynamical systems. Thus far many of these phe-nomena such as the so-called noise-induced chaos[He & Habib 2013; T´el et al. 2008; Lin & Young2008] intermittent switching behavior [Armbrusteret al. 2003; Stone & Armbruster 1999] noise-induced switching between cycles in the heteroclinicnetworks [Armbruster et al. 2003; Stone & Arm-bruster 1999] synchronization [Zhou et al. 2003]a lowering of the escape energy [Kraut & Feudel2003a; Hunt et al. 1996] transient chaos [T´el et al.2008; Kraut & Feudel 2002] are observed andfound that belong respectively to diﬀerent subjectssuch as ﬂuid mechanics [Aubry et al. 1988] neu-rology [Dtchetgnia Djeundam et al. 2013] biology[Billings & Schwartz 2002] stochastic stability ofstructures [Roy 1996] population ecology [Ellner &Turchin 1995] chemistry [Naeh et al. 1990] lasertheory [Zhou et al. 2003] communications theory[Hunt et al. 1996] and so on. It is noted that fora nonlinear dynamical system which has structural†Author for correspondence1750137-1Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liuinstability even a small noise may result in thesekinds of phenomena. Thus far most research meth-ods emphasized on the numerical evaluation of themaximal Lyapunov exponent [He & Habib 2013;Lin & Young 2008; Aubry et al. 1988; Dtchetg-nia Djeundam et al. 2013; Ellner & Turchin 1995]which is used to characterize the chaotic prop-erties of signals and the fractal dimensions [T´elet al. 2008; Hunt et al. 1996] Stone and Holmesinvestigated the behaviors of the heteroclinic net-works under random perturbations [Aubry et al.1988] a noise-induced transition from a periodicmotion to a quasi-periodic or chaotic motion wasfound numerically in a Rayleigh–Duﬃng system[Hikihara et al. 2012] and furthermore via areduced-order Fokker–Planck–Kolmogorov (FPK)equation a disappearance of a noise-induced energylocalization was found within a monostable Duﬀ-ing system. In [Roy 1997a; Roy & Nauman 1995]noise-induced chaos orswitchingbehavior was found in multistable piecewise linearsystems however under the assumption of ellipticorbits and a two-dimensional Itˆo system this phe-nomenon cannot be analyzed analytically. Accord-ing to the recent researches by the authors for anoise-excited piecewise linear system a chaotic sad-dle was found to exist among a wide range of param-eters [Kong et al. 2016] which reﬂects the fact thatwithin the same range of parameters noise-inducedchaos can exist.intermittentRecently the concept of quasi-potential wasalways used to characterize these complicated phe-nomena especially for the nonlinear system whichexhibits transient chaos or unattractive chaoticattractors. The unattractive chaotic attractors werefound to be very common in a periodic window andthe quasi-potential plateaus caused by chaotic sad-dles were also observed [Kraut & Feudel 2003b](see Figs. 4–6 in [Kraut & Feudel 2003b]). Thequasi-potential was ﬁrst introduced by Freidlin andWentzell [2012] in order to investigate the behav-ior of dynamical systems that were under randomperturbation. However up till now the methodof quasi-potential is mostly used in the numericalsimulations of the complicated nonlinear dynami-cal systems which exhibit chaos or chaotic saddlesand the reason is simply because the dynamicalstructures of the nonlinear stochastic systems thatprocess these phenomena are complicated. Fur-thermore the quasi-potential was used by someresearchers to measure MFPT. In the study ofstability problem stochastic stability is alwaysdeﬁned as the stability of invariant density [Young1986] and an oscillator can be called stochasti-cally unstable if the MFPT is ﬁnite [Klosek-Dygaset al. 1988]. In this paper the phenomenon ofnoise-induced chaos obviously implies the stochas-tic instability of a regular attractor and MFPTcan characterize the enhancement of noise-inducedescape if the chaotic saddles exist [Hong et al.2010].Based on the previous work of the authors[Kong et al. 2016] a piecewise linear system isinvestigated again in the present paper for whichthe phenomenon of noise-induced chaos is inves-tigated furthermore. To measure the complicatedphenomenon quantitatively a high-dimensional Itˆosystem is obtained and the relevant MFPT is esti-mated via the singular perturbation method. In thepresent analysis the eﬀect of boundary layer is con-sidered which reﬂects the algebraic rate of changeof MFPT with respect to time and as a result themore precise estimation of MFPT is then obtained.Whereas within the analysis process the authorsare challenged wherein an ill-conditioned matrixarises and hinders the estimation of the MFPT. Tosolve the problem by ignoring that term in the for-mula of MFPT which relates to the ill-conditionedmatrix an approximate result is obtained. Despiteits complex forms the approximate analytical resultreﬂects successfully the inﬂuence of chaotic saddleson the exit behavior and matches the numericalresults well.This paper is organized as follows. In Sec. 2 inthe absence of noise excitation the global dynami-cal properties of the piecewise linear system drivenby only a harmonic excitation are illustrated byusing GCMD. In Sec. 3 for the system driven byboth a harmonic excitation and a Gaussian whitenoise an averaged four-dimensional Itˆo systemis obtained via the stochastic averaging method.Based on the analysis upon the boundary layerfunction and the rays method the solution of theFPK equation is obtained along each ray by solv-ing a set of ordinary diﬀerential equations (ODEs)which are formulated in Sec. 4. In Sec. 5 on thebasis of Laplace approximation method and thework of Z. Schuss and his co-workers the expressionfor the MFPT is formulated. As noted before theill-conditioned matrix induced by the term withinthe expression of MFPT arises to solve the prob-lem one more approximation method is applied1750137-2Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137and ﬁnally a comparison between the approximateresult and the numerical result simulated by MonteCarlo method is given. The conclusions and somediscussions are summarized in Sec. 6.2. Global AnalysisIn this section the global dynamical properties for adeterministic piecewise linear system is investigatedin advance by using GCMD prior to the inves-tigation on the MFPT of the relevant stochasticsystem because of the complexity of the dynam-ical behaviors. GCMD is known as a powerfulnumerical method for inspecting and illustratingthe global and local properties of a dynamical sys-tem [Kong et al. 2016; Hong 2010; Hong et al.2010; Han et al. 2015; Han et al. 2014; Yue et al.2012; Hong & Xu 2003; Hsu 1995; Dellnitz et al.2001]. And some eﬃcient algorithms were devisedfor obtaining invariant sets in high-dimensional sys-tems [Dellnitz et al. 2001]. For the piecewise linearsystem its equation of motion is given bywhere f(x) is a piecewise linear restoring forcewhich is deﬁned asdt + f(x) = F (t)d2xdt2 + 2ζ dxx + 1 x ≤ −1|x| < 10x − 1 x ≥ 1f(x) =(1)(2)Noise-Induced Chaos in a Piecewise Linear Systemand F (t) is the external excitation i.e.F (t) = f0 + f1cos(ωt) + ξ(t)(3)ξ(t) is the random process that has a zero meanand the autocorrelation function (cid:3)ξ(t)ξ(t + τ)(cid:4) =2Dδ(t) and D is assumed as a small parameter. Inmechanical systems there are many buﬀers bar-riers and elastic stops to limit the amplitude ofvibrations such as vibration dampers in a vehicleand fenders. And the clearance or backlash betweengears cannot be avoided to model the vibration ofgear lapping system. All these systems are subjectedto piecewise linear restoring forces. Equation (1) isa classical and generalized model with a clearancetype nonlinearity used to model many systems inpractical engineering projects especially the vibra-tion and impact systems within mechanical andcivil engineering. In this piecewise linear systemmany complicated dynamical behaviors have beenfound.In [Kong et al. 2016] the dynamics of a deter-ministic piecewise linear system are investigatednumerically. Through the Monte Carlo simulationthe noise-induced chaos is found twice if the fre-quency of the external excitation ω is around 0.49or belongs to the interval [0.7 1.4] thus ω = 0.89is chosen to be as the typical case to show thisphenomenon numerically. In the remainder of thisresearch the values of some parameters are set asζ = 0.01 f0 = f1 = 0.25. For the system deﬁned inEq. (1) excited by a very small noise Fig. 1 displaysthe signal history and the corresponding projection1.510.5y0−0.5−10Noise is added 1.510.5y0−0.520004000T6000800010000(a)−1−3−2−10x(b)123(a) The time history of y-component of the Poincar´e points on (4) are shown versus the time T which stands for theFig. 1.period of the external excitation T = 2π/ω and (b) the projection of the same signal onto the Poincar´e section (4). ω = 0.89D = 6e − 5 are chosen in this ﬁgure.1750137-3Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liuon a Poincar´e section which is deﬁned by(cid:5)(x y t)| t = t0 mod(cid:6)2πωΣ =(4)where y-coordinate denotes˙x. The comparisonbetween the signal in Fig. 1(a) before and afternoise is added the fact that the chaotic saddle existsin the deterministic system and the fact that thephenomenon of noise-induced chaos exists in thestochastic system are both veriﬁed. And the pro-jection of this signal is displayed in Fig. 1(b).Due to the complexity of the dynamical behav-iors of the system deﬁned in Eq. (1) in this sectionthe GCMD method is applied to ﬁnd the reﬁnedstructure of the phase portrait on the Poincar´e sec-tion deﬁned in Eq. (4)in the case of no noiseexcitation (i.e. D = 0). The GCMD method is anumerical method which is based on the general-ized cell-mapping (GCM) method [Hsu 1995 1987]and graph theory and with the help of the the-ory of digraph this method extracts much moreinformation through numerical computation suchas attractors saddles and even chaotic saddles.Although via this method the accurate positionsof the ﬁxed points cannot be obtained the areasthat they occupy are given which are suﬃcient forthe qualitative analysis.In Fig. 2 the green parts are two period-1attractors the black parts are the regular andchaotic saddles and the blue and red partsare the stable and unstable manifolds of saddlesrespectively. The remaining pictures in this paperFig. 2. The phase diagram projected on the Poincar´e sec-tion by using GCMD method ω = 0.89. The attractors areshown by green points and the saddle and chaotic saddlesare shown by black points. The blue and red points are thestable and unstable manifolds of saddles respectively.obtained from the GCMD method follow the sameconvention. A cell space of 2000 × 2000 is used tocover the phase diagram in Fig. 2 and there areone period-1 stable solution with a large amplitude(the green part in the upper right) one period-1 sta-ble solution with a small amplitude (the green partin the lower left) one period-1 saddle (the blackpoint located in the insertion of the blue and redline in the left) and one big black part that hasa reﬁned structure which is a chaotic saddle. Thischaotic saddle is a set of saddles interconnectedby the manifold of each other [Kong et al. 2016;Hong & Xu 2003]. It is noted that if the resolu-tion of this method is raised extremely each of thetwo period-1 stable solutions and the period-1 sad-dle solution depicted in Fig. 2 should turn out to bea point respectively on the phase portrait howeverthe simulation process is uneconomical. Because thepurpose of the investigation here is to just show theglobal properties of this system not to determinethe exact positions of these solutions the resultsshown in Fig. 2 are enough here.By comparing Fig. 1(b) and the chaotic sad-dle in Fig. 2 it is observed that the phenomenonof noise-induced chaos is a kind of exit problemin which the system status exits from the basinof the stable attractor to the chaotic saddle undereven a small noise excitation. That is how thenoise-induced chaos occurs. Furthermore due to thenonattracting property of the chaotic saddle it isimpossible for the system to stay at the chaoticsaddle for a long time instead it moves back tothe stable attractor intermittently. The intermit-tent switching behavior occurs as a result. So thenoise-induced chaos is a kind of exit behavior fordynamical systems wherein the attractors and thechaotic saddles coexist. And we note that in gen-eral noise-induced chaos is not only a simple transi-tion between chaotic saddles and regular attractors.For example Lin and Young studied noise-inducedchaos without intermittent switching behavior in atwo-dimensional system [Lin & Young 2008].In orderto characterize both the attrac-tor and the chaotic saddle a further analysis isrequired with a smaller scope and a ﬁner resolution.Figure 3(a) shows the reﬁned structure just aroundthe period-1 stable solution with a small amplitude.In this small scale it is easy to ﬁnd that there isa period-3 saddle [three black parts connected toeach other in Fig. 3(a)] around the attractor whichdemonstrates that the period-3 saddle is the nearest1750137-4Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137Noise-Induced Chaos in a Piecewise Linear System0.40.20−0.2−0.410.5y0−0.500.51(a)−1−0.500.51x(b)1.522.5Fig. 3.(a) The phase diagram projected on the Poincar´e section (4) in a small scale around the period-1 stable cycle with asmall amplitude by using GCMD method at ω = 0.89 and (b) the two-dimensional phase trajectory on (x y) plane which isintegrated numerically by choosing the right black point in Fig. 3(a) as the initial point.part of the chaotic saddle from period-1 attractorand corresponds to the edge of the quasi-potentialplateau [Kraut & Feudel 2003b; Hunt et al. 1996].Choosing the right black point on the Poincar´esection as the initial point the trajectory of thisperiod-3 saddle is obtained numerically and plottedin Fig. 3(b). Thus we expect the analysis of the phe-nomenon of noise-induced chaos can be replaced bythe analysis of the transition between the period-1attractor and the period-3 saddle.3. Simpliﬁcation of the SystemThrough the global analysis by GCMD method inSec. 2 one period-3 saddle is found just around theperiod-1 attractor which corresponds to the edge ofquasi-potential plateau. Thus the analysis on thephenomenon of noise-induced chaos can be replacedby the analysis on the exit behavior between thisperiod-3 saddle and the period-1 attractor. Sinceboth the white noise excitation and the dampingcoeﬃcient of the system are small the stochasticaveraging method which is used widely for theanalysis of the exit problems [Roy 1996 1997b1994b 1997a 1994a 1995; Rodrigues et al. 2010;Chen & Zhu 2010; Chen et al. 2009] can be appliedto simplify the system (1). It is noted that theperiod-1 attractor and this period-3 saddle are bothstructurally stable thus it is reasonable to use anapproximation method such as the stochastic aver-aging method to deal with this exit problem [Guck-enheimer & Holmes 1983] and the averaged systemas the substitution of the original piecewise linearsystem will facilitate the analysis of the MFPT.Via the comparison between the trajectorydepicted in Fig. 3(b) and an elliptic trajectory itis clear that the amplitude of this trajectory isobviously period-3. Then the power spectrum ofthe motion around the period-3 saddle displayedin Fig. 4 shows that the subharmonic frequenciesof orders 1/3 and 2/3 are present beside the mainoscillating frequency. Thus by adding two period-3harmonic components to the amplitude of the ellip-tic trajectory the approximate solution to Eq. (1)|)ωX(|1051041031021010X: 0.8805Y: 5929X: 0.2953Y: 1733X: 0.5852Y: 20540.20.40.6ω0.811.2Fig. 4. The power spectrum of the motion around theperiod-3 saddle found in Fig. 3.1750137-5Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liuis assumed as followsx(t) = c + r(t) cos(ωt + θ(t))+ r3(t) cos+ ϑ(t)(cid:8)(cid:8)(cid:7)(cid:7)2ωt3ω3+ r13(t) cos+ Θ(t)(5)where c is a constant which is used to express theoﬀset of the trajectory caused by the constant forcef0 r is the amplitude of the period-1 componentr3 and r13 are the amplitudes of the period-3 com-ponents respectively and θ ϑ and Θ are the phasesof these components respectively. However theseven independent variables contained in Eq. (5)will lead to much diﬃculty in the following calcula-tion. Since the last term in Eq. (5) is the one withthe minimum energy according to Fig. 4 it is rea-sonable to ignore the last term to simplify the fol-lowing calculation. And the following results shownin Fig. 6 demonstrate that this approximation isappropriate and valid. Then the new approximatesolution is given below(cid:7)(cid:8)(cid:8)x(t) = c + r(t) cos(ωt + θ(t)) + r3(t) cos2ωt3= c + a(t) cos(ωt) − b(t) sin(ωt) + d(t) cos(cid:7)(cid:9)y(t) = −ωr(t) sin(ωt + θ(t)) − 2ω3r3(t) sin2ωt3(cid:7)= −ω[a(t) sin(ωt) + b(t) cos(ωt)] − 2ω3+ ϑ(t)2ωt3(cid:7)(cid:8)2ωt3− e(t) sin(cid:8)(6)+ ϑ(t)(cid:7)(cid:8)+ e(t) cos(cid:7)(cid:8)(cid:10).2ωt32ωt3d(t) sinThe van der Pol variables a b c d and e areintroduced to transform the periodic solutions inthe phase space (x y) to the ﬁxed points in the vander Pol phase space (a b c d e). Via the compari-son between Figs. 6(b) and 3(b) it is easy to verifythat the expressions in Eq. (6) give well the approx-imations of either the period-3 solution [comparingFig. 6(b) with Fig. 3(b)] under the condition thatr3(t) (cid:5)= 0 or d(t)e(t) (cid:5)= 0 or the period-1 solution ifr3(t) = 0 or d(t) = 0 e(t) = 0 [He & Habib 2013;Roy & Nauman 1995]. With the assumption of theapproximate solution given in Eq. (6) it is reason-able to consider all the variables of a b c d and eas slow variables.N0 =ω6πN2i−1 =0(cid:11) 6π/ω(cid:11) 6π/ω(cid:11) 6π/ωω3π0N2i =ω3πf(x) sin0f(x)dt(cid:7)f(x) cos(cid:7)(i + 1) ωt3(cid:8)(i + 1)ωt3dt.Next the piecewise linear function deﬁned inEq. (2) is expanded along the trajectories given byEq. (6) [Roy 1997b] for which the expansion isgiven by(cid:7)(cid:8)2ωt3(cid:7)(cid:8)2ωt3f(x(t)) = N0 + N1 cos+ N2 sin+ N3 cos(ωt) + N4 sin(ωt)(7)where each of Ni i = 0 . . .  4 is the piecewise func-tion of the van der Pol variables c a b d and e andcan be obtained easily through a simple integral off(x) or f(x) multiples of an orthogonal triangle ifc a b d and e are given i.e.(cid:8)dtfor i = 1 2.(8)Since N0 N1 N2 N3 N4 are all piecewise and nonsmooth functions wherein the variations of N0 andN2 versus the variables a are displayed in Fig. 5 then the approximation expression given in Eq. (7) is still1750137-6Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137Noise-Induced Chaos in a Piecewise Linear System2N0N 3210−1−2−3−4N0N2−20a24Fig. 5. The multivariate functions N0(c a b d e) N2(c a bd e) plotted versus the variable a.piecewise and nonsmooth in the van der Pol vari-able space just like that given in Eq. (2) which isthe piecewise and nonsmooth function of the statevariable.Consider the state equations for Eq. (1) asgiven by (cid:12)˙x(t) = y(t)˙y(t) + 2ζy(t) + f(x) = F (t).(9)After substituting Eq. (6) into Eq. (9) through aseries of algebraic calculations four governing equa-tions on the variables of (a b d e) are obtained.Then the substitutions of Eqs. (7) and (3) intoEq. (9) and the application of the stochastic averag-ing method lead to a set of averaged Itˆo stochasticdiﬀerential equations i.e.√Dω dW1(t)da(t) =db(t) =1212dd(t) =de(t) =112112ω2b(t) + N4 − 2ζa(t)ωdt +−2ζb(t)ω + N3 − f1 − ω2a(t)ωωdt +9N2 + 4ω2e(t) − 12d(t)ωζωdt +−12ζe(t)ω − 4ω2d(t) + 9N1ωdt +√Dω dW2(t)√3D2ω dW3(t)√3D2ω dW4(t)(10)where each Wi(t) for i = 1 . . .  4 is an unit Wiener process.However Eq. (10) is not complete there is a lack of governing equation for the van der Pol variable cwhich is an algebraic one and is obtained through the condition that the integral average value of ¨c(t) over1.510.50dses0.5y0se ds−0.50.811.2ω(a)1.41.6−0.50.811.41.61.2x(b)(a) Nonvanishing ds es of the solutions of Eq. (13) are plotted versus ω and (b) the two-dimensional phase trajectoryFig. 6.on (x y) plane corresponding to the period-3 saddle when ω = 1.4.1750137-7Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liuthe interval of [0 6π/ω] for the variable t vanishes i.e.a cos(ωt) − b sin(ωt) +¨x(t) + ω2(cid:11) 6π/ω(cid:5)ω6π0(cid:9)(cid:7)49d cos(cid:7)(cid:8)23ωt(cid:8)(cid:8)(cid:10)(cid:6)(cid:7)23ωt− e sindt = 0.(11)By substituting Eqs. (1) (3) and (7) into Eq. (11)the algebraic equation is then obtained i.e.N0 = f0.(12)With Eqs. (10) and (12) a complete set of gov-erning equations for (a b c d e) is gained whichunder the assumption given in Eq. (6) is an Itˆo sys-tem with a higher dimension than those of the Itˆosystem considered in [Roy 1997a; Roy & Nauman1995]. Based upon this system the phenomenon ofnoise-induced chaos is predicted.Firstly the nonlinear deterministic version ofthe averaged equations is considered to whichcs as bs ds and es represent the steady-state solu-tions. The equations are given byω2bs + N4 − 2ζasω2ω= 0−2ζbsω + N3 − f1 − ω2as2ω9N2 + 4ω2es − 12dsωζ12ω= 0= 0(13)−12ζesω − 4ω2ds + 9N1= 0N0(as bs cs ds es) − f0 = 0.12ωIt is noted that the hyperbolic ﬁxed points ofthe averaged system correspond to the hyperbolicperiodic orbits in the original system respectively[Guckenheimer & Holmes 1983]. Among all thethree solutions to Eq. (13) there is only one solu-tion possessing the nonvanishing ds and es whichare the functions of ω and are depicted in Fig. 6(a)and under the condition that ω = 1.4 the two-dimensional phase trajectory on (x y) plane whichcorresponds to the period-3 saddleis shown inFig. 6(b).4. Estimation of the StationaryProbability Density FunctionIn Sec. 4 the stationary probability density functionof the four-dimensional Itˆo system that is deﬁned inEq. (10) is investigated in D which is the domainof interest and is completely inside the basin ofattraction of the stable equilibrium point mentionedabove [Fig. 3(a) shows that the basin of attractionis very small]. The boundary of D is denoted by ∂D.Suppose that x0 is a ﬁve-dimensional vectorwhich is deﬁned byx0 = [x1 x2 x3 x4 x5]T = [a b d ec]T .(14)Since x5 is governed by the algebraic equation (12)then x5 is a function of x1 x2 x3 x4. Thus the vec-tor stochastic process of x = [x1 x2 x3 x4]T which is deﬁned by Eq. (10) forms a vector diﬀu-sion process and its stationary probability densityfunction p(x) satisﬁes the following FPK equationL[p(x)]4(cid:13)=(cid:5)−∂i[bi(x)p(x)] +ij=1∂∂xi∂i =∂2ij =∂2∂xi∂xj(cid:6)ij[aij(x)p(x)]∂2= 02(15)where  = D/ω2 is a small quantity and bi and aijare respectively the drift coeﬃcients and the diﬀu-sion coeﬃcients which are deﬁned by1212112112A(x) =b1(x) =b2(x) =b3(x) =b4(x) = =1 00 10 00 00094000094a11 a12 a13 a14a21 a22 a23 a24a31 a32 a33 a34a41 a42 a43 a44ω2b + N4(x) − 2ζaωω−2ζbω + N3(x) − f1 − ω2aω9N2(x) + 4ω2e − 12dωζ−12ζeω − 4ω2d + 9N1(x)ωω.(16)1750137-8Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137Noise-Induced Chaos in a Piecewise Linear SystemFor the matrix A aij = 0 for i (cid:5)= j i j = 1 . . .  4and aii (cid:5)= 0 for i = 1 . . .  4. This particular simpliﬁ-cation is suitable for all the systems that are underthe external random excitations and dramaticallyreduces the complexity of the solution process forthe FPK equation (15).For p(x) the normalization condition andboundary condition are respectively given by(cid:11)p(x)dx = 1R5p(x) = 0 x ∈ ∂D.(17)(18)In order to ﬁnd the solution to the FPK equa-tion (15) in the limit  → 0 the Wenzel–Kramers–Brillouin (WKB) approximation is applied andbased on which the approximation of the expres-sion of the stationary probability density functionasymptotically is assumed as− ψ(x)(cid:10) + O()p(x) = w(x) expq(x)(19)(cid:9)wherein the function ψ(x) is the quasi-potentialfunction w(x) is a prefactor function and q(x) isthe boundary layer function which satisﬁes the fol-lowing condition(cid:12)q(x) = 0 x ∈ ˜Dq(x) = 1 x /∈ ˜D(20)where ˜D denotes a thin boundary layer close to theboundary ∂D. It is impossible to obtain the solu-tion over the entire R4 however the solution overthe domain D is investigated instead.By substituting Eq. (19) into Eq. (15) the fol-lowing equation is obtained4(cid:13)q(x)L(z(x )) − z(x )L(ψ(x))(cid:8)aii∂iψ(x)∂iz(x ) = 0(cid:7)+ i=1z(x ) = w(x) exp− ψ(x)(21)Based on Eq. (20) Eq. (21) is divided into two equa-tions i.e.L[z(x )] = 0 x /∈ ˜D4(cid:13)−z(x )L(q(x)) + i=1aii∂iq(x)∂iz(x ) = 0x ∈ ˜D(23)which means that the ﬁrst term of Eq. (21) vanishesoutside the boundary layer and the other terms van-ish inside the boundary layer.The solution to the ﬁrst equation of Eq. (23)is then investigated. By expanding the ﬁrst equa-tion of Eq. (23) and identifying the terms of theorder −1 and 0 two independent partial diﬀeren-tial equations of the ﬁrst order are obtained the onethat governs ψ(x) is the Hamilton–Jacobi equation(24)and the other one that governs w(x) is the transportequationL(ψ(x)) = 0− 4(cid:13)i=1(cid:7)− 4(cid:13)i=1((bi(x) + aii∂iψ(x))∂iw(x))(cid:8)∂ibi(x) +12aii∂2iiψ(x)w(x) = 0.(25)Based on the theory of partial diﬀerential equationof the ﬁrst order Eqs. (24) and (25) are both solv-able by using the rays method [Ludwig 1975]. Therays of these two equations are governed by ordi-nary diﬀerential equations and along each ray ψ(x)and w(x) are also governed by ordinary diﬀeren-tial equations respectively. With appropriate initialconditions a family of rays is found to cover thedomain D and then approximate stationary proba-bility density function is obtained in the domain D.The equations governing the rays of Eq. (24) aregiven belowwhere under the condition that aij = 0 for i (cid:5)=j i j = 1 . . .  4 the operator L is the substitu-tion of the adjoint operator of the Fokker–Planckoperator L deﬁned in Eq. (15) i.e.˙xi = bi(x) + aiipi(x)˙pi = − 4(cid:13)j=1(∂ibj(x)pj(x))for i = 1 . . .  44(cid:13)i=1L =bi∂i +124(cid:13)iaii∂2ii.(22)(26)where pi(x) = ∂iψ(x). Supposing that the coordi-nate along the rays is denoted by s then ˙xi ˙pi are1750137-9Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liurespectively deﬁned by ˙xi = dxieach ray ψ(x) is governed byds  ˙pi = ∂2ψ(x)∂xi∂s . Along4(cid:13)i=1˙ψ =12aiip2i .(27)With Eqs. (26) and (27) a family of rays that coversthe whole domain of D is ﬁrst obtained and thenthe function of ψ(x) on each ray is determined.Via the same method the rays deﬁned byEq. (25) are obtained which are the same as thosethat are given by Eq. (26). Along each of the raysw(x) is governed by(cid:9)˙w = − 4(cid:13)i=1∂ibi(x) +12(cid:10)aii∂ipi(x)w(x).(28)It is noted that Eq. (28) is diﬀerent from Eq. (27)for Eq. (28) there are second partial derivatives ofψ(x) involved thus w(x) cannot be determined byEqs. (26) and (28).To determine these second partial derivativesof ψ(x) each equation in Eq. (26) needs to be dif-ferentiated with respect to the three coordinates ofthe four-dimensional space where the rays lie i.e.θ ϕ φ. Thus along each ray 24 ODEs are obtainedi.e.lj∂jbi + aiiqimj∂jbi + aiirinj∂jbi + aiisij=1j=1˙ni = ∂φ ˙xi =˙mi = ∂ϕ ˙xi =˙li = ∂θ ˙xi =4(cid:13)4(cid:13)4(cid:13)˙qi = ∂θ ˙pi = − 4(cid:13)˙ri = ∂ϕ ˙pi = − 4(cid:13)˙si = ∂ϕ ˙pi = − 4(cid:13)k=1k=1j=1j=14(cid:13)4(cid:13)4(cid:13)j=1k=1j=1for i = 1 . . .  4.(29)[(lj ∂2ijbk)pk + qj∂ibj][(mj∂2ijbk)pk + rj∂ibj][(mj∂2ijbk)pk + rj∂ibj]However the second partial derivatives that arise in Eq. (28) cannot be solved directly from the above 24ODEs but from the following relationships based on the chain rule i.e.4(cid:13)4(cid:13)4(cid:13)4(cid:13)qi =lj∂2ijψri =mj∂2ijψsi =nj∂2ijψ˙pi =˙xj∂2ijψfor i = 1 . . .  4.(30)j=1j=1j=1j=1The situation now is that the 12 second partialderivatives of ψ should be solved from the 16 lin-ear algebraic equations which is obviously over-constrained so the least square method is appliedhere. Denote the inhomogeneous term and the coef-ﬁcient matrix of the above linear equations by Nand C respectively then the solution vector x whichconsists of 12 second partial derivatives of ψ is eval-uated byX = (CTC)−1CN.(31)Thus far 34 ODEs are obtained from the com-bination of Eqs. (26) (29) and (30) however thecorresponding initial conditions are missing whichaccording to characteristics theory of ﬁrst order par-tial diﬀerential equation [Ludwig 1975] will be dis-cussed on a small hypersphere in R4 with radius α.1750137-10Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.(32)(33)(34)August 28 201713:51 WSPC/S0218-12741750137Noise-Induced Chaos in a Piecewise Linear SystemSince in the limit α → 0 the stationary probability density function is approximately Gaussian around thestable equilibrium point then the initial conditions are readily derived i.e.x20 = x∗4 + α cos(θ)x40 = x∗2 + α sin(θ) sin(ϕ) cos(φ)x10 = x∗1 + α sin(θ) sin(ϕ) sin(φ)x30 = x∗4(cid:13)3 + α sin(θ) cos(ϕ)kij(xj0 − x∗j )4(cid:13)pi0 =j=1for i = 1 . . .  4kij(xi0 − x∗i )(xj0 − x∗j) w0 = 1ψ0 =123 x∗ij=1where x∗ = (x∗element of the inverse of the covariance matrix S which is derived from2 x∗1 x∗4) is the particular stable equilibrium point given in Sec. 3 and each kij is theB = [∂jbi(x∗)] BS + SB = −A K = S−1.The other initial conditions are given as belowl4 = −α sin(θ)l1 = α cos(θ) sin(ϕ) sin(φ)m1 = α sin(θ) cos(ϕ) sin(φ) m2 = α sin(θ) cos(ϕ) cos(φ) m3 = −α sin(θ) sin(ϕ) m4 = 0r1 = α sin(θ) sin(ϕ) cos(φ)r2 = −α sin(θ) sin(ϕ) sin(φ)l2 = α cos(θ) sin(ϕ) cos(φ)l3 = α cos(θ) cos(ϕ)r3 = 0r4 = 04(cid:13)4(cid:13)4(cid:13)ni =kijljqi =kijmjsi =kijnjfor i = 1 . . .  4.j=1j=1j=1In brief the solution to the ﬁrst equation of Eq. (23)is obtained.Next the solution to the second equation inEq. (23) which is the governing equation of theboundary layer function q(x) is then investigated.Before the analysis it is supposed that the bound-ary ∂D is noncharacteristic which means that thefollowing condition holds everywhere on the bound-ary ∂D i.e.b · v < 0(35)where b is the drift vector of the system (10) andv is the outer normal vector on the boundary ∂D.This condition is satisﬁed readily by choosing thedomain D appropriately.In order to analyze the boundary layer functionin the thin boundary layer ˜D two new local coordi-nates are introduced (ρ s) = (ρ s1 s2 s3) whereρ = ρ(x) characterizes the distance between thepoint x and the boundary ∂D and s = [s1 s2 s3]are the local coordinates along the boundary. Sinceevery point x near ∂D corresponds to only one raydeﬁned by Eq. (26) then only one point xb on theboundary is found via each ray which correspondsto x. Thus if the condition Eq. (35) is satisﬁed ρis deﬁned byρ(x) = ψ(xb) − ψ(x).(36)And a stretched variable needs to be introduced i.e.µ =ρ.(37)Based on the new local coordinates the new bound-ary layer function is then given byQ(µ s) = q(x).(38)With the new boundary layer function and newlocal variables via a simple computation and iden-tifying the terms of the order −1 a new boundarylayer equation governing Q(µ s) is obtained i.e.L(ψ(x))∂µQ(µ s)4(cid:13)(cid:20)+12(cid:21)aii(∂iψ(x))2µµQ(µ s) = 0∂2i=1(39)of which the simple form can then be obtainedon the basis of the Hamilton–Jacobi equation (24)1750137-11Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liui.e.µµQ(µ s) + ∂µQ(µ s) = 0.∂2(40)Combined with the boundary condition given inEq. (20) the solution is ﬁnally given byQ(µ s) = 1 − exp(−µ)(41)which is the ﬁrst-order approximation of the bound-ary layer function.With the boundary layer function deﬁned inEq. (41) the 34 ODEs given by Eqs. (26) to (29)combined with their initial conditions given byEqs. (32) to (34) and the 16 linear equations deﬁnedin Eq. (30) the stationary probability density func-tion deﬁned in Eq. (19) are obtained along eachray in the domain D. Although this method leadsto a set of 34 ODEs which is a little complicatedthrough it the stationary probability density func-tion for a high-dimensional Itˆo system is obtainedwhich is the preparation for the estimation of theMFPT.5. Estimation of the MFPTIn Sec. 5 the exit problem of the noise-driven piece-wise linear system that exhibits transient chaosis investigated. Based on the singular perturbedmethod some results of MFPT and the distribu-tion of exit points were given by Z. Schuss and hisco-workers [Naeh et al. 1990; Bobrovsky & Schuss1982] and the quasi-potential plateaus were foundin the systems that exhibit transient chaos [Kraut &Feudel 2003b]. In this work based on the conceptof the quasi-potential plateaus the phenomenon ofnoise-induced chaos which is found in Sec. 2 is mea-sured by MFPT. The simpliﬁed system with respectto the period-3 saddle and the period-1 attrac-tor has been established in Sec. 3. Then with thepreparation that has been completed in Sec. 4 thephenomenon of noise-induced chaos can be stud-ied through analyzing the exit problem between theperiod-1 attractor and the period-3 saddle in thesimpliﬁed system given by Eqs. (10) and (12).In the following context the derivation ofMFPT is just sketched at ﬁrst for the complete-ness of this paper. In addition since the model hereis about a system with much higher dimensions andmore complicated dynamical structures the resultsobtained by Z. Schuss and his co-workers are notconcise enough to deal with the problem directlyhere thus some approximation method is neededto simplify the formula.(cid:11)(cid:11)T0 =Suppose that τ(x) is the exit time of the tra-jectory which starts at the point x ∈ D and reachesthe boundary ∂D for the ﬁrst time. The Pontryaginequation which τ(x) needs to satisfy is given asLτ(x) = −1 x ∈ Dx ∈ ∂Dτ(x) = 0(42)where D is the domain within a four-dimensionalspace then ∂D is a three-dimensional hypersurface.The solution to Eq. (42) is assumed asτ(x) = T0τq(x)(43)where τq(x) is the boundary layer function and T0is a constant which according to the second inter-pretation in [Naeh et al. 1990] is formulated byDp(x)dx1dx2dx3dx4J(x) · v(x)ds1ds2ds3∂D(44)where J is the stationary probability current den-sity and the components Ji of which are deﬁnedasJi = bip(x) − 2∂j(aijp(x))(45)4(cid:13)j=1and v is the outer normal vector of the boundary∂D each si for i = 1 . . .  3 is the variable of inte-gration along the boundary ∂D.Before the calculation for Eq. (44) the domainD and its boundary ∂D are needed to be deﬁnedspeciﬁcally [Roy 1997b; Roy & Nauman 1995; Roy1994b 1997a 1995] i.e.(cid:12)D = {x| ψ(x) ≤ ˆψ}∂D = {x| ψ(x) = ˆψ}(46)where ˆψ is a constant determined artiﬁcially. Thecondition Eq. (35) of noncharacteristic boundariesis satisﬁed by an appropriate choice of ˆψ. And underthe deﬁnition of Eq. (46) the outer normal vectorof ∂D is derived i.e.[p1v =p4].(47)p3p2(cid:22)(cid:23)(cid:23)(cid:24) 4(cid:13)p2ii=11750137-12Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137Noise-Induced Chaos in a Piecewise Linear SystemT0 =DBy substituting Eqs. (19) (45) (47) and (41)into Eq. (44) the formula of MFPT is derived:(cid:8)− ψ(x)(cid:11)(cid:11)∂Dw(x) exp(cid:7)4(cid:13)w(x)(cid:22)(cid:23)(cid:23)(cid:24) 4(cid:13)2ij=1i (x)p2q(x)dx1dx2dx3dx4(cid:8)(cid:7)− ψ(xb).ds1ds2ds3aijpi(x)pj(x) expi=1(48)However due to the high-dimensional integral inEq. (48) this result is hardly available for a directcalculation then the Laplace approximation andthe condition given in Eq. (20) are applied to reducethe complexity of the calculation which leads to(cid:22)(cid:23)(cid:23)(cid:24) 4(cid:13)2π√2(cid:30)T0 =i (xb) expp2i=1det(H(x∗))w(xb)(cid:8)(cid:29)(cid:7)4(cid:13)ψ(xb)det(H(xb))aijpi(xb)pj(xb)ij=1(49)where the matrix H is the Hessian matrix of ψ(x)and xb is the point where the ray and the bound-ary intersect. In comparison with Eq. (48) Eq. (49)is simpler and furthermore for the estimation ofMFPT there is no need to calculate a family ofrays that cover the domain D just a calculation forone ray which connects the initial point x0 that isgiven in Eq. (32) and the point of xb is enough.With Eq. (49) the MFPT under a set of param-eters mentioned above is estimated and compareswith the numerical result simulated by the MonteCarlo method. However during the analysis pro-cess the authors have to face a new problem ofwhich the matrix CTC that is deﬁned in Eq. (31)is ill-conditioned and directly leads to its inaccu-rate inverse matrix. The condition number is usedto characterize how ill-conditioned the matrix CTCis. Figure 7(a) shows the variation of the conditionnumber of the matrix CTC versus the coordinate salong one ray.In Fig. 7(a) the ill-conditioned degree is veryserious. For example if s = 34.25 the conditionnumber of CTC is 2.1358 × 109. Under this condi-tion if there are ten linear equations the coeﬃcientmatrix of full rank can be selected from the 16 lin-ear equations in Eq. (30) there are 2304 choices inwhich the maximum condition number of the coef-ﬁcient matrix is 1.18 × 108 and the minimum oneis 4.41 × 104. There is no doubt that the problemof ill-conditioned matrix may lead to the incorrectsolution or no solution of w(x) to Eq. (19). AndFig. 7(b) shows how the ill-conditioned matrix inﬂu-ences the prefactor function w(x) and the functionw(x) is O(1) before the matrix gets ill-conditioned.It is noted that these numerical complexities arecaused by the nonsmoothness eﬀect of the piecewiselinear function and cannot be avoided by choosingx 1042.5 rebmuNnoitidnoC21.510.5001020s(a)30403.532.5w(x)21.5101020s(b)3040(a) The condition number of the matrix CTC in Eq. (31) during the process of calculation for one ray. In order toFig. 7.display the variation of the condition number the peak value of the condition number does not appear in this ﬁgure and(b) the function w(x) in Eq. (19) during the same process for the same ray.1750137-13Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. Liua much smaller step size in numerical process [Hu1995]. Detailed discussions about these numericalcomplexities are beyond the scope of this paperand will be given in the further work. Here we fol-low the method used in [Roy 1996] and [Ludwig1975]. Since w(x) is of the order O(1) before thematrix gets ill-conditioned as shown in Fig. 7(b)the term w(xb) in Eq. (49) is then ignored withoutloss of much accuracy. The ﬁnal solution of MFPTis obtained i.e.(cid:22)(cid:23)(cid:23)(cid:24) 4(cid:13)(cid:30)i=1√2T0 ≈2πi (xb) expp2(cid:8)(cid:29)ψ(xb)(cid:7)4(cid:13)det(H(xb)).det(H(x∗))aijpi(xb)pj(xb)ij=1(50)It is remarked that in Eq. (31) the matrix C isinvolved in the calculation of the matrix H soit must be ensured that the matrix C is not ill-conditioned during the calculation.As discussed in Sec. 2 the case of ω = 0.89is selected as the typical one for the estimation ofthe MFPT. There are two solutions to Eq. (13) oneis the period-1 solution which is under the condi-tion that as = −4.6291 bs = −1.7403 cs = 0.2874ds = 2.9451e − 4 es = −5.0282e − 6 the otheris the period-3 solution which is under the condi-tion that as = −0.6931 bs = −0.0779 cs = 0.6308ds = 0.2529 es = 1.1417. These results demon-strate that the approximation Eq. (6) gives appro-priate expressions of both the subharmonic solution(period-3) and the harmonic solution (period-1)and the dynamical behaviors such as the subhar-monic motions and the harmonic ones given by thedeterministic version of the averaged system matchthe ones of the original system.On the basis of the solutions obtained aboveand the result given in Eq. (50) domain D is deﬁnedvia Eq. (46) wherein ˆψ = 0.00069 is selected. Andthe methods used above like the averaging methodand WKB approximation are valid in the case → 0 i.e. D → 0 while the Monte Carlo simu-lations can be only applied with ﬁnite noise inten-sity D. Thus D−1 values in Fig. 8 are chosen tovary between 5000 and 18 000. When D is chosen inthis interval D is very small and of order O(10−5)and meanwhile the numerical simulations do notcost too much time. Then the ﬁnal result of MFPTis estimated and the curve is plotted in Fig. 8i)T(emT egassaP  tsriFnaeM105102.4104102.26000700080001031020.60.81Monte Carlo simulationsolution of Eq. (50)1.21/D1.41.61.8x 104Fig. 8. The MFPT with Eq. (50) is compared with the resultsimulated by the Monte Carlo method versus D−1. Inset: Thedetails of a part of the main ﬁgure when D is large.which matches the result given by the Monte Carlosimulation. The simulation results are obtained byaveraging 10 000 samples at each data point. Thesetwo curves are both displayed versus D−1 on thesemi-logarithmic plot to get a clear observation ofthe almost linear relationship between the loga-rithm of MFPT and D−1 according to Eq. (50).As the noise intensity D becomes largei.e.D−1 becomes small the error between these tworesults also becomes large as shown in the inset ofFig. 8. This fact is caused by Eq. (50) which isvalid in the case D → 0. When the noise inten-i.e. D−1 becomes largesity D becomes smallone can claim the agreement between these tworesults. These results demonstrate that the tran-sitions between the attractor and the saddle insimpliﬁed system Eq. (10) can be used to analyzethe noise-induced chaos and also demonstrate thatEq. (50) is useful to estimate the phenomenon ofnoise-induced chaos and the inﬂuence of the chaoticsaddle in the exit problem quantitatively.6. Conclusions and DiscussionsThe phenomenon of noise-induced chaosin apiecewise linear system that is subjected to bothGaussian white noise and harmonic excitation isinvestigated. This phenomenon depends on the exis-tence of a chaotic saddle around a regular attrac-tor. Noise drives the system to switch between thechaotic saddle and the regular attractor intermit-tently. So the onset and termination of this phe-nomenon are partly determined by the deterministicbifurcation which creates and destroys the chaotic1750137-14Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137saddle. And we choose the concept of MFPT tomeasure this phenomenon of noise-induced chaosquantitatively. After the onset of the noise-inducedchaos a tiny noise can induce large displacementsfrom the deterministic stable state and the MFPTis expected to be ﬁnite. Compared to regular situa-tions such a tiny noise could be expected to inducetiny displacements from the deterministic stablestate and the MFPT is expected to be inﬁnite.For the relevant system in the absence of noisethrough the numerical GCMD method a chaoticsaddle is ﬁrst found just around the attractor. Itis noted that for the system of structural instabil-ity even a very small noise excitation leads to atransition between the chaotic attractor and a peri-odic motion [Fig. 1(a)]. Based on an assumptionof the approximate expression about the trajectoryof the periodic motions the piecewise linear sys-tem is simpliﬁed to a four-dimensional Itˆo systemby using stochastic averaging method. Thus for thenoise-driven system the investigation on the phe-nomenon of noise-induced chaos in the original sys-tem is replaced by the one on the phenomenon oftransition between a stable attractor and a saddle inthe simpliﬁed system. Then the formula of MFPT isobtained by using the singular perturbation methodand the Laplace approximation. In this process theeﬀect of boundary layer is considered.Within the analysis a serious problem theauthors have to face is that an ill-conditionedmatrix arises and hinders the calculation of MFPT.Since the condition number of this matrix is toolarge to obtain the accurate inverse matrix onemore approximation is applied to simplify the ﬁnalform of MFPT. The comparison between the ﬁnalresult and the numerical simulation conﬁrms thatthe approximation method is available to character-ize the phenomenon of noise-induced chaos quanti-tatively and the transitions between the attractorand the saddle in simpliﬁed system can be used toanalyze the noise-induced chaos.Although Eq. (1) is a general piecewise linearsystem we have noticed that in other types of non-linear systems not all noise-induced chaos can beapproximated to transitions between an attractorand a saddle. If this approximation fails the ana-lytical procedure cannot work. Besides this whenthe intensity of noise exceeds some level noise-induced chaos disappears in this piecewise linearsystem. To characterize both the onset and termi-nation of noise-induced chaos more concepts andNoise-Induced Chaos in a Piecewise Linear Systemtechniques of random dynamics should be involvedlike random attractors and the stochastic bifurca-tion. To devise a more general method to studythe onset and termination of noise-induced chaosphenomenon in more complicated nonlinear systemswith multiple stable periodic attractors will be thefuture direction of our works.AcknowledgmentsThis work was supported by the National NaturalScience Foundation of China (No. 11472126) theResearch Fund of State Key Laboratory of Mechan-ics and Control of Mechanical Structures (NanjingUniversity of Aeronautics and Astronautics) (GrantNo. MCMS-0116G01) and a Project Funded bythe Priority Academic Program Development ofJiangsu Higher Education Institutions (PAPD).ReferencesArmbruster D. Stone E. & Kirk V. [2003] “Noisy het-eroclinic networks” Chaos 13 71–79.Aubry N. Holmes P. Lumley J. L. & Stone E. [1988]“The dynamics of coherent structures in the wallregion of a turbulent boundary layer” J. Fluid Mech.192 115–173.Billings L. & Schwartz I. B. [2002] “Exciting chaos withnoise: Unexpected dynamics in epidemic outbreaks”J. Math. Biol. 44 31–48.Bobrovsky B. Z. & Schuss Z. [1982] “A singular pertur-bation method for the computation of the mean ﬁrstpassage time in a nonlinear ﬁlter” SIAM J. Appl.Math. 42 174–187.Chen L. C. Deng M. L. & Zhu W. Q. [2009] “Firstpassage failure of quasi integrable-Hamiltonian sys-tems under combined harmonic and white noise exci-tations” Acta Mech. 206 133–148.Chen L. C. & Zhu W. Q. [2010] “First passage failure ofquasi-partial integrable generalized Hamiltonian sys-tems” Int. J. Non-Lin. Mech. 45 56–62.Dellnitz M. Froyland G. & Junge O. [2001] “The algo-rithms behind GAIO-set oriented numerical methodsfor dynamical systems” Ergodic Theory Analysisand Eﬃcient Simulation of Dynamical Systems(Springer) pp. 145–174.Dtchetgnia Djeundam S. R. Yamapi R. KofaneT. C. & Aziz-Alaoui M. A. [2013] “Deterministic andstochastic bifurcations in the Hindmarsh–Rose neu-ronal model” Chaos 23 033125.Ellner S. & Turchin P. [1995] “Chaos in a noisy world:New method and evidence from time series analysis”Am. Nat. 145 343–375.1750137-15Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.August 28 201713:51 WSPC/S0218-12741750137C. Kong & X.-B. LiuFreidlin M. I. & Wentzell A. D. [2012] Random Pertur-bations of Dynamical Systems 3rd edition (Springer).[1983] NonlinearOscillations Dynamical Systems and Bifurcations ofVector Fields (Springer-Verlag NY).Guckenheimer J. & Holmes P.Han Q. Xu W. & Yue X. [2014] “Global bifurcationanalysis of a Duﬃng–Van der Pol oscillator with para-metric excitation” Int. J. Bifurcation and Chaos 241450051-1–11.Han Q. Xu W. Yue X. & Zhang Y. [2015] “First-passage time statistics in a bistable system subject toPoisson white noise by the generalized cell mappingmethod” Commun. Nonlin. Sci. Numer. Simul. 23220–228.He T. & Habib S. [2013] “Chaos and noise” Chaos 23033123.Hikihara T. Perkins E. & Balachandran B.[2012]“Noise-enhanced response of nonlinear oscillators”Procedia IUTAM 5 59–68.Hong L. & Xu J. [2003] “Chaotic saddles in Wadabasin boundaries and their bifurcations by the gener-alized cell-mapping digraph (GCMD) method” Non-lin. Dyn. 32 371–385.Hong L. [2010] “A fuzzy crisis in a Duﬃng–van der Polsystem” Chin. Phys. B 19 1–6.Hsu C. S.Hong L. Zhang Y. & Jiang J. [2010] “A hyperchaoticcrisis” Int. J. Bifurcation and Chaos 20 1193–1200.[1987] Cell-to-Cell Mapping: A Methodof Global Analyis for Nonlinear Systems (Springer-Verlag NY).Hsu C. S. [1995] “Global analysis of dynamical systemsusing posets and digraphs” Int. J. Bifurcation andChaos 5 1085–1118.Hu H. [1995] “Simulation complexities in the dynamicsof a continuously piecewise-linear oscillator” ChaosSolit. Fract. 5 2201–2212.Hunt B. Ott E. & Yorke J. [1996] “Fractal dimensionsof chaotic saddles of dynamical systems” Phys. Rev.E 54 4819–4823.Klosek-Dygas M. Matkowsky B. J. & Schuss Z. [1988]“Stochastic stability of nonlinear oscillators” SIAMJ. Appl. Math. 48 1115–1127.Kong C. Gao X. & Liu X. [2016] “On the global anal-ysis of a piecewise linear system that is excited by aGaussian white noise” J. Comput. Nonlin. Dyn. 11051029-1.Kraut S. & Feudel U. [2002] “Multistability noise andattractor hopping: The crucial role of chaotic sad-dles” Phys. Rev. E 66 1–4.Kraut S. & Feudel U. [2003a] “Enhancement of noise-induced escape through the existence of a chaotic sad-dle” Phys. Rev. E 67 015204.Kraut S. & Feudel U. [2003b] “Noise-induced escapethrough a chaotic saddle: Lowering of the activationenergy” Physica D 181 222–234.Lin K. K. & Young L.-S. [2008] “Shear-induced chaos”Nonlinearity 21 899–922.Ludwig D. [1975] “Persistence of dynamical systemsunder random perturbations” SIAM Rev. 17 605–640.Naeh T. K(cid:5)losek M. M. Matkowsky B. J. & Schuss Z.[1990] “A direct approach to the exit problem” SIAMJ. Appl. Math. 50 595–627.Rodrigues C. S. Grebogi C. & De Moura A. P. S.[2010] “Escape from attracting sets in randomly per-turbed systems” Phys. Rev. E 82 1–5.Roy R. [1994a] “Averaging method for strongly non-linear oscillators with periodic excitations” Int. J.Non-Lin. Mech. 29 737–753.Roy R. [1994b] “Noise perturbations of a non-linear sys-tem with multiple steady states” Int. J. Non-Lin.Mech. 29 755–773.Roy R. V. [1995] “Noise-induced transitions in weakly-nonlinear oscillators near resonance” J. Appl. Mech.62 496–504.Roy R. V. & Nauman E. [1995] “Noise-induced eﬀectson a non-linear oscillator” J. Sound Vibr. 183 269–295.Roy R. [1996] “Probabilistic analysis of a nonlinear pen-dulum” Acta Mech. 115 87–101.Roy R. [1997a] “Asymptotic analysis of ﬁrst-passageproblems” Int. J. Non-Lin. Mech. 32 173–186.Roy R. V. [1997b] “Global stability analysis of nonlineardynamical systems” Uncertainty Modeling in FiniteElement Fatigure and Stability of Systems Series onStability Vibration and Control of Systems Series BVol. 9 (World Scientiﬁc) pp. 261–295.Stone E. & Armbruster D.[1999] “Noise and O(1)amplitude eﬀects on heteroclinic cycles” Chaos 9499–506.T´el T. Lai Y.-C. & Gruiz M. [2008] “Noise-inducedlong deterministic tran-chaos: A consequence ofsients” Int. J. Bifurcation and Chaos 18 509–520.Young L.-S. [1986] “Stochastic stability of hyperbolicattractors” Ergod. Th. Dyn. Syst. 6 311–319.Yue X. Xu W. & Zhang Y. [2012] “Global bifurca-tion analysis of Rayleigh–Duﬃng oscillator throughthe composite cell coordinate system method” Non-lin. Dyn. 69 437–457.Zhou C. S. Kurths J. Allaria E. Boccaletti S.Meucci R. & Arecchi F. T.[2003] “Constructiveeﬀects of noise in homoclinic chaotic systems” Phys.Rev. E 67 066220.1750137-16Int. J. Bifurcation Chaos 2017.27. Downloaded from www.worldscientific.comby HONG KONG POLYTECHNIC UNIVERSITY on 04/20/19. Re-use and distribution is strictly not permitted except for Open Access articles.
Observing_the_State_of_a_Linear_S Observing the State of a Linear SystemDAVID G. LUENBERGER STUDENT MEMBER IEEESummary-In much of modern control theory designs are basedon the assunption that the state vector of the system to be controlledis available for measurement. In many practical situations only a fewoutput quantities are available. Application of theories which assumethat the state vector is known is severely limited in these cases. Inthis paper it is shown that the state vector of a linear system can bereconstructed from observations of the system inputs and outputs.It is shown that the observer which reconstructs the state vectoris itself a linear system whose complexity decreases as the number ofoutput quantities available increases. The observer may be incorpo-rated in the control of a system which does not have its state vectoravailable for measurement. The observer supplies the state vectorbut at the expense of adding poles to the over-all system.I. INTRODUCTIONI N THE PAST few years there has been an increasingpercentage of control system literature written fromthe "state variable" point of view [1]-[8]. In thecase of a continuous time-invariant linear system thestate variable representation of the system is of theform:y(t) = Ay(t) +Bx(t)wherey(t) is an (n X 1) state vectorx(t) is an (m X1) input vectorA is an (nXn) transition matrixB is an (nXm) distribution matrix.This state variable representation has some con-ceptual advantages over the more conventional transferfunction representation. The state vector y(t) containsenough information to completely summarize the pastbehavior of the system and the future behavior isgoverned by a simple first-order differential equation.The properties of the system are determined by the con-stant matrices A and B. Thus the study of the systemcan be carried out in the field of matrix theory which isnot only well developed but has many notational andconceptual advantages over other methods.When faced with the problem of controlling a systemsome scheme must be devised to choose the input vectorx(t) so that the system behaves in an acceptable man-ner. Since the state vector y(t) contains all the essentialinformation about the system it is reasonable to basethe choice of x(t) solely on the values of y(t) and per-haps also t. In other words x is determined by a relationof the form x(t) = F[y(t) t].This is in fact the approach taken in a large portionof present day control system literature. Several newReceived November 2 1963. This research was partially sup-The author is with the Department of Electrical Engineeringported by a grant from Westinghouse Electric Corporation.Stanford University Stanford Calif.74techniques have been developed to find the function Ffor special classes of control problems. These techniquesinclude dynamic programming [8]- [10] Pontryagin'smaximum principle [11] and methods based on Lya-punov's theory [2] [12].In most control situations however the state vectoris not available for direct measurement. This meansthat it is not possible to evaluate the function F[y(t) t].In these cases either the method must be abandoned or areasonable substitute for the state vector must be found.In this paper it is shown how the available system in-puts and outputs may be used to construct an estimateof the system state vector. The device which recon-structs the state vector is called an observer. The ob-server itself as a time-invariant linear system driven bythe inputs and outputs of the system it observes.Kalman [3] [13] [14] has done some work on thisproblem primarily for sampled-data systems. He hastreated both the nonrandom problem and the problemof estimating the state when measurements of the out-puts are corrupted by noise. In this paper only the non-statistical problem is discussed but for that case a fairlycomplete theory is developed.It is shown that the time constants of an observer canbe chosen arbitrarily and that the number of dynamicelements required by the observer decreases as moreoutput measurements become available. The novel pointof view taken in this paper leads to a simple conceptualunderstanding of the observer process.II. OBSERVATION OF A FREE DYNAMIC SYSTEMAs a first step toward the construction of an observerit is useful to consider a slightly more general problem.Instead of requiring that the observer reconstruct thestate vector itself require only that it reconstruct someconstant linear transformation of the state vector. Thisproblem is simpler than the previous problem and itssolution provides a great deal of insight into the theoryof observers.Assuming it were possible to build a system which re-constructs some constant linear transformation T of thestate vector y it is clear that it would then be possibleto reconstruct the state vector itself provided that thetransformation T were invertible. This is the approachtaken in this paper. It is first shown that it is relativelysimple to build a system which will reconstruct somelinear transformation of the state vector and then it isshown how to guarantee that the transformation ob-tained is invertible.The first result concerns systems which have no in-puts. (Such systems are called free systems.) The situa-Luenberger: State of a Linear System75tion which is investigated is illustrated in Fig. 1. Thefree system is used to drive another linear system withstate vector z. In this situation it is nearly always truethat z will be a constant linear transformation of thestate vector of the free system.Theorem 1 (Observation of a Free System): Let S1 be afree system: y =Ay which drives S2: X=Bz+Cy. If Aand B have no common eigenvalues then there is aconstant linear transformation T such that if z(o)= Ty(o) then z(t) = Ty(t) for all t > 0. Or more generallyz(t) = Ty(t) + eSt[z(o) - Ty(o)].Proof: Notice that there is no need for A and B tobe the same size; they only have to be square.Suppose that such a transformation did exist; i.e.suppose that for all tz(t) = Ty(t).The two systems are governed byy= Ayx = Bz +Cybut using the relation z = TyTy = TAyTy=BTy+Cy.(1)(2)(3)Now since the left sides agree so must the right sides.This implies that T satisfiesTA-BT= C.(4)Since A and B have no common eigenvalues (4) willhave a unique solution T [15 ]. It will now be shown thatT has the properties of the theorem. Using (3)z-Ty =Bz-TAy+Cy.By using (4) this becomesz- Ty= B(z -Ty).(5)(6)This is a simple first-order differential equation in thevariable z - Ty. It has the well-known solutionz(t) = Ty(t) + eBt[z(o) - Ty(o)](7)which proves the theorem.The result of Theorem 1 may be easily interpreted interms of familiar linear system theory concepts. As asimple example consider the situation described byFig. 2. Here both SI and S2 are first order systems. It isclear from the figure that y(t) = y(o)eXt and that ay(o)extis the signal which drives S2. By elementary transformtheory it may be verified thatz(t) =y(o)X-t + elltLz(o) -a(8aX - Ay(o)I(8)Fig. 1-A simple observer.Fig. 2-Observation of a first-order system.So if the initial condition on z(o) is chosen asthen for all t>0z(o) =aX _ py(o)z(t)=ay(t)(9)(10)which is just a constant multiple of y. This type ofreasoning may be easily extended to higher-order sys-tems.The results of Theorem 1 would be of little practicalvalue if they could not be extended to include nonfreesystems. Fortunately thisextension is relatively straight-forward.Assume now that the plant or system Si that is tobe observed is governed byy= Ay+ Dx(11)where x is an input vector. As before an observer forthis system will be driven by the state vector y. Inaddition it is natural to expect that the observer mustalso be driven by the input vector x. Consider the sys-tem S2 governed by= Bz + Cy + Gx.(12)As before let T satisfy TA - B T = C. Then it followsthatx-Ty-=Bz-TAy + Cy + (G-TD)x(13)or using (4)z- T = B(z - Ty) + (G - TD)x.(14)By choosing G = TD the differential equation above canbe easily integrated givingz(t) = Ty(t) + eBt[z(o) - Ty(o)].(15)This shows that the results for free systems contained inTheorem 1 will also apply to nonfree systems providedthat the input drive satisfiesG = TD.(16)76IEEE TRANSACTIONS ON MILITARY ELECTRONICSAprilThis is what one might intuitively expect. The systemwhich produces Ty is driven with an input just equal toT times the input used to drive the system which pro-duces y.In applications then an observer can be designed fora system by assuming that the system is free; then anadditional input drive can be added to the observer inorder to satisfy (16). For this reason it is possible tocontinue to develop the theory and design techniquesfor free systems only.III. OBSERVATION OF THE ENTIRE STATE VECTORIt was shown in the last section that "almost" anylinear system will follow a free system which is drivingit. In fact the state vectors of the two systems will berelated by a constant linear transformation. The ques-tion which naturally arises now is: How does one guar-antee that the transformation obtained will be in-vertible?One way to insure that the transformation will beinvertible is to force it to be the identity transformation.This requirement guarantees that (after the initialtransient) the state vector of the observer will equal thestate vector of the plant.In the notation used here vectors such as a are com-monly column vectors whereas row vectors are repre-sented as transposes of column vectors such as a'.Assume that the plant has a single output vy= Ayv = aly(17)and that the corresponding observer is driven by v as itsonly inputorB=Rz+bv= Bz + baty.under these conditions z= Ty where T satisfiesForcing T= I givesTA -BT = ba'.B =A-ba'(18)(19)(20)(21)which prescribes the observer in this case. In (21) A anda' are given as part of the plant hence chosing a vectorb will prescribe B and the observer will be obtained.This solution to the observer problem is illustrated inFig. 3 and is the solution obtained by Kalman [3 ] usingother methods. For a sampled-data system he deter-mined the vector b so that the transient would die outin minimum time. In the continuous case presumablythe vector b would be chosen to make the transient dieout quickly.Fig. 3-Observation of the entire state vector.IV. REDUCTION OF DYNAMIC ORDERThe observer constructed above by requiring T= Ipossesses a certain degree of mathematical simplicity.The state vector of the observer is equal to the statevector of the system being observed. Further examina-tion will reveal a certain degree of redundancy in theobserver system. The observer constructs the entirestate vector when the output of the plant which repre-sents part of the state vector is available by directmeasurement.Intuitively it seems that this redundancy might beeliminated thereby giving a simpler observer system.In this section it is shown that this redundancy can beeliminated by reducing the dynamic order of the ob-server. It is possible however to choose the pole loca-tions of the observer in a fairly arbitrary manner.The results of this section rely heavily on the con-cepts of controllability and observability introduced byKalman [3] and on properties of the matrix equationTA - B T = C. Some new properties of this equation aredeveloped but first a motivation for these results isgiven in the form of a rough sketch of the method thatis used to reduce the dynamic order of the observer.Consider the problem of building an observer for annth-order system Si which has only one output. Let thissystem drive an (n - 1)th-order system S2. Then by theresults of Section II each state variable of S2 is a time-invariant linear combination of the state variables ofSi. Thus the n -1 state variables of S2 together withthe output of S1 give n quantities each of which is alinear combination of the state variables of Si. If thesedifferent combinations are linearly independent itispossible to find the state variables of SI by simple mati ix(no dynamics) operations. The scheme is illustrated inFig. 4.Another way to describe the method is in terms ofmatrix inversion. The state vector of S2 is given byz = Ty; but z has only n -1 components while y has ncomponents. This means that T is an (n-1) Xn matrixand so it cannot be inverted. However if another com-ponent that is a linear combination of the componentsof y is adjoined to the z vector one obtains an n-dimen-sional vector z1 = T1y where T is now an nXn matrixwhich may possess an inverse. The component adjoinedto the z vector in the scheme of Fig. 4 is the output of Si.1 964Luenberger: State of a Linear System77Fig. 4-Reduction of the dynamic order.It is appropriate at this point to review the definitionsof controllability and observability for linear time-invariant systems. A discussion of the physical inter-pretations of these definitions can be found in [3] and[16].Definition: The nth-order system y-Ay+Bx is saidto be completely controllable if the collection of vectors= 12 .. mspans n dimensions. (The bi are the m columns of thenXm matrix B.)As a notational convenience this situation will some-times be described by writing "(A B) is completelycontrollable. "Definition: The system-=Ay with output vectorv = B'y is said to be completely observable if (A' B) iscompletely controllable. As a notational conveniencethis situation will sometimes be described by writing"(A B') is completely observable."In the special case that A is diagonal with distincteigenvalues and B is just a column vector there is asimple condition which is equivalent to complete con-trollability [16].Lemma 1: Let A be diagonal with distinct roots. Then(A b) is completely controllable if and only if eachcomponent bi is nonzero.The following theorem which is proved in Appendix Iconnects complete controllability and complete ob-servability with the matrix equation TA -B T = C.Theorem 2: Let A and B be n X n matrices with nocommon eigenvalues. Let a and b be vectors such that(A a') is completely observable and (B b) is completelycontrollable. Let T be the unique solution of TA -BT= ba'. Then T is invertible.With this Theorem it is easy to derive a result concern-ing the dynamic order of an observer for a single outputsystem.Theorem 3: Let Si: y=Ay v=a'y be an nth-ordercompletely observable system. Let.I*n be aset of distinct complex constants distinct from theeigenvalues of A. An observer can be built for Si whichis of (n - 1)th-order and which has n -1 of the /.ti's aseigenvalues of its transition matrix.1u2.Proof: As a first attempt let S1 drive the nth-ordersystemMi0 11Y20=O1Z + VI.An IjjJ(22)where the /ii are arbitrary except that IAi 'tj for i$jand AiXk for all i and k. Now (under proper initialconditions) z = Ty and by Theorem 2 the n rows ti of Tare independent. It is clear that there is one ti whichmay be replaced by a' so that the (row) vectors tn a' will be independent. (If thist1 t2is not clear see Lemma 2 in Appendix IL.)By removing the ith dynamic element from the ob-server an (n - 1)th-degree system (with state vector z1)is obtained. The state vector y may be recovered fromthe n -1 components of zi and the output a'y since* ti1.z1V=t2ti-iti+ltnaty = YTy(23)and the matrix on the right is invertible. This provesTheorem 3.Note: By employing here the methods used in Ap-pendix I in the proof of Theorem 2 it can be shownthat the n-1 eigenvalues of the observer can in factbe chosen arbitrarily provided only that they are dis-tinct from those of A.At this point it is natural to ask whether these resultscan be extended to systems with more than one output.Theorem 4 which is proved in Appendix II states thatan nth-order system with m independent outputs canbe observed with n-m "arbitrary" dynamic elements.Theorem 4: Let S1 be a completely observable nth-order system with m independent outputs. Then an ob-server S2 may be built for Si using only n-im dynamicelements. (As illustrated by the proof the eigenvalues ofthe observer are essentially arbitrary.)In order to illustrate the results obtained in this sec-tion consider the system shown in Fig. 5. It may be ex-pressed in matrix form asY = _--[21-O2O -1_ Y + 1_X.(24)It will be assumed that y is the only measurable output.78IEEE TRANSACTIONS ON MILITARY ELECTRONICSAprilTo build an observer for this system observer eigen-values must be chosen. According to Theorem 3 anobserver can be constructed for this system using a sin-gle dynamic element. Suppose it is decided to requirethe observer to have -3 as its eigenvalue. The observerwill have a single state variable z and will be driven byy and x. The state variable z will satisfy0 ]y + kx(25)where k is determined by the input relation given by(16).1 Then z= Ty where T satisfiesz = -3z+ [ 1TA+3T=[ 1(26)This equation is easily solved giving T = [ 1- 1/2 ]. Toevaluate k (16) is used01.k=[ 1-1/2 ]=-1/2.(27)It is easy to see how to combine y and z to produce Y2.The final system is shown in Fig. 6. In the figure 92represents the observer's estimate of Y2.Fig. 5-A secoe pFig. 5-A second-order plant.Fig. 6-Observer and plant.V. APPLICATION TO CONTROL PROBLEMSThe primary justification for an investigation of ob-servers is its eventual application to control system de-sign. A control system can be regarded as performingthree operations: it measures certain plant inputs andoutputs; on the basis of these measurements it computescertain other input signals and it applies these input sig-nals to the plant. The philosophy behind this paper isthat the computational aspect of control should be di-vided into two parts. First the state vector should beconstructed; this is the job of the observer. Then theinputs can be calculated using this value for the statevector.A primary considerationthat arises when thisphilosophy is used is the extent that use of the estimatedoHerercorresponds to D and k corresponds to G in (16).state vector rather than the actual state vector de-teriorates the performance of control. Various criteriacan be used to measure this deterioration. One of themost important considerations is the effect that an ob-server may have on pole locations. It would be unde-sirable for example if an otherwise stable control de-sign became unstable when an observer is used to real-ize it. It is shown in this section that an observer has noeffect on a control system's closed-loop pole locationsother than to add the poles of the observer itself.Consider a linear plant: y-Ay+Dx which has all ofits state variables measurable and all of its input com-ponents available for control. It is then possible to de-sign a linear feedback system by putting x= Fy. Thisis a feedback system without dynamics. The closed-loopplant would be governed by y= (A +DF)y so that theeigenvalues of A +DF are the closed-loop poles of thesystem.Suppose the same plant is given except that not allstate variables are measurable. In this case an observerfor the plant might be used to construct an estimate9 of the plant state vector. The vector 9 could then beused to put x= F9. The closed-loop poles of this systemcan be found in terms of the poles of the observer andthe poles of the system above. Suppose the observer isgoverned by z=Bz+Cy+TDx where TA-BT== C.Then 9 is a linear combination of y and zwhere9 = Hy + KzH+KT = I.Putting x = F9 the over-all system becomesy = Ay + DF(Hy + Kz)z = Bz + Cy + TDF(Hy + Kz)or in matrix formA + DFHy [_yz LTC+TDFH B+TDFK ZJDFK(28)(29)(30)(Theorem 5: The eigenvalues of the over-all system(31) are the eigenvalues of A +DF and the eigenvaluesof B.Proof: For an eigenvalue XAy + DFHy + DFKz = XyCy + TDFHy + Bz + TDFKz = Xz.Multiplying (32) by T and subtracting (33)(TA - C)y - Bz = X(Ty - z).Using TA - B T this becomes(32)(33)(34)B(Ty - z) = X(Ty - z).(35)This equation can be satisfied if X is an eigenvalue of Bor if Ty =z. This shows that all eigenvalues of B (in-cluding multiplicity) are eigenvalues of the over-all sys-tem (31).1964Luenberger: State of a Linear System79Now if Ty= z (32) becomes(A + DFH + DFKT)y = Xy(36)but using (29) this reduces to(A + DF)y = Xy.(37)This equation immediately shows that all eigenvalues ofA +DF (including multiplicity) are also eigenvalues ofthe over-all system (31). This proves the theorem.Theorem 5 demonstrates that as far as pole locationproblems are concerned it is possible to design a feed-back system assuming the state were available and thenadd an observer to construct the state. There is still theproblem of what feedback coefficients to use if the statewere available.For a single input system it is possible to find feed-back coefficients to place the closed-loop poles anywherein the complex plane. This result can be obtained froma canonical form given by Kalman [17] or by a simpleapplication of Theorem 2.****Theorem 6: Given a completely controllable singleinput system: y=Ay+bx and a set of complex con-stants pi MU2* * tAn; there is a vector c such that ifclosed-loop system will havex=c'y the resultingAli A2 A as its eigenvalues.Proof: First assume that each ui is distinct from theeigenvalues of A. Let B be a matrix in Jordan formwhich has the ui as its eigenvalues and has only oneJordan block associated with each distinct eigenvalue[31. Let c1 be any vector such that (B c1') is completelyobservable. By Theorem 2 the equationTB - AT = bci'(38)has a unique solution T which is invertible. Let c'=c1'T-l thenA + bc' = TBT-1(39)which says A +bc' is similar to B. This establishes theresult.In case some of the Ai are not distinct from the eigen-values of A proceed in two steps. First choose coeffi-cients to make the eigenvalues distinct from those of Aand from the ui. Then move the eigenvalues of the re-sulting system to the /ui. This proves Theorem 6.Finally the results of Theorems 4-6 may be collectedto obtain a result for systems that do not have theirstate vector available. Suppose one is given an nth-order system with m independent outputs. Accordingto Theorem 4 an observer can be designed which hasn - m essentially arbitrary eigenvalues. If the state vec-tor were available constant feedback coefficients couldbe found to place the closed-loop eigenvalues arbitrarilyby the method of Theorem 6. Then according to Theo-rem 5 if the observer's estimate of the state is used inplace of the actual state the resulting system will havethe eigenvalues of the observer and the eigenvalues ofthe constant coefficient feedback system. This result isexpressed in Theorem 7.Theorem 7: Let S be an nth-order single input com-pletely controllable completely observable system withm independent outputs. Then a feedback network canbe designed for S which is (n-m)th-order and the re-sulting 2n - m poles of the closed-loop system are essen-tially arbitrary.VI. CONCLUSIONSIt has been shown that the state vector of a linear sys-tem can be reconstructed from observations of its inputsand outputs. The observer which performs the recon-struction is itself a linear system with arbitrary timeconstants. It has been shown that the dynamic orderof an observer which observes an nth-order system withm outputs is n- m. Hence when more outputs are avail-able a simpler observer may be constructed.Observers may be incorporated in the design of con-trol systems. If a feedback system has been designedbased on knowledge of the state then incorporation ofan observer to construct the state does not change thepole locations of the system. The observer simply addsits own poles to the system. Much work remains how-ever in the area of incorporation of observers in controlsystem design. The effects of parameter variations useof design criteria other than pole location and considera-tion of systems which are "marginally" observableshould be investigated.Most of the results given can be easily extended toinclude sampled-data systems. The necessary proofsare in fact often simpler in the sampled case. Likewisemany of the results can be extended to include time-varying linear systems.APPENDIX ITheorem 2: Let A and B be n Xn matrices with nocommon eigenvalues. Let a and b be vectors such that(A a') is completely observable and (B b) is completelycontrollable. Let T be the unique solution of TA - B T= ba'. Then T is invertible.Proof: Without loss of generality it may be assumedthat A is in Jordan Canonical Form [13] [18]. A willconsist of several Jordan blocks but since (A a') iscompletely observable no two blocks are associated withthe same eigenvalue [3]. Furthermore the componentof the vector a which corresponds to the top of a Jordanblock must be nonzero [19] [20]. Partition the matrixT into columnsT = [t| t2...tn.Then if a particular Jordan block with eigenvalue X islocated with its top row in the kth row of A and extendsto row k+q it is possible to express the correspondingcolumns of T astk = ak(XI - B)-lbti = (XI -B)-'(aib -ti-1)k < i -< k + q -1.(40)80IEEE TRANSACTIONS ON MILITARY ELECTRONICSHence the vectors ti will be linearly dependent only iffor some set of aai not all zerowhere P is a polynomial of degree less than p. But sinceeach (A'- ujI)-' is nonsingular this implies thatqiA;ZX oaii(IXi -B)-lb = 0.i 1=This equation can be multiplied by the nonsingularmatrixto obtain7 (IX - B)>iP(B)b = Owhere P is a polynomial of degree n-I or less. But un-less P=O which implies that each aii is zero this con-dition contradicts the complete controllability of (B b).Hence the vectors ti must be linearly independent.APPENDIX II* *YIYM xil xi2Lemma 2: Let x1X2'In order to prove the general statement concerningthe dynamic order of an observer the following well-known lemma [21] will be used.-* x be n linearly independ-ent vectors in an n-dimensional space. LetYi Y2* Ymalso be independent. Then there are n - m xi's such thatYli Y2Theorem 4: Let S1 be a completely observable nth-order system with m independent outputs. Then an ob-server S2 may be built for S1 using only n - m dynamicelements. (As illustrated by the proof the eigenvaluesof the observer are essentially arbitrary.)ainy. Then since S1tion of vectorsProof: Let the m outputs be given by al'y a2'y*is completely observable the collec-* xin__ are independent.(A')iaj1l2i -Oj.1-1 2;** n 1mspans n dimensions.Let p be the order of the minimal polynomial of A.To each output of S1 connect a completely controllablepth-order system with distinct eigenvalues. Consider thesystem driven by al'y. The p state variables of this sys-tem (under proper initial conditions) arezi= [b(A' - u)-lal]'y(41)where a diagonal form for this system has been assumed.Lemma1It will be shown that the vectors (A'-ujI)-'ajp generate the same space as the vectors1n. Assume that we can find ai'sguarantees that each bi is not zero.*2*i=(A')ka k=1 2**such thatpi=1ai(A'- ujI) = 0.This can be rewritten asP(A')fi (A -u jI)-l = 0i=l(42)(43)P(A') = 0.(44)Since this polynomial has a degree less than the minimalpolynomial each a5 = 0 in the original combination(42). This implies that any polynomial in A' can bewritten as a linear combination of the (A'-ujI)-1. Inparticular the vectors (A'-ujI)-1a1 i=1 2 .. I Pgenerate the same space as the vectors (A)ka k= 1 2. n.This same argument applies to each ai. Hence theoutput vectors from all observing systems span n dimen-sions. Now from Lemma 2 n dimensions can be spanned am and n-im dy-with m output vectors a a2namics. This proves Theorem 4.REFERENCES[11][21"Control system analysis and design via the[1] R. E. Kalman and J. E. Bertram "A unified approach to thetheory of sampling systems" J. Franklin Inst. vol. 267 pp.405-436; May 1959.'secondmethod' of Lyapunov-I. Continuous-Time Systems" Journalof Basic Engineering Trans. ASME Series D vol. 82 pp. 171-393; June 1960.[3] R. E. Kalman "On the General Theory of Control Systems"Proc. of the FirstIFA C Moscow Congress; 1960.[4] J. E. Bertram and P.E. Sarachik "On Optimal Computer Con-trol " Proc. of the First IFA C Moscow Congress; 1960.[51 E. B. Lee "Design of optimum multivariable control systems"J. of Basic Engrg. Trans. ASME Series D vol. 83 pp. 85-90;March 1961.[6] R. BellmanI. Glicksberg and 0. Gross "On the 'Bang-Bang'control problem" Quart. Appl. Math. vol. 14 pp. 11-16; 1961.[7] H. L. Groginsky "On a property of optimum controllers withboundedness constraints" IRE TRANS. ON AUTOMATIC CON-TROLvol. AC-6 pp. 98-1 10; May 1961.[81 K. K. Maitra "An Application of the Theory of Dynamic Pro-gramming to Optimal Synthesis of Linear Control Systems"Proc. of Dynamic Programming Workshop ed J. E. GibsonPurdue University LafayetteInd.; 1961.[9] R. Bellman "Dynamic Programming" Princeton UniversityPress N. J.; 1957.[10]R. Bellman and R. Kalaba "Dynamic programming and feed-back control" Proc. of the First IFA C Moscow Congress; 1960.V. G. Boltyanski R. V. Gamkrelidze E. F. Mischenko and L. S.Pontryagin "The maximum principle in the theory of optimalprocesses of control" Proc. of the First IFA C Moscow Congress;1960.112] J. La Salle and S. Lefshetz"Stability by Liapunov's DirectMethod with Applications" Academic Press New York N. Y.;1961.[13] R. E. Kalman "A new approach to linear filtering and pre-diction theory" J. of Basic Engrg. Trans. A SME Series Dvol. 82 pp. 35-45; March 1960.R.E. Kalman and R. S. Bucy "New results in linear filteringTrans. ASMEand prediction theory" J.Series D vol. 83 pp. 95-108; March 1961.[15] F. R. Gantmacher "The Theory of Matrices" Chelsea NewYork vol. 1 especially pp. 215-225; 1959.[16] E. G. Gilbert "Controllability and observability in multivari-able control systems" J. Soc. Indust. Appl.Math. Series A:On Control vol. 1 pp. 128-151; 1963.[17] R.E. Kalman "Mathematical description of linear dynamicalsystems" J. Soc. Indust. Appl. Math. Series A: On Control vol.1 pp. 152-192; 1963.[18] D. G. Luenberger "Special Decomposition of Linear Transfor-mations in Finite Dimensional Spaces" Mimeographed NotesStanford University Calif.[19] Y. C. Ho "What constitutes a controllable system?" IRETRANS. ON AUTOMATIC CONTROL (Correspondence) vol. AC-7p. 76; April 1962.[20] D. G. Luenberger"Determining the State of a Linear Systemwith Observers of Low Dynamic Order" Ph.D. dissertationDept. of Elec. Engrg. Stanford University Calif.; 1963.[21] P. R. Halmos"Finite-Dimensional Vector Spaces" D. VanNostrand Co. Inc. Princeton N.J. especially p.11; 1958.of Basic Engrg.[141
2006_Book_LinearS LINEARSYSTEMSP a n o s   J .   A n t s a k l i sA n t h o n y   N .   M i c h e lBirkhäuserPanos J. Antsaklis Anthony N. Michel Linear Systems Birkhauser Boston • Basel • Berlin Panos J. Antsaklis Dept. of Electrical  Engineering University  of Notre Dame 275 Fitzpatrick  Hall Notre Dame IN 46556 USA Anthony  N. Michel Dept. of Electrical  Engineering University  of Notre  Dame 275 Fitzpatrick  Hall Notre Dame IN  46556 USA Cover design by Mary Burgess. AMS  Subject  Classification:  34A30  34H05  47N70  65F05  93-XX  93-01  93Axx  93A30  93Bxx 93B03 93B05 93B07 93B10 93B11 93B12 93B15 93B17 93B18 93B20 93B25 93B50 93B52 93B55 93B60 93Cxx 93C05 93C10 93C15 93C35 93C55 93C57 93C62 93Dxx 93D05 93D10 93D15 93D20 93D25 93D30 Library of Congress Cataloging-in-Publication Data Antsaklis Panos J. Linear systems / Panos J. Antsaklis Anthony N. Michel. p. cm. Originally published : New York : McGraw-Hill ©1997. Includes bibliographical references and index. ISBN 0-8176-4334-2 (alk. paper) -  ISBN 0-8176-4435-0 (e-ISBN) 1. Linear control systems. 2. Control theory. 3. Signal processing. I. Michel Anthony N. II. Title. TJ220.A58 2005 629.8^32-dc22 2005053096 ISBN-10 0-8176-4434-2 ISBN-13 978-0-8176-4434-5 elSBN 0-8176-4435-0 Printed on acid-free paper. ©2006 Birkhauser Boston 2nd Corrected Printing Originally published by McGraw-Hill Englewood Cliffs NJ1997. BirkhdUSer All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Birkhauser Boston c/o Springer Sciences-Business  Media Inc. 233 Spring Street New York NY 10013 USA) except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with  any form  of  information  storage  and retrieval electronic  adaptation  computer  software  or by  similar or dissimilar methodology now known or hereafter developed is forbidden. The use in this publication  of  trade names trademarks  service  marks  and  similar  terms even  if  they  are not identified as such is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights. Printed in the United States of America. (KeS/IBT) 9 8 7 6 5 4 3 21 www. birkhauser.com To Our  Families To Melinda and our daughter Lily and to my parents Dr  loannis and Marina Antsaklis —Panos J. Antsaklis To Leone and our children Mary Kathy John Tony  and Pat —Anthony N. Michel Mechanics  is the paradise  of  the  mathematical sciences  because  by  means  of  it  one  comes  to the fruits  of mathematics. LEONARDO  DA  VINCI 1452-1519 C O N T E N TS Preface Mathematical Descriptions of Systems 1.1 Introduction A.  Physical  Processes  Models  and  Mathematical Descriptions  2/B.  Classification  of Systems  3 / C. Finite-Dimensional  Systems  4/D.  Chapter  Description  6/ E.  Guidelines for  the Reader  7 1.2 1.3 Preliminaries A.  Notation  8/B.  Continuous  Functions  9 Initial-Value  Problems A. Systems  of First-Order  Ordinary  Differential Equations  10/B.  Classification  of Systems  of  First-Order Ordinary  Differential  Equations  11 /  C.  nth-Order  Ordinary Differential  Equations  12 1.4 Examples  of  Initial-Value  Problems *1.5  More  Mathematical  Preliminaries A. Sequences  17 /B.  Sequences  of Functions  18/C  The Weierstrass M-Test  21 *1.6 Existence  of  Solutions  of  Initial-Value  Problems A. The Ascoli-Arzela  Lemma  22 /B.  e-Approximate Solutions  23 /  C.  The  Cauchy-Peano  Existence  Theorem  25 *1.7  Continuation  of  Solutions A. Zom's  Lemma  26/B.  Continuable  Solutions  27/ C  Continuation  of Solutions  to the Boundary  ofD  28 *1.8  Uniqueness  of  Solutions A. The Gronwall  Inequality  29 /B.  Unique  Solutions  30 *1.9  Continuous  Dependence  of  Solutions  on  Initial Conditions  and  Parameters 1.10 Systems  of  First-Order  Ordinary  Differential  Equations A. More  Mathematical  Preliminaries:  Vector Spaces  37/ B.  Further  Mathematical  Preliminaries:  Normed  Linear Spaces  41 /  C. Additional  Mathematical  Preliminaries: Convergence  44 /D.  Solutions  of Systems  of  First-Order Ordinary  Differential  Equations:  Existence  Continuation Uniqueness  and  Continuous  Dependence  on  Initial Conditions  45 XV 1 2 8 10 13 17 21 26 29 33 37 ^ Contents 47 54 55 58 60 65 79 80 80 81 94 94 96 1.11 Systems  of Linear  First-Order  Ordinary  Differential Equations A. Linearization  48 /B.  Examples  52 1.12  Linear  Systems:  Existence  Uniqueness Continuation  and Continuity  witli Respect to  Parameters of  Solutions 1.13 1.14 1.15 1.16 Solutions  of Linear  State  Equations State-Space  Description  of  Continuous-Time Systems State-Space  Description  of Discrete-Time  Systems Input-Output  Description  of Systems A. External  Description  of Systems:  General Considerations  65 /B.  Linear  Discrete-Time  Systems  68/ C. The Dirac  Delta  Distribution  72 /D.  Linear Continuous-Time  Systems  76 1.17 Summary 1.18  Notes 1.19  References 1.20  Exercises 2  Response of Linear  Systems 2.1 11 Introduction A. Chapter  Description  94/B.  Guidelines for  the Reader 96 Background  Material A. Linear  Sub spaces  97 /B.  Linear  Independence  97 / C. Bases  99/D.  Linear  Transformations  100/ E. Representation  of Linear  Transformations  by Matrices  104/  "^F.  Some  Properties  of Matrices  107/ *G. Determinants  of Matrices  111 /H.  Solving  Linear Algebraic  Equations  115/1.  Equivalence  and Similarity  116/J.  Eigenvalues  and Eigenvectors  121 / K. Direct  Sums  of Linear  Subspaces  126/L.  Some Canonical  Forms of Matrices  127/M.  Minimal Polynomials  132 /N.  Nilpotent  Operators  134/0.  The Jordan  Canonical  Form 135 2.3  Linear  Homogeneous  and Nonhomogeneous  Equations A. The Fundamental  Matrix  139 /B.  The State  Transition Matrix  143 /C.  Nonhomogeneous  Equations  145/D.  How to Determine  ^(t^to)  146 lA Linear  Systems  with  Constant  Coefficients A. Some  Properties  ofe^^  148/B.  How to  Determine e^^ 150/C  Modes  and Asymptotic  Behavior  of Time-Invariant  Systems  156 *2.5  Linear  Periodic  Systems 138 148 161 2.6 2.7 State  Equation  and  Input-Output  Description  of Continuous-Time  Systems A.  Response  of Linear  Continuous-Time  Systems  165 / B.  Transfer  Functions  168/C.  Equivalence  of  Internal Representations  170 State  Equation  and  Input-Output  Description  of Discrete-Time  Systems A.  Response  of Linear  Discrete-Time  Systems  174/B.  The Transfer  Function  and  the z-Transform  177/C.  Equivalence Sampled-Data of Internal  Representations  180/D. Systems  182 /E.  Modes  and Asymptotic  Behavior  of Time-Invariant  Systems  186 Summary 2.8  An  Important  Comment  on  Notation 2.9 2.10  Notes 2.11 2.12 References Exercises Controllability Observability and Special Forms 3.1 Introduction A. Brief  Introduction  to Reachability  and  Observability  215  / B.  Chapter  Description  223 /  C.  Guidelines for  the Reader  225 PART  1  Controllability  and  Observability 3.2  Reachability  and  Controllability A.  Continuous-Time  Time-Varying  Systems  227/ B.  Continuous-Time  Time-Invariant  Systems  235  / C  Discrete-Time  Systems  241 3.3  Observability  and  Constructibility A.  Continuous-Time  Time-Varying  Systems  248  / B.  Continuous-Time  Time-Invariant  Systems  252  / C  Discrete-Time  Systems  257 PART  2  Special  Forms  for  Time-Invariant  Systems 3.4 *3.5 3.6 Special  Forms A.  Standard  Forms for  Uncontrollable  and  Unobservable Systems  263/B.  Eigenvalue/Eigenvector  Tests for Controllability  and  Observability  272 /  C  Relating State-Space  and Input-Output  Descriptions  275  / D.  Controller  and  Observer  Forms  278 Poles  and  Zeros Summary 165  Contents 174 190 191 191 192 193 214 215 226 226 247 263 263 298 308 xii Contents 3.7  Notes 3.8  References 3.9 Exercises 4  State Feedback and State Observers 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 Introduction A.  A Brief  Introduction  to State-Feedback  Controllers  and State  Observers  322 /B.  Chapter  Description  325  / C. Guidelines for  the Reader  326 Linear  State  Feedback A.  Continuous-Time  Systems  326/B.  Eigenvalue Assignment  328 /  C.  The Linear  Quadratic  Regulator  (LQR): Continuous-Time  Case 342 /D. Relations  345 /E.  Discrete-Time  Systems  348/E  The Linear  Quadratic  Regulator  (LQR):  Discrete-Time  Case  348 Input-Output Linear  State  Observers A.  Eull-Order  Observers:  Continuous-Time  Systems  350/ B.  Reduced-Order  Observers:  Continuous-Time Systems  355 /  C.  Optimal  State  Estimation: Continuous-Time  Systems  357 /D.  Eull-Order  Observers: Discrete-Time  Systems  358/E.  Reduced-Order  Observers: Discrete-Time  Systems  362 /  E  Optimal  State  Estimation: Discrete-Time  Systems  362 Observer-Based  Dynamic  Controllers A.  State-Space  Analysis  364/B.  Transfer  Function Analysis  367 Summary Notes References Exercises 5  Realization  Theory and Algorithms 5.1 5.2 5.3 Introduction A. Chapter  Description  384/B.  Guidelines for  the Reader  384 State-Space  Realizations  of  External  Descriptions A.  Continuous-Time  Systems  385 /B.  Discrete-Time Systems  388 Existence  and  Minimality  of  Realizations A.  Existence  of Realizations  390 /B.  Minimality  of Realizations  394 /  C.  The  Order of  Minimal Realizations  397/D.  Minimality  of  Realizations: Discrete-Time  Systems  401 310 311 311 321 322 326 350 363 370 370 371 372 383 384 385 389 5.4 5.5 5.6 5.7 5.8 Realization  Algoritlims A. Realizations  Using Duality  402 /B.  Realizations  in Controller/Observer  Form 404 /  C.  Realizations  with  Matrix A Diagonal  417/D.  Realizations  with Matrix  A  in Block Companion  Form 418/E.  Realizations  Using Singular  Value Decomposition  423 Summary Notes References Exercises Stability 6.1 Introduction A. Chapter  Description  433 /B.  Guidelines for  the Reader  434 6.2  Matliematical  Background  Material A. Bilinear  Functionals  and  Congruence  435 /B.  Euclidean Vector Spaces  437/C  Linear  Transformations  on  Euclidean Vector Spaces  441 PART  1  Lyapunov  Stability 6.3 The  Concept  of  an  Equilibrium 6.4  Qualitative  Characterizations  of  an  Equilibrium 6.5 6.6 6.7 6.8 Lyapunov  Stability  of  Linear  Systems Some  Geometric  and  Algebraic  Stability  Criteria A. Some  Graphical  Criteria  462 /B.  Some  Algebraic Criteria  465 The  Matrix  Lyapunov  Equation Linearization PART  2  Input-Output  Stability  of  Continuous-Time Systems 6.9 Input-Output  Stability PART  3  Stability  of Discrete-Time  Systems 6.10  Discrete-Time  Systems A. Preliminaries  489/B.  Lyapunov  Stability  of  an Equilibrium  492 /  C.  Linear  Systems  495 /D.  The Schur-Cohn  Criterion  498/E.  The Matrix  Lyapunov Equation  499/F  Linearization  503 /G. Stability  505 Input-Output 6.11 Summary 6.12  Notes 402 Contents 424 424 425 425 432 432 434 445 445 447 452 461 468 477 481 481 489 489 508 509 xiv Contents 6.13  References 6.14 Exercises 7  Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems 7.1 Introduction A. A Brief  Introduction  to Polynomial  and  Fractional Descriptions  518/B.  Chapter  Description  522/ C. Guidelines for  the Reader  523 PART  1  Analysis  of  Systems 7.2 7.3 Background  Material  on  Polynomial  Matrices A. Rank  and Linear  Independence  524 /B.  Unimodular  and Column  (Row) Reduced  Matrices  526/C.  Hermite  and Smith  Forms 531 /D.  Coprimeness  and  Common Divisors  535/E.  The Diophantine  Equation  540 Systems Represented by Polynomial  Matrix Descriptions A. Equivalence  of Representations  554/B.  Controllability Observability  Stability  and Realizations  560/ C. Interconnected  Systems  568 PART  2  Synthesis  of  Control  Systems 7.4 7.5 7.6 7.7 7.8 Feedback  Control  Systems A. Stabilizing  Feedback  Controllers  589 /B.  State  Feedback Control  and State Estimation  605 /  C.  Stabilizing  Feedback Controllers  Using Proper  and Stable  MFDs  611 /D.  Two Degrees  of Freedom  Feedback  Controllers  622 Summary Notes References Exercises Appendix  Numerical  Considerations A.l A.2 A.3 A.4 Introduction Solving  Linear  Algebraic  Equations Singular  Values  and  Singular  Value  Decomposition Solving  Polynomial  and  Rational  Matrix  Equations Using  Interpolation  Methods A.5 References Index 510 511 517 518 524 524 553 589 589 634 635 636 638 645 645 646 648 653 659 661 PREFACE This text is intended primarily for  first-year  graduate students and advanced  under graduates  in  engineering  who  are  interested  in  control  systems  signal  processing and communication  systems. It is also appropriate for students in applied  mathemat ics economics and certain  areas in the physical  and biological  sciences.  Designed for a challenging one-semester  systems course the book presents an introduction to systems theory with an emphasis on control theory. It can also be used as supplemen tary  material  for  advanced  systems  and  control  courses  and  as  a general  reference on the  subject. The prerequisites  for using this book are topics covered in a typical  undergrad uate  curriculum  in  engineering  and  the  sciences:  undergraduate-level  differential equations linear  algebra  Laplace  transforms  and the modeling  of electric  circuits and simple mechanical  systems. The  study  of  linear  systems  is  a  foundation  for  several  disciplines  including control and signal processing. It is therefore very important that the coverage of linear systems be comprehensive and give readers sufficient  breadth and depth in analysis and  synthesis  techniques  of  such  systems. We believe  that  the best preparation  for this is a firm understanding of the fundamentals  that govern the behavior of complex systems. Indeed  only  a thorough  understanding  of  system behavior  enables  one to take full  advantage  of the various  options  available  in the  design  of the best  kinds of control  systems  and  signal processors. Therefore  the primary  aim of this text  is to  provide  an  understanding  of  these  fundamentals  by  emphasizing  mathematical descriptions of systems and their properties. In writing this book our goal was to clearly present the fundamental  concepts of systems theory in a self-contained  text. In addition to covering the fundamental  prin ciples we provide  sufficient  background  in analysis  and  algebra  to enable  readers to move on to advanced topics in the systems area. The book is designed to highlight the main results  and  distinguish  them from  supporting  results  and  extensions. Fur thermore  we present  the  material  in  a  sufficiently  broad  context  to give  readers  a clear picture of the dynamical behavior of linear systems and the limitations of such systems. The  theory  of  linear  systems  is  a  mature  topic  and  there  are  literally  thou sands  of  scholarly  papers  reporting  research  on  this  subject.  This  book  empha sizes  fundamental  results  that  are  widely  accepted  as  essential  to  the  subject.  For those  readers  interested  in  further  detail  the  end-of-chapter  material  includes additional  results  in  the  exercise  sections  as  well  as  pertinent  references  and notes. The  book  covers  both  continuous-time  and  discrete-time  systems  which  may be time-varying  or time-invariant.  The material  is organized  in  such  a manner  that it  is  possible  to  concentrate  only  on  the  time-invariant  case  if  desired.  The  time-invariant case is treated in separate sections and the results are presented so that they can be developed independently  of the time-varying case. This type of  organization provides considerable  flexibility  in covering the material. xvi Preface Although the text is designed to serve primarily the needs of graduate students it should also prove valuable to researchers and practitioners we tried to make it easy to  use  and  the  book  should  prove  valuable  for  self-study.  Many  simple  examples are included  to clarify  the material  and to encourage readers  to actively  participate in the learning process. The exercises at the end of each chapter introduce  additional supporting  concepts  and results and encourage readers to gain additional insight by using what was learned. The exercises also encourage readers to comment interpret and  visualize  results  (e.g. responses)  making  use  of  computer  programs  to  aid  in calculations  and the generation of graphical results when  appropriate. Over  the  past  several  years  the  material  has  been  class-tested  in  a  first-year graduate-level  course  on  linear  systems  and  its  development  has  been  influenced greatly by  student feedback.  Although  there are many  ways of using this book in a course we suggest in the following  several useful guidelines. Because any course on linear systems will most likely serve students with different  educational  experiences from  a variety  of  disciplines  and  institutions.  Chapters  1 and  2 provide  necessary background  material  and  develop  certain  systems  fundamentals.  Armed  with  this foundation we develop essential results on controllability and observability  (Chapter 3) on state observers  and  state feedback  (Chapter 4) and on realization  of  systems (Chapter 5). Chapters  6 and 7 address basic issues concerning  stabihty  (Chapter  6) and  the  representation  of  systems  using  polynomial  matrices  and  matrix  fractions (Chapter 7). The appendix presents  supplementary  material  (concerning  numerical aspects). How to use this book At the beginning  of each chapter is a detailed  description  of the chapter's  con tents along with guidelines for readers. This material should be consulted when de signing a course based on this book. In the following  we give a general overview of the book's  contents with  suggested  topics for  an introductory  one-semester  course in linear systems. From Chapter  1 covering a first course in linear systems should include the fol lowing: all the material on systems  (Section  1.1); the material on initial-value prob lems  (Sections  1.3  and  1.4);  the  material  on  systems  of  linear  first-order  ordinary differential  equations  (Sections  1.11  1.12  and  1.13); the material on state equation descriptions  of  continuous-time  systems  (Section  1.14)  and  discrete-time  systems (Section  1.15);  and  the  material  on  input-output  descriptions  of  systems  (Section 1.16). The mathematical  background  material  in  Sections  1.2  1.5  and  Subsections I.IOA  to  1.IOC  is included  for review  and to establish  some needed  notation.  This material  should  not  require  formal  class  time.  In  Subsection  1.1 OD  [dealing  with existence  continuation  uniqueness  and  continuous  dependence  (on  initial  condi tions  and  parameters)  of  solutions  of  initial-value  problems]  the  coverage  should emphasize the results and their implications rather than the proofs  of those results. From  Chapter  2  a first course  in  linear  systems  should  include  essentially  all the material  from  the following  sections:  Section  2.3  (dealing  with  systems  of  lin ear homogeneous  and nonhomogeneous  first-order  ordinary  differential  equations); Section  2.4  (dealing  with  systems  of  linear  first-order  ordinary  differential  equa tions  with constant  coefficients);  and  Sections  2.6  and  2.7  (which  address  the  state equation  description  the  input-output  description  and  important  properties  such xvii Preface as asymptotic  stability of continuous-time  and discrete-time linear systems respec- tively). Section  2.5  (concerned  with linear periodic  systems)  may be omitted  with- out  any  loss  of  continuity.  As in Chapter  1 the mathematical  background  material in Section 2.2 (dealing with linear algebra and matrices) is included for review  and to establish some important notation and should not require much formal  class time. For Chapter 3 it is best to consider the material in two parts. From Part 1 include Section 3.1 (where the concepts of controllability  and observability  are introduced); and Subsections  3.2B  and  3.3B  (where these concepts  are developed  in greater de tail  for  continuous-time  time-invariant  systems).  From  Part  2 include  Subsections 3.4A  and  3.4D  (where  special  forms  of  system  descriptions  are  considered);  and Subsection  3.4B  (where  an  additional  controllability  and  observability  test  is  pre sented). Similarly the course should include the following  material from  Chapter 4: Section 4.1  (where  state feedback  and  state observers  are introduced);  Subsections 4.2A  and 4.2B  (linear  state feedback  and eigenvalue  assignment by  state  feedback are treated  in  detail);  and  Subsection  4.3A  (where  the  emphasis  is  on  identity  ob servers);  and  Section  4.4  (where  observer-based  controllers  are  developed).  Also the  course  should  include  material  from  Chapter  5:  Section  5.2  (where  realization theory is introduced); Section 5.3 (where the existence minimality and the order of minimal  realizations  are  developed);  and  Subsections  5.4A  5.4B  5.4C  and  5.4E (where realization  algorithms are presented). The  material  outlined  above  constitutes  the  major  portion  of  a  first  course  in linear systems. The course is rounded out if time permits with selected topics  from Chapter 6 (stability) and Chapter 7 (polynomial matrix system descriptions and frac tional representations of transfer function  matrices of linear time-invariant systems). The choice of these topics and where they are presented throughout the course de pends on the interests of the instructor  and the students. We have been using this textbook in a one-semester first-year graduate course in electrical engineering. Typically we spend the first half  of the course on Chapter  1 Chapter 2 and Part  1  of Chapter 3. The second half of the course is devoted to Part 2 of Chapter 3 and Chapters 4 and 5. Selected topics from  stability theory and matrix fractional  descriptions  of  systems  from  Chapters  6  and  7  are  included  as  needed. Detailed  coverage of Chapters  1 and 2 with only  selective coverage of Chapters 3 4  and  5 would  be  appropriate  in  a course  that  emphasizes  mathematical  systems theory. Chapter 6 can also be used as an introduction to a second-level graduate  course on  nonlinear  systems  and  stability.  Similarly  Chapter  7  stands  alone  and  can  be used  in  an  advanced  linear  systems  course  or  as  an  introduction  to  a  multi-input/ multi-output linear control course. There is enough material in Chapters 3 through 7 for courses taught at several levels in a graduate  program. Acknowledgments We are indebted to our students for their feedback  and constructive  suggestions during the evolution of this book. In particular we would like to thank B. Hu I. Kon-stantopoulos and X. Koutsoukos and also Dr. K. Wang for their help and suggestions during the final editing of the manuscript.  Special thanks go to Clarice Staunton  for her  patience  in  typing  the  many  versions  of  our  manuscript.  We  are  also  very  ap preciative of our excellent working relation with the staff of McGraw-Hill especially xviii Preface with  Lynn  Cox  the  Electrical  Engineering  Editor.  Finally  we  are  both  indebted to  many  individuals  who  have  shaped  our  views  of  systems  theory  in  particular. Bill  Wolovich  Brown  University;  Boyd  Pearson  Rice  University;  David  Mayne Imperial  College  of  the  University  of  London  (England);  Sherman  Wu  Marquette University; and Wolfgang  Hahn the Technical University  of Graz (Austria). The present  printing  of Linear  Systems  by  Birkhauser  is in  response  to  many requests by colleagues from  the US and around the world who wanted to start using or  continue  using  the  book  which  was  out  of  print.  Colleagues  who  had  used  the book in the classroom or as a reference as well as our own students identified  several items that needed to be changed. These corrections which mostly constituted  minor typos have been incorporated in the current printing of the book. This publication of Linear  Systems  was  made  possible  primarily  because  of  Tom  Grasso  Birkhauser's Computational  Sciences  and Engineering  Editor  whom  we would  like to thank  for his professionalism  encouragement  and  support. A new  companion  book  entitled A  Linear  Systems  Primer  is forthcoming.  The Primer-bdiscd  on the more complete treatment  of the present Linear  Systems  book-contains  a treatment  of  linear  systems  that  focuses  primarily  on  the  time-invariant case  using  streamlined  presentation  of  the  material  with  less  formal  and  more intuitive proofs without sacrificing  rigor. Panos J. Antsaklis  and Anthony  N.  Michel Notre Dame Indiana August 2005 CHAPTER  1 Mathematical Descriptions of Systems The dynamical behavior of systems can be understood by studying their mathemati cal descriptions. The flight path of an airplane subject to certain engine thrust rudder and elevator angles and particular wind conditions or the behavior of an automobile on cruise control when climbing a certain hill can be predicted using  mathematical descriptions of the pertinent behavior. Mathematical equations typically  differential or difference  equations are used to describe the behavior of processes and to predict their response to certain inputs. Although computer simulation is an excellent tool for verifying  predicted behavior and thus for enhancing our understanding of processes it is certainly not an adequate substitute for generating the information  captured in a mathematical  model when  such a model is available. But computer  simulations  do complement mathematical descriptions. To be able to study the behavior of processes using mathematical  descriptions  such  as differential  and difference  equations  one needs  a  good  working  understanding  of  certain  important  mathematical  concepts and procedures. Only in this way can one seriously  attempt to study the behavior of complex  systems  and eventually  design processes  that exhibit the desired  complex behavior. This chapter  develops  mathematical  descriptions  for  the types of  systems  with which  we  are  concerned  namely  linear  continuous-time  and  linear  discrete-time finite-dimensional  systems.  Since  such  systems  are  frequently  the  result  of  a  lin earization  process  of  nonlinear  systems  or  the  result  of  the  modeling  process  of physical  systems in which the nonlinear effects  have been suppressed  or neglected the  origins  of  these  linear  systems  are  frequently  nonlinear  systems.  For  this  rea son here  and  in Chapter  6 when  we  deal  with  certain  qualitative  aspects  (such  as existence uniqueness continuation and continuity with respect to parameters of so lutions  of  system  equations  stability  of  an equilibrium  and  so forth)  we  consider linear as well as nonlinear system models although the remainder of the book deals exclusively  with linear  systems. 2 Lhi^^Systems 1.1 INTRODUCTION A  systematic  study  of  (physical) phenomena  usually  begins  with  a modeling  pro cess.  Examples  of models include electric circuits consisting of interconnections of resistors inductors  capacitors transistors  diodes voltage  or current  sources etc.; mechanical circuits consisting of interconnections  of point masses springs viscous dampers  (dashpots)  applied  forces  etc.; and  verbal  characterizations  of  economic and societal systems among others. Next appropriate laws ox principles  are invoked to generate equations  that describe the models (e.g. Kirchhoff's  current and voltage laws  Newton's  laws  conservation  laws  and  so forth).  When  using  an  expression such as "we consider a system  described by ordinary differential  equations" we will have in mind a phenomenon  described by an appropriate set of ordinary  differential equations  (not the description of the physical phenomenon  itself). A.  Physical  Processes Models and Mathematical  Descriptions A  physical  process  (physical  system)  will  typically  give  rise  to  several  different models  depending  on  what  questions  are  being  asked.  For  instance  in  the  study of the voltage-current  characteristics  of a transistor  (the physical process) one may utilize  a  circuit  (the  model)  that  is  valid  at  low  frequencies  or  a  circuit  (a  second model)  that  is  valid  at high  frequencies;  alternatively  if  semiconductor  impurities are of interest a third model quite different  from  the preceding two is appropriate. Over the centuries a great deal of progress has been made in developing  math ematical  descriptions  of  physical  phenomena  (using  models  of  such  phenomena). In  doing  so  we  have  invoked  laws  (or principles)  of  physics  chemistry  biology economics  etc. to  derive  mathematical  expressions  (usually  equations)  that  char acterize the evolution  (in time) of the variables  of interest. The availability  of  such mathematical  descriptions  enables  us to make use  of the vast resources  offered  by the  many  areas  of  applied  and  pure  mathematics  to conduct  qualitative  and  quan titative  studies  of  the behavior  of  processes.  A  given  model  of  a physical  process may  give  rise  to  several  different  mathematical  descriptions.  For  example  when applying Kirchhoff's  voltage and current laws to the low-frequency  transistor model mentioned  earlier  one can  derive  a set of differential  and  algebraic  equations or a set consisting of only differential  equations or a set of integro-differential  equations and so forth.  This process  of mathematical  modeling  ''from a physical  phenomenon to a model  to a mathematical  description"  is essential  in science  and  engineering. To capture phenomena of interest accurately and in tractable mathematical form is a demanding  task  as can be imagined  and requires  a thorough understanding  of the physical process involved. For this reason the mathematical description of complex electrical  systems  such  as power  systems  is  typically  accomplished  by  electrical engineers  the  equations  of  flight  dynamics  of  an  aircraft  are  derived  by  aeronau tical engineers the equations  of chemical processes  are arrived  at by chemists  and chemical  engineers  and  the  equations  that  characterize  the  behavior  of  economic systems are provided by economists. In most nontrivial cases this type of modeling process  is  close  to  an  art form  since  a good  mathematical  description  must  be  de tailed enough to accurately describe the phenomena of interest and at the same time CHAPTER  1: Mathematical Descriptions of Systems simple enough to be amenable to analysis. Depending  on the applications  on hand a  given  mathematical  description  of  a process  may  be  further  simplified  before  it is used  in  analysis  and  especially  in  design  procedures. For  example  using  the fi nite  element  method  one  can  derive  a  set  of  first-order  differential  equations  that describe  the motion  of  a space  antenna.  Typically  such  mathematical  descriptions contain hundreds  of differential  equations. Whereas  all of these equations  are quite useful  in simulating the motion of the antenna a lower order model is more  suitable for the control design that for example may aim to counteract the effects  of certain disturbances.  Simpler mathematical  models  are required  mainly  because  of our in ability to deal effectively  with hundreds  of variables  and their interactions. In  such simplified  mathematical  descriptions  only  those  variables  (and  their  interactions) that have significant  effects  on the phenomena of interest are included. A point that cannot be overemphasized is that the mathematical descriptions we will encounter characterize processes only approximately. Most often this is the case because the complexity  of physical systems defies  exact mathematical  formulation. In many  other cases however  it is our own choice that a mathematical  description of a given process  approximate the actual phenomena by only a certain  desired  de gree  of  accuracy.  As  discussed  earlier  this  is done  in the  interest  of  mathematical simplicity. For example in the description of RLC circuits one could use nonlinear differential  equations that take into consideration parasitic effects  in the capacitors; however most often it suffices  to use linear ordinary differential  equations with con stant coefficients  to describe the voltage-current relations of such circuits since typ ically  such  a description  provides  an  adequate  approximation  and  since it is  much easier to work with linear rather than nonlinear differential  equations. In this book it will generally be assumed that the mathematical  description  of a system in question is given. In other words we assume that the modeling of the pro cess in question has taken place and that equations describing the process are given. Our  main  objective  will be to present  a qualitative  theory  of  an important  class of systems—finite-dimensional  linear  systems—by  studying  the  equations  represent ing such  systems. B.  Classification  of  Systems For our purposes a comprehensive  classification  of systems is not particularly  illu minating. However an enumeration of the more common classes of systems encoun tered in engineering  and  science may be quite useful  if for  no other reason than to show  that the classes  of  systems  considered  in this book  although  very  important are quite  specialized. As  pointed  out  earlier  the  particular  set  of  equations  describing  a  given  sys tem  will  in  general  depend  on  the  effects  one  wishes  to  capture.  Thus  one  can speak of lumped parameter  ox finite-dimensional systems  and distributed  parameter or  infinite-dimensional  systems' continuous-time  and  discrete-time  systems'  linear and nonlinear  systems' time-varying  and  time-invariant  systems' deterministic  and stochastic  systems'  appropriate  combinations  of  the  above  called  hybrid  systems' and perhaps others. The appropriate mathematical settings for  finite-dimensional  systems are finite-dimensional vector spaces and for infinite-dimensional  systems they are most  often 4 Linear Systems infinite-dimensional  linear  spaces. Continuous-time  finite-dimensional  systems  are usually described by ordinary differential  equations or certain kinds of integral equa tions while discrete-time  finite-dimensional  systems are usually characterized by or dinary difference  equations or discrete-time counterparts to those integral equations. Equations  used  to describe infinite  dimensional  systems include partial  differential equations  Volterra  integro-differential  equations  functional  differential  equations and so forth. Hybrid system descriptions involve two or more different  types of equa tions.  Nondeterministic  systems  are  described  by  stochastic  counterparts  to  those equations  (e.g. Ito differential  equations). In a broader context not addressed  in this book most of the systems  described by the equations enumerated  generate dynamical  systems.  It has become  customary in the engineering literature to use the term "dynamical system" rather loosely and it has even been applied to cases where the original definition  does not exactly fit. (For a  discussion  of  general  dynamical  systems  refer  e.g.  to Michel  and  Wang  [13].) We will address in this book dynamical  systems determined by ordinary  differential equations or ordinary  difference  equations considered  next. C.  Finite-Dimensional  Systems The  dynamical  systems  we  will  be  concerned  with  are  continuous-time  and discrete-time  finite-dimensional  systems—primarily  linear  systems.  However  since such systems are frequently  a consequence of a linearization process it is important when  dealing  with  fundamental  qualitative  issues  that  we  have  an  understanding of the origins  of  such linear  systems. In particular  when  dealing  with  questions  of existence and uniqueness of solutions of the equations describing a class of systems and with stability properties of such systems we may consider nonlinear models as well. Continuous-time  finite-dimensional  dynamical  systems  that we will consider are described by equations of the  form ^i  —  fii^y  ^l yi  =  gi{txi...Xnui...Um) '  '  -y  ^m  U\ .  .  .  Um) I  .  .  .  n i  = i  =  1 • • • A (Lla) (1.1b) where  Ui i  =  1...  m  denote  inputs  or  stimuli;  yi  i  =  1... /? denote  outputs or responses' xf  i  =  1...  n denote state  variables;  t denotes time; xi  denotes  the time  derivative  of  xt;  ft  i  =  1...  n  are real-valued  functions  of  1 +  ^  +  m real variables; and gi i  =  \...  p  are real-valued functions  of 1 -h ^ + m real variables. A complete description of such systems will usually also require a set of initial  con ditions  Xi(to)  =  Xio i  =  \.. .n  where  fo denotes  initial  time.  We will  elaborate later on restrictions  that need to be imposed  on the  ft  gt  and  ui and on the  origins of the term "state variables." Equations  (1.1a) and (1.1b) can be represented in vector form  as X =  f{tx  u) y  =  g{txu) (1.2a) (1.2b) where x is the state vector with components xt u is the input vector with components Ui y is the output vector with components  j /  and/  and g are vector-valued  functions CHAPTER  1: Mathematical Descriptions of Systems with components  ft  and gi respectively.  We call  (1.2a) a state  equation  and  (1.2b) an output  equation. Important  special  cases  of  (1.2a)  and  (1.2b)  are  the  linear  time-varying  state equation  and output  equation  given by X =  A{t)x  +  B{t)u y  -  C{t)x  +  D{t)u (1.3a) (1.3b) where A B C and D are real nXnnXmpXn and pXm  matrices respectively whose  elements  are  time-varying.  Restrictions  oi^ these  matrices  will  be  provided later. Linear  time-invariant  state  and output  equations  given by X =  Ax  +  Bu y  =  Cx  +  Du (1.4a) (1.4b) constitute important  special cases of (1.3a) and (1.3b) respectively. Equations (1.3a) (1.3b) and (1.4a) (1.4b) may arise in the modeling process or they may be a consequence of linearization  of (1.1a) and (1.1b). Discrete-time  finite-dimensional  dynamical  systems  are described by  equations of the  form Xi(k  -\-  I)  =  fi(h  xi{k)...  Xn{k) ui(k\ ...  Xn{k) ui(k\ yi(k)  =  gi(k  xi{k\ ...  Um{k)\ . . .  Um(k)\ /  =  1.. /  =  1.. . n .p (1.5a) (1.5b) or in vector  form x(k  +  1)  =  f(k  x(kl  u(k)) y(k)  =  g(k  x(kl  u(k)) (1.6a) (1.6b) where k is an integer that denotes discrete  time  and all other symbols are defined  as before.  A  complete  description  of  such  systems  involves  a set of  initial  conditions x(fco)  =  Xy^Q  where feo denotes  initial  time.  The  corresponding  linear  time-varying and time-invariant  state and output equations are given by and x(k  +  1)  =  A(k)x(k)  +  Bik)u(k) y(k)  =  C(k)x(k)  +  D(k)u(k) x(k  +  1)  -  Ax(k)  +  Bu(k) y(k)  =  Cx(k)  +  Du(k) (1.7a) (1.7b) (1.8a) (1.8b) respectively  where all symbols in (1.7a) (1.7b) and in (1.8a) (1.8b) are defined  as in (1.3a) (1.3b) and (1.4a) (1.4b) respectively. This  type  of  system  characterization  is  called  state-space  description  or state-variable  description  or  internal  description  of  finite-dimensional  systems. Another  way  of  describing  continuous-time  and  discrete-time  finite-dimensional dynamical  systems involves operators that establish  a relationship between the sys tem  inputs  and  outputs.  Such  characterization  called  input-output  description  or external  description  of a system will be addressed later in this  chapter. Linear Systems D.  Chapter  Description In this book we will make liberal use of certain aspects of analysis and algebra. To help the reader  recall  some of these facts  we will provide throughout  such  background material as needed. This is done e.g. in the second section where we provide some of the notation used and where we recall certain facts concerning continuous  functions. In the third section  we present the initial-value problem for  nth-order  ordinary differential  equations  and for  systems  of  first-order  ordinary  differential  equations and  we  give  a  classification  of  ordinary  differential  equations.  We  also  show  that the study of nth-order ordinary  differential  equations can be reduced to the study of systems of first-order ordinary differential  equations. In the fourth  section  we give several specific examples of initial-value problems determined by ordinary differential  equations. In the fifth section  we provide mathematical  background  material  dealing  with sequences sequences of functions  and the Weierstrass  M-test. In the sixth  section  we establish  conditions under which initial-value  problems for  ordinary  differential  equations  possess  solutions.  This  is  accomplished  in  two stages. First we establish  an existence result for e-approximate  solutions of  which the  Euler  method  is  a  special  case. Next  we  state  and  prove  a preUminary  result called  the  Ascoli-Arzela  Lemma  that  we  use  together  with  the  existence  result for  6-approximate  solutions  to  establish  a  result  for  the  existence  of  solutions  of initial-value problems. These solutions need not be unique. (This result is called the Peano-Cauchy  Theorem.) In  the  seventh  section  we  make  use  of  Zom's  Lemma  to  establish  a  result that  enables  us  to  determine  the  extent  (in  time)  of  the  existence  of  solutions  of initial-value problems. This is called continuation of solutions. In the eighth  section  we prove a result that ensures  the uniqueness  of  solutions of  initial-value  problems.  In  doing  so  we  utilize  a  useful  result  called  the  Gron-wall Inequality  that  we also prove. One of the results  of this  section called  Picard iteration provides a method of constructing  solutions  iteratively. In the ninth  section  we  show  that under  reasonable  conditions  the  solutions of initial-value problems depend continuously  on initial conditions and system param eters. To simplify  our presentation  we  consider  in  Sections  6 to 9 the  case  of  scalar first-order  ordinary  differential  equations. In the tenth  section  we extend  all results to  the  case  of  systems  of  first-order  ordinary  differential  equations.  In  the  process of  accomplishing  this  we  introduce  additional  mathematical  background  material concerning vector spaces normed linear spaces and convergence  on normed  linear spaces. The results  in  Sections  6 to  10 pertain  to differential  equations  that in  general are nonlinear. In the eleventh  section  we address linearization of such equations and provide several specific  examples. We utilize the results of Section  10 to establish in the twelfth  section  conditions for the existence uniqueness continuation and continuity with respect to initial con ditions  and  parameters  of  solutions  of  initial-value  problems  determined  by  linear ordinary differential  equations. In  the  thirteenth  section  we  determine  the  solutions  of  linear  ordinary  differ ential  equations.  To  arrive  at  some  of  our  results  (the  Peano-Baker  series  and  the CHAPTER  1: Mathematical Descriptions of Systems matrix  exponential)  we  make  use  of  the  Picard  iteration  considered  in  Sections  8 and  10. In this  section  we introduce  for  the first time  the notions  of  state  and  state transition  matrix.  We  also  present  the  variations  of  constants  formula  for  solving linear nonhomogeneous  ordinary differential  equations  and introduce the notions of homogeneous  and particular  solutions. Summarizing  the  purpose  of  Sections  3  to  13  is  to  provide  material  dealing with  ordinary  differential  equations  and  initial-value  problems  that  is  essential  in the study of continuous-time  finite-dimensional  systems. This material enables us to give  the  state-space  equation  representation  of  continuous-time  finite-dimensional systems. This  is  accomplished  in  tho fourteenth  section.  We consider  nonlinear  as well as linear systems that may be time-varying  or time-invariant. In  the  fifteenth  section  we  present  the  state-space  equation  representation  of finite-dimensional  discrete-time  systems.  In  doing  so  we  introduce  systems  of first-order  ordinary  difference  equations  nth-order  ordinary  difference  equations initial-value problems involving such equations solutions of equations the transition matrix and so forth. Finally  in  the  sixteenth  section  we  consider  an  alternative  description  of  the systems considered herein called the input-output representation  of systems. In the process of accomplishing  this we introduce  several important general properties of systems  (such  as  causality  systems  with  memory  linearity  time  invariance  and so  forth).  We  emphasize  linear  discrete-time  and  linear  continuous-time  systems. For  the  former  we  introduce  the  notion  of  pulse  response  while  for  the  latter  we introduce the concepts of impulse response and the integral representation  of linear continuous-time systems. For both continuous-time and discrete-time linear systems we make  a connection  between  the  state-space representation  and the  input-output description of systems and we introduce the concept of system transfer  function.  In the first subsection of this section we encounter Dirac delta distributions. This chapter has been organized in such a way that proofs of results may be omit ted  without  much  loss  of  continuity  should  time  constraints  be  a factor.  However the concepts  (including the statements of most theorems) introduced in this  chapter are of fundamental  importance and will be utilized throughout the remainder of this book. E.  Guidelines  for the  Reader In  a first reading  certain  material  may  be  omitted  without  loss of continuity.  Such material  is  identified  throughout  the  book  by  starring  the  section  or  subsection title. A typical graduate course in linear  systems will include the following  material from  this  chapter: Mathematical  description  and classification  of systems  (Section  1.1). Initial-value problems with examples  (Sections  1.3  and  1.4). Material on vector spaces and the results concerning existence and  uniqueness of solutions of systems of first-order ordinary differential  equations  (Sections 1.10  and  1.12). Linearization  of nonlinear  systems with examples  (Section  1.11). Linear Systems Solutions of the linear state equations  x  =  A(t)x  and x  =  A(t)x  + g(t)  (Section 1.13). State-variable descriptions  of continuous-time  and discrete-time  systems (Sections  1.14  and  1.15). Input-output  description of systems  (Section  1.16). 1.2 PRELIMINARIES We will employ a consistent notation and use certain facts  from  the calculus analy sis and linear algebra. We will summarize this type of material as needed in various sections throughout the book. This is the first such  section. A.  Notation Let  V and  W be sets.  Then  VUWVnWV-WmdVxW denote the  union intersection  difference  and  Cartesian  product  of  V  and  W respectively.  If  V is  a subset  of W we write V  C  W' if x is an element  of V we write x  G  V; and \fx  is not an element of V we write x  ^V.  We let V\dV  V and int V denote the  complement boundary  closure  and interior  of  V respectively. Let 0  denote the  empty  set  let R  denote the  real numbers  let  R^  =  {x  ^  R \ x>  0} (i.e. R^  denotes the set of nonnegative real numbers) let Z denote the  inte gers  and let Z^  =  {x G Z  : x  >  0}. We will let /  C  i? denote open closed or half-open intervals. Thus for abGR a  ^  b J  may  be  of the  form  J  =  (a b)  =  {x  G  R  : a  <  x  <  b} or J  =  [ab]  = {x  G R  :  a  ^  X ^  b} or J  =  [a b)  =  {x  G R  :  a  ^  x  <  b} or J  =  (a b]  =  {x  E R  :  a  <  X ^  b}. Let R^ denote the real n-space.  If x  E  R^  then ''xi and  x^  =  (xi...  Xn) denotes the transpose  of the vector x.  Also let R^^^  denote the set ofmXn real matrices. If A G i^^X"  then [Clij] an an ail ^ ml CLynl Clin and A^  =  [aji]  G R^^^  denotes the transpose  of the matrix A. Similarly  let  C"  denote the  set of n-vectors  with  complex  components  and  let (^mxn  denote the set of mX  n matrices with complex  elements. Let  f  : V  -^  W  denote  a mapping  or function  from  a  set  V  into  a  set  W  and denote by D(f)  and R(f)  the domain  and the range of/  respectively. Also let f~^  : R(f)  -^  D(f) if it exists denote the inverse  off. CHAPTER  1: Mathematical Descriptions of Systems B.  Continuous  Functions First iQiJCR denote an open interval  and consider  a function  f  : J  ^  R.  Recall that/ is said to be continuous  at the point  to G J if lim^_>/Q f(t)  =  f(to)  exists i.e. if for every e  >  0 there exists a 5  >  0 such that \f(t)  -  f(to)\  <  e whenever \t-to\  <  8 and t  ^  J.  The function/  is said to be continuous  on J or simply continuous  if it is continuous at each point in  /. In the above definition  8 depends on the choice of to and e i.e. 8  =  8(6 to). If at each to ^  J  it is true that there is a 5  >  0 independent of fo [i-e. 8  =  8(e)]  such that 1/(0  -  /(^o)|  <  ^ whenever  \t -to\<8 and f E  /  then/ is said to be  uniformly continuous  (on  J). Let C(J R)  =  {f  : J  ^  R\  f  is continuous on  / }. Now  suppose  that /  contains  one or both  endpoints.  Then  continuity  is  interpreted as being one-sided at these points. For example if 7  =  [a b] then /  E  C(J R) will mean that  /  G  C((a b) R)  and that Um^^^+ f(t)  =  f(a)  and lim^^^-  f(t)  = f(b) exist. With k any positive integer and with J an open interval we will use the notation C^J  R)  =  {f  :J^ R\ the derivative  f^J^ exists on /  and fj^  G  C(J R)  for  j  =  0h...k  where/^^^  =  /} and will call/ in this case a C^-function.  Also we will call/ apiecewise  C^-function if /  G C^~^(J R) and /^^~^^ has continuous derivatives for all r G 7 with the possi ble exception of a finite set of points where  /"^^^ may have jump discontinuities. As before  when /  contains one or both endpoints then the existence  and continuity of derivatives is one-sided  at these points. For  any  subset  D  of  the  n-space  R^  with  nonempty  interior  we  can  define C(D R)  and  C^(D  R)  in  a  similar  manner  as  before.  Thus  /  G  C(D R)  indi cates  that  at  every  point  xo  =  (:v:io...  Xno)^  ^  D  lim;c->xo fM ^  f(^o)  exists or equivalently  at every  XQ G D  it is true  that  for  every  e  >  0 there  exists  a S  = 8(e  Xo) >  0 such that \f(x)  -  f(xo)\  <  e whenever  \xi  -  xio| H ^-1-^/2 -  x^o] <  8 and X G D. Also we define  C^(D  R)  as C\D R)  =  {f :D R dJf dx'l' C(D  R) i\  +  '"  + in  =  j j  =  l...k mdfGC(DR)} (i.e.  ii.. .in  take on all possible positive  integer  values  such that their  sum  is7). When D contains its boundary  (or part of its boundary) then the continuity of/  and the  existence  and  continuity  of partial  derivatives  of/  d^fldx^^ • • •o'xjf  /i  +  • • •  -h in  =  j  j  =  1...  ^ will have to be interpreted in the appropriate way at the bound ary points. Recall that if K  C  R^ K  7^  0  and K is compact  (i.e. K is closed and bounded) and if /  G  C(K  R) then/ is uniformly  continuous (on K) and/  attains its maximum and minimum on  K. Finally let D be a subset of R'^ with nonempty interior and Ictf-.D-^R^. Then : D ->  7^ /  =  1...  m. We say that  /  G  C(D R"^)  if /  =  (fh  ...  /m)^ where  / 10 Linear Systems /;.  E  C(A  / ? )  / - !  . . .  m and that for  some positive integer K  f  E. C^(D  R"^) if ft  ^  C^{D R)i  =  I... m. 1.3 INITIAL-VALUE  PROBLEMS In  this  section  we  make  precise  the  meaning  of  several  concepts  that  arise  in  the study of continuous-time  finite-dimensional  dynamical  systems. A.  Systems  of First-Order  Ordinary  Differential  Equations Let  D  C  R^^^  denote  a domain  i.e.  an  open  nonempty  and  connected  subset  of /^«+i  We call  R^^^  the  {t x)-space\  we denote  elements  of R^^^  by  {t x)  and  ele ments  of  R^  hy  X  =  {x\...  Xnf.  Next  we  consider  the  functions  ft  E  C{D  R) i  =  I..  .n  and if xt is a function  of t let jc-"^  =  d^xtldt^  denote the nth derivative of Xi with respect to t (provided that it exists). In particular when n  == 1 we usually write (1) . dxi We call the system of equations given by Xi  =  fiit  x i  . . .  Xn) i  =  1...  n (Ei) a system  of n first-order ordinary  differential  equations.  By a solution  of the system of equations  (Ei) we  shall mean  n continuously  differentiable  functions  (/>!...(/)„ defined on an interval /  =  (a b) [i.e. 4>  ^  C^U^ ^")] such that (f (f)\{t)...  (j)n{t))  E D for  all f E  /  and such that (/>/(0  =  fiit  Mt)... (/>^(0) i  = l.n for  all t  E  /. Next  we  let  (to x\o... x„o)  E  D.  Then  the  initial-value  problem  associated with  (Ei) is given by Xi  =  fi(t  xi...  Xn\ i  = l...n Xi(to)  =  Xi^ i  = l...n. A  set  of  functions  {(pi...  c/)^} is  a  solution  of  the  initial-value  problem  (//)  if {(1)1...  (pn}  is  a  solution  of  (£"/)  on  some  interval  /  containing  ^o  and  if ((t)l(to) (pnOo))  =  (-^lO  '•-  Xno)' ... In  Fig.  1.1  the  solution  of  a  hypothetical  initial-value  problem  is  depicted graphically  when  n  =  1. Note  that  (/>(T)  =  / ( r  x)  =  tanm  where  m  is  the  slope of  the  line  L  that  is  tangent  to  the  plot  of  the  curve  (/)(0  vs.  t  at  the  point (T  X). In  dealing  with  systems  of  equations  we  will  find  it  convenient  to  utilize the  vector  notation  x  =  (x\...  XnY  XQ =  (xio...  ^^Q)^'  ^  ^  (^i^  • • •' 4^n)^ f(t  X)  = X  = (xi... xn)\  and \l^ f(s  ci>(s)) ds  =  [\l  fi(s  (/>(^)) ds... \l  fn(s  cl>(s)) dsf. x)  . . .  fn(t  x)Y Xn))^  =  (f\(t (fi(t  Xi... fn(t  Xi... Xn)  ... Solution (p 11 CHAPTER  1: Mathematical Descriptions of Systems FIGURE 1.1 Solution of an initial-value problem when n  = I With  the  above notation  we  can  express  the  system  of  first-order  ordinary  dif ferential  equations  (£"/) by X =  fit  x) iE) and the initial-value problem (//)  by (/) We leave it to the reader to prove that the initial-value  problem  (/)  can be  equiva-lently expressed by the integral  equation X =  fit  x) xito)  =  XQ. m  =  X0+  C fisclis))ds JtQ iV) w^here cj) denotes a solution of  (/). B.  Classification  of Systems  of First-Order  Ordinary Differential  Equations Systems  of  first-order  ordinary  differential  equations  have been  classified  in  many ways. We enumerate here some of the more important cases. If in iE\  fit  X) ^  fix)  for all it x)  G D then X = fix). (A) We call (A) an autonomous  system  of first-order ordinary differential  equations. If it-{-Tx)ED  whenever it x)  G D and if fit  x)  =  fit  + T x) for all it  x) D  then iE)  assumes the  form X =  fit  x)  =  fit  +  T X). iP) We call such an equation a. periodic  system  of first-order differential  equations  with period  T. The smallest  T  >  0 for which  (P) is true is called the least period  of this system of equations. When in (£") fit  x)  =  Ait)x  where Ait)  =  [atjit)]  is a real nX  n matrix with elements aij that are defined  and at least piecewise continuous on a ^interval /  then 12 Linear Systems we have x  =  A{t)x (LH) and refer to (LH)  as a linear homogeneous  system  of first-order ordinary  differential equations. If for (LH)  A(t)  is defined  for all real t and if there is a T  >  0 such that A(t)  = A(t  +  T)  for  all t then we have X =  A(t)x  =  A(t  +  T)x. (LP) This  system  is  called  a  linear  periodic  system  of  first-order  ordinary  differential equations. Next  if  in  (E)  f(t  x)  =  A(t)x  +  g(t)  where  A(t)  is  as  defined  in  (LH)  and g(t)  =  [g\(Oy • • • > gn(t)V  is a real /z-vector with elements  gi that are defined  and at least piecewise continuous on a ^interval /  then we have X =  A(t)x  +  g(t). (LN) In this case we speak of a linear nonhomogeneous  system  of first-order ordinary differential  equations. Finally if in (E)  f(t  x)  =  Ax  where A  =  [atj] G R^^^^ then we have This  type  of  system  is  called  a  linear autonomous  homogeneous  system  of  first-order ordinary differential  equations. X =  Ax. (L) C.  nth-Order  Ordinary  Differential  Equations Thus  far  we  have  been  concerned  with  systems  of  first-order  ordinary  differen tial equations. It is also possible to characterize  initial-value problems by means of nth-order ordinary differential  equations. To this end we let /z be a real function that is defined and continuous on a domain/) of the real (r y... }^^)-space [i.e./)  C  R^^^ D is a domain and h  G  C(D R)]. Then / -)  =  h(ty/'\.../--'^) (En) is an nth-order  ordinary  dijferential  equation. A  solution  of  (En) is  a function  (f)  G  C"(/  R)  that  satisfies  (t (f)(t)  4>^^\t)... (/)^"-i>(0)  G D for  alH  G /  and cly^^\t) = h(tcfy(tlcj>^'\t)...cf>^--'\t)) for  all r G /  where /  =  (a b) is a r-interval. Now for  a given  (to xio... x„o)  G /) the initial-value  problem  for  (£„) is A  function  (/> is  a  solution  of  (/„)  if  0  is  a  solution  of  Eq.  (En)  on  some  interval containing  to and if (/)(^)  =  xio •..  <p^^~^\to)  =  Xno-As  in  the  case  of  systems  of  first-order  ordinary  differential  equations  we can point to several important  special  cases. Specifically  we consider  equations of 13 CHAPTER  1: Mathematical Descriptions of Systems the  form /«) + a_i(oy in-1) ^  ai(t)/^^  + ao(t)y  -  g(t) (3.1) where a/  G  C(J R) i  =  01.. nonhomogeneous  ordinary  differential  equation  of order  n. .n-l and^  E  C(/ R). Wereferto  (3.1) as  a/mear If in (3.1) we let ^(0  ^  0 then /^^  +  a-i(0/"-^^  +  • • • +  ^i(0/^^  +  ao(Oy  =  0. (3.2) We call (3.2) a linear  homogeneous  ordinary  differential  equation  of order  n. If in (3.2) we have ai{t)  =  at  i  =  01. .n  -  I  then /-^  +  an-i/^-'^ + +  aiy^^^ +  aoj 0 (3.3) and we call (3.3) a linear autonomous  homogeneous  ordinary  differential  equation of order  n. As in the case of  systems  of  first-order  ordinary  differential  equations  we  can dt^ne  periodic  and linear periodic  ordinary  differential  equations  of order n in the obvious way. It  turns  out  that  the  theory  of  nth-order  ordinary  differential  equations  can  be reduced to the theory  of a system of n  first-order  ordinary  differential  equations. To demonstrate  this  we  let  y  =  xi j^^^  =  X2... y^^~^^  =  Xn in  Eq.  (/„).  We  now obtain the system of first-order ordinary differential  equations X\  =  X2 X2  =  X3 Xn  =  h(tXi ...Xn) (3.4) that is defined  for  all (t xi...  Xn) E  D. Assume that 4>  =  ( 0 1  . . .  0^)^  is a solu tion of (3.4) on an interval J.  Since 02  =  (pi 03  =  0 2 . . . ..  0«  =  0 i " ^\  and  since h(t 0 i ( O  . . . 0.(0)  -  hit 0i(00^/^(0...  0 r ' \ O) it  follows  that  the  first  component  0i  of  the  vector  0  is  a  solution  of  Eq.  (En) on  the  interval  /.  Conversely  if  0i  is  a  solution  of  (En)  on  /  then  the  vector (0 0^^\  . . .  0^""^^)^  is  clearly  a  solution  of  (3.4). Moreover  if  0i(ro)  =  ^lo. • • •. 0i'^~^\^o)  =  ^no^ then the vector 0  satisfies  0(^o)  ^  ^o  =  (-^lo. • • •.  ^no)^-1.4 EXAMPLES OF INITIAL-VALUE PROBLEMS We now give several specific  examples of initial-value problems. EXAMPLE 4.1.  The mechanical  system of Fig.  1.2  consists of two point masses Mi and M2 that are acted upon by viscous damping forces (determined by viscous damping constants B Bi  and B2) spring forces  (specified  by the spring constants K K\  and K2) and external forces  f\  and /i. The initial displacements of Mi  and M2 at ^  =  0 are given by yi(0) and y2(0) respectively and their initial velocities are given by y\(0) 14 Linear  Systems v77'y//////yyy////yy//7?\^ FIGURE 1.2 An example of a mechanical  circuit and  J2(0).  The  arrows  in Fig.  1.2 indicate  positive  directions  of displacement  for M\ and  M2. Newton's  second  law  yields  the  following  coupled  second-order  ordinary  differ ential  equations  that  describe  the motions  of the  masses  in Fig.  1.2 (letting  y^^^ = d^yldf  =  y\ Miyi  +{B + Bi)yi  + (K + Ki)y -  Bh  -  ^ J2  =  / i (0 M2y2  +  (5 +  B2)y2  + (^  +  ^2)^2  -  ^i ji  -  Kyi  =  -  f2(t) (4.1) with initial data ji(0) 3^2(0) yi(0) and J2(0). Letting x\  = y\ X2 = y\ X'i = 3^2 and x^  = ^2 we can express Eq.  (4.1)  equiv-alently by the system of first-order ordinary  differential  equations x{ X2 x?> M. 0 Ki+K Ml 0 K .  M2 1 Bi+B Ml 0 B Ah 0 K Ml 0 0 B Wi 1 K  + K2 5  + ^2 M2 M2 J j pi 0 U2 Us L-^4 (4.2) /2(0 M2 with initial data given by  x(0)  =  (xi(0) X2(0) ^3(0) ^4(6))^. EXAMPLE4.2.  Using the node voltages vi V2 and V3 and applying Kirchhoff's  current law we can describe the behavior  of the electric circuit given in Fig.  1.3  by the  system of  first-order  ordinary  differential  equations 1 /I /I 1 Ci 1 1 1 R2C2 1 RiCi (Ri V L 1 ^ 2 Ci 1 RiC: 2 ^2 1 0 Rl L 0 [vi' V9 LV3. + V /^iCi V RiC 0 (4.3) To  complete  the  description  of this  circuit  we specify  the  initial  data  at ^o = 0 • given by vi (0) V2(0) and V3(0). EXAMPLE  4.3.  Figure  1.4 represents  a  simplified  model  of an armature  voltage-controlled  dc servomotor  consisting  of a stationary  field  and  a rotating  armature  and load.  We  assume  that  all effects  of the  field  are  negligible  in the description  of this • A A /V 6 v^  ^VW C^^ 15 CHAPTER  1: Mathematical Descriptions of Systems FIGURE  1.3 An  example  of  an  electric circuit Armature FIGURE  1.4 An example of an electromechanical  system system.  The  various  parameters  and  variables  in  Fig.  1.4  are:  Ca  =  externally  apphed armature  voltage  ia =  armature  current  Ra =  resistance  of  the  armature  winding.  La  = armature  winding  inductance  Cm  =  back-emf  voltage  induced  by  the  rotating  armature winding  B  =  viscous  damping  due  to  bearing  friction  /  =  moment  of  inertia  of  the armature and load and  0  =  shaft  position. The back-emf  voltage  (with the polarity  as shown) is given by em  =  KeO where  KQ  >0  is 2i constant  and the torque  T  generated by the motor is given by T=KTia-Application of Newton's  second law and Kirchhoff's  voltage law yields and at je+Be  =  T{t) (4.4) (4.5) (4.6) (4.7) Combining  (4.4)  to  (4.7)  and  letting  xi  =  eX2  =  9  and  X3 =  4  yields  the  system  of first-order  ordinary differential  equations 16 Linear  Systems 1 0 B KT J  T -0 0 0 0 0 _ pi IU2I + I "" |. \x2 Us J [Ta\ + (4.8) A  suitable  set  of  initial  data  for  (4.8)  is  given  by  ^o  =  0  and  (xi(0) X2(0) X3(0))^  = • (^(0) ^(OX/(0))^ EXAMPLE  4.4.  A much  Studied ordinary  differential  equation is given by X  +  f(x)x  +  g(x)  =  0 (4.9) where /  G  C\R  R)  and g  G  C\R  R) When  f(x)  >  0  for  all  x  G  /?  and  xg(x)  >  0  for  all  x  7^ 0  then  (4.9)  is  called the Lienard  Equation.  This  equation  can  be  used  to represent  e.g.  RLC  circuits  with nonlinear circuit  elements. Another important  special case of (4.9) is the van der Pol Equation  given by x-e{\ -  x^)x  + x  =  0 (4.10) where e  >  0 is a parameter. This equation has been used to represent certain  electronic oscillators. If in (4.9) f(x)  ^  0 we obtain X  +  g(x)  =  0. (4.11) When xg(x)  >  0 for all x  9^  0 then  (4.11) represents various models of so-called  "mass on a nonlinear spring." In particular if g(x)  =  ^(1 + a^x^)x  where ^  >  0 and a^  >  0 are parameters then g represents the restoring force of a hard spring. If g(x)  = k(l-a^x^)x where  ^  >  0 and  a^  >  0 are parameters then g represents  the restoring  force  of  a  soft spring.  Finally if g(x)  =  x then g represents the restoring force of a linear spring.  (See Figs.  1.5  and  1.6.) For another special case of (4.9) let f(x)  =  0 and g(x)  =  k sin jc where  /: >  0 is a parameter. Then  (4.9) assumes the  form X +  ^ sin X =  0. (4.12) This equation  describes  the motion  of a point mass moving  in a circular path  about  the axis of rotation normal to a constant gravitational field  as shown in Fig.  1.7. The param eter k  depends  on the radius  / of the circular path  the gravitational  acceleration  g  and the mass. The symbol x denotes the angle of deflection  measured from  the vertical. The present model is called  a simple  pendulum. //////// FIGURE  1.5 Mass on a nonlinear  spring 9(x)i / V-X / 17 CHAPTER  1: Mathematical Descriptions of Systems (a) Soft spring (b) Hard spring (c) Linear spring FIGURE  1.6 Letting  x\  =  x  and  X2 =  x  the  second-order  ordinary  differential  equation  (4.9) can be represented by the system of  first-order  ordinary differential  equations given by Xi  =  X2 Xl  =  -f(Xi)X2 - g(Xi). (4.13) The required initial data for  (4.13) are given by  xi(0)  and X2(0). FIGURE  1.7 Model of a simple  pendulum *1.5 M O RE  M A T H E M A T I C AL  P R E L I M I N A R I ES At  this  point  v^e  need  to  review  additional  material  from  the  calculus  and  anal ysis. A.  Sequences Let /  denote  an  index  set  (usually  the  set of positive  integers). A  sequence a  real  sequence) such  a  sequence  by  {x„} rather  than  {/(n)}. in 7^ (i.e. is  a  mapping  of /  into  R  say  f{n)  =  x„.  It is  customary  to  denote *Throughout the book starred sections subsections or items may be omitted to conserve time without loss of continuity. 18 Linear Systems Let  {xn}  be  a  sequence  in  R  n  =  1 2 3  . . .  and  let  ni  n2...  n^...  be  a j.  Then strictly increasing sequence of positive integers i.e. ni  >  Uj whenever  /  > the sequence {x„J  is called a subsequence  of {x„}. Recall  that  a real  sequence  {xn} is  said  to  converge  to  an  element  in  R  if  for every 6  >  0 there is an integer N  such that for all n  >  N\x-  Xn\ <  e. In general A^ depends  on e i.e. N  =  N(€).  We call x the limit  of {x„} and we usually  write this as lim„_>oo Xn =  x  or Xn -^  x  as n ^  ^.  If there is no x  E  /^ to which the  sequence converges then we say that {xn}  diverges. A real sequence {xn} is said to be a Cauchy sequence  or a fundamental  sequence if for  every  e  >  0 there is an integer A^ =  N(e)  such that  |x^  -  x^|  <  e  whenever mn^N. It  is  easy  to  show  that  every  convergent  sequence  is  also  a Cauchy  sequence. One  of  the  fundamental  results  in  analysis  shows  that  for  R  the  converse  to  this statement is also true: every real Cauchy  sequence is a convergent  sequence  (i.e. it converges  to an element in R).  To express  this property  we say that R is  complete. Other important fundamental  properties that follow  from  the completeness  of R  in clude the Bolzano-Weierstrass  (B-W) property  and the Heine-Borel  (H-B)  property. The  B-W  property  states  that  every  bounded  sequence  of  real  numbers  contains  a convergent  subsequence. To present the H-B property  we require the following  additional concept: by a finite (or countable or uncountable)  open  covering  of a set £  C  /^ we mean a finite (or countable or uncountable) collection {Ga} of open sets such that E  C  UaGa.  The H-B property  states that every open covering of a compact set K contains 3. finite open subcovering of K.  (Recall that aset  K  C  Ris  compact  if it is closed and bounded.) B.  Sequences  of  Functions Next we consider sequences  of functions.  For our purposes we let £" be a nonempty subset of ^  and we let {/„} n  =  1 2 3  . .. denote a collection of real-valued  func tions defined  on E  (i.e. for  each  n  G I  where /  denotes the positive integers there is a mapping  fn  :  E  -^  R). We say that the sequence of functions  {fn} is pointwise  convergent  to a  function /  on E  if  lim^^oo fn(t)  =  f{t)  for  all  t  G E  i.e.  if  for  every  e  >  0  and  for  every t  E  E  there exists an integer N  that may depend on e  and t [i.e. N  =  N(e  t)] such that  1/^(0  -  f(t)\  <  e whenever  n>  N. The sequence {/„} is said to converge  uniformly  to a function/  on E if for  every 6  >  0 there is an integer N that depends only on e  [i.e. N  =  N(e)]  such that 1/^(0  ~ f(t)\  <  e  whenever  n>  Nfor  all t  G  E. For example the sequence {/„} specified  by fM ^ t\ 0  <  r <  1 (5.1) is pointwise  convergent  to the  function but  it  is  not  uniformly  convergent  to /.  Note  also  that  whereas  for  each  n  = fn  in  (5.1)  is  continuous  on  E  =  [0 1] the  limiting  function/  in  (5.2) 12... is not continuous on  E. As another example we note that the sequence {/„} specified  by (^  =  1 2  3  . . .) 19 fn(t) ^ t+ 1 - n - 00  <  r  <  00 is pointwise  convergent  and  uniformly  convergent to the  function f{t)  =  t -co  <  t  <co. CHAPTER  1: Mathematical Descriptions of Systems (5.3) (5.4) Note  also  that  in the  above  example  the  / „  n  =  1 2 3  . .. given  in  (5.3)  as  well  as /  given  in  (5.4)  are  continuous  on  R. THEOREM  5.1.  Let  /„  G  C(E  R)  n  =  1 2  . . .  and  assume  that  the  sequence  {/„} converges uniformly  t o/  on E. Then  /  G  C{E  R). Proof  Let  to G E.  We must  show  that  lim^^^^ f(t)  =  f(to)  or equivalently  we  must show  that for  every  e  >  0 there  exists  a 8  =  8(6 to) >  0 such  that  \f(t)  -  f(to)\  <  e whenever  |^ -  ^o| <  ^• Since  /„  converges  t o/  uniformly  on E  given  e  >  0 there exists N  =  N(e)  such that  \fN(t)  -  f(t)\  <  e/3  for  all  t  G  E.  Also  since  fN  G  C(E  R)  there  exists  8  = 8(e  to) >  0  such  that  |/iv(0  -  /iv(^)|  <  ^/3  whenever  \t -  to\ <  8.  Therefore  when ever  1^  -  ^1 <  8 we have 1/(0  -  /(^0)|  =  1/(0  -  fN(t)  +  fN(t)  -  fN(t0)  +  fN(to)  - f(to)\ ^  1 / (0  - € € / v ( 0|  +  | / v (0  - € <  3 +  3 +  3  = e. fN(to)\  +  \fN(to)  ~ f(to)\ THEOREM  5.2.  Let  /„  G  C(E  R) n  =  1 2  . . .  and £" be a bounded  subset of R.  As sume that the sequence {/«} converges uniformly  t o/  on E.  Then lim fn(t)dt \imfn{t)dt  = f(t)dt. E  "-^°° JE (5.5) Proof  We  have fn(t)dt- f(t)dt= Unit)- fit)]  dt. E JE JE Also  let  /  =  (a b)  denote  a  bounded  interval  with  the  property  that  J  D  E  and  let L(J)  =  (b-  a). Since f  converges t o/ in t uniformly on a bounded set E we can choose for a given e  >  0  an  A^ =  N(€)  such  that  \fn(t)  -  f(t)\  <  elL{J)  for  dXX  t  G E  whenever  n>  N. Therefore fn(t)dt- f(t)dt \fn(t)-f(t)\dt UJ)  = € L(J) whenever  n>  N. As  an  example  consider fn{t)  = 0^ t  ^  n n  <  t n  =  1 2 3 The  sequence  {fn}  converges  uniformly  to the function  f(t)  =  0  for 20 Linear Systems all t. It is easily  shown that while fn(t)dt  =  1 r  OC lim 7 1 -^  CO  0 r  00 lim/„(0^^  =  0. Jo  n^oo Theorem 5.2 does not apply in this case since the interval E is not  bounded. As another example consider fn(t)  =  n^te~''\ 0  <  ^ <  1 n  =  1 2 3 in this case that The sequence {/„} converges pointwise on [0 1]. It is easily  shown while lim fn(t)dt  =  1 lim  fn(t)dt  =  0. Jo  «-^°° Theorem  5.2  does not apply  in this case since {/„} is not uniformly  convergent  on [0 1]. The point of the above two examples is this: in the case of sequences of functions care must be taken when interchanging  limits and  integration. The  next  result  is  called  the  Cauchy  criterion for  the  uniform  convergence  of functions. THEOREM 5.3.  Let fn'.E-^  R n  =  1 2 3 The sequence of functions  {/„} con verges uniformly  on E if and only if for every e > 0 there exists an integer A^ =  N(e) such that \fn(t) -  fm(t)\ < € for Sillt G E whenever n>  N and m>  N. Proof. Assume that {/„} converges uniformly  on E to the limit function /.  Then there exists an integer A^ =  N{e) such that when n>  Nwe  have l / « ( 0 - / ( O l <| foralUG^. This implies that \fn(t) -  fm(t)\ =  \fn(t) "  f(t)  + f(t)  " fM\ ^\fn(t)-f(t)\ + \f(t)-f^(t)\<e for all r E £• whenever n>  N and m>  N. Conversely assume that the Cauchy condition holds i.e. for all t E. E \fn{t) -  f^{t)\  < e when n^  Nm^  N. (5.6) This implies that the sequence {/n(0} converges for every ^ E £• to a limit that we call f(f).  (This follows  since in R every Cauchy sequence converges to an element in R.) We must show that this convergence is uniform.  To this end we let e >  0 be given and pick A^ >  0 so that (5.6) holds. Fix n and let m ^  ^  in (5.6). Since fmit) -^  f(t)  as m ^  00 this yields for all t G. E \fn(t)-f(t)\^e foralln^A^. • C.  The Weierstrass  M-test For an  infinite  series  of real-valued  functions  written  X j =i  fj(0^  with each  fj  de fined  on  a  set £" C  R  convergence  is  defined  in  terms  of  the  sequences  of  partial sums 21 CHAPTER  1: Mathematical Descriptions of Systems n Sn{t) = ^fj(t). 7 = 1 The  series  2 J =i  fn(t)  is  said  to  converge  pointwise  to  the  function/  if  for  every t  GE Km =  0. 7 = 1 Also the  series Xj  = i fj(t)  is  said to converge  uniformly  tofonE of partial sums {sn} converges uniformly  t o/  on E. The next result is called the  Weierstrass  M-test. THEOREM 5.4.  Let fn'. E ^  Rn  =  \23 stants Mn  such that \fn{t)\  <  Mn for dXXt GE  and Suppose there exist nonnegative con if the  sequence XM„  <  00. Then the sum X^=i /n(0 converges uniformly on E. Proof If  X^=i Mn converges then for  arbitrary  6 >  0 there are m >  /2 sufficiently large so that Y\fM j = n m The uniform convergence of X^= i fn{t) follows now from Theorem 5.3. *1.6 EXISTENCE OF SOLUTIONS OF INITIAL-VALUE PROBLEMS In  this  section  we  address  the  following  question:  under  what  conditions  has  the initial-value problem  (/)  at least  one solution  for  a given  set of initial data  (to  XQ)! The significance  of this question is illustrated by the following  two examples. 1.  For the initial-value  problem X =  g(x) x(0)  =  0 t^O (6.1) where I g(x)  =  0 X  =  0 X ^  0 no differentiable  function  cf) exists that satisfies  (6.1). Hence no solution  (as de fined in Section  1.3)  exists for this initial-value  problem. 22 Linear Systems 2.  The initial-value  problem x  =  x^'^ xit^)  =  0 (6.2) has the  solution  (j){t) =  [2it  -  ^)/3]^^^  (determined  by  separation  of  variables). This solution is not unique since i//(0  =  0 is clearly also a solution. To  simplify  our  presentation  we  will  consider  in  this  section  and  in  Sections 1.7  to  1.9  one-dimensional  initial-value problems  [i.e. we will assume that for  (/) n  =  I]. Later we will  show how these results  are modified  for  higher  dimensional systems.  Thus  we  have  a domain  D  C  R^  f  E  C{D R)  we  are  given  the  scalar differential  equation X =  fit  x) (E') we are given the initial data (to XQ) G D and we seek a solution  (or solutions) to the one-dimensional  initial-value  problem In doing so it suffices  to find a solution of the integral  equation X =  f(t  X\ x(to)  =  XQ. m  =  xo^ f  f(scl>(s))ds. J to (/') (V) We will solve the above problem in stages. First we establish an existence result for  a  sequence  of  approximate  solutions  of  (/').  Next  we  show  that  this  sequence converges to the actual  solution of (/')  using  a preliminary  convergence result. We will establish this preliminary  result first. A.  The Ascoli-Arzela  Lemma We will require the following  concepts. DEFINITION 6.1.  Let ^  be a family of real-valued functions  defined on a set £  C R. (i)  9^ is called uniformly bounded if there is a nonnegative constant M such that \f{f)\  < M for 2i\\t^E  and for all /  G  9^. (ii)  ^  is called equicontinuous on E if for every € >  0 there isaS  = 6 ( e ) >0  (inde pendent of t\ t2 and/)  such that |/(^i)  -  /(^2)| <  ^ whenever |ri  -  ^2! <  ^ for all tut2  G£  and for a l l/  G 9^. • The next result is known as the Ascoli-Arzela  Lemma. THEOREM 6.1.  Let £ be a closed and bounded subset of R and let {/^} be a sequence of functions in C(E R). If {/m} is equicontinuous and uniformly bounded on E then there is a subsequence {m^} of {m} and a function  /  G C(E R) such that {/m^} converges to/ uniformly on E. Proof. Let {r^} be a dense subset of £" (i.e. {r^} =  E). (For example let {r^} be the enu meration of the rational numbers contained in an interval [a b] C R.) The sequence of real numbers {/m(^i)} is bounded since {/^} is uniformly bounded on E. By the Bolzano-Weierstrass Theorem (see Section 1.5) {fm(ri)} contains a convergent subsequence that we label {/im(^i)}- We denote the point to which this subsequence converges by  /(ri) and the sequence of functions obtained in this way by {fim}- Consider next the sequence of real numbers {/im(^2)} which is also bounded and contains a convergent subsequence that we label {/2m(^2)}- We denote the point to which {/2m(^2)} converges by /(r2) and 23 CHAPTER 1: Mathematical Descriptions of Systems we  label  the  sequence  of  functions  obtained  in  this  way  by  {fim}-  Continuing  we  ob tain the subsequence {/^^}  of the sequence {fk-im}  and the real number  /(r^)  such that fkmiXk) -^  fi^k)  as  m ^  00 for  ^  =  1 2 3 Since  the  sequence  {fkm} is  a  subse quence of all the preceding  sequences {fjm} for  1 <  7 <  /: -  1 it will converge at each point Kj  with  1 <  j  <  /:. Next we generate a subsequence by  ''diagonalizing"  the preceding infinite  collec tion of sequences. In doing so we set gm  =  fmm for  all m. If the terms  fkm are  arranged as the elements of a semi-infinite  matrix as shown in Fig.  1.8 then the elements gm are the diagonal elements of this matrix. Since [gm] is eventually  a subsequence of every sequence {fum} we have gmirk)  -^ f(rk)  as m ^  00 for  /:  =  1 2 3  We now  show that {gm} converges uniformly  on  E. Fix e  >  0. For any rational number  rj  G E  there exists Mj  =  Mj{e)  such that  {gmiXj)  ~ Sn{rj)\ <  e for all mn>  Mj(e).  By the equicontinuity of {/„} there is a 5  >  0 such that \gn(ti) -  gn(t2)\ <  € foY all u whcu ti t2 E:  E  and 1^1  -12\  <  8. Therefore for  \t -  rj\  < 8 and mn^  Mj{e)  we have \gm(t)  -  gn{t)\  < \gm{t)  "  gm{rj)\  +  \gm{rj)  "  gn{rj)\ +  \gn{rj)  -  gn{t)\  <  3e. 8} covers By construction the collection of neighborhoods B{rj  8)  =  {t E  R'.\t-rj\< E  (i.e.  Uy B(rj 8)  D E).  Since  Eh  a  closed  and  bounded  subset  of R  (i.e.  since  E  is compact) by the Heine-Borel Theorem there is ?i finite subcollection of the above neigh borhoods say {B{rji 8)...  B(rjL 8)} that covers E i.e. B(rji  5) U • • •  U B(rjL 8)  D  D (see  Section  1.5).  Let  M(e)  =  max{M;i(e)..  .MJL(€)}. If  m  and  n  are  larger  than M(e)  and  if  t is  any  point  in  E  then  t  E  B(rji 8)  for  some  /  G  { 1  . . .  L}.  Therefore IgmiO  ~  gn(t)\  <  36  for  all  ^ E  £• whenever  mn>  M(e).  This  shows  that  {gm} con verges  uniformly  on £" to a function/  by  Theorem  5.3. Furthermore  /  E  C(E  R)  by Theorem 5.1. • /ll  /12  /l3  /l4 /2I /23  /24 /3I  /32  /33  /34 fll FIGURE  1.8. Diagonalization  of a collection of  sequences B.  6-Approximate  Solutions We will require the following concept. DEFINITION6.2.  A real-valued function  (p defined and continuous on a ^-interval J  = (a b) containing  ^  is called an e-approximate  solution of (/')  if <^(^)  =  XQ  and (i)  {t (/)(0)  E  D for  all t  E  /; (ii)  (j)  has  a continuous  derivative  on /  except  possibly  on  a  finite  set /  of points  in J where there are jump discontinuities  allowed; (iii)  |(/)(0 -  f{t  c/>(0)| <  e  for alU  E  /  -  /. • Recall  that if /  in the  above  definition  is not empty  (j) is  said  to have apiecewise continuous  derivative  on  J. 24 Linear  Systems Now  let S  =  {{tx)  eD: \t-to\  < a |JC-JCO|  <b} (6.3) be  a  fixed  rectangle  in D  containing  (^0-^0) as  shown  in Fig.  1.9.  Since /  G  C{D^R) it is bounded  on  S  and  there  is  an M  >  0  such  that  \f{tx)\  <M  for  all  {tx)  e  S.  We define  (see  Fig.  1.9) mm b " 'M (6.4) We  now  prove  the  following  existence  result. xo  + b rs LZ y^.xo) \\ xo + b to + a {to^Xo)^^^>^ to + a (a) (b) FIGURE  1.9 (a) Case c = b/M  (b) case c = a THEOREM 6.2.  If f  eC{DR)  and if c is as defined  in (6.4) then for  any  e >  0 there is an e-approximate  solution of  (/')  on the interval  |^ — ^o|  <  c. Since  / is  continuous  on  S  a  closed  and  bounded  set  then  / Proof  Given e >  0 we shall show that there is an e-approximate  solution on  [^o ^0 +  c]. The proof  for  the interval  [^0 — c to]  is  similar.  The  approximate  solution  will be  made up of a finite number of straight line segments joined at their ends to achieve continuity. is  uniformly continuous  on  S.  Hence  there  is  a  5  >  0  such  that  \f(tx)  — f(sy)\  <  e  whenever (tx)  and  (sy)  are  in  S  with  \t — s\  <  d  and  \x — y\  <  d.  Now  subdivide  the  interval [^0 ^0 +  c] into m equal subintervals by a partition  ^0 <  ^1 <h  <  • • •  <tm  = to-\-c  where tj^i  — tj  < min{5 d/M}  and where M  is the bound  for /  given above. On the  interval let  ^(t)  be  the  line  segment  issuing  from  (^o-^o) with  slope  /(^o-^o)-  On to <t  <ti h  <  t  <  t2  ^Qt (j)(t) be  the  line  segment  starting  at  (^1^(^1))  with  slope f{t\(^{t\)). Continuing  in  this  manner  we  define  0  over  to < t  <  tm-  A  typical  solution  is  shown in  Fig.  1.10.  The  resulting  <p  is  piecewise  linear  and  hence  piecewise  continuously differentiable  and (l){to)  =xo.  Indeed  on tj  <t  < tj^i  we have (^it)  =  ^{tj)+f{tj^{tj)){t-tj). (6.5) Since the slopes of the linear segments in (6.5) are bounded between  ±M then  (^  0(0) cannot leave S before  time tm = to-\-c  (see Fig.  1.10). To see that  0  is an 6-approximate  solution  we use (6.5) to obtain \<i>it)-f{t<l>{t))\ = \fitjcj>itj))-fitcj>it))\<e. This inequality  is true by the choice of  5  since  \tj -•t\<\tj--tj^i\  <  5  and \(t>it)-Htj)\<M\t--tj\  <M d. This completes the proof. 25 CHAPTER  1: Mathematical Descriptions of Systems (^0. ^o) FIGURE 1.10 Typical e-approximate solution ^0+^ The approximations  defined  in the proof  of Theorem  6.2  are called Euler  poly gons  and (6.5) with t  =  tj+i  assumes the  form 0(O+i)  =  ^(0)  +  f(^P  ^(0))(0+i  -  0)^ (6.6) which  is  called  Euler's  method.  This  technique  and  more  sophisticated  piecewise polynomial  approximations  are common  in determining  numerical  approximations to solutions of (/')  via computer  solutions. C.  The  Cauchy-Peano  Existence  Theorem We are now in a position to state and prove the main result of this  section. THEOREM 6.3.  If /  G C{D R) and (^0 -^o) G D then (/')  has a solution defined on \t -  to\  ^  c [where c is defined in (6.4)]. Proof. Let  {e^}  m  =  1 2...  be  a monotone  decreasing  sequence  of positive num bers  tending  to  zero  e.g.  6^  =  l/m.  Let  c  be  defined  in  (6.4)  and  let  c/)^ be  the e^-approximate solution given in Theorem 6.2. Then |0m(O "" <Pm(s)\  ^  M\t -  s\ for all t s in [to  -  cto + c] and for all m >  1. Therefore {(f)m} is an equicontinuous sequence. Now since \ct>m(t)\ n(to)\  +  \(l>m(t)  - (t>m(to)\  ^  k o|  +  Mc the sequence is also uniformly  bounded. By the Ascoli-Arzela Lemma (Theorem 6.1) there is a subsequence {(/>mj that converges uniformly on /  =  [^o -  c ^  + c] to a con tinuous function  0. Next define emit)  =  (f)m{t)  - fit (j)mit)) (6.7) at those points where 4>m exists. From the proof of Theorem 6.2 em is piecewise contin uous and \emit)\  ^  6^ on 7 where 4>m exists. Integrating (6.7) and rearranging terms we obtain (t>mit)  =  ^0  + [ / ( ^  (t>mis))  +  emis)] J to ds. (6.8) 26 Linear Systems Now since {4>mj} tends to cf) uniformly on / and since/ is uniformly continuous on ^^^ ^^^ '^ [defined in (6.3)] it follows that f(t  (l>mj(t)) tends to f(t  (f){t)) uniformly on /. To see this we note that for every 6'  >  0 there is an A/^ =  N{d') such that |(^^^ (0 - 0(01 < 3'  for allr  E  /  whenever  k > N  and also for  every e  >  0 there is a 6  =  6(e)  such that for all t E  J \f{t p)  -  f(t  q)\ < e for all (t pi  (t q) e  S whenever  \p -  q\ < 8. Pick A^ large enough so that  |6m^(0 ~  </>(0l < ^  for  alH  E  /  whenever  k>  N.  Then \fit  (l)m^(t))  -  fit  0(0)1 <  e for all f E /  whenever k>  N. Using Theorem 5.2 we now obtain lim  I  f(s(l)^^(s))ds  =  \ s =  I  \imf(s4>mk(s))ds \imf(s(l)mk(s))ds (6.9) f(scl(s))ds to Also observing that em^(s)ds<\ \em^(s)\ds  =  \  e^^ds  <  €mj^c JtQ JtQ JtQ and recalling that lim^^^oo 6^^  = 0  we obtain lim  I  em^(s)ds  = 0. (6.10) Letting in (6.8) m  = rrik and using (6.9) and (6.10) we finally obtain lim 0^(0  =  m  =  xo +  f  f(scl>(s))ds (V) • which completes the proof. Theorem  6.3  establishes  the  existence  of  a  solution  of  (/')  "locally''  i.e.  only on some sufficiently  short time interval. In general this theorem cannot be  changed to assert  the  existence  of  a  solution  for  all  t  >  to or for  all  r <  ^Q. As  an  example consider the initial-value  problem X =  1 -\-  x^ x(0)  -  jco  =  0 which has a solution given by (pit)  =  tant.  This solution exists only when  -7r/2  < t  <  77/2. *1.7 CONTINUATION  OF  SOLUTIONS Once the existence of a solution of an initial-value problem has been established over some time interval it is reasonable to ask w^hether this solution can be extended to a larger time interval. We call this process continuation  of solutions.  In this section we address this problem for the scalar initial-value problem  (/'). We shall consider  the continuation  of  solutions  of  an initial-value  problem  (/)  characterized  by  a  system of equations later (in Section  LIO). A.  Zorn's  Lemma In the proof  of the main result of this  section  we will require  a fundamental  result from analysis called Zorn's Lemma that we will present without proof. To state this lemma we need to introduce the following  concepts. K partially  ordered set {A :) consists of a set A and a relation <  on A such that for any a b and c in A 1.  a ^  a 2.  a ^  b and Z?  <  c implies that a 3.  a  <  Z? and b ^  a implies that a  b. 27 CHAPTER 1: Mathematical Descriptions of Systems A chain is a subset AQ of A such that for all a and b in AQ either a ^  b or b ^  a. An wp/^^r bound for a chain AQ is an element ^o E A such that b ^  ao for all Z?  E  AQ. A maximal  element  for A if it exists is an element ai  of A such that for all b in A a\  ^  b implies (3i  =  b. THEOREM 7.1.  (ZORN'S LEMMA).  If each chain in a partially ordered set (A <) has an upper bound then A has a maxiinal element. • B.  Continuable  Solutions Now let (/) be a solution of (£"') on an interval /. By a continuation  of cf) we mean an extension (j)o of (/) to a larger interval  /Q in such a way that the extension solves (£"') on  JQ\ then (/) is said to be continued  or extended  to the larger interval  JQ. When no such continuation is possible then cf) is called  noncontinuable. To illustrate  these  ideas  consider  the differential  equation  x  =  x^  that  has a solution 0(0  -  (1 - 0 " ^ o n/  =  (-11). This  solution is continuable to the left  to -oo and is noncontinuable to the right. As a second example consider the differential  equation x  =  x^^^ that has a solution il/(t)^OonJ  =  (-10). This  solution  is continuable  to the right  in more  than  one way. For example  both i/^i(0  =  0 and il/2(t) =  {ItH^f^  are solutions of i:  =  x^'^ for t >  0. The solution ip can also be continued to the left  using  ijj^it) =  0 for alW < - 1. THEOREM 7.2.  Let /  E C{D R) with/ bounded on D. Suppose (/> is a solution of {E') on the interval J  = (a b). Then (i)  the limits lim  (j){t) <t>{a^) and lim  (/)(0 cj>{b-) exist and (ii)  if {a (f)(a'^))  [respectively (Z? (t>(b'))] is in £) then the solution 0 can be continued to the left past the point t  = a (respectively to the right past the point t  = b). Proof We give the proof for the endpoint b. The proof for the endpoint a is similar. Let M be a bound for \f{t x)\ on D fix ^o E / and let (f){t{y)  = XQ. Then for to < t < u < b the solution 0 satisfies (V)  and thus. |c^(^^) -  ml  = f(sct>(s))ds \f(scl(s))\ds (7.1) Mds  = M(u -  t). 28 Linear  Systems Given  any  sequence  {tm} C  (tob)  tending  monotonically  to  Z?  we  see  from  (7.1) ^^^^  {4^(^m)}  is  a  Cauchy  sequence  and  therefore  the  Hmit  (/>(/?")  exists  (see  Sub section  1.5 A). Next  if  (b (i>{b~))  G  D  then  by  Theorem  6.3  there  is  a  solution  (/)o of  (£')  that satisfies  (/>o(^)  =  4>(b~). The solution (/)o will be defined on some interval Z?  <  t  ^  b + c for  some  c  >  0. Define  (f)o(t) =  (j){t)  on a  <  t  <  b. Then  cf^o  is  continuous  on a  <  r  < b  + c and  satisfies (/>o(0  =  -^0 +  [  f(s  (l)o(s))ds a<t<b (7.2) and (/)o(0  -  (^(Z?") 4- Jb f(s  (l)o(s))ds b  <  t  <  b  + c. The limit of (7.2) as t tends to b is seen to be (/>(/?")  =  xo + f{s(l)o(s))ds. JtQ Therefore (j)Q{t)  =  xo+\ Jto f(s(l)o(s))ds  + Jb f(s(l)o{s))ds =  ^0 + f(s  (t)o(s)) ds b  <  t  <  b +  c. Jto Hence </>o  solves (V)  ona  <  t  <  b + c and therefore  (J)Q  solves (/')  ona  <  t  <  b +  c. C.  Continuation of Solutions to the Boundary of D We are now in a position to prove the following result. THEOREM  7.3.  If  /  G  C{D R)  and  if  (/) is  a  solution  of  (E')  on  an  open  interval  7 then  (/) can be continued  to a maximal  open interval  /*  D 7 in such a way that  {t (pit)) tends to dD  ast-^ extended  solution (^* on /*  is noncontinuable. (?/* when o'D is not empty  and  \t\  + \4>(t)\ -^  (^ifdD is empty. The Proof  Let 0  be a given solution of (£") on J.  The graph  of (/> is the  set Gr(cl>)  =  {(t m) -t^Jl Given  any  two  solutions  (j)\ and  (/>2 of  {E')  that  extend  </> we  define  ^\  <  (/>2 if  and only  if  Gr((pi)  C  Gr{4>i)^  i.e.  if  and  only  if  (f)2  is  an  extension  of  cpi. The  relation  < determines  a partial ordering  on continuations  of (f) over open intervals. If {4>a  : a  G A} is any chain of such extensions then  U{Gr((/)Q) : a  G A} is the graph of a continuation of (j) that we call (/)A. This (pA is an upper bound for the chain. By Zorn's Lemma  (Theorem 7.1)  there  is  a maximal  element  (/>*. Clearly  c^* is  a  noncontinuable  extension  of  the original  solution (/>. Now let 7* denote the domain of (/>*. By Theorem 7.2 the interval /*  must be open for  otherwise  0*  could not be maximal.  Call /*  =  {a b). If  b  =  ^  then we know  that D is unbounded and \t\ + |0(O| -^  o^ as r ^  b~.So  let us assume that b  <^  and assume for  purposes  of  contradiction  that  (t </>*(0)  does  not  approach  dD  on  any  sequence {tm} that  approaches  b~.  Then  (t (p^it))  must remain  in  a compact  subset K  of D  when ^ E  [c Z?)  for  any  c  G  (<2 b).  Since/  must  be bounded  on K  then  by  Theorem  7.2  we can continue (/)*past b. But this is impossible since (^*is noncontinuable. We have shown that {t (/)*(0) must approach SD on some sequence {tm} that approaches b~. We now claim that {t (^*(0) -^  dD as r ^  h~ .\i  this is not the case there exists a sequence {T;„} that approaches h~ and a point (Z? ^)  E^  D such that (t)^{Tm) —>  ^. Let e be one-third the distance from {b ^) to dD. We can assume without loss of generality that Tm<tm<  Tm+x  (T^ (/>*(T^))  ^  5((Z? ^  e\  and (^^ (/>*(r^)) £  5((Z7 ^) 2e) for all m >  1. Let M be a bound for \f{t x)\ over ^((Z? ^) 2e). It now follows from the proper ties of (£") that 29 CHAPTER  1: Mathematical Descriptions of Systems /fe(^*(5))J^  +  {tm -  Tm) <  (M  -h  \){tm  "  T^) i.e. ^^ -  T^ >  el{M + 1) for all m. But this is impossible since tm conclude that {t (/>*(0) -^dD2iSt->b~. The proof for the endpoint t  = ais  similar. b  and b < ^. We *1.8 UNIQUENESS  OF  SOLUTIONS We now  establish  conditions  for  the  uniqueness  of  solutions  of  initial-value  prob lems determined  by  scalar  first-order  ordinary  differential  equations. Later in Sec tion  LIO  we  will  address  the  uniqueness  of  solutions  of  initial-value  problems characterized by systems of first-order ordinary  differential  equations. A.  The  Gronwall  Inequality We will require the following  preliminary  result on several occasions. THEOREM  8.1.  (GRONWALL  INEQUALITY).  Let  r k  G  C([a  b] R)  and  suppose that r{t) >  0 and k(t) >  0 for all t G [a b]. Let 6 be a given nonnegative constant. If foralU  E  [(3 Z?] then r ( 0^  d + k(s)r(s)ds Ja r(t)<  8e^ak(s)ds for all r G [ab]. Proof. Let R(t)  =  8 + \^ k(s)r(s)ds.  Then  r(t) <  R(tX R(a)  =  8 R(t)  =  k(t)r(t) k{t)R(t) and ^(0  -  k(t)R(t) <  0 (8.1) for all t G [a b]. Let K(t)  = e'^a ^(^)^^ Then K(t)  =  -k(t)e-^akis)ds ^ -K{t)k{t). Multiplying both sides of (8.1) by ^(0 we obtain K{t)R{t) -  K(t)k(t)R(t)  <  0 ^ Linear Systems or or ^-(0^(0  + ^ ( 0 ^ (0  ^  0 j -[K(t)R(t)]^0. (8.2) Integrating  (8.2) from a to t we obtain or or or K(t)R(t)  -  K(a)R(a)  <  0 K(t)R(t)  -  6 <  0 e-^i^^'^'^'R(t)-8^ 0 r(t)^  R(t)^  8e^ik{s)ds^ which is the desired inequaUty. • B.  Unique  Solutions Before  addressing  the uniqueness  issue we need to introduce  the notion of Lipschitz continuity. DEFINITION  8.1.  A function  /  E  C(A R) D C R\  is said to satisfy  a Lipschitz  con dition  on D (with respect to x) with Lipschitz  constant  L if \f(tx)-f(ty)\^L\x-y\ for all (t x) {t y) in D. The function/  is said to be Lipschitz  continuous  mxonD case. in this • For  example  if /  E  C (A R)  and if dfldx exists  and is continuous  on D  then /  is Lipschitz  continuous  on any compact  and convex  subset  Do  ofD.  To show  this let  LQ  be a bound  for \(df/dx)(t x)\ on DQ. Let {t x)  and (t y) be in DQ. Since  DQ  is convex the straight  line that connects  (t x) and (t y) is a subset of DQ  By the Mean Value  Theorem  there is a point z on this  line  such  that |/(r  X) -  fit  y)\  = (tz)(x-y) dx Lo|x  -  y\. THEOREM  8.2.  If /  G C{D R) and if/  satisfies  a Lipschitz  condition  (with  respect to x) on D with Lipschitz constant L then the initial-value problem (/') has at most one solution on any interval \t -  to\ ^  d. Proof  Suppose for some d > 0 there are two solutions (/>! and (/)2 on |r -  fo| —  d. Since both solutions  solve (V)  on ^o — t — to + d we have 1^1(0-02(01 [f(sMs))-f(sMs))]ds to < JtQ \f(sMs))-f(sMs))\ds <  f  L\cl>i(s)-ct>2(s)\ds. JtQ Applying the Gronwall Inequality  (Theorem 8.1) with k  = L and 8  =  0 it follows  that |(/>i(0 -  02(01 ^  0 on the interval to^  t <  to + d. Thus 0i(O  =  02(0 on this interval. • The proof for the interval  to -  d ^  t ^  to proceeds  similarly. COROLLARY 8.3.  If/  and dfldx  are both in C(A R) then for any (^  XQ) G D and any J containing ^o if a solution of (/')  exists on 7 it must be unique. Proof.  Let (j)\  and (/)2 be two solutions of (/')  on /  and define b  = sup{/ >  to  : 01(0  -  02(0} a  = mf{t  <  ro : 0i(O  -^ </>2(0}. 31 CHAPTER  1: Mathematical Descriptions of Systems We claim that a and Z? are the endpoints of J {dJ  = {a b}). For if b is not an endpoint of/  then by continuity we would have 01 (Z?) =  (piib). Since (b 4>\{b))  E Z) and D is a domain we know that there exists 6 >  0 such that DQ = {(t x):\t-b\^ e Mb)\^€}ca Clearly  DQ is a compact and convex subset of D. Now from  the comments  following Definition 8.1 and Theorem 8.2 we have that 0i(O  =  02(0 for ^ ^  [bb + e'] for some 0 <  e'  <  e. This contradicts the definition  of b. We conclude that b is an endpoint of 7 and so is a. It follows that 0i(O  = 02(O> t ^  J which implies the uniqueness of the solution of (/'). • Using Theorems 7.3  and 8.2 we can prove the following  continuation  result. THEOREM 8.4.  Let /  E  C(J  X  R R) for some open interval J  C R and let/  satisfy a Lipschitz condition on J  X R (with respect to x). Then for any (to XQ) E  /  X R the initial value problem (/') has a unique solution that exists on the entire interval /. Proof The local existence and uniqueness of solutions 0(/ to xo) of (/')  are clear from Theorem 8.2. Now if 0(0^ (V)  and therefore •- (j)(t to Xo) is a solution defined on ^  <  t < c then 0  satisfies 0(0  -  ^0  = [f{s <f){s)) -  f(s  Xo)] ds + JtQ Jto f{s  xo)ds and  10(0 -  xo| ^ L\<p(s) -  xo\ds + 8 where 8  =  [max^Q<^<c \f(s  xo)\](c -  to). By the Gronwall Inequality we have 10(0 -  xol <  6 exp[L(c -  to)l to  ^  t < c. Hence |0(O| is bounded on [to c) whenever 0(0 is a solution defined on [to c) c E. J. Let /  =  (a b) and assume that 0  is a noncontinuable solution of (/')  that is defined on /*  =  (a' b'). We must prove that b'  = b. If this is not the case we have b' < b. We have shown that |0(O| is bounded say |0(O| ^  M for t E  [to b'). Let D  = J  X [-M  -  1 M + 1]. Applying Theorem 7.3 we have that (t 0(0) -^  dD as ^ -^  b'. Since b' < b we must have |0(O| -^  M + 1 as r ^  b'. But this contradicts our assumption that |0(O| ^  M for t E  [to b'). Therefore b'  =  b. A similar argument works for r <  ^. • Successive approximation of solution If a solution 0  of (/')  is unique then the e-approximate  solutions constructed in the proof of Theorem 6.2 will tend to 0  as e -^  0^ and this is the basis for justifying Euler's  method  a numerical  method  of  constructing  approximations  to  0.  Now^ if we  assume  that/  satisfies  a Lipschitz  condition  an  alternative  classical  method  of approximation  is the  method  of  successive  approximations  (also  known  as  Picard iterations).  Specifically  let  /  E  C{D R) D  C  R^  and  5' be  a rectangle  in D  cen tered  at  (^  XQ) (see Fig.  1.9)  and  let  c and M  be  defined  by Eq.  (6.4).  Successive 32 Linear  Systems approximations for  (/')  or  equivalently  for  ( V )  are  defined  as ^m+i(0  =  -^0 + ft ho f(s  (f)m{s))ds m  -  0  1 2  . .. (8.3) for  |r  -  ^ol  — c. For  this  sequence  {4>m}^  we  have  the  following  result. THEOREM8.5.  I f /E  C(A  /?) and if/  is Lipschitz continuous on S (with respect to x) with  constant L then  the  successive  approximations  (f)m m  =  0 1 2  . ..  given  in  (8.3) exist  on  I? -  ^01 — c  are  continuous  there  and  converge  uniformly  as  m ^  oo to  the unique  solution of  (/'). Proof.  We  give  the  proof  for  the  interval  ^  <  ^ <  to +  c.  The  proof  for  the  interval to -  c  ^  t  ^  to proceeds  similarly. Using induction on the integer m we first prove the following  statements: (i)  (j)fn  exists on  [to to +  c] (ii)  (l>meC\[to>to  + (iii)  10^(0  -  xo\^  M(t~ clRl to) on  [to to + c] for  all m >  0. Each  statement  is  clearly  true  when  m  =  0.  Assume  that  each  statement  is  true for  a  fixed  integer  m  >  0.  By  (iii)  and  by  the  choice  of  c  it  follows  that  (t 4>m(t))  £ 5  C  D  for  all t  G  [^0 to + c]. Therefore  f(t  (pmit)) exists  and is continuous  in t while \f(t  (t>m(t))\  ^  M  on the time interval. This in turn means  that 4>m+l(t)  =  Xo+ \ J to f(s(t)m(s))ds exists that (1)^+1  G  C\[to  to +  c] R)  and that \(l>m+l(t)  -  -^ol  = f{s4>m{s))ds M{t  -  to). This completes the induction on m. Next we define ^mit)  =  4>m+\(t)  -  (t>mit).  Then |^m(0| : | / ( ^  (l)m{s))  - f{s (t)m-\is))\ds\ < L\4)m{s)- (f)ni-\{s)\ds =  L\ <tm-\{s)ds. Jto JtQ Notice in particular  that l^o(Ol f(s  xo) ds  <  M{t  -  to). The above two estimates  show  that 1^1 (01 ^  L\  M(s-to)ds  = JtQ LMjt  -  tof 2! and that |O2(0|  ^  L [LM{s  -  tofl2\]ds  • Jto L^Mjt  -  tof 3! and by induction that Therefore the mth term of the series \^m(t)\ (m+  1)!  * is bounded on the interval [to to + c] by ^  ^——TTT • Now since M  (Lc)"^^^ L  (m+  1)!' 33 CHAPTER  1: Mathematical Descriptions of Systems (8.4) nLc =  >  ^: i-  <  00 Ley Id it follows from the Weierstrass M-test (see Theorem 5.4) that the series (8.4) converges uniformly  to a continuous function  (f).  This in turn means that the sequence of partial sums m </>0 +  ^{4>k+\ -  4>k)  =  4>0  +  (<pl  -  M  +  '••  +  (0m+l  -  </>m)  =  4>m+\ k = 0 tends uniformly to (^ as m —>  oo. Since the bound (iii) given above is true for all c/)^ it is also true in the limit i.e. 10(0 -  xo\ <  M(t  -  to). Therefore f(t  (p(t)) exists and is a continuous function oft. Using an identical argument as in the proof of Theorem 6.3 it now follows that (/)(0  =  lim 0^+1(0  = xo+  lim f(s(l)m(s))ds =  xo-h  \  f{s(j){s))ds to <  t ^  to +  C. Therefore (f) solves {V). • We  will  consider  the  application  of  Theorem  8.5  to  specific  cases  (linear  sys tems) in Section  1.13. *1.9 CONTINUOUS DEPENDENCE OF SOLUTIONS ON INITIAL CONDITIONS AND PARAMETERS In practice  it frequently  happens  that  an  initial-value  problem  may  exhibit  depen dence on some parameter  A. An example of such a class of problems is given by X =  fit  X A) X{T)  =  ^ (n.p where /  G  C{J  X  RX  D R) J  C  Ris  m  open interval and D  C  R If it is assumed that for each pair of compact  subsets  JQ C  J  and DQ C  D  there exists a constant L  =  LJ^DO >  0 such that for all (t X) G  JQ X  DQ  x y  G  R \f(txX)-f(tyX)\^L\x-yl (9.1) 34 Linear Systems then by previous results we know that for every r  ^  J\E.  D and ^  G /? the initial-value  problem  (/{  r  P  ^^^  ^ unique  solution  (j){t) =  (f){ty r ^ A) that  exists  for  all ^ E  /.  It turns out that this solution depends continuously on the initial data (T ^ A). We express this in the following  result. THEOREM9.1.  L c t /E  C{J XRXD  R) where J  C Risan  open interval and D C R. Assume that for each pair of compact subsets JQ  C J and Do C D there exists a constant L  =  LJQDQ  > 0 such that for all (t X)  E.  JQXDQ  xy  G R the Lipschitz condition (9.1) is true. Then the initial-value problem (I'xre^ ^^^ ^ unique solution (pit r ^ A) where cf)  G C(J  X J  X R X DR).  Furthermore if Z) is a set such that for every XQ  G D there exists 6 >  0 so that  [XQ  -  e XQ  + e] H D C D then </)(^ r ^ A) ->  (j){t TQ ^O> AQ) uni formly for ? E 7o as (r ^ A) -^  (TQ & AQ) where Jo is any compact subset of/. • It  is  because  of  uniform  convergence  that  we  require  the  restrictions  on D  in Theorem  9.1. However  in practice most  sets that are of interest to us  satisfy  these assumptions including open and closed  sets in R intervals  such as {a b\  and  {a b) sequences  such  as {(1/n)  : n  G N}  {0} U {{lln)  : n  G N)  {m  +  (1/n)  : mn  E.  N} {m : m G A/^} U {m +  (l/n)  \ mn  EL  N}  and so forth. Applying Theorem 9.1 to the initial-value  problem X =  f(t  X A) (nr) where  it  is  assumed  that  ^A  depends  continuously  on  A we  obtain  the  following result. COROLLARY 9.2.  Let /  G C(J  X R X D R) where J  C R is an open interval and D C R. Assume that for each pair of compact subsets Jo E J and Do E D there exists a constant L  =  LJ^DQ  > 0 such that for all (t X)  E  JQ X Do xyER the Lipschitz con dition (9.1) is true. Then the initial-value problem (/j^^) has a unique solution (/>(? r A) where  (f)  E  C(J  X J  X D R). Furthermore if D is  a set  such  that  for  every  AQ G D there exists an 6 >  0 so that  [Ao -  6 Ao  + e] fl D C D then (/)(^ r. A) -^  (j){t TQ AQ) uniformly for r G /o as (r A) -^  (TO AO) where Jo is any compact subset of J. • Proof of Theorem 9 J.  For the solution (/>(/ r ^ A) we first show that 4> E  C(J  X J X RXD  R). By (/{ ^^) we have that cl>(t T ^ A) =  ^ +  f  f(s  cl>(s T ^ A) A) ds. (9.2) J to We want to show that for (to TQ ^O AQ) G /  X /  X /? X D (l>(tm>  Tm ^m  A^)  -^ (f)(to  To &  Ao) as  {tm Tm U> Am) -^  (^0 To & Ao) where  (tm Tm UXm)  E  J  X J  X RXD m G A^. By (9.2) we have for  each (p{tm  Tm ^m  A^)  - (j){to  To &  Ao) =  ^m  -  &  + I f{s ^{S  Tm ^m  ^m\  Xm)  ds f{s  (f){s To & Ao) Ao) ds I •^0 =  ^m  -  &  + ( / ( 5  (I>{S  Tm ^m>  K\  ^m)  " f(s (^(5  To &  Ao)  A^))  ds m. {f{s  ct)(s To & Ao) A^) -  /(5 (t)(s To ^0 Ao) Ao)) ds + f(s  (/)(5 To & Ao) Ao) ds J to fTm f(s(l)(sTo^o\oXXo)ds. Denote 35 CHAPTER  1: Mathematical Descriptions of Systems (9.3) {ab) [ba] \fa>b. Since fo '^b £  «/ and 7 is an open interval it follows that for m sufficiently  large (T^ tm) {to tm) and (TO T^) are contained in /.  Also given  (TO ^Q AQ) E. JxjxD (f)(s To ^o Ao) is a continuous function  on J. Note that since /  E  C (/  X RXD  R) we can assume that for m sufficiently  large. niax  \f(s  (i){s To & Ao) Ao)| ^  M S^{tQtrn) max  1/(5 (/>(5 To & Ao) Ao)| <  M S&{TQTm) and for  some M  >  0. We now have \4>itm  Tm  ^m  ^m) "  </>(^  ^o  &  A o )| <  1^^ -  &|  +  M(\tm  -  tQ\  +  \Tm  -  Tol) 1/(5 0(5 Tm ^m A^) A^) -  f(s  (t){s To & Ao) km)\ds 1/(5  (/)(5 To &  Ao)  A ^)  - / ( 5  (f){s  To fo  Ao)  Ao)|(i5  . Without  loss of generality  we  assume that  ^  >  To. Then  for  m  sufficiendy  large  there exists e  >  0 such that  [TO -  e ^  +  e]  C  /  and \4>itm Tm ^m  ^m)  "  4>{k  To &  Ao)| < \^m  -  &|  +  M{\tm -  k\  +  \Tm  "  To|) 1/(5  (/)(5  Tm U  Xm)  ^m)  " f{s (^(5  To &  AQ)  A ; „ )|  ds 1/(5 (^(5 To & Ao) Am)  -  f(s  (j^is To ^0 Ao) Ao)| J5 TQ-e + To-e \^rn -  &|  +  M ( | ^^  -  ^1  +  \Tm "  To|) to+6 1/(5 (/)(5 To & Ao) A;„) -  /(5 (f){s To & Ao) Ao)| ds rto+e +  L 10(5  Tm ^m  Am)  "  0 ( ^  To &  Ao)| <i5. By the Gronwall Inequality we obtain that \(t>{tm  Tm ^m  K) "  0(^0  TQ &  Ao)|  < (|^m  "  &|  +  M{\tm '  ^o|  +  Vm  "  To|) + [ ' ' ^'  1/(5  0 ( 5  TO &  Ao)  km)  - J To-6 f{s  0 ( 5  To &  Ao)  Ao)| J5)^^(^0-0+2.)_ (9  4) 36 Linear Systems Since f(s  cl)(s  TQ & AQ) A) G C([TO  -  eJo  + e]X  RX  {Xj  R) (with 5 A as variables and {A^} C D since A^ ^  AQ G D) and since  [TQ -  6 ^  4- e] x {A;„} is a compact  subset of /  X D we have by Theorem 5.2 and (9.4) as m ^  oo that l im \(l)(tm>  Tm ^m  A m) " (/>(^0  TQ  &  A o )|  =  0. Thus (I)GC(JXJXRXDR). Similarly  under  the  assumption  on  D  we  can  prove  that  </>(^ r ^ A)  -^ <l>(t To ^o> Ao)  uniformly  for t  G JQ as (r ^ A) -^  (TQ ^O> AQ)  where  JQ is any com pact subset of J. In place of (9.3) we have (/>(? To +  AT & + A^ Ao + AA) -  0(r ^ & AQ) =  A^ +  [ (f(s  (l)(s To +  AT & + A^ Ao + AA) Ao + AA) JTQ+AT -  f(s  (j){s To & Ao) Ao + AA)) J^ + {f{s (j){s To & Ao) Ao + AA) -  f{s  (Pis  TQ fo Ao) Ao)) ds JTQ+AT Jro /(5(/)(^To&Ao)Ao)J^. (9.5) Let  ||A||  =  V(AT)2  + (A^)2 + (AA)2  ^^^ax  ^  max{^ : r G /o} and /min  =  min{f  : r E JQ}.  Then  there  exists  e  >  0  such  that  when  ||A|| <  6 <  TO +  AT r >  C <  TO -  6 tmax  >  U  < tmin  TQ  +  €  >  =  Trr  C  J  <  TQ TQ  +  AT  >C  [TO -  6  TQ  +  e]  C  J  and [Ao -  6 Ao + 6] n  D  ^  DAO  C D. Note  that  both  7^^ and D^^  are compact  subsets  of J and D respectively. For (t A) E rQXDAolet M  =  max 5G[To-eTo+e] 1/(5 (/)(5 To & Ao) Ao)|. By the Lipschitz  condition we obtain |(/)(r To +  AT ^0 + A^ Ao + AA) -  (/)(? TO ^O AO)| <  |A^| +  M | A T| +  I 1/(5 (/)(5 To & AO) AO + AA) -  f(s  cf^is  TQ & Ao) Ao)| ds +  L I |(/)(5 To +  AT & + A^ Ao + AA) -  (/>(5 To & Ao)| J^. Again using the Gronwall Inequality  we have that |0(/ To +  AT & + A|^ Ao + AA) -  (/>(r TQ & Ao)| <  (|A^|  +  M | A T| +  i ^ET-^0 1/(5 (/>(5 TO & Ao) Ao + AA) -  f(s  (/>(5 To & Ao) Ao)| ^5) By  the first part  of the  proof  we already  know  that  f(s  (/)(5 To ^o. Ao) A) E  €(1^^  X R  X  DXQ R) which  implies  t h a t/  is uniformly  continuous  on the compact  set Tj^  X DXQ.  Therefore  by Theorem  5.2 and (9.6) we know  that  (/)(^ T ^ A) -^  (pit TQ io Ao) • uniformly  as (T ^ A) -^  (TO ^O AO) for t ^  JQ. 1.10 SYSTEMS  OF FIRST-ORDER  ORDINARY DIFFERENTIAL  EQUATIONS In  Sections  1.6  to  1.9  we addressed  the existence  of solutions the continuation  of solutions the uniqueness  of solutions and the continuous  dependence  of  solutions on initial  data and parameters  for the scalar  initial-value  problem  for  ordinary dif ferential  equations [characterized  by  {E')  and (/')  or by (V)  (resp. by  (/j^^.)]. In this  section  we  show  that  these  results  can be  extended  to  initial-value  problems characterized  by systems  of equations  [determined  by {E) and (/)  or by (V) (resp. by  (/AT))] with no essential changes in proofs. Before we can accomplish this how ever we need to introduce additional background  material. 37 CHAPTER  1: Mathematical Descriptions of Systems A.  More Mathematical Preliminaries: Vector Spaces We will require the notion of vector space or linear space over a field. DEFINITION  10.1.  Let F be a set containing more than one element and let there be two operations "+" and "•" defined on F (i.e."+" and "•" are mappings of F XF into F) called addition and multiplication respectively. Then for each a p  G F there is a unique element a + (3  E. F called the sum of a and jB and a unique element a(3  = a - (3  E^ F called the product of a and f3. We say that {F; + •} is afield provided that the following axioms are satisfied: (i)  a  + (/3 + y)  =  (a + /3) + 7 and a  • (jS • y)  =  (a • j8) • 7 for all a jS y G F (i.e. "+" and "•" are associative operations); (ii)  a  + (3 = (3  + a  and a  - (3 == /3 • a  for all a /3 E F  (i.e. "+" and "•" are (iii)  a  • (j8 + y)  =  a  • jS + a  •  y  for  all a /3 y  G F  (i.e.  "•" is  distributive commutative operations); over "+"); (iv)  There exists an element Of G F such that Of + a  =  a  for all a  E F (i.e. Of is the identity element of F with respect to "+"); (v)  There exists an element IfEF^lfj^Of such that If  - a  = a for all a  E F (i.e.. If  is the identity element of F with respect to "•"); (vi)  For every a  E F there exists an element  -a  G F such that a  + (-a)  =  Of (i.e. -a  is the additive inverse of F); (vii)  For any a  T^ Of there exists an o;"^ G F such that a  - (a~^) =  If  (i.e. a~^ is the multiplicative inverse of F). • In the sequel we will usually  speak of a field F rather than "a field {F; +  •}." Perhaps  the most  widely  known  fields  are the field of real numbers  R  and the field  of complex  numbers  C. Another field we will encounter  (see Chapter 2) is the field  of rational functions  (i.e. rational fractions  over polynomials). As a third example we let F  =  {0 1} and define  on F  (binary)  addition  as 0 + 0 : ^ 0 =1  +  1 1 + 0 = 1 = 0 +1  and (binary) multipHcation as 1 • 0  =  0 • 1  = 0 • 0  =  0 1 • 1 =  1. It is easily verified  that {F; + •} is a field. As a fourth  example let P denote the set of polynomials  with real  coefficients and  define  addition  " +"  and  multiplication  "•"  on  P  in  the  usual  manner.  Then {F; + •} is not a field since e.g. axiom (vii) in Definition  10.1 is violated  (i.e. the multiplicative  inverse of a polynomial p  G P is not necessarily  a polynomial). 38 Linear Svstems DEFINITION  10.2.  Let V be a nonempty set let F be  a field l e t " +" denote a mapping of V X V into  V and let "•" denote a mapping of F  X y  into V. Let the members x  ^  V be called vectors  let the elements a  G F be called scalars  let the operation " +"  defined on V be called vector addition  and let the mapping "•" be called scalar  multiplication  or multiplication  of vectors  by scalars.  Then for each  xy  ELV there is a unique  element X + y  EV  called the sum ofx  and y  and for each x  ELV and a  G F there is a unique element ax  = a  • x  E  V called the multiple  ofx  by a. We say that the nonempty set V and the field F along with the two mappings of vector addition and scalar multiplication constitute a vector  space  or a linear  space  if the following  axioms are satisfied: xySV. (i)  X + y  = y + xfoY  every (ii)  X + (y + z)  = (x + y) + zfoY  every  xyzE.  V. (Hi)  There is a unique vector in V called  the zero  vector  or the null  vector  or the origin  that  is denoted  by Oy and has the property  that  Oy +  x  == x for all xEV. (iv)  a:(x + y)  = ax  + ay  for all a  G F  and for all xy  EV. (v)  (a  + P)x  =  ax + j8x for all a ^  G F and for all x E V. (vi)  (a;/3)x  =  Q:(/3X) for  all a  jS  G F  and for  all x  E  V. (vii)  OFX  = Oy for all x E V. (viii)  Ipx  =  X for all x  G V. • In  subsequent  applications  when  the meaning  is  clear  from  context  we  will write 0 in place of OF  1 in place of Ip  and 0 in place of Oy. To indicate the relation ship  between  the set of vectors  V and the underlying  field  F we sometimes  refer  to a  vector  space  V  over  the  field  F and we  signify  this  by  writing  (V F).  However usually  when  the  field  in question  is clear  from  context  we speak  of a vector  space V. If F is  the field of real numbers R we call the space a real  vector  space.  Similarly if F  is the  field  of complex  numbers  C we speak  of a complex  vector space. Examples  of vector  spaces EXAMPLE  10.1.  Let V  = F^ denote the set of all ordered  /z-tuples of elements  from a  field F. Thus if x E  F'' then  x  =  {x\...  Xnf  where  xt  G F  i  =  1... n. With xy  GF"^ and a  E F let vector addition and scalar multiplication be defined as X  + y  =  {Xi . . .  Xnf  + {y\ . . .  ynf ^  {x^-y...Xn + ynf and ax  = a{xi...  Xnf  =  {axi.. .axnf. (10.1) (10.2) In this case the null vector is defined  as 0  =  (0...  0)^ and the vector  -x is defined as Xnf  =  ( - x i  . . .  -Xnf.  Thcu wc utilizc the properties of the field F -X  =  -(xi... all axioms of Definition  10.2 are readily verified  and therefore F^ is a vector space. We call this space the space  F" ofn-tuples  of elements  ofF.  If in particular we let F  =  R we  have  /?" the n-dimensional  real  coordinate  space.  Similarly if we let F  =  C we have C" the n-dimensional  complex  coordinate  space. • We  note  that the set of points  in R^  {x\  X2) that  satisfy  the linear  equation Xi  +  X2 +  C  == 0 C 7^ 0 with  addition  and multiplication  defined  as in Eqs.  (10.1)  and (10.2) is not  a  vector space  (why?). EXAMPLE  10.2.  Let y  =  7?°° denote the set of all infinite  sequences of real numbers X  =  {Xi X2 .  . .  Xk .  . .}  = {Xi} let  vector  addition  be  defined  similarly  as  in  (10.1)  and  let  scalar  multiplication  be defined  as in (10.2). It is again an easy matter to show that this space is a vector  space. On  some  occasions  we  will find it convenient  to modify  V  = R°° io  consist  of  the • set of all real infinite  sequences  {xi}  i eZ. EXAMPLE  10.3.  Let  1 <  /? <  oo and define  V  =  lphy 39 CHAPTER  1: Mathematical Descriptions of Systems xeR"^ <^ oo 1  < / ? << i=l {xeR^^ : sup{|x;|} <oo}. (10.3) Define  vector  addition  and  scalar  multiplication  on  Ip as in  (10.1)  and  (10.2)  respec tively. It can be verified  that this space called the Ip-space  is a vector space. • In  proving  that  Z^ 1 <  p  <  ^  is  indeed  a  vector  space  in  establishing  some  of the  properties  of  norms  defined  on  the  /^-spaces  (see  Examples  10.10  and  10.11)  in defining  linear  transformations  on  /^-spaces  (see  e.g..  Example  10.8)  and  in  many other applications  we make use of the Holder  and  Minkowski  Inequalities infinite sums  given  below.  (These  inequalities  are  of  course  also  valid  fox  finite  sums.)  For proofs  of  these  results refer  e.g.  to Michel  and  Herget  [12 pp.  268-270]. for Holder's Inequality states  that  if  pq  e  R  are  such  that  1  <  p  <  oo and  1/p  + l/q  =  1  and  if  {xi}  and  {yi}  are  sequences  in  either  R  or  C  and  if  YtLi  \^i\^  <  "^ and  YiLi  \yi\^  <  "^^ then / oo \ ^ IP f oo \ V^ Minkowski's Inequality states  that  if  p  e  R  where  1 <  p  <  ^  and  if  {xi}  and {yi}  are  sequences  in  either R  or C  and  if  YiLi  \^i\^  <  "^ and  YiLi  \yi\^  <  "^^ then / oo \ ^ / P / oo \ ^ / P / oo \  VP [l\^i^yi\n  <[l\^i\n  +ll\yi\n • (M) If in  particular  p  =  q  =  2  then  (Hs)  reduces  to  the  Schwarz  Inequality  for  sums. EXAMPLE  10.4.  Let  V  =  C{[ab]R).  We  note  that  x  =  y  if  and  only  if  x{t)  =  y{t) for  all  t  G [ab]  and  that  the  null  vector  is  the  function  that  is  zero  for  all  t  G [ab]. Let  F  denote  the  field  of  real  numbers  let  a  G F  and  let  vector  addition  and  scalar multiplication be defined  pointwise by {x + y){t)=x{t)+y{t) foralU  G [ab] ^j^(^ {ax){t)  =  ax{t) foY?illt  e[ab]. (10.4) (10.5) Then  clearly x-\-y  eV  whenever xy  eVax eV  whenever  a  G  F  and x G V  and  all the  axioms  of  a vector  space  are  satisfied.  We call this  space  the  space  of  real-valued continuous functions  on  [a b] and we frequently  denote it simply by  C[ab]. • EXAMPLE  10.5.  Let  I  < p  < oo and let V  denote the set of all real-valued  functions  x on the interval  \a b] such that \x{t)\Pdt<' / Ja (10.6) 40 Linear Svstems Let F  = R and let vector addition and scalar multiplication be defined as in (10.4) and (10-5) respectively. It can be verified that this space is a vector space. In this book we will usually  assume that in (10.6) integration is in the Riemann sense. When integration in (10.6) is in the Lebesgue sense then the vector space under discussion is called an L^-space (or the space Lp[a b]). • In proving that the L^-spaces are indeed vector spaces in establishing properties of norms  defined  on L^-spaces  (see e.g.  Example  10.12) in defining  linear trans formations  on Lp-spaces  (ee e.g.. Example  10.12) and in many other  applications we make use of the Holder  and Minkowski  Inequalities  for  integrals  given  below. (These inequalities  are valid  when integration  is in the Riemann  and the  Lebesgue senses.)  For proofs  of these results refer  e.g.  to Michel  and  Herget  [12 pp.  268-270]. Holder's  Inequality  states  that  if  pq  ^  R  are  such  that  1  <  j9  <  oo  and lip  -\-  1/q  =  1 if  [a b] is  an  interval  on  the  real  line  if  /  ^  : [a b] -^  R  and if \a  1/(01^ dt<^ and  j/  \g(t)\^ dt  <  oo then \l//7 /  \[/q (  [  1/(01" ^M  ij  \g(t)\''dt\  . (HI) Minkowski's  Inequality  states that if p  G  R where 1 ^  p  <'^ii  f  g  :  [ab\^' R  and if //  1/(01" dt  <'x> and //  [^(Ol'' dt  <  oo then \\lp \A/p I  I  \\lp (  \^\f(t)±g{t)Ydt\ <  M  1/(01"^H  ^\\\g{t)Ydt\ . (M) If  in  particular  p  =  q  =  j then  (///)  reduces  to  the  Schwarz  Inequality  for integrals. EXAMPLE  10.6.  Let  V denote the set of all continuous real-valued functions  on the interval [a b]  such that sup  \x(t)\ < ^. a<t<:b (10.7) Let F  = R and let vector addition and scalar multiplication be defined  as in (10.4) and (10.5) respectively. It can readily be verified that this space is a vector space. In some applications it is necessary to expand the above space to the set of measur able real-valued functions  on [a b] and to replace (10.7) by ess  sup  \x(t)\ < 00 a<t<b (10.8) where ess  sup denotes the essential supremum i.e. ess  sup  \x(t)\ =  inf {M : m{t:  \x(t)\ > M}  =  0} a<t<b where m denotes the Lebesgue measure. In this case the vector space under discussion is called the Loo-space. • Next we consider linear  transformations. DEFINITION  10.3.  A mapping T of a linear space V into a linear space W where V and  W are vector spaces over the same field F is called  a linear transformation  or a linear operator provided that (L-i) (L-ii) T{x + y)  = T{x) + T(y) for all  xyeV. T(ax)  = aT(x) for all x e  V and a  G F. • In  Section  1.16  we  will  discuss  in  detail  the  representation  of  linear  systems by means of linear operators. This discussion  will be continued in Chapter 2. In the following  we consider three specific  examples of linear transformations. 41 CHAPTER 1: Mathematical EXAMPLE 10.7.  Let (V R)  = (/?« R) and (W R)  = {R"^ R) be vector spaces defined as in Example 10.1 let A =  [a/y] G /?^>'^ and let T : V ^  VF be defined by the equation Descriptions of Systems y =  Ax y^R'' R\ It is easily verified using the properties of matrices that 7 is a linear transformation.  • EXAMPLE  10.8.  Let  (V R)  =  (Ip R) be  the  vector  space  defined  in  Example  10.3 (modified  to  consist  of  sequences  {x/} /  G Z  in  place  of  {x/} /  =  12...).  Let h  : Z  X Z  ^ /?bea  function  having  the property  that  for  each  x  E  V  the  infinite sum ^ h(n k)x(k) exists and defines a function  of n on Z. Let T :V  ^  F be defined by 00 y{n)  =  ^_^  h{n k)x(k). k= - co It is easily verified that T is a linear transformation. The  existence  of  the  above  sum  is  ensured  under  appropriate  assumptions. For example  by  using  the  Holder  Inequality  it  is  readily  shown  that  if  e.g.  for fixed n {h(n k)} G I2  and {x(k)} G fc then the above sum is well defined.  The above sum • exists also if e.g. {x(k)} G L  and {h(n k)} G /i for fixed n. EXAMPLE  10.9.  Let (V R)  denote  the  vector  space  given  in  Example  10.5 and let k G C([a b] X [a b] R) have the property that for each x E  V the Riemann integral k(st)x(t)dt exists and defines a continuous function of s on [a b]. Let T  : V ^  V be defined by (Tx)(s)  = y(s)  = k{st)x{t)dt. rb Ja It is readily verified that T is a linear transformation of V into V. B.  Further Mathematical  Preliminaries:  Normed  Linear  Spaces In  the following  we require  for  (V; F)  that F be  either  the  field  of real  numbers  R or the  field  of  complex  numbers  C. For  such  linear  spaces  we  say  that  a  function II • II : y  ->  7?^ is a norm  if (N-i)  ||x|| >  0 for  every  vector  x  E  V  and  \\x\\  =  0 if  and only if x is the  null vector (i.e. x  =  0); (N-ii)  For  every  scalar  a  E  F  and  for  every  vector  x  E  K ||Q:X||  =  Ic^Hkll where  |Q:| denotes the absolute value of a  when F  =  R and the modulus when F  =  C; (N-iii)  For every x and y in V \\x +  y\\ <  ||xi| + ||y||. (This inequality is called the triangle  inequality.) 42 Linear Systems We call a vector space on which a norm has been defined  a normed  vector  space ^^ ^ normed  linear  space. EXAMPLE 10.10.  On the linear space (/?" R) we define for every x  = {xi...  XnY and \\A\p  =  Zl^'-lM ||x||oo =  max{|x/| : I <  i <  n}. ' l^P<^ (10.9) (10.10) Using Minkowski's Inequality for finite sums (M^) it is an easy matter to show that for every/? 1 <  /> <  oo || • ||^ is a norm on R^. In addition to || • ||oo of particular interest to us will be the cases p  =  \  and p  = 2 i.e. IWIi  =i^M and \\x\\2  -^\T\xi?\ . (10.11) (10.12) The norm || • ||i is sometimes referred to as the taxicab norm or Manhattan norm  while II • II2 is called the Euclidean norm. The foregoing norms are related by the inequalities \\x\\o.  <  IWIi <  ^IWJoo ll^lloo <  llxlb <  >||x||oo Iklb ^  IWIi <  v^lWb. (10.13) (10.14) (10.15) Also for p  =  2 we obtain from the Holder Inequality for finite sums (//^) the Schwarz Inequality \x^y\ •T^\ ^ ^xtyt \l/2  /  „ xl/2 ^Ei^'H  E NI i=l (10.16) forallx y E /?". The assertions made in the above example turn out to be also true for the  space (C" C). We ask the reader to verify  these relations. EXAMPLE 10.11.  On the space Ip given in Example 10.3 let \\4p = (XM']  i^p<^ and ||x||oo =  sup|x/|. Using Minkowski's Inequahty for infinite sums (Ms) it is an easy matter to show that II • lip is a norm for every p  1 <  /? <  00. • EXAMPLE 10.12.  On the space given in Example 10.5 let x(t)\Pdt]  1 <  p  < 00. Using Minkowski's Inequality for integrals (M/) it can readily be verified  that || •  \\p is a norm for every p  1 <  p  <  00.  Also on the space of continuous functions  given in 43 CHAPTER  1 : Mathematical Descriptions of Systems Example  10.6 assume that (10.7) holds. Then ||x||oo  =  sup  \x(t)\ a<t<b is  easily  shown  to  define  a  norm.  Furthermore  expression  (10.8)  can  also  be  used  to define  a norm. • EXAMPLE  10.13.  We can  also define  the norm  of  a matrix.  To this  end  consider  the set  of  real  m  X  n  matrices  R^^^  =  V  and  F  =  R.  It  is  easily  verified  that  (V F)  = (R^^'^  R) is a vector space where vector addition is defined  as matrix addition and mul tiplication of vectors by scalars is defined  as multiplication  of matrices by scalars. For  a  given  norm I  : R^""^  -^  i?+  by It is easily verified  that \\u on  i?"  and  a  given  norm  ||  • ||v  on  R"^  we  define sup{||Ax||v  : X G /?^ with \\x\\u  =  1}. (10.17) (M-i) (M-ii) (M-iii) (M-iv) (M-v) ||Ax||v  <  ||A||vJx||« for  any  x  G /?^ ||A +  5 | U < | | A |U  +  ||5|U; ||aA|U  =  |a|||A|U  for all a  G  R; ||A||vM ^  0 and  ||A||VM  =  0 if and only if A is the zero matrix  (ie. A  =  0); ||A|U  ^  XT=iT%i \aij\ for any p-vector norms defined  on i?^ and /?^. Properties (M-ii) to (M-iv) clearly show that ||  • ||VM defines  a norm on R^^^  and jus tifies the use of the term matrix norm.  Since the matrix norm ||  • ||VM depends on the choice of the vector norms ||  • \\u and ||  • ||v defined  onU  =  R^ and V  =  R^  respectively  we say that the matrix  norm  || • is induced  by the vector norms  ||  • \\u  and  ||  • ||v. In  par-ticular if  ||  • ||«  =  ||  • ||^ and llv  =  II • II;? then the notation  ||A||; is frequently  used to denote the norm of A. As a specific  case let A  =  [atj] G  R  Then it is easily verified  that /  m ||A||i  =  m a x ^ l a  -^ ||A||2  =  [maxA(A^A)]i/^ where max A(A-^ A) denotes the largest eigenvalue of A^A  and ||A||.  =  m a x ^ | f l  vl . When  it  is  clear  from  context  which  vector  spaces  and  vector  norms  are  being used the indicated  subscripts on the matrix norms are usually not used. For example if A  G /?^^" and B  G /?"><^ it can be shown that (M-vi) ||A5||^ B\\. In  (M-vi)  we  have  omitted  subscripts  on  the  matrix  norms  to indicate  inducing  vector norms. • We  conclude  this  subsection  by  noting  that  it  is  possible  to  define  norms  on ^j^mxn^ 7?) that need  not be  induced  by  vector  norms. Furthermore  the  entire  discus sion  given  in  Example  10.13  holds  also  for  norms  defined  on  complex  spaces  e.g. (C^x"  C). 44 Linear Systems C.  Additional  Mathematical  Preliminaries:  Convergence Although  most of what we will present in this  subsection  is true in a rather  general setting we will confine  ourselves to the spaces {R^ R) or {C^  C). Using  the concept  of norm we can define  distance  between  vectors x  and y  in R^  [or in C^] by d{x  y)  =  \\x -  y\\. The three basic properties of distance are given next and are a consequence of the axioms of a norm: (D-i) (D-ii) (D-iii) \\x -  y\\>  0 for all vectors  x y and \\x -  y\\ =  0 if and only if  x  =  y; \\x -  y\\ =  \\y -  x\\ for  all vectors x y; \\x -  z\\ ^  \\x -  y\\ +  \\y —  z\\ for all vectors x y z. We can  now  define  spherical  neighborhood  in  R^  (in  C")  with  center  XQ and radius  /^ >  0 as B{xo h)  =  {xER'' : \\x -  xo\\ <  h}. If in particular the center of a spherical neighborhood with radius h is the origin then we write B(0 h)  =  B{h)  i.e. B(h)  =  {xER'' : ||x|| <  h}. We shall use the notation B{xo h)  =  {xG  R""  :  \\x -  xo\\  <  h} and B(h)  =  {x  G R""  : \\x\\  <  h}. The introduction of vector and matrix norms enables us to generalize the notions of convergence of sequences continuity of functions and the like. We will not retrace here  the  entire  presentation  given  in  Sections  1.2  and  1.5.  Instead  to  demonstrate what is involved in these generalizations we consider a few  specific  cases. A sequence  of vectors  {xm)  =  {{x\m • • •. ^nmf}  C  R^  is  said to converge  to a vector X E  R^  (i.e. x^  ^  x as m ^  oo) if lim  \\xm  —  x\\ =  0 or  equivalently  if  for  every  e  >  0  there  exists  an  integer  A^  =  N(e)  such  that Ikm  ~  -^11  <  ^  whenever  m  >  N.  (In  this  definition  ||  • || denotes  a  norm  on  R^.) Using the properties of norms it is easily shown that x^  -^  x if and only if for  each coordinate one has  x^rn -^  Xk^s  m-^  ^  k  =  1.. .n. The above allows the generalization  of many  of the properties  of R to R^  (e.g. the Bolzano-Weierstrass  property and the Heine-Borel  property). As another example consider/>6>m^/5'^ convergence  of a sequence of functions. We  say  that  a  sequence  of  functions  {f^} fk  - D ^  R'^ D  C  R^  k  =  1 2  . . .  is pointwise convergent to a function  f  : D ^  R^ if for every e  >  0 and every  x  E:  D there is an integer N  =  N(e  x)  such that ||/^(jc)  -  f(x)\\  <  e  whenever  k^  N.  (In the above definition  ||  • || denotes a norm on R"^) Using the properties of norms it is again easy to show that /^(x) ->  f(x)  for all x  G Z) if and only if for each coordinate one has fik(x)  -^  fi(x)  as fc ^  oo /  =  1...  n for  all x  E  D. As  a third  example  consider  continuity  of a function  f  : D ^  R^  where D  is an open subset of R^.  The function/  is said to be continuous  at point  XQ  G Dif  for every e  >  0 there is a §  =  8(e  XQ) >  0 such that ll/(-^)  ~  /(-^o) IIF  <  ^ whenever  || x -  XQIU  <  ^-In the  above  definition  | on/?^. \\Y is  a norm  defined  on  R^  and  ||  • ||x is  a norm  defined 45 CHAPTER  1: Mathematical Descriptions of Systems Next let g(t)  =  [gi{t\  . . .  gn(t)V  be a vector-valued function  defined  on some interval J  C  R. Assume that each component of ^ is differentiable  and integrable on /.  As pointed out earlier differentiation  and integration of ^ are defined  component wise e.g. di dt  (0 = (0 dg\ dt b dgn dt  (0 and g{t)dt  = g\{t)dt.. gn(t)dt It is easily verified  that for b>  a g{t)dt\\ i)\\dt Ja where again ||  • || denotes a norm on  R^. Finally if D is an open connected  nonempty  set in the  {t x)-space  RX  R^  and if /  : D ^  R^ then/ is said to satisfy  a Lipschitz  condition  with Lipschitz  constant L (with respect to x) if for  all {t x)  and {t y) in  D \\f{tx)-f{ty)\\^L\\x-yl This is an obvious extension of the notion of a Lipschitz condition for  scalar-valued functions. D.  Solutions  of Systems  of First-Order  Ordinary  Differential  Equations: Existence  Continuation  Uniqueness  and  Continuous  Dependence on Initial  Conditions It  turns  out  that  every  result  given  in  Sections  1.6  to  1.9  can  be  restated  in  vector form and proved using the same methods as in the scalar case and invoking obvious modifications  (such  as the  replacement  of  absolute  values  of  scalars  by  the  norms of vectors or the norms of matrices and  so forth). In the following  we restate  these results in vector form  and ask the reader to prove these results. We  have  a  domain  D  C  R^^^  f  G  C(A  R^)  and  we  are  given  the  system  of first-order  ordinary differential  equations We are given  (to XQ) G D  and  seek a solution  (or solutions) to the  initial-value X =  fit  x). (E) problem In doing so it suffices  to find a solution of the integral  equation X =  fit  X) x(to)  =  XQ. (/>(0  =  xo+ \ Jto f(scl)(s))ds (/) (V) As in the scalar case this can be accomplished by the use of 6-approximate solutions. 46 Linear Systems DEFINITION  10.4.  A  function  0  defined  and  continuous  on  a  ^interval  /  =  {cib) containing  ^o is called an e-approximate  solution  of  (/)  if  0 (^o) = ^o and (i) (ii) (iii) (t(l)(t))  e  D for 2i\\te  J; (p  has a continuous  derivative on /  except possibly  on a finite set /  of points in  / where there are jump discontinuities  allowed; II 0 (0  -  / (^ 0(0)  II <  e for alU  G /  - /  where  || • || denotes a norm on R"^. • Now  let S  =  {{tx) : l^-^ol  <a\xi-Xio\ and  let  (^o^-^o)  ^  S.  Since  /  G C{D^R^) Mi  >  0  such  that  \fi{tx) \ <  Mi  for  all  (fjc)  G 5 / =  1  . . . n.  Define <bi i=l...n} (10.18) it  is  bounded  on  S  and  hence  there  are cD Ci  =  min  <  a  —  >   bi\ i=\...n c  =  min/  {c/}. THEOREM  10.1.  li  f  eC{DW) there is an e-approximate  solution of  (/)  on the interval  |^ — ^o| ^  <^- and if  c is as defined  in  (10.19) then  for  any  e >  0 • In  the  proof  of  the  next  result  we  require  a  slight  generalization  of  the  Ascoli-Arzela  Lemma  given  in  Theorem  6.1.  To  this  end  we  let  ^  denote  a  family  of real-valued  functions  defined  on  a  set  G  C  /^^  Then  ^ is  called  uniformly  bounded if  there  is  a nonnegative  constant  M  such  that  \f{x)  \ <  M  for  all x  in  G  and  for  all  / in  ^.  Furthermore  ^ is called  equicontinuous  on  G if  for  any  e >  0 there  is  a  5  >  0 (independent  of  x^y  and  /)  such  that  \f{x)  — f{y)\  <  e  whenever  || x —j  ||<  5  for all X and  j  in  G  and  for  ail  f  e  ^ (||  • ||  denotes  a  norm  on  R^).  The  Ascoli-Arzela Lemma  now  reads  as  follows. THEOREM  10.2.  Let  G  be  a  closed  and  bounded  subset  of  R^  and  let  {fm}  be  a sequence  of  functions  in  C{GR).  If  {fm}  is  equicontinuous  and  uniformly  bounded on  G  then  there  is  a  subsequence  {rrik} and  a functuion  /  G C{GR)  such  that  {fmk} converges uniformly  to /  on G. • THEOREM  10.3.  If /  G  C(DR'')  and  (toxo)  G Z) then  (/)  has  a solution  defined  on m \t-to\<c. THEOREM  10.4.  Let /  G C(DR'^)  with /  bounded on D.  Suppose that 0  is a solution of  (E)  on the interval /  =  {ab).  Then (i) the two  limits lim  0(r)  =  0(<2+) and lim  0(r)  = (l)(b~) (ii) exist; if  {a(p{a'^))  [respectively  {b(p{b~)]  is in Z) the  solution  (p  can be continued  to • the left  past the point t = a (resp. to the right past the point t = b). THEOREM  10.5.  If /  G  C{DR'^)  and if  0  is  a solution  of  (E)  on  an open  interval  / then  (p  can be continued to a maximal open interval /*  D /  in such a way that  (t(p(t)) tends  to  dD  as ^ ^  dJ*  when  dD  is not  empty  and  |^|+  || (p(t)  | |^  oo if  dD  is  empty. • The extended  solution  0* on /*  is noncontinuable. THEOREM  10.6.  If  /  G C{DR'^)  and  if  /  satisfies  a Lipschitz  condition  on  D  with Lipschitz  constant L  (with respect  to x) then  the initial-value  problem  (/)  has  at  most one solution on any interval  |^ — ^o| ^  <^- • COROLLARY  10.7.  If /  E  C{D /?«) and dfildxj  E  C(A /?") (/ j  =  \...n) then for  any  (to  XQ)  E  D  and  any  J  containing  ^o a  solution  of  (/)  exists  on  /  and  is ^"^n^^- THEOREM 10.8.  Let /  E  C{J X /?« R^) for some open interval 7 C /? and let/ satisfy a Lipschitz condition on /  X /?« (with respect to x). Then for any (^  XQ) E /  X  /?" the initial-value problem (/) has a unique solution that exists on the entire interval J. • 47 CHAPTER 1 • •  Mathematical Descriptions of Systems Next  let  /  E  C{D R")  let  5  C  D  be  the  set  defined  in  (10.18)  centered  at (to JCo) and let c be defined  in (10.19). Successive  approximations  for  (/) or equiv-alently for  (V) are defined  as <i>oit)  =  xo f{s(f)m{s))ds m  =  0 1 2  . .. (10.20) for  \t -  ^1 <  c. THEOREM 10.9.  If /  E  C{D E^) and if/  is Lipschitz continuous on S with constant L (with respect to x) then the successive approximations (/>^ m  = 0 1 2...  given in (10.20) exist on \t -  to\  <  c are continuous there and converge uniformly  as m ^  oo to the unique solution of (/). • In the final result of this  subsection  we address initial-value problems that ex hibit dependence on some parameter  X G G  C  R^  given by X —  f(t  X A) (/Ar) v^here /  E  C(J  X  R^  X  G R^)  /  C  /^ is an open interval and ^x depends  continu ously on A. THEOREM 10.10.  Let /  E  C{J X  /?" X G 7?") where J  C Rism  open interval and G G R^. Assume that for each pair of compact subsets Jo C J and Go C G there exists a constant L  =  LJQGQ  > 0 such that for  all (t X) G JQ X Go xy  E  jR" the Lipschitz condition \\f{txX)-f(t.yX)\\^L\\x-y\\ is  true.  Then  the  initial-value  problem  (/AT)  has  a unique  solution  (t)(t r A) where (j)  E  C(J  X J  X G /?"). Furthermore if D is a set such that for all Ao E D there exists e  >  0 such that  [Ao -  e Ao  + e] Pi D C D then 0(r r A) -^  (f){t TQ AQ) uniformly  for to E  JQ as (T A) —>  (TO AO) where JQ is any compact subset of 7. • Theorem  10.10  is  a  generalization  of  Corollary  9.2  from  the  one-dimensional case  (/{^)  to the  /z-dimensional  case  (/AT)-  The  generalization  of  Theorem  9.1  for the one-dimensional  case  {i'xrp)  ^^ the n-dimensional  case  (/AT^)  is of course  also readily established. We leave the details to the reader. 1.11. SYSTEMS  OF LINEAR  FIRST-ORDER  ORDINARY DIFFERENTIAL  EQUATIONS In this section we v^ill address linear ordinary differential  equations of the  form X =  A(t)x  +  g(t) (LN) 48 Linear Systems and and and X =  A(t)x X =  Ax  -\-  g(t) X =  Ax (LH) (11.1) (L) where x  G /?^ A(t)  =  [aij(t)]  G  C(R  /^^><") g  G  C(R  R"") and A  G 7^"^^ Linear equations of the type enumerated above may arise in a natural manner in the  modeUng  process  of  physical  systems  (see  Section  1.4  for  specific  examples) or  in  the  process  of  linearizing  equations  of  the  form  (E)  or  some  other  kind  of form. A.  Linearization We consider the system of first-order ordinary  differential  equations  given by X =  fit  xl (E) where f  :  RX  D->  R""  md  D  C  R""  is some domain. If  f  GC\RXD R"") and if (/) is a given  solution of (E)  defined  for  all t  G R then we can linearize  (E)  about cj) in the following  manner. Define  8x  =  x  -  4>{t)  so that dt = fit 8x + m)  -  fit  m) =  ^it(l>it))Sx  +  FitSx) dx (11.2) where idfldx)it  x) denotes the Jacobian  matrix of fit  x)  =  (/i it x)... with respect to A: =  ( x i  . . .  x„)^ i.e.. /„(f  x)Y it x)  = dx 'J^itx) dX\ ••• 'J^itx) dXn itx) dxi itx) dXn (11.3) and Fit  8x)  ^  [fit  Sx  +  ct>it))  -  fit  (/.(O)]  -  ^-fit  4>ii))8x. (11.4) dx It turns out that Fit  8x)  is <9(||5x||) as ||5x|| -^  0 uniformly  in / on compact subsets of R  i.e. for any compact  subset I  G Rvi/e  have sup lim HSx\ho\ei \\Fit  8x) \\8x\\ =  0. To prove this we will use the fact that for each  /  =  !  . . .  « fit  Sx  + <t>it))  -  fit  <j>it))  =  iSxf Jo •1 I  Vfit  si8x)  + (f>it))ds =  y  5x-  f  ^ i t  si8x)  +  ^it))ds (11.5) pi Jo  dXj where Sxt  =  {Sx)i  and Wfi  =  i ^ .. . .  ^ I . To verify  (11.5) we let \dXi dXfi  ' g(s)  =  Mts(8x)  + cl>(t)) and use the fact  that ^(1) - ^(0) = fi(t sx + m) - fiit m) 49 CHAPTER  1: Mathematical Descriptions of Systems •1 g'{s)ds= rl f  dfi{ts{8x)  +  m) Jo 1  n dx -(ts{8x)  + 4>(t))ds  Sxj =  (8xy Jo Vfi(tsi8x)  +  (l)(t))ds. Next we note that the /th component of F(t  Sx)  is given by Fi(t Sx)  = ^^^Sxj 0  o'Xj dXj ^(ts(Sx) +  ci>(t))-^(tm) ds. axj dXj Choose \\Sx\\ =  Q11=^i(Sxi)^)^^^  and let /  be a compact interval in R.  Then ^. hm / s up \Ft(tSx)\\ 11^  II \^U^^j\o = lim  I  sup ^(t^s(Sx) +  ci>(t))-^(tm) ds\ dXj WSxW dXj (X;=(Sxi)^)i'MS}=(sup^J yi{ts{8x) \_dxj +  <f>{t))-^{t<t>{t)) dXj AI2 dsf < lim lis^lho iZ%y(Sxj)2) 1/2 111 lim \\8. ^ (sup ^(t^ss(x) + m)-^it^m) ds =  0 where we have made use of the Schwarz  Inequality. To establish  equality  (equal  to  zero)  in  the  last  line  of  the  above  equation  re quires  perhaps  a bit  of extra  work.  Since I  C  Ris  compact  and  cf) is continuous  it follows  that  the  set  (/>(/) is  compact.  Since  (f)(1)  C  D  and Z) is  a domain  we  have dist{(j){I) dD)  =  d>  0. Clearly then Xo  =  {(/)(0 +  I -^  ](Sh . --^Sn)^  : r G /  -1  <  ^/  <  1  /  =  1...  n} C  D \2jn ^ Linear Systems and XQ is a compact subset of D since (pit) +  [d/(2 ^)](si... vector-function  of (t Si...  Sn). Sn)^ is a continuous Now for  ||§x|| <  d/2  0  <  ^ <  1 ^ G /  we have that s(Sx)  +  (/)(0  G  ZQ.  Since {dfldxj){t  x)  is uniformly  continuous  on the compact  set /  X XQ we conclude  the equality  (equal to zero). Finally  since  the  above  argument  is  true  for  all  /  =  1...  n  it  follows  that F{t 8x)  is (9(||Sx||) as ||Sx|| -^  0 uniformly  in t on compact subsets of i^. Letting we obtain from  (11.2) the equation ^-f{tcl>{t))  = A{t\ dx ^^ 4  Si  =  A{t)hx  +  Fit  Sx\ (11.6) at Associated  with (11.6) we have the linear differential  equation z  =  A(t)z (LH) called the linearized  equation  of {E) about the solution  0. In  applications  the linearization  {LH)  of  (£*) about  a given  solution  (/> is  fre quently used as a means of approximating a nonlinear process by a linear one (in the vicinity of (/>). In Chapter 6 where we will study the stability properties of equilibria of {E)  [which  are specific  kinds of solutions of  (£")] we will show under what con ditions  it makes  sense to deduce  qualitative properties  of  a nonlinear  process  from its  linearization. Of special interest is the case when in (£")/ is independent of t i.e. X =  fix) (A) and (/) is a constant solution of (A) say (f)(t) =  XQ for  all t  ^  R. Under these condi tions we have ^^ ^  Sx  =  Adx  +  F(Sxl where lim  ^ ^ ^ j^  =  0 (11.7) (11.8) and A  denotes  the Jacobian  {dfldx){x^).  Again  associated  with  (11.7) we have  the linear differential  equation z  =  Az called the linearized  equation  of (A) about the solution (j){t)  =  XQ. We can generalize the above to equations of the  form X =  fit  X u) (Eu) where  f  : R  X  Di  X  D2 -^  R"" md  Di  C  /?"  D2  C  R"^ are  some  domains.  If f  E  C^(RX  Di  X D2 R^)  and if (j){t) is a given  solution of {Eu) that we assume to exist  for  all  ^ G /? and  that  is  determined  by  the initial  condition  XQ  and  the  given specific function ip E  C{R R^)  i.e. m  =  fit  m  ifjit)) t  G  R then we can linearize (Eu) in the following  manner. Define Sx  =  x-  cf)(t) and 8u  = u  -  ijj{t). Then d{8x) dt 8x  =  X-  (f)(t) =  f(t  X  u)  -  fit  (j){t) ijj{t)) =  fit  8x + 0(0 8u + ^(0) -  fit  0(0 ^(0) 51 CHAPTER  1: Mathematical Descriptions of Systems =  ^Mt  cj)it\ iPit))8x  +  ^Mt  cl)it) iljit))8u ax du +  Fiit8xu)  +  F2it8u\ (11.9) where Fife  8x  u)  =  fit  Sx + (/>(0 u)-fit ^ /. (j)it)  u)-^it ax cfyit)  il/it))8x  is oi\\8x\\ as  \\8x\\ 0  uniformly  in  t  on  compact  subsets  of  R  for  fixed  u  i.e.  for  fixed u  and  for  any  compact  subset  I  C  R  lim||§;^;||_^o (sup^^/ where \\Fiit  8xu \\Sx\\ and F2{t 8u)  =  fit  m  8u  + iff{t))  -  fit  <^it) iPit)) -  -fit  0(0  «A(0)SM ^f au is  6>(||Sw||) as  ||Sw|| -^  0 uniformly  in  t on compact  subsets  of  R i.e.  for  any  com-pact  subset  I  C  R lim||§;^||^o  sup^^^ ^F2it 8u 115^11 =  0  and where  idfldx)i • ) and idfldu)i of/  with respect to w respectively. '  ) denote the Jacobian matrix of/  with respect to x and the Jacobian matrix Letting it cl)it) if/it))  =  Ait) and dx it cj>it)  iPit))  =  Bit) du we obtain from  (11.9) di8x) dt =  8x  =  Ait)8x  +  Bit)8u  +  Flit  8x  u) +  F2it 8u). (11.10) Associated  with (11.10) we have z  =  Ait)z  +  Bit)v iLN) and  call  iLN)  the  linearized  equation  of  iEu)  about  the  solution  0  and  the  input function  ijj. As  in  the  case  of  the  linearization  of  (£")  by  (L//)  the  linearization  iLN)  of system iEu) about a given solution 0  and a given input 0  is often used in attempting to capture the qualitative properties of a nonlinear process by a linear process (in the vicinity  of  0  and  0).  In  doing  so great  care  must  be  exercised  to  avoid  erroneous conclusions. The motivation  of linearization is of course very obvious: much more is known about linear ordinary differential  equations than about nonlinear ones. For example the  explicit  forms  of  the  solutions  of  (L)  and  (11.1)  are  known;  the  structures  of the  solutions  of  (L//)  iLN)  and  iLN)  are known;  the  qualitative  properties  of  the solutions of linear equations  are known; and so  forth. 52 Linear  Systems B.  Examples We now consider some specific cases. EXAMPLE  11.1.  We consider the simple  pendulum  discussed  in Example 4.4  and de scribed by the  equation where  ^  >  0 is a constant. Letting  xi  =  x  and X2 =  i: ( I L l l)  can be expressed  as x  + A:sinx  =  0 ( H . H) Xi  =  X2 X2 =  —/:sinxi. (11.12) It  is  easily  verified  that  (/)i(0  =  0  and  </>2(0  =  0  is  a  solution  of  (11.12).  Letting fi(xi  X2)  =  X2 and  f2(xi  X2)  =  - ^ s i n x i  the  Jacobian  of  f(xiX2)  =  (fi(xi  X2\ f2{x\  X2)Y  evaluated  at {x\  X2Y  =  (0 0)-^ is given by /(O) 0 —/rcosxi 1 0_ 0  1 0 -k JC2=0 The linearized equation of (11.12) about the solution (j)i{t) =  0 <j^2(0 =  0 is given by EXAMPLEII.2.  The system of equations 0  1 -k 0 xi  =  ax\  —  bxiX2  —  cx\ X2 =  dx2  —  ex\X2  —  fx\ (11.13) describes the growth of two competing  species (e.g. two species of small fish) that prey on  each  other  (e.g.  the  adult  members  of  one  species  prey  on  the  young  members  of the  other  species  and  vice  versa).  In  (11.13)  a  b  c  d  e  a n d/  are  positive  parame ters  and it is assumed  that  xi  >  0 and  X2 ^  0. For (11.13) 4>i{t)  =  (i)\(t 0 0)  =  0 and 02(0  =" (}>2{t 0 0)  =  0 r >  0 is a solution of (11.13). A simple computation  yields A  =  ^ ( 0)  = dx a  0 0  d and thus the system of  equations constitutes  the  linearized  equation  of  (11.13)  about  the  solution  01 (0 r>  0. 0 02(0  = 0 EXAMPLE  11.3.  Consider  a unit  mass  subjected  to an inverse  square  law  force  field as  depicted  in  Fig.  1.11. In  this  figure  r denotes  radius  and  6  denotes  angle  and  it  is assumed that the unit mass (representing e.g. a satellite) can thrust in the radial and in the tangential  directions  with thrusts  u\  and  U2 respectively. The equations that  govern this system are given by r  =  rO^  -H-  Ml -lOr r + 1 -U2. r (11.14) y^ / \ N 53 CHAPTER  1: Mathematical Descriptions of Systems FIGURE  1.11 A unit mass subjected  to an inverse  square law force  field When  r(0)  =  ro r(0)  =  0 ^(0)  =  ^o ^(0)  =  COQ  and  uxif)  ^  0 U2(t)  ^  0 for  r >  0 it is easily verified  that the system of equations  (11.14) has as a solution the circular  orbit given by r(t)  =  TQ  =  constant 6(t)  =  coo  =  constant (11.15) for all r >  0 which implies  that ^ (0  =  (Oot + Oo (11.16) where  COQ  = {klriy^. If  we  let  xi  =  r  X2  =  r  x^  =  0  and  X4 =  6  the  equations  of  motion  (11.14) assume the  form k x\ +1 (11.17) Xi = X2 X2 = Xi T2 -X3 = X4 X4 = -2X2X4 Xi u U2 Xx The linearized equation of (11.17) about the solution (11.16)  [with u\{t)  =  0 U2{t)  =  0] is given by r.  n Zl Z2 Z3 ZA_ "  0 3col 0 0 — 0 1 0 0 0 0 -2COQ_  0 IVQCOQ  I 0 1 0 r- -Ul U2 Us [Z4_ 0 1 0 0 + r- 0  1 0 0 1 -| Vi |_V2. ^0-' EXAMPLE  11.4.  In  this  example  we  consider  systems  described  by  equations  of  the form X  +  Af(x)  +  Bg(x)  =  u (11.18) where  xGR'^.A  =  [atj] G 7?^><" B  =  [btj] G  7?"><" with an  >  0 bu  >  0  1 <  /  <  ^ fge C^R"  R%  u  G  C(/?+ /?") and f{x)  =  0 g(x)  =  0 if and only if x  =  0. Equation  (11.18)  can  be  used  to  model  a  great  variety  of  physical  systems.  In particular  (11.18) has been used to model a large class of integrated  circuits  consisting 54 Linear  Systems of  (nonlinear)  transistors  and  diodes  (linear)  capacitors  and  resistors  and  current  and voltage sources. (Figure  1.12  gives a symbolic representation  of such circuits.) For such circuits we assume that f(x)  =  [fi(xi)... fn{Xn)Y -tfH-» ^  ^ + FIGURE  1.12 Integrated  circuit If  u{t)  =  0 for all t  >  0 then (/)/(0  =  0 r >  0  1 <  /  <  n is a solution of (11.18). The system of equations  (11.18) can be expressed  equivalently  as Xi 7 = 1 / - = ! . (11.19) The linearized equation of (11.19) about the solution (/>/(0  =  0 and the input  Ui(t)  =  0 t  >  0 /  =  1...  w is given by Zi =  -^[atjfjiO) +  bijg'jmzj + V- (11.20) 7 = 1 where /;(0)  =  (dfj/dxj)(0)  and ^;.(0)  =  (dgj/dxj)(0\ i  = l...n. 1.12 LINEAR SYSTEMS: EXISTENCE UNIQUENESS  CONTINUATION AND CONTINUITY WITH RESPECT TO PARAMETERS OF SOLUTIONS In this  section  we  address  nonhomogeneous  systems  of  first-order  ordinary  differen tial  equations  given  by X  =  A(t)x  +  g(tX (LN) where  x  G  R^  A(t)  =  [aij(t)]  is  a real  nX  n matrix  and  g  is  a real  n-vector-valued function. THEOREM  12.1.  Suppose thatA  G  C{J /?">^")andg  G  C(7 /?'') where 7 is some open interval.  Then  for  any  to G  J  and  any  XQ G  /?" equation  (LN)  has  a  unique  solution satisfying  x(to)  =  XQ. This  solution  exists  on the  entire  interval /  and  is continuous  in (t to xo). Proof.  The function  f(t  x)  =  A(t)x  + g(t) is continuous in (t jc) and moreover for any compact  subinterval  Jo G J  there is an  LQ ^  0 such that \\f(t  X) -  fit  y)\\i  =  \\A(t)(x  -  y)\\i  <  ||A(0||i||x  -  y\\i - i ^^^  ko-(Ol  Ik -  y\\i ^  Lo\\x -  y\\i for all (t x) (t y)  G  JQXR^  where LQ is defined in the obvious way. Therefore/  satisfies a Lipschitz condition on Jo X  R^. If (^0. -^o) G JoXR^  then the continuity of/  implies the existence of solutions (The orems  6.3  and  10.3) while  the Lipschitz  condition  implies  the uniqueness  of  solutions (Theorems 8.2 and 106). These solutions exist for the entire interval JQ  (Theorems 8.4 and 10.8). Since this argument holds for any compact subinterval JQ  C /  the solutions exist and are unique for alU G /. Furthermore the solutions are continuous with respect to ^0 and XQ (Theorems 9.1 and 10.10 modified for the case where A and g do not depend on any parameters A). • 55 CHAPTER  1: Mathematical Descriptions of Systems For the case when  in  {LN)  the matrix A  and the vector g depend  continuously on parameters  A and  [x respectively it is possible to modify  Theorem  12.1 and its proof in the obvious way to show that the unique solutions of the system of equations X =  Ait  A)x +  g{t fji) (LNx^) are  continuous  in  A  and  /x  as  well.  [Assume  that  A  G  C(J  X  R^ R^^^)  and g G  C (/  X R^ /?'^)andfollowaprocedurethatissimilartotheproofofTheoreml2.1.] 1.13 SOLUTIONS  OF LINEAR  STATE  EQUATIONS In this  section  we determine  the  specific  form  of the  solutions  of  systems  of  linear first-order ordinary differential  equations. We will revisit this topic in much  greater detail in Chapter 2. Homogeneous  equations We begin by considering linear homogeneous  systems X =  A(t)x (LH) where  A  G  C(R  R""^"").  By  Theorem  12.1 for  every  XQ G  /?^ {LH)  has  a  unique solution that exists for all t  G R. We will now use Theorem  10.9 to derive an expres sion  for  the  solution  (/)(f ^o ^o)  for  (LH)  fovtE.R  with  0(^0 to XQ)  =  ^Q.  In  this case the successive approximations  given in (10.20) assume the  form (poit to xo)  =  xo (f>i{t to Xo) =  xo+\ A{s)xods J to (t>2{t  to  Xo)  =  Xo+ \  A(s)(l)i(s to  Xo)ds Jto (t>m(t to Xo) =  ^0 + JtQ A(s)(l)fn-i(s  to Xo)ds or (i>m{ttoxo)  =  ^0 + A{si)xodsi  + A{sx) A{s2)xods2dsi  + Si J to + ft Jto fSl A(si)\ Jto J to C^m-l to Afe)---| ho A(Stn)XodSm"'dSi |/  + ft Jto A(si)dsi  + ft Jto A(s2)ds2dsi  + fsi A(si) Jto Sm-l +  \  A(si)r A(s2y Jto Jto A(Sm)dSni"'dSi Xo (13.1) 56 Linear Systems where  /  denotes  the  n  X n  identity  matrix.  By  Theorem  10.9 the  sequence  {(l)m} m  =  012... determined by (13.1) converges uniformly  as m —^ oo to the unique solution (/)(f to XQ) of (LH)  on compact  subsets of R. We thus have where  <b{t to)  = / + (/)(/  to  Xo)  =  ^(t to)Xo A{si)dsi  + I rt JtQ A{si) rs J  to A(s2)ds2dsi Si +  I  A(s) I  Afe) S2 Jto JtQ +  f  Ais)^ A{S2) Jto Jto A{Sy)dS2dS2dS\ + Sm-\ A{Sm)  dSm  dSm-  l'"dsi + -• Expression  (13.3) is called the Peano-Baker  series. From expression  (13.3) we immediately  note that (13.2) (13.3) (13.4) Furthermore by differentiating  expression (13.3) with respect to time and  substitut ing into {LH)  we obtain that ^(tt)  =  L ^{tto)  =  A{t)^{tto). (13.5) From  (13.2)  it  is  clear  that  once  the  initial  data  are  specified  and  once  the nXn  matrix ^{t  to) is known the entire behavior of system {LH)  evolving in time t  is  known.  This  has  motivated  the  state  terminology:  x{to)  =  xo  is  the  state  of the system  (LH)  at  time  to (j>{t to xo)  is the  state  of  the  system  (LH)  at  time  t  the solution (f) is called the state vector  of {LH)  the components of ^  are called the state variables  of {LH)  and the matrix ^{t  to) that maps x{to) into cl){t to xo) is called the state  transition  matrix  for  {LH).  Also the vector space containing the state vectors is called the state  space  for  {LH). We can specialize the preceding discussion to Hnear systems of equations X =  Ax. {L) In this case the mth term in (13.3) assumes the  form A{si) A(S2)\  Afe)-to Jto A{Sm)  dSyj t  rsi  rs2 ds\ Sm-\ to =  A^ Jto  Jto Jto Jto IdSn ' ds\  = A'"(t  -  to)" ml and expression  (13.1) for  0^  assumes now the  form (j)ni{t  to  Xo)  =  ^+2: A'{t  -  to)^ Xo. k=i k\ We conclude once more from Theorem 10.9 that {c/)^} converges uniformly as m ^  oo to the unique solution </>(? to xo) of (L) on compact  subsets ofR.  We have (f){t to Xo) = A\t -  to)'' k\ Xo =  <|)(r to)xo  =  0(?  -  to)xo (13.6) 57 CHAPTER  1: Mathematical Descriptions of Systems where  ^(t  -  to) denotes  the  state  transition  matrix  for  (L).  [Note  that  by  writing ^(t  to)  =  ^(t  -  to) we have used a slight abuse of notation.] By making the analogy with the scalar e^  =  I  + ^1=i{a^lk\)  usage of the notation ^ A^ - ' + 1 :^ i c -1  k\ (13.7) should be clear. We call e^  a matrix exponential.  In Chapter 2 we will explore several ways of determining  e^  for a given A. Nonhomogeneous  equations Next we consider linear nonhomogeneous systems of ordinary differential  equa tions X =  A(t)x  +  g(t) (LN) where  A  G  C{RW^'') and  g  G  C(R  R"").  Again  by  Theorem  12.1  for  every xo G /?" (LN)  has a unique solution that exists for all t  E:  R. Instead of deriving  the complete  solution  of  (LN)  for  a given  set of  initial  data  x(to)  =  ;co we  will  guess the  solution  and  verify  that it indeed  satisfies  (LN).  To this  end let us assume  that the solution is of the  form (t)(t to xo)  =  ^(t  to)xo + 0(r  s)g(s)ds (13.8) where 3>(r to) denotes the state transition matrix for  (LH). To show that (13.8) is indeed the solution of (LN)  we first let t  =  to. In view of (13.4) and  (13.8) we have (f)(to to xo)  =  xo. Next by differentiating  both  sides of (13.8) and by using (13.4) (13.5) and (13.8) we have (j)(t  to  Xo)  =  ^(t to)xo  +  ^(t t)g(t) + 4>(r s)g(s)  ds =  A(t)^(t to)xo +  g(t)  + A(t)^(t s)g(s)ds =  A(t){^(t  to)xo  + ^(ts)g(s)ds^  +  g(t) =  A(t)(f)(t to xo)  +  g(t) i.e.  (j)(t to Xo) given  in  (13.8)  satisfies  (LN).  Therefore  (j)(t to xo)  is  the  unique solution of (LN).  Equation (13.8) is called the variation  of constants formula  which is discussed further in Subsection 2.3C of Chapter 2. In the exercise section of Chap ter 2 (refer  to Exercise  2.33) we ask the reader  (with hints) to derive  the  variation of constants formula  (13.8) using change  of  variables. We note that when  xo  =  0 (13.8) reduces to (/)(^ ^00)  =  ^p(t)  = ^(ts)g(s)ds (13.9) and when  XQ 7^ 0 but g(t)  =  0 for all r G /? (13.8) reduces to (l)(t to Xo) =  (/)/z(0  =  ^(t  ^)^0-(13.10) 58 Linear Systems Therefore the total solution  of {LN) may be viewed as consisting of a component that is due to the initial conditions {t^  XQ) and another component that is due to the  'Jorc-ing term''  g(t).  This type of  separation  is in general possible only in linear  systems of differential  equations. We call (l)p  3. particular  solution  of the  nonhomogeneous system  (LN)  and call (f)h the homogeneous  solution. From  (13.8)  it  is  clear  that  for  given  initial  conditions  x(to)  =  XQ and  given forcing  term g(t)  the behavior  of system  (LN)  summarized  by  </>  is known  for  all t. Thus (/)(r to xo)  specifies  the state  vector  of  (LH)  at time  t. The components </>/ of (/) /  =  1...  n represent the state  variables  for  (LH)  and the vector  space that contains the state vectors is the state  space  for  (LH). Before  closing this section it should be pointed out that in applications the ma trix A(t)  and the vector  g(t)  in  (LN)  may be only piecewise  continuous  rather  than continuous as assumed above [i.e. A(t)  and g(t) may have (at most) a finite number of  discontinuities  over  any  finite  time  interval]. In  such  cases  the  derivative  of  x with respect  to t  [i.e. the right-hand  side in  (LN)]  will be discontinuous  at a finite number of instants over any finite time interval; however the state itself x will still be  continuous  at  these  instants  [i.e. the  solutions  of  (LN)  will  still  be  continuous over R].  In  such  cases  all the  results  presented  concerning  existence  uniqueness continuation of solutions and so forth as well as the explicit expressions of solutions of  (LN)  are either  still valid  or can be modified  in the obvious  way. For  example should g(t)  experience a discontinuity  at say ti  >  to then expression (13.8) will be modified  to read (/>(r to Xo) =  ^(t  to)xo + ^(t  s)g(s)ds to^ t<ti (13.11) (t)(ttixx)-^^(tti)xi +  \  ^(ts)g(s)ds t^tu (13.12) where xi  =  lim^^^- (l)(t to xo). 1.14 STATE-SPACE  DESCRIPTION  OF  CONTINUOUS-TIME  SYSTEMS Returning  now to Section  1 let us consider  once more  systems  described  by  equa tions of the  form X =  f(tx  u) y^g(txul (14.1a) (14.1b) where  x  ^  R"" y  ^  RP  u  ^  R"^  f  : R  X  R"" X  R"^ -^  R\  md  g  : R  X  R""  X R^  ^  RP. Here  t  denotes  time  and  u  and  y  denote  system  input  and  system  out put  respectively. Equation  (14.1a) is called the state  equation  (14.1b) is called the output  equation  and  (14.1a)  and  (14.1b)  constitute  the  state-space  description  of continuous-time  finite-dimensional  systems. The system input may be a function  of t only (i.e. u : R->  R^)  or as in the case of feedback  control systems  it may be a function  of t and x (i.e. u : RX  R^ -^  R^). In either  case for  a given  (i.e. specified)  u we let  f(t  x u)  =  F(t  x)  and  rewrite (14.1a)  as X =  F(t  x). (14.2) 59 CHAPTER  1: Mathematical Descriptions of Systems Now  according  to Theorems  10.8  and  10.10 if  F  G  CiR  X /^" R"")  and  if  for  any compact  subinterval  JQ C  R  there  is  a constant  Lj^  such  that  \\F(t x)  -  F(t  x)\\  < Lj^\\x -  x\\ for  all t  G  JQ and for all x x  G R^  then the following  are true: 1.  For any (^  XQ) ^  Rx  R^  Eq. (14.2) has a unique solution (f)(t to XQ)  satisfying (/)(ro to xo)  ="  XQ that exists for all t  ^  R. 2.  The solution 0  is continuous in t to and XQ. 3.  If F depends continuously on parameters (say \  G R^) and if XQ depends  contin uously on A the solution (/> is continuous in A as well. Thus  if  the  above  conditions  are  satisfied  then  for  a  given  to XQ  and  u  Eq. (14.1a) will have  a unique  solution that exists  for  t  E. R.  Therefore  as already  dis cussed  in  Section  13  (^(^ to xo)  characterizes  the  state  of  the  system  at  time  t. Moreover under these conditions the system will have a unique response for t  E  R determined by Eq. (14.1b). We usually assume that g  G  C(R  X i?^ X /^^ RP) or that gG C\RXR''XR'^RP). An important  special case of (14.1a) and (14.1b) is systems described by linear time-varying  equations of the  form X =  A(t)x  +  B(t)u y  =  C(t)x  +  D(t)u (14.3a) (14.3b) where  A  G  C(R  R'"'"'')  B  G  C(R  R'"'"'^)  C  G  C{R  RP"""")  and  D  G  C(R  RP"""^). Such equations may arise in the modeling process of a physical system or they may be a consequence of a linearization process as discussed in Section 11. By  applying  the  results  of  Section  12  we  see  that  for  every  initial  condition x{to)  =  Xo and for every given input w  : R^ solution  that exists  for  alltGR and that is continuous  in  (t to xo). Moreover  if A and  B  depend  continuously  on parameters  say  A G  /?^ then  the  solutions  will  be continuous in the parameters as well. Indeed in accordance with (13.8) this solution is given by i?'" system (14.3a) possesses a unique (/)(r to Xo) =  ^(t  to)xo + ^(t s)B(s)u(s)ds (14.4) Jto where ^(t  to) denotes the state transition matrix of the system of equations X =  A{t)x. (14.5) By using (14.3b) and (14.4) we obtain the system  response  as y(t)  =  C(t)<^(t to)xo +  C(t) ^(t  s)B(s)u(s)ds  +  D{t)u{t\ (14.6) JtQ When in (14.3a) and (14.3b) A{t)  ^  A B{t)  ^  B  C{t)  =  C and D(t)  =  D we have the important linear time-invariant  case given by X =  Ax-\-  Bu y  =  Cx  + Du. (14.7a) (14.7b) In  accordance  with  (13.6)  (13.7)  (13.8)  and  (14.4)  the  solution  of  (14.7a)  is given by (t){ttoxo)  =  e^'^'-'^^xo  + e''^'~'^Bu(s)ds (14.8) ho 60 Linear Systems and the response of the system is given by y(t)  =  Ce^^^'-'^ho  +  C  [  e^^'-'^Bu{s)ds  +  Du{t\ (14.9) Linearity We  have  referred  to  systems  described  by  the  hnear  equations  (14.3a)  and (14.3b)  [resp.  (14.7a)  and  (14.7b)]  as  linear  systems.  In  the  following  we  estab lish precisely  the  sense in which  this  linearity  is to be understood.  To this  end  for (14.3a)  and  (14.3b)  we first let  yi  and  y2 denote  system  outputs  that  correspond  to system inputs given by u\  and W2 respectively under the condition  that  XQ  =  0. By invoking  (14.6) it is clear that the system output corresponding  to the system  input u  =  a\U\  •^-  ^2^2 where «i  and a2  are real scalars is given by y  =  a i ji  +  aiyi^ i.e.. y{t)  =  C{t)'  <b{t s)B{s)[a\Ui{s)  +  a2U2{s)\ds  +  D{t)[aiUi{t)  +  a2W2(0] =  aiC(r) (&(r s)B(s)ui(s)ds  +  0:2^(0 *(^  s)B(s)u2(s)ds +  aiD(t)ui(t)  +  a2D(t)u2(t) =  aiyiit)  + a2y2{t\ (14.10) Next  for  (14.3a)  and  (14.3b)  we let  yi  and  j2  denote  system  outputs  that  cor respond  to  initial  conditions  XQ  and  XQ    respectively  under  the  condition  that u(t)  =  Q for  all  t  ^  R  Again  by  invoking  (14.6)  it  is  clear  that  the  system  out put corresponding to the initial condition XQ =  aix^^^ + o^2^^o\ where a 1  and a 2 are real scalars is given by  y  =  aiyi  -{-  0:2J2. i-^-? y(t)  =  C(00(rfo)[«i4'^  +  «24'^] =  aiC(t)^(t =  aiyi(t)  + a2y2{tl to)x^d^  +  a2C{t)^{t to)x^o^ (14.11) Equations  (14.10)  and  (14.11)  show  that  for  systems  described  by  the  linear equations  (14.3a)  (14.3b)  [and  hence  also  by  (14.7a)  (14.7b)]  a  superposition principle  holds in terms of the input u and the corresponding  output y of the  system under the assumption of zero initial conditions and in terms of the initial conditions XQ and the corresponding output y under the assumption of zero input. It is important to note however  that  such  a superposition  principle  will in general  not hold  under conditions that combine nontrivial inputs and nontrivial initial conditions. For exam ple with xo #  0 given and with inputs ui  and U2 resulting in corresponding  outputs yi  and j2  in (14.3a) and  (14.3b) it does not follow  that the input aiwi  +  ^2^2  will result in an output a\yi  + a2}^2-1.15 STATE-SPACE  DESCRIPTION  OF DISCRETE-TIME  SYSTEMS State-space  representation The  state-space  description  of discrete-time  finite-dimensional  dynamical  sys tems is given by equations of the  form Xi(k  +  1)  =  fi(k  xi(k)...  Xn(k) u\{k)...  Utn{k)) yi{k)  =  gi{k xi(k)...  Xn(kl  ui(k)...  Um(k)) i  =  I..  .n i  =  1... p (15.1a) (15.1b) for fc =  k{)k{)+\...  where fco is an integer. (In the following we let Zdenote the set of integers  and we let Z+  denote the set of nonnegative  integers.) Letting  x{kY  = (Xiikl =  (/l(  • X  . . .  / .(  • ))  U(k)^  =  (Ui(kl  . . . Um(k)X f{'f y(kf  =  (yi(k\...yp(k)) ))  we  can  rewrite and  g(  - V  =  (gi(  'l...gm(' (15.1a) and (15.1b) more compactly  as . . . Xn(k)l 61 CHAPTER  1: Mathematical Descriptions of Systems x(k  +  1)  =  f(k  x{k)  u(k)) y{k)  =  g{K x{k)  u{k)). (15.2a) (15.2b) Throughout  this  section  we  will  assume  that  f  : Z  X  R^  X  R"^ -^  R^  and  g  : Z X Since /  is  a  function  for  given  ko x{ko)  =  XQ  and  for  given  u(k)  k  =  ko ko + I...  Eq.  (15.2a) possesses  a unique  solution  x(k)  that exists  for  all  k  =  ko ^0 +  1. Furthermore  under  these  conditions  y(k)  is uniquely  defined  for  k  = koko  + I.... As in the case of continuous-time  finite-dimensional  systems  [see Eqs.  (14.1a) and  (14.1b)] ko denotes  initial  time  k denotes  time  u{k)  denotes the  system  input (evaluated at time k) y(k)  denotes the system output  or system response  (evaluated at  time  k)  x(k)  characterizes  the  state  (evaluated  at  time  k)  Xi(k)  i  =  I.. .n denote the state  variables  (15.2a) is called the state  equation  and (15.2b) is called the output  equation. A  moment's  reflection  should  make  it  clear  that  in  the  case  of  discrete-time finite-dimensional  dynamical  systems described by (15.2a) (15.2b) questions con cerning existence uniqueness and continuation of solutions are not an issue as was the case in continuous-time  systems. Furthermore continuity  with respect to initial data  x(ko)  =  XQ or with  respect  to  system  parameters  is  not  an  issue  either  pro vided that /(  • ) and g(  • ) have appropriate continuity properties. In the case of continuous-time  systems described by ordinary differential  equa tions [see Eqs. (14.1a) and (14.1b)] we allow time t to evolve "forward"  and  "back ward." Note however that in the case of discrete-time systems described by (15.2a) and  (15.2b)  we  restrict  the  evolution  of  time  k  in  the  forward  direction  to  en sure  uniqueness  of  solutions.  (We  will  revisit  this  issue  in  further  detail  in  Chap ter 2.) Special  important  cases  of  (15.2a)  (15.2b)  are  linear  time-varying  systems given by x(k  +1)  =  A(k)x(k)  +  B(k)u(k) y(k)  =  C(k)x(k)  +  D(k)u(k) (15.3a) (15.3b) i?"^"  B  : Z-^ RP"""" and  D  : Z  ^  RP"""^.  When where  A  : Z-^ A(k)  =  A B(k)  =  B  C(k)  =  C and D(k)  =  £) we have linear  time-invariant  sys tems given by i^"><^  C  :Z^ x(k  +  1)  =  Ax(k)  +  Bu(k) y(k)  =  Cx(k)  4-  Du(kl (15.4a) (15.4b) 62 Linear Systems As in the case of continuous-time  finite-dimensional  dynamical  systems many ^f the qualitative properties of discrete-time  finite-dimensional  systems can be stud ied in terms of initial-value  problems  given by x(k  +  1)  =  f(k  x(k)\ x(ko)  =  xo (ID) where  x  E. R"" f  : Z  X  R""  -^  R""  ko  G  Z  and  ^  =  ^o 'to  +  1  . . ..  We  call  the equation x(k  +  1)  =  f(k  x(k)\ (ED) a system  offirst-order  ordinary difference  equations.  Special important cases of  (ED) include autonomous  systems  described  by periodic  systems  given by x(k  +  1)  - f(x(k)) x(k  +  1)  =  f(k  x(k))  =  f(k  +  K x(k)) for fixed K  G Z"^ and for all  k  G Z  linear  homogeneous  systems  given by linear periodic  systems  characterized  by x(k  +  1)  =  A(k)x(k) x(k  +1)  =  A(k)x(k)  =  A(k  +  K)x(k) (AD) (PD) (LHD) (LPD) for fixed K  E. Z^  and for all  kE  Z  linear  nonhomogeneous  systems  given by and linear autonomous  homogeneous  systems  characterized  by x(k  +  1)  =  A(k)x(k)  +  g(k) x(k  +  1)  =  Ax(k). (LND) (LD) In these equations  all symbols used  are defined  in the obvious way by making  ref erence  to  the  corresponding  systems  of  ordinary  differential  equations  (see  Sub section  1.3B). Difference  equations of order n Thus  far  we  have  addressed  systems  of  first-order  difference  equations.  As  in the continuous-time case it is also possible to characterize initial-value problems by nth-order ordinary  difference  equations say y(k  +n)  =  h(k  y(k)  y(k  +  I)..  .y(k  + n -  \)) (E^D) where  h  : Z  X  R^ ^  R  n  G Z^  k  =  ko ko -\-  1 —  By  specifying  an initial  time ko G  Z  and  by  specifying  y(kQ) y(ko  +  1)...  y(ko  +  n  -  1)  we  again  have  an initial-value problem given by y(k  + n)  =  h(k  y(k)  y(k  -h 1)..  .y(k  + n  -  I)) y(ko)  =  xio... y(ko  -\-  n -  I)  =  x^o-UnD) We  call  (EfiD) an  nth-order  ordinary  difference  equation  and  we  note  once  more that in the case of initial-value  problems  described  by  such equations there are no difficult  issues involving the existence uniqueness  and continuation of solutions. We can  reduce  the  study  of  (IUD)  to the  study  of  initial-value  problems  deter mined  by  systems  of  first-order  ordinary  difference  equations. To accomplish  this. we let in (Ino) y(k)  =  xi(k)  y(k  +  1)  =  X2(k)...  y(k  +  n -  1) obtain the system of first-order  ordinary  difference  equations xi(k  +  1)  =  X2(k) Xn-l(k  +  1)  == Xn(k) Xn(k +  1)  =  h(k  xi(k)...  Xn(k)). Xn(k). We now 63 CHAPTER  1: Mathematical Descriptions of Systems (15.5) Equations  (15.5) together  with  the initial  data  XQ  =  (xio... Xno) are  equivalent to the initial-value problem (Ino) in the sense that these two problems will  generate identical  solutions  [and in the  sense that the transformation  of (Ino)  into (15.5)  can be reversed unambiguously  and uniquely]. As  in  the  case  of  systems  of  first-order  ordinary  difference  equations  we  can point to several important  special  cases  of nth-order  ordinary  difference  equations including equations of the  form y(k  + n)  + an-i(k)y(k -\-  n -  1)+  • • +  ai{k)y(k  +  1) +  ao(k)y{k)  =  g{k\ (15.6) y(k  + n)  -\-  an-i(k)y(k  + n-  1)+  • • +  ai(k)y(k  +  1) +  ao(k)y(k)  =  0 -\-  n -  1)+  • • and  y{k  +  n)  +  a^-iyik +  aiy(k  +  1) +  aoy(k)  =  0. (15.7) (15.8) We  call  (15.6)  a  linear  nonhomogeneous  ordinary  difference  equation  of  order n;  we  call  (15.7)  a  linear  homogeneous  ordinary  difference  equation  of  order  n; and we call (15.8) a linear autonomous  homogeneous  ordinary  difference  equation of order n. As in the case of systems of first-order ordinary difference  equations we can dtfmQ periodic  and linear periodic  ordinary  difference  equations  of order n in the obvious way. Solutions of state  equations Returning now to linear homogeneous  systems we observe that x{k  +\)  =  A(k)x(k) (LHD) x(k  +  2)  =  A(k  +  l)x(k  +  1)  -  A(k  + l)A(k)x(k) x(n)  =  A(n  -  l)A(n  -  2) • • • A()t + l)A{k)x(k) n-\ =  YlA(j)x(k) i.e. the state of the system at time n is related to the state at time k by means of the nX  n matrix  YI^JZ\A(J) (as can easily be proved by induction). This  suggests  that the state  transition  matrix  for  (LHD) is given by n-l ^(n k) = Yl Mjl n> k and that ^(k  k)  =  I. (15.9) (15.10) 64 Linear Systems As in the continuous-time  case the solution to the initial-value  problem ^(^ +  1)  ^  A(k)x(k) x(ko)  =  Xk^ ko G Z is now given by n-l x(n)  =  ^(n  ko)xk  =  n ^U)^h n  >  ko. (15.12) Continuing  let  us  next  consider  initial-value  problems  determined  by  linear nonhomogeneous  systems (LND) x(k+l)  =  A(k)x(k)  + 8(k) ^^^^3^ x(ko)  = Xk^. Then x(ko  +  1)  -  A{ko)x(ko)  +  g(ko) x(ko  +  2)  =  A(ko  +  l)x(ko  +  1) +  g(ko  +  1) -  A(ko  +  l)A(ko)x(ko)  +  A(ko  +  l)g(ko)  +  g(ko  +  1) x(ko  +  3)  =  A(ko  +  2)4^0  +  2) -h ^(^o  +  2) -  A(fco  +  2)A(ko  +  l)A(/:o)4/:o)  +  A(ko  +  2)A(/:o  +  l)g(^o) +  A(ko  +  2)^(fco  +  1) +  g(ko  +  2) =  0(/:o  +  3 ^o)^;to  +  ^(^0  +  3 /:o +  l)g(^o) +  cD(/:o +  3 /:o +  2)g(ko  +  1) +  *(/:o  +  3 /CQ +  3)g(^o  +  2) and  so  forth.  For fc >  A:o +  1 we  easily  obtain  the  expression  for  the  solution  of (15.13)  as x(k)  =  ^(k  ko)Xk +  X  ^ ( ^' ^' +  1)^0'). (15.14) k-i We note that when  x^^  =  0 (15.14) reduces to xp{k)= k-i ^^{kJ+DgUl ; = ^o (15.15) and when  x^^ T^  0 but ^(/:)  =  0 (15.14) reduces to (15.16) Therefore the total solution  of (15.13) consists of the sum of its particular  solution Xp(k)  and its homogeneous  solution  Xh(k). Xh{k)  =  ^{kko)xk. System  response Finally we observe that in view of (15.3b)  and (15.14) the system  response  of the system (15.3a) (15.3b) is of the  form k-\ y(k)  =  C(kmk  ko)Xk  +  C(k) X  ^(k  J +  ^)BUMj)  + D(kMk\ k > ko and yiko)  =  C(ko)Xk  + D{ko)u(ko). (15.17) (15.18) Discrete time systems as discussed  above arise in several ways including  the numerical  solution  of  ordinary  differential  equations  (see  e.g.  our  discussion  of Euler's  method  in  Subsection  1.6B); the representation  of sampled-data  systems  at  Mathematical Descriptions of discrete  points  in  time  (which  will  be  discussed  in  further  detail  in  Chapter  2); in the modeling process of systems that are defined  only at discrete points in time (e.g. Systems digital computer  systems); and so forth. 65 CHAPTER 1: As  a  specific  example  of  a  discrete-time  system  we  consider  a  second-order section  digital  filter  in direct  form X\{k  +  1)  =  X2{k) X2(k +  1)  =  axi{k)  +  bx2(k)  +  u(k) y(k)  =  xi(k\ (15.19a) (15.19b) k  G Z^  where  xi(k)  and  X2(k) denote  the  state  variables  u(k)  denotes  the  input and y(k)  denotes the output of the digital filter. We depict system (15.19a) (15.19b) in block diagram form in Fig.  1.13. ujk)  + X2(/f +  1^ + /v 1 x(k  + 1) Unit delay x(k) =  y(k) Unit delay b FIGURE 1.13 Second-order section digital filter in direct form a 1.16 INPUT-OUTPUT  DESCRIPTION  OF  SYSTEMS This  section  consists  of  four  subsections.  First  we  consider  rather  general  aspects of the input-output  description  of  systems. Because  of their  simplicity  we  address the characterization  of linear discrete-time  systems next. In the third  subsection  we provide  a  foundation  for  the  impulse  response  of  linear  continuous-time  systems. Finally we address the external description  of linear continuous-time  systems. A.  External  Description  of Systems: General  Considerations The  state-space  representation  of  systems  presupposes  knowledge  of  the  internal structure  of the system. When  this  structure  is unknown  it may  still be possible to arrive at a system description—an  external  description—that  relates  system  inputs to  system  outputs.  In  linear  system  theory  a great  deal  of  attention  is  given  to re lating  the  internal  description  of  systems  (the  state  representation)  to  the  external description  (the input-output  description). 66 Linear Systems In the present context we view system  inputs  and system  outputs  as elements of two real vector spaces  U and  7 respectively  and we view a system as being repre sented by an operator  T that relates elements of  U to elements of  F. For  u  E. U and J  G  7  we will assume that  u:  R^  R^  and y  \ R-^  RP m  the case of  continuous-time  systems  and  that  u  : Z  —>  R^  and  y  : Z  ^  RP in  the  case  of  discrete-time systems.  For  continuous-time  systems  we define  vector  addition  (on  U) and  multi plication of vectors by scalars (on  L^) as and (ui  +  U2)(t)  =" ui(t)  +  U2(t) {au){t)  =  au(t) (16.1) (16.2) for all ui  U2  E  Ua  G R  and t  E  R. We similarly  define  vector addition  and mul tiplication  of  vectors  by  scalars  on  Y.  Furthermore  for  discrete-time  systems  we define  these operations  on  U and  Y analogously. In this case the elements  of  U and Y  are real  sequences  that  we  denote  e.g.  by  w =  {uk} or u  =  {u(k)}.  (It  is  easily verified  that under these rather general conditions  U and  Y satisfy  all the axioms of a vector space both for the continuous-time case and the discrete-time case.) In the continuous-time  case as well  as in the discrete-time  case the  system  is  represented hy  T  : U ->  Y and we write y  =  T(u). (16.3) In the subsequent development we will impose restrictions on the vector spaces U  F  and  on  the  operator  T  as  needed.  For  example  if  T is  a linear  operator  the system is called a linear  system.  In this case we have 3;  =  T(aiui  +  ^2^2) =  aiT(ui)  +  a2T(u2) =  aiyi  +  0:2^2 (16.4) for all a i 0^2 E  /?and ui  U2  G U where j/  =  T{ui)  E  7 /  =  12  and j  E  Y. Equa tion (16.4) represents the well-known  principle  of superposition  of linear  systems. (SISO) system.  Systems  for  which  m  >  1  p  >  1  are  called  multi-input/multi-output (MIMO)  systems. If  in  the  above  m  =  p  =  1 we  speak  of  a single-input/single-output We  say that  a system  is memoryless  or without  memory  if  its output  for  each value of the independent  variable  {t or k)  is dependent  only  on the input  evaluated at the same value of the independent  variable  [e.g. yit\)  depends only on u{t\)  and y{k\)  depends  only  on  u{k\)l.  An  example  of  such  a  system  is the  resistor  circuit shown  in Fig.  1.14  where the current  i{t)  =  u(t)  denotes  the  system  input  at  time t and the voltage across the resistor v(t)  =  Ri(t)  =  y(t)  denotes the system  output at time  t. v{t) FIGURE 1.14 Resistor circuit A system that is not memoryless is said to have memory. An example of a con- tinuous time system with  memory  is the capacitor circuit shown in Fig.  1.15  where the current  i(t)  =  u(t)  represents  the  system  input  at time  t and  the voltage  across  Mathematical the capacitor 67 CHAPTER 1: Descriptions of Systems y{t)  =  v(t)  -  -  I /(T)  dr 1  r t denotes  the  system  output  at time  t. Another  example  of  a continuous-time  system with memory is described by the scalar  equation y(t)  =  u(t  -  1) K and an example of a discrete-time system with memory is characterized by the scalar equation y(n)  =  ^ x(k) n k  G  Z. A system is said to be causal if its output at any time say ti  (or ki)  depends only on values  of  the  input  evaluated  for  t  ^  ti  (for  /: <  k\).  Thus  y(ti)  depends  only on u{t) t  ^  ti  [or y(ki)  depends only on u(k) fc <  ki].  Such a system is referred  to as being nonanticipative  since the system output does not anticipate future  values of the input. To make the above concept a bit more precise we define  the function  Uj :  R^ R"^ fovu^U by Uj{t) _  J  u{t\ 0 /<  T t  >  T and we similarly define the function  yj  \ R -^  RP fox y  ^  Y. K  system that is repre sented by the mapping  y  ^^ T(u)  is said to be causal  if and only if (T(u))r - (T(Ur))r for  all  TG  R  for  all  uGU. Equivalently  this system is causal if and only if for  uv^U that and  u^  =  Vr it is true (T(u))r  =  (T(v))r for all r  ^  R. For example the discrete-time  system described by the scalar  equation y(n)  =  u(n)  -  u(n  +1) n  G  Z is  not  causal.  Neither  is  the  continuous-time  system  characterized  by  the  scalar equation y(t)  =  x(t  +1) t  E  R. v{t) FIGURE 1.15 Capacitor circuit 68 Linear Systems It  should be pointed  out that  systems  that  are not causal  are by  no means  use-1^^^- ^^^  example causality  is not  of fundamental  importance in  image-processing applications  where the independent  variable  is not time. Even  when time is the in dependent variable noncausal systems may play an important role. For example in the processing of data that have been recorded (such as speech meteorological  data demographic data stock market fluctuations etc.) one is not constrained to process ing the data causally. An example of this would be the smoothing of data over a time interval say by means of the  system >'^"^ = 2MTT  S  "(« - ^)-k=-M A  system is said to be time-invariant  if a time shift  in the input  signal causes a corresponding time shift in the output signal. To make this concept more precise for fixed a  E  i? we introduce the shift  operator  Qa  - U -^  L^ as Qaii(t)  =  u{t -  a\ uG  Uj  ^  R. A  system that is represented  by the mapping  y  =  T(u)  is said to be  time-invariant if and only if TQaiu)  =  Qa(T(u))  =  Qaiy) for  any  a  E  R  and  any  w E  t/.  If  a  system  is  not  time-invariant  it  is  said  to  be time-varying. For example a system described by the relation is time-invariant. To see this consider the inputs  ui(t)  and U2(t) == ui(t  -  to). Then y(t)  =  cos  u(t) yi(t)  =  COSWi(r) yiit)  ==  C0SU2(t)  = COSUi(t-to) and As a second example consider a system described by the relation yi(t  -  ^o)  =  cosui(t  -  to)  =  y2(t). y(n)  =  nu(n) and consider two inputs  ui{n)  and  U2(n) =  u\{n~  no). Then y\{n)  — nui(n) and yiM  =  nu2(n)  =  nu\{n  —  no). However if we shift  the output yi{n)  by no we obtain yi{n  -  no)  =  (n  -  no)ui(n  -  ^o)  ^  yiin). Therefore  this system is not  time-invariant. B.  Linear Discrete-Time  Systems In this subsection  we investigate the representation  of linear discrete-time  systems. We begin our discussion by considering  SISO  systems. In  the  following  we  employ  the  discrete-time  impulse  (or  unit pulse  or  unit sample)  which is defined  as 8{n)  =  \ [ 0 I I ^7^0 n  E Z n  =  0. (16.5) Note that if  {p{n)}  denotes the unit step sequence  i.e. then and J l ^ ^ ^ ^ ~ | o 5{n)  = p{n)  — p{n—  1) / I > 0  / I G Z n < 0  n G Z p{n) ^lk=oS{n-k) ^0 n>0 n<0. Furthermore note that an arbitrary  sequence  {x{n)}  can be expressed as x{n)= ^ x{k)5{n-k). 69 CHAPTER  1: Mathematical Descriptions of Systems (16.6) (16.7) (16.8) In Example  10.8 we showed that a transformation  T  :U  ^Y  determined by the equation y{n)= ^ k=-oo h{nk)u{k) (16.9) where y =  {y{k)}  G F w =  {w(^)} G 6^ and /z: Z x Z ^  /^ is a linear  transformation. We also noted in Example  10.8 that for (16.9) to make any sense we need to impose restrictions  on {h{nk)}  and  {u{k)}.  For example if for every fixed n {h{nk)}  G h and  {u{k)}  ^  h  =  U  then  it  follows  from  the  Holder  Inequality  (resp.  Schwarz Inequality)  that  (16.9)  is  well  defined.  There  are  of  course  other  conditions  that one  might  want  to  impose  on  (16.9)  (refer  to  Example  10.8).  For  example  if  for every  fixed  nYk^_^\h{nk)\  <  oo (i.e.  for  every  fixed  n{h{nk)}  G h)  and  if sup^^2 W(^)\ <  "^ (i-^-' {i^{k)}  ^  loo) then (16.9) is also well  defined. We  shall  now  elaborate  on the  suitability  of  (16.9)  to represent  linear  discrete-time  systems.  To  this  end  we  will  agree  once  and  for  all  that  in  the  ensuing discussion  all  assumptions  on  {h{nk)}  and  {u{k)}  dire  satisfied  that  ensure  that (16.9) is well  defined. We will view y eY  and ueU  SLS system outputs and system inputs respectively and we let r  : L^ ^  F denote a linear transformation  that relates u to y. We first con sider the case when u{k) =Ofork<ko k  ko ^  Z. Also we assume that for ^ >  n  > ^0 the inputs u{k) do not contribute to the system output at time n (i.e. the system is causal).  Under these assumptions and in view of the linearity of T and by invoking the representation of signals by (16.8) we obtain for j  =  {y{n)}n  G Z the expression y{n)  = r ( i r_  uik)d{n  -k))  = Ti^Uo  <k)5{n-k))= ^l=k  h{nk)u{k)n  > ko dindy{n)  =  0/i  <  ^o where  T{5{n  — k))  =  {T5){n  — k)  = h{nk)  represents  the response  of  T  to  a unit  pulse  (resp.  discrete-time  impulse  or unit sample) occurring atn  =  k. Y.U  u{k)T{5{n-k)) = When  the  assumptions  in  the  preceding  discussion  are  no  longer  valid  then a  different  argument  than  the  one  given  above  needs  to  be  used  to  arrive  at  the system  representation.  Indeed  for  infinite  sums  the  interchanging  of  the  order  of the  summation  operation  X  with  the  linear  transformation  T  is  no  longer  valid. We  refer  the  reader  to  a  paper  by  I.  W.  Sandberg  ("A  Representation  Theorem for  Linear  Discrete-Space  Systems" IEEE  Transactions  on  Circuits and  Systems-I Vol.  45  No.  5  pp.  578-580  May  1998.)  for  a  derivation  of  the  representation of  general  linear  discrete-time  systems.  In  that  paper  it  is  shown  that  an  extra term  needs  to  be  added  to  the  right-hand  side  of  equation  (16.9)  even  in  the  rep resentation  of  general  linear  time-invariant  causal  discrete-time  systems.  (In the  proof  the  Hahn-Banach  Theorem  (which  is  concerned  with  the  extension  of 70 Linear Systems bounded  linear  functionals)  is  employed  and  the  extra  required  term  is  given  by lim/^oo T{Yk^^~^  u{k)5{n  — ^) + Er=Q+l u{k)5{n  — k))  with  c/  ^  ©o  as  / ^  ©o. For a  statement  and  proof  of  the  Hahn-Banach  theorem  refer  e.g.  to  reference  [12 pp.  367-370]  given  at the  end  of  this  chapter.)  In  that  paper  it is  also  pointed  out however that cases with such extra non-zero terms are not necessarily of importance in applications. In particular if inputs and outputs are defined  (to be non-zero) on just the non-negative  integers  then  for  causal  systems  no  additional  term  is needed  (or more specifically the extra term is zero) as seen in our earlier argument. In any event throughout  the present  book we will concern  ourselves  with  linear discrete-time  sys tems which  can be  represented  by equation  (16.9) for  the  single-input/single-output case (and appropriate generalizations  for multi-input/multi-output  cases). Next  suppose  that  T  represents  a  time-invariant  system.  This  means  that  if {/z(/i0)} is the response to  {5(/i)} then by time invariance the response to {d{n — ^)}is simply {h{n — k0)}.  By a slight abuse of notation welet/z(/i —^0) =h{n  — k). Then (16.9) assumes the  form y{n)= ^ u{k)h{n-k). (16.10) Expression  (16.10) is called a convolution sum and is written more compactly  as Now by a substitution  of variables we obtain for  (16.10) the alternative  expression y{n)  =  u{n)  ^h{n). and therefore we have 3^(^) =  ^  h{k)u{n  — k)^ y{n)  =  u{n) *h{n)  =  h{n) * w(/i) i.e. the convolution operation  * commutes. As  a  specific  example  consider  a  linear  time-invariant  discrete-time  system with unit impulse response given by ^^""^  =  {  0' n <  0/  =  ''''^^''^' 0 <  ^ <  1. where p{n)  is the unit step sequence given in (16.6). It is an easy matter to show that the response of this system to an input given by is y{n)  = 0 /i<  0 u{n) =  p{n)  — p{n — N) y{n)  =  Ya"-'  = a'^- -  =  -  0 < n < A ^ yS) l—a~^ I—a and y{n)  =  Y  a^-^  =  a ^ ^ —%  = ic^o l—a~^ ^ I—a N<n. Proceeding  with reference  to (16.9) we note that h{nk)  represents the  system output at time n due to a 5-function  input applied  at time k. Now if  system (16.9) is causal  then  its  output  will be identically  zero before  an input  is  applied.  Hence  a linear system (16.9) is causal if and only if Therefore  when the system (16.9) is causal we have in  fact h{n k)  =0 for all k  and all n <  k. y{n)= ^  h{nk)u(k). (16.11a) We can rewrite (16.1 la) as y{^)  ^  ^  h{n^k)u{k)-\-  ^  h{n^k)u{k) k=-oo k=ko ^y{ko-l)^f^h{nk)u{k). k=ko (16.11b) 71 CHAPTER  1: Mathematical Descriptions of Systems We say that the discrete-time  system described by (16.9) is at rest 3tk = ko  ^Z ko. Accordingly if system  (16.9) ko implies that y{k)  =Ofork> if u{k) =Ofork> is known to be at rest at ^ = ^o» we have y{^)  =  ^  h{n^k)u{k). k=kQ Furthermore if system  (16.9) is known to be causal and at rest at ^ =  ^o» its input-output description  assumes the form  [in view of (16.1 lb)] y{^)  =  ^  h{n^k)u{k). k=kQ (16.12) Next  turning to linear  discrete-time MIMO  systems  we can generalize  (16.9) to y{n)=  X  H{nk)u{k) (16.13) where y \Z  ^RP u\Z ^W^.dind 'hii{nk) h2i{nk) hi2{nk) h22{nk) himin.k)-h2m{nk) H{nk) (16.14) hpi{nk) hp2{nk) where  hij{n^k)  represents  the system  response  at time  n of the  i\h component  of y due to a discrete-time  impulse  5 applied at time k at the jth component of u while the inputs  at all other components  of u are being  held  zero.  The matrix H is called the discrete-time  unit impulse  response matrix of the system. Similarly it follows  that the system (16.13) is causal if and only if H{n^ k) =0 for all k and all n < k and that the input-output description of linear discrete-time causal  systems is given by n y{n)=  ^  H{nk)u(k). (16.15) A discrete-time  system described by (16.15) is said to be at rest at k = ko ^Z  if u{k) = 0 for ^ > ^0 implies that y{k)  = 0 for ^ > ^o- Accordingly if system  (16.13) is known to be at rest at ^ = ^o we have y{n)= J^H{nk)u{k). k=ko (16.16) 72 Linear Systems Moreover if a linear discrete-time  system that is at rest at ko is known to be causal then its input-output  description reduces to n y(n)  =  ^  H(n  k)u(k). k=ko (16.17) Finally as in (16.10) it is easily shown that the unit impulse response H(n  k) of a linear time-invariant  discrete-time MIMO system depends only on the  difference of n and k i.e. by a slight abuse of notation we can write H{n  k)  =  H(n  -  k0)  =  H(n  -  k) (16.18) for all n and k. Accordingly linear time-invariant causal discrete-time MIMO sys tems that are at rest at k  =  ko are described by equations of the  form n y(n)  =  ^H(n- k)u(k) (16.19) We  conclude  by  supposing  that  the  system  on  hand  is  described  by  the  state and output equations  (15.3a) and  (15.3b) under the assumption that x(ko)  =  0 i.e. the  system  is  at  rest  at  ^  =  ^o- Then  according  to  Eqs.  (15.17)  and  (15.18)  we obtain Hin  k)  =  < C(«)0(n k +  \)B{k) Din) 0 n>  k n  =  k n <  k. Furthermore for the time-invariant case we obtain H{n -k)  = CA^'-^^^^^B \D 0 n n n >  k =  k < k. (16.20) (16.21) C.  The Dirac Delta  Distribution For any linear time-invariant operator P from  C(R  R) to itself we say that P  admits an  integral  representation  if there exists  an integrable  function  (in the Riemann  or Lebesgue  sense) gp  : R^  R  such that for  any /  G  C(R  R) {Pf){x)  =  (/  * gp){x)  ^ r  00 J  - co f(T)gpix -  T)  dr. We call gp a kernel  of the integral  representation  of  P. For the identity operator /  [defined by If  =  f  for any /  E  C(R  R)] an integral representation  for which gp is a function  in the usual sense does not exist (see e.g. Z.  Szmydt  Fourier  Transformation  and  Linear  Differential  Equations  D.  Reidel Publishing Company Boston  1977). However there exists a sequence of  functions {(/)„} such that for  any /  G  C(R  R) (If)(x)  =  fix)  =  l i m (/  * c/>)(x). (16.22) To prove  (16.22)  we  will  make  use  of  functions  {(pn) given  by «(1  --  n\x\). 0«W  =  < 0 if  IJCI  < -n if  \x\ >  - n 73 CHAPTER 1: Mathematical Descriptions of Systems n  =  1 2 3 A  plot  of  (/)„ is  depicted  in  Fig.  1.16. We  first  establish  the  following  useful  property  of {(/)„}. FIGURE  1.16 LEMMA  16.1.  L e t/  be  a continuous  real-valued  function  defined  on R  and  let  (/>„ be defined  as above (Fig.  1.16). Then for any a  E  R lim f{T)(f>n{a  -T)dT  = f(a). (16.23) Proof  It is easy to verify  that for  n  =  1 2... Then (/>„(T) dr  = 4>n{T) dr  =  1. J-l/n (pnici^  ~  T)  dr a-l/n \ln (f)n{a —  T)  dr (j)n{T)dT  =  1. (16.24) We  first  assume  t h a t/  is  a  nonnegative  function.  By  the  continuity  of/  we  may suppose that/  assumes  a maximum value  f(bn)  and a minimum value  /(c„) where bn and Cn  E  \a  ~  -a  +  -\.  Then I n n\ r  a+l/n (l>n(a  -  T)f{T)  dr  = 3 Ja-l/n (f)n{a  -  T)f(T)  dr r a+l/n f(bn) Ma -T)dT  =  f{bn) (16.25) Ja-\ln where we have used  (16.24). In a similar manner we can show  that ct>n{a-T)f{T)dT^ f{Cn). (16.26) Since  bn and  Cn 1 a  -  -a  +  -n 1 n  it  follows  that  lim„_^oo bn  =  lim„^oo Cn =  a.  Thus (16.25) and (16.26) together with the continuity  of/  imply  (16.23). 74 Linear Systems To remove the assumption that /  is a nonnegative function  we recall that every continuous  function  /  can  be  written  as  the  sum  of  two  nonnegative  functions /  = /+—/_ where .  J/W ^ ^ ^ " [ 0 if/(x)>0 if/(x)<0 [-/(x) if/(x)<0. Then lim  / f{T)(l)n{a-T)dT =  lim  / /+(T)0„(a —T)(iT—lim  / /_(T)0„(a —T)(iT = U{a)-U{a)=f{a). • The above result when applied to (16.22) now allows us to J^^n^  a generalized function  5  (also called  a distribution)  as the kernel  of  di formal  or symbolic  integral representation  of the identity operator/  i.e. f{x)  =  lim  r f{T)Ux-T)dT = / / ( T ) 5 ( J C - T ) JT (16.27) (16.28) (16.29) /f  is emphasized  that expression  (16.28)  is not an integral at all (in the Riemann  or Lebesgue  sense) but only  a symbolic  representation.  The generalized function  d  is called the unit impulse or the Dirac delta  distribution. =  /*5(jc). In applications we frequently  encounter functions  /  G C{R^R).  If we extend  / to be defined  on all ofRby letting f{x)  =  0 for x <  0 then (16.23) becomes lim  / f{T)^n{ci-T)dT  = f{a) (16.30) for any a  >  0 where we have used the fact that in the proof  of Lemma  16.1 we need /  to be continuous only in a neighborhood of a. Therefore for /  G C{R^R)  (16.27) to (16.30) yield lim  rf{T)Ut-^)dT^ n^^Jo rf{T)5{t-T)dT Jo = f{t) (16.31) for  any t  > 0. Since the 0^ are even functions  we have 0^(f  — T) =  0^(T — f)  which allows for the representation  d{t — T) =  d{T— t).  We obtain from  (16.31) that lim  rf{T)U^-t)dT^ n^^Jo rf{T)5{T-t)dT Jo =  f{t) for any t > 0. Changing the variable  T^ =  T — f  we obtain lim  r f{T'+t)U^')dT'^ r f{T'  + t)5{T')dT'=f{t) for any t > 0. Taking the limit f ^  0+ we obtain lim  r n^^Jo- f{T'+  t)UT')dT' ^  r Jo-f{T')5{T')dT'  = f{Q) (16.32) where as in (16.24) J^- f{T')5{T')dT' tion 0f\imn^ooSQf{T'+t)(^n{^')dT'. is not an integral but a symbolic  representa 75 CHAPTER  1: Mathematical Descriptions of Systems Now  let  s  denote  a  complex  variable.  If  in  (16.31)  to  (16.32)  we  let  / ( T)  = e~^^ ^ T >  0 then we obtain the Laplace  transform lim  /  e-'^(l)n{T)dT= n^^  Jo- I  e-'^5{T)dT=l. Jo-Symbolically  we denote (16.33) by ^ ( 5)  =  1 (16.33) (16.34) and we say that the Laplace transform  of the unit impulse function  or the Dirac delta distribution is equal to one. Next we point out another important property  of  5. Consider  a  (time-invariant) operator P and assume that P admits an integral representation  with kernel gp.  If in (16.31) we let /  =  gp we have \im{P^n){t)=gp{t) (16.35) and we write this (symbolically)  as PS  =  gp. (16.36) This  shows  that  the  impulse  response  of  a  linear  time-invariant  continuous-time system  with  integral representation  is equal  to the kernel  of the integral  representa tion of the system.  Symbolically this is depicted in Fig.  1.17. Next  for  any  linear  time-varying  operator  P  from  C{R^R)  to  itself  we  say that P admits an integral representation  if there exists  an integrable function  (in the Riemann or Lebesgue sense) gp : RxR^ R  such that for any /  G  C{RR) {Pf){ri) = iyit)gp{rit)dt. (16.37) Again  we  call  gp  a kernel  of  the  integral  representation  of  P.  It  turns  out  that  the impulse  response  of  a  linear  time-varying  continuous-time  system  with  integral 9)1 ' n • • • • 5 FIGURE 1.17 p • • • • p 9n • • • • 9p oo 76 Linear Systems representation  is again  equal  to the kernel  of the integral  representation  of the sys-^^^- To see this we first observe that if /z G  C{R  X R R)  and if in Lemma  16.1 we replace /  G  C{R R) by h then all the ensuing relationships  still hold with  obvious modifications.  In particular  as in (16.27) we have for  all t  ^  R lim h{t T)(l>n(V  -T)dT  =  I  h{t 7)8(7] -  T) dj  =  h(t 7]).  (16.38) Also as in (16.31) we have lim «^°°Jo h(t T)(j)n{r]  -T)dT  = Jo h(t T)8(r] -T)dT  =  hit  r/) (16.39) for  7]>0. Now let hit  T)  =  grit  r). Then (16.38)  yields r  00 ^ 00 lim gpit  r)cl)niv -r)dr = gp(t  7)8(7] -7)d7  =  grit  T]) (16.40) which establishes our assertion. The common interpretation of (16.40) is that gp(t 17) represents  the response of the system at time t due to an impulse applied  at time  r/. In  establishing  the  results  of  the  present  subsection  we  have  made  use  of  a specific  sequence of functions  {(/>„} given by n(\  -- n\x\) ^n(x) =  { 0 if Ixl <  -n  if \x\  >  -n n= \2... We  wish  to emphasize  that  there  are  many  other  functions  that  could  have  served this purpose.  An  example  of  a commonly  used  sequence  of functions  employed  in the development  of the Dirac delta distribution is given by {ipn} where ^n(t)  =  \  In' I 0 '^'  "  ""' \t\  >  n n  =  \2 this is given by {r/^} where Another  example  of  a  sequence  of  functions  that  can  be  utilized  for f  n^te'""^ Vn(t)  =  \ 0 0 <  ^ <  00 elsewhere. n= 12... D.  Linear  Continuous-Time  Systems We let P denote a linear time-varying operator from  C(R  R^)  =  Uto  C(R RP)  =  Y and we assume that P admits an integral  representation  given by y(t)  =  (Pu)(t)  =  i  Hp(t7)u(7)d7 (16.41) where  Hp  : R  X  R^ j^pxm^ u  ^  U and  j  G  F  and  where  Hp  is  assumed  to  be integrable. This means that each element of Hp hp..  : RX  R-^  Ris  integrable  (in the Riemann  or Lebesgue  sense). Now  let  yi  and  j2  denote the response  of  system  (16.41)  corresponding  to the input  ui  and  U2  respectively  let  ai  and  0^2 be  real  scalars  and  let  y  denote  the response of system (16.41) corresponding to the input aiui  +  0:2^2  =  u. Then y  =  P(u)  — P(aiU\  +  ^2^2)  — Hp(t  T)[aiUi(T)  +  «2W2(T)]  dr 77 CHAPTER  1: Mathematical Descriptions of Systems —  ai \  Hp{t  T)U\(T)  dr J—00 -\- a2 \  Hp(t  T)U2(T)  dr J—cc =  aiP(u\) (16.42) which  shows  that  system  (16.41)  is indeed  a linear  system  in the  sense  defined  in (16.4). -\- a2P{u2)  =  cxiyi  + a2y2y Next  we let all components  of  U(T)  in  (16.41) be zero except for  the jth  com ponent. Then the /th component of y(t)  in (16.41) assumes the  form yi(t)  = hp.j(tT)Uj(T)dT. J —00 (16.43) According  to the results  of the previous  subsection  [see Eq.  (16.40)] hp.j(t r)  de notes the response of the /th component of the output of system (16.41) measured at time t due to an impulse applied to thejth  component of the input of system (16.41) applied  at time r  while  all the remaining  components  of the input  are zero. There fore  we call Hp(t  r)  =  [hp.j(t r)] the impulse  response  matrix  of system (16.41). Now suppose that it is known that system (16.41) is causal.  Then its output will be identically zero before an input is applied. It follows that system (16.41) is causal if and only if Hp(t  T)  =  0 for all r  and for  all t  <  r. Therefore  when  system (16.41) is causal we have in fact  that y(t)  = Hp{t  T)U{T)  dr. (16.44) We can rewrite (16.44) as y(t)  = Hp{t  T)U(T)  dr  + Hp(t  T)U(T)  dr =  y(to)  + Hp(t  T)U{T)  dr. (16.45) We  say  that  the  continuous-time  system  (16.41)  is  at  rest  at  t  =  to if  u(t)  = 0  for  r ^  to implies  that  y(t)  =  0  for  ^ >  ^o- Note  that  our  problem  formulation mandates  that the system be at rest at to  =  -^.  Also note that if a system  (16.41) is known to be causal and to be at rest at  ^  =  ^o^ then according to (16.45) we have y(t) Hp(t  T)U(T)  dr. J to (16.46) Next  suppose  that  it  is known  that  the  system  (16.41)  is  time-invariant.  This means that if in (16.43) hp..(t r) is the response yt at time t due to an impulse applied at time  r  at thejth  component  of the input  [i.e.  UJ(T)  =  8(t)]  with  all other  input components  set to zero then a  -r  time shift  in the input  [i.e. Uj(t -  r)  =  8{t  -  r)] will result in a corresponding  -  r time shift in the response resulting in hp. .(t-T0). 78 Linear Systems Since this argument holds for all tTGR and for all /  =  1... p  and j  =  1...  m we  have  Hp(t  r)  =  Hp(t  -  r 0).  If  we  define  (using  a  sHght  abuse  of  notation) Hp{t  -  TG)  =  Hp{t  -  T) then (16.41) assumes the  form y(t)  = J  —cc Hp(t  -  T)U(T)  dr. (16.47) Note that (16.47) is consistent with the definition  of the integral representation  of a linear time-invariant  operator introduced in the previous  subsection. The right-hand  side of (16.47) is the familiar  convolution  integral  of Hp  and u and is written more compactly  as y(t)  =  (Hp  * u)(t). (16.48) We note that since Hp(t  -  r) represents responses at time t due to impulse inputs applied  at time r  then Hp(t)  represents responses  at time t due to impulse  function inputs  applied  at r  =  0. Therefore  a linear time-invariant  system  (16.47) is causal if and only if Hp(t)  =  0 for all t  <  0. If it is known that the linear time-invariant system (16.47) is causal and is at rest at ^0 then we have rt y(t)  =  Hp(t-T)u(T)dT. to (16.49) In  this  case  it  is  customary  to  choose  without  loss  of  generality  to  =  0.  We  thus have y(t)  =  f  Hp(t  -  T)U(T)  dr Jo t  >  0. (16.50) If we take the Laplace transform  of both sides of (16.50) provided it exists we obtain y{s)  =  Hp(s)uis\ (16.51) where  y(s)  =  [yi{s\  ..  .ypJis)fHp(s)  =  [hp.j(s)l  md  u(s)  =  [ui(s\  ...  Um(s)]\ where the yi(s)  Uj(s) and  hp..(s)  denote the Laplace transforms  of  yi{t)  Uj(t)  and hpij(t)  respectively. Consistent with Eq. (16.34) we note that Hp(s)  represents  the Laplace  transform  of  the impulse  response  matrix  Hp(t).  We call Hp(s)  a  transfer function  matrix. Now suppose that the input-output relation of a system is specified  by the  state and output equations  (14.3a) (14.3b) repeated here as X =  A(t)x  +  B(t)u y  =  C(t)x  +  D(t)u. (16.52a) (16.52b) If we assume that x(to)  =  0 so that the system is at rest at to  =  0 we obtain  for the response of this  system y(t)  =  [  C{t)^{tT)B{T)u{T)dT  + D{t)u{t) (16.53) =  [  {C{t)^{t  T)B(T)  +  D(t)8(t  -  T)]U(T)  dr (16.54) JtQ where in (16.54) we have made use of the interpretation of 8 given in Subsection  C. Comparing  (16.54)  with (16.46) we conclude that the impulse response matrix  for system (16.52a) (16.52b) is given by M  (tr^-l ^^^)*(^^ ^)^(^)  +  ^WS(r  -  T) t^  T (16.55) 79 CHAPTER  1: Mathematical Descriptions of Systems Finally for  time-invariant  systems  described by the state and output  equations (14.7a) (14.7b) repeated here as X =  Ax  +  Bu y  =  Cx  -\-  Du we obtain for the impulse response matrix the  expression r)  =  f  Ce'^^'-^^B +  D8(t  -  r) Hp(t 0 or as is more commonly  written. r >  T t  < T Hp(t)  = Ce^'B  +  D8(t\ 0 t^  0 t<0. We will pursue the topics of this section further  in Chapter 2. 1.17 SUMMARY (16.56a) (16.56b) (16.57) (16.58) In  this  chapter  we  addressed  mathematical  descriptions  of  systems.  First  initial-value problems determined by systems of first-order nonlinear ordinary  differential equations were introduced. Conditions for existence and uniqueness of solutions and approaches to determine such solutions were established. The main reason for con sidering nonlinear mathematical descriptions of systems in a book on linear  systems is that the origins  of most linear  systems  are nonlinear  systems. Specifically  linear systems  are frequently  obtained from  nonlinear  systems by  a linearization  process or are the result of modeling where the nonlinear effects  of a physical process  have been  suppressed  or neglected.  Accordingly  the validity  of linear mathematical  de scriptions  must  always  be  interpreted  in  the context  of the nonlinear  systems  they approximate. The linearization of nonlinear systems along a given solution (and a given input) was discussed in Section  1.11. The solutions of linear (time-varying) state equations were obtained  in Section  1.13  using the method  of successive  approximations.  The response  of  a  linear  continuous  time  system  to  an  input  given  initial  states  was derived in Section  1.14. The  development  of  the  theory  of  discrete-time  systems  parallels  that  of continuous-time  systems and was addressed in Section  1.15. State-space representations  provide detailed  descriptions  of the internal  behav ior of  a system  while  input-output  descriptions  of  systems  emphasize  external be havior  and  how  a system  interacts  with  its  environment.  Input-output  descriptions of  linear  systems  addressed  in  Section  1.16  involve  the  convolution  integral  for continuous-time  systems  and the convolution  sum for discrete-time  systems. 80 Linear Systems In the next chapter both  state-space  descriptions  and input-output  descriptions ^f systems are revisited and their dynamic behavior is studied in detail. 1.18 NOTES Standard  references  on  linear  algebra  include  Birkhoff  and  MacLane  [2] Halmos [9] and Gantmacher  [8]. For more recent texts on this  subject  refer  e.g. to  Strang 122] and Michel and Herget [12]. Excellent sources on analysis at the elementary level include Apostol [1] Rudin [17] and Taylor  [24]. For treatments  at an intermediate level consult e.g.  Royden [16] Taylor  [23] Naylor and Sell  [15] and Michel and Herget [12]. For  a classic  reference  on ordinary  differential  equations  see Coddington  and Levinson  [6]. Other excellent  sources include Brauer and Nohel  [3] Hartman  [10] and  Simmons  [21]. Our treatment  of ordinary  differential  equations  in this  chapter was greatly influenced by Coddington and Levinson  [6] and Miller and Michel [14]. An original  standard reference  on linear  systems is Zadeh  and Desoer  [25]. Of the  many  excellent  texts  on  this  subject  the  reader  may  want  to refer  to  Brockett [4]  Kailath  [11] and  Chen  [5]. For  more  recent  texts  on  linear  systems  consult e.g. Rugh  [18] and DeCarlo [7]. In  Section  16  we  showed  that  continuous-time  finite-dimensional  linear  sys tems described by state equations have an input-output description given by integral equations. For a general and comprehensive treatment of the integral  representation of linear systems refer  to Sandberg  [19] [20]. 1.19 REFERENCES 1.  T. M. Apostol Mathematical Analysis Addison-Wesley Reading MA 1974. 2.  G. Birkhoff and S. MacLane A Survey of Modern Algebra Macmillan New York 1965. 3.  F. Brauer and J. A. Nohel Qualitative Theory of Ordinary Differential Equations Ben jamin New York 1969. 4.  R. W. Brockett Finite Dimensional Linear Systems Wiley New York 1970. 5.  C. T. Chen Linear System Theory and Design Holt Rinehart and Winston New York 1984. 6.  E. A. Coddington and N. Levinson Theory of Ordinary Differential Equations McGraw-Hill New York 1955. 7.  R. A. DeCarlo Linear Systems Prentice-Hall Englewood Cliffs NJ 1989. 8.  F. R. Gantmacher Theory of Matrices Vols. I II Chelsea New York 1959. 9.  R R. Halmos Finite Dimensional Vector Spaces D. Van Nostrand Princeton NJ 1958. 10.  P. Hartman Ordinary Differential Equations Wiley New York 1964. 11.  T. Kailath Linear Systems Prentice-Hall Englewood Cliffs NJ 1980. 12.  A. N. Michel and C. J. Herget Applied Algebra and Functional Analysis Dover New York 1993. 13.  A. N. Michel and K. Wang Qualitative Theory of Dynamical Systems Marcel Dekker New York 1995. 14.  R. K. Miller  and A. N. Michel  Ordinary  Differential  Equations  Academic Press New 81 York  1982. 15.  A. W. Nay lor and G. R. Sell Linear  Operator  Theory  in Engineering  and Science  Holt Rinehart  and Winston New York 1971. 16.  H. L. Royden Real Analysis  Macmillan  New  York  1965. 17.  W. Rudin Principles  of Mathematical  Analysis  McGraw-Hill New York  1976. 18.  W. J. Rugh Linear  System  Theory  Second Edition Prentice-Hall Englewood Cliffs NJ CHAPTER  1: Mathematical Descriptions of Systems 1996 19.  I. W. Sandberg  "Linear  maps  and  impulse  responses" IEEE  Transactions  on  Circuits and Systems  Vol. 35 No. 2 pp. 201-206  1988. 20.  I. W. Sandberg "Integral representations for linear maps" IEEE  Transactions  on  Circuits and Systems  Vol. 35 No. 5 pp. 536-544  1988. 21.  G. F. Simmons Differential  Equations  McGraw-Hill New York  1972. 22.  G. Strang Linear  Algebra  and Its Applications  Academic Press New York  1980. 23.  A. E. Taylor Introduction  to Functional  Analysis  Wiley New York  1958. 24.  A. E. Taylor Advanced  Calculus  Blaisdell New York  1965. 25.  L.  A.  Zadeh  and  C.  A.  Desoer  Linear  System  Theory—The  State  Space  Approach McGraw-Hill New York 1963. 1.20 EXERCISES 1.1.  (Hamiltonian  dynamical  systems)  Conservative  dynamical  systems  also  called Hamiltonian  dynamical  systems  are  those  systems  that  contain  no  energy-dissipating elements. Such  systems  with n degrees  of freedom  can be characterized  by means  of a Hamiltonian  function  H{p  q) where  q^  =  {q\...  qn) denotes n generalized  position coordinates  and  p^  =  {p\...  Pn) denotes  n  generalized  momentum  coordinates.  We assume that H{p  ^) is of the  form H{pq)  =  T(qq)  + W(qX (20.1) where  T denotes  the kinetic  energy  and  W denotes  the potential  energy  of the  system. These energy terms are obtained from  the path-independent  line  integrals T(qq)=\ p(q^fd^=\ JO Jo  / ^i ^pt(qOd^i (20.2) (20.3) where  fj  =  1...  n denote generalized potential  forces. For the integral  (20.2) to be path-independent  it is necessary  and sufficient  that dpi{qq)  _ dpj{qq) dqj dqt ij=l... n. (20.4) A similar statement  can be made about Eq. (20.3). Conservative dynamical systems are described by the system of 2n ordinary  differ ential  equations qi  =  -z—{pq) I  = l...n "^P^ Pi  =  -—-(pq) oqt I  = h...n. (20.5) 82 Linear Systems Note that if we compute the derivative of H(p q) with respect to t for (20.5) [i.e. along the solutions qi{t) Pi{t\ i  =  I..  .n]  then we obtain by the chain rule. -^(p(t\q(t)) i=l ^  dH ^ JH^  ^  OH ^ JH  ^  ^ ^ - S ^^^' '^-^'' '^ ^ S ^^^' 'W'  '^ ^ '• In other words in a conservative system (20.5) the Hamiltonian i.e. the total energy will be constant along the solutions (20.5). This constant is determined by the initial data (p(0)^(0)). (a)  In Fig.  1.18 Mi  and M2 denote point masses  ^1 K2 K  denote spring constants and xi X2 denote the displacements of the masses M\ and M2. Use the Hamiltonian formulation of dynamical systems described above to derive a system of  first-order ordinary differential  equations that characterize this system. Verify your answer by using Newton's second law of motion to derive the same system of equations. As sume that x\{0\  x\{0) ^2(0) i:2(0) are given. K A A /77777777777V7777777777777777777777777A FIGURE 1.18 Example of a conservative dynamical system M 1 h^A/W-^2  —VW  i Ko (b)  In Fig. 1.19 a point mass m is moving in a circular path about the axis of rotation normal to a constant gravitational field (this is called the simple pendulum problem). Here / is the radius of the circular path g is the gravitational  acceleration  and x denotes the angle of deflection  measured  from  the vertical. Use the Hamiltonian formulation of dynamical systems described above to derive a system of  first-order ordinary differential  equations that characterize this system. Verify your answer by FIGURE 1.19 Simple pendulum using Newton's  second  law  of motion  to derive the  same  system of equations. As sume that x(0)  and i:(0) are given (c)  Determine  a system of  first-order  ordinary  differential  equations  that  characterizes the  two-link  pendulum  depicted  in  Fig.  1.20.  Assume  that  ^i(O) ^2(0) ^i(O)  and ^2(0) are given. 83 CHAPTER  1: Mathematical Descriptions of Systems y////////y FIGURE  1.20 Two-link  pendulum 1.2.  (Lagrange's  equation) If a dynamical  system contains elements that dissipate  energy such as viscous friction elements in mechanical systems and resistors in electric circuits then we can use Lagrange's  equation  to describe such systems. (In the following we use some of the same notation used in Exercise  1.1.)  For a system with n degrees of  freedom this equation is given by d  idL \ dL ..^dD /  =  1...  n (20.6) where q^  =  {q\ ••-qn)  denotes the generalized position vector. The function  L{q q) is called the Lagrangian  and is defined  as L{q q)  =  T(q  q) -  W{ql i.e. the difference  between the kinetic energy  T and the potential energy  W. The function  D{q) denotes Rayleigh's  dissipation  function  which we shall  assume to be of the  form ^(^)- la^iMp ^  i=\  7=1 where [jS/y] is a positive semidefinite matrix (i.e. [/3/y] is symmetric and all its eigenval ues  are nonnegative).  The  dissipation  function  D  represents  one-half  the rate  at  which energy  is  dissipated  as  heat.  It  is  produced  by  friction  in  mechanical  systems  and  by resistance in electric circuits. Finally  ft  in  Eq.  (20.6)  denotes  an  applied  force  and  includes  all  external  forces associated  with the qt coordinate. The force  ft  is defined  as being positive when it acts to increase the value of the coordinate  qt. (a)  In  Fig.  1.21  M\  and  M2  denote  point  masses;  Ki  K2 K  denote  spring  constants; yh  yi  denote the  displacements  of masses  M\  and M2 respectively;  and  B\  B2 B denote  viscous  damping  coefficients.  Use  the  Lagrange  formulation  of  dynamical 84 Linear  Systems y77'Y/////yyyyy////y//7?^ 81 Bp FIGURE  1.21 An example of a mechanical  system with energy  dissipation systems described  above to derive two second-order  ordinary differential  equations that characterize this system. Transform  these equations into a system of first order ordinary differential  equations. Verify your answer by using Newton's second law of motion  to derive  the  same  system  equations. Assume  that  ji(0) ji(0) j2(0) J2(0) are given. (b)  Consider the capacitor microphone depicted in Fig.  1.22.  Here we have a capacitor constructed from  a fixed plate and a moving plate with mass M. The moving plate is suspended from the fixed frame by a spring that has a spring constant k and also some damping expressed by the damping constant B. Sound waves exert an external  force fit)  on the moving  plate. The  output  voltage  v^ which  appears  across  the  resistor R  will reproduce electrically  the sound-wave patterns that strike the moving plate. When  fit)  =  0  there  is  a  charge  q^  on  the  capacitor.  This  produces  a  force of  attraction  between  the plates  that  stretches  the  spring  by  an  amount  x\  and  the space between the plates is XQ. When sound waves exert a force on the moving plate there will be a resulting motion displacement x that is measured from the equilibrium position. The distance between the plates will then be  XQ -  x  and the charge on the plates will be ^0 +  ^• Moving plate with mass M -Xi  -\- X-K ->AAN— ^B fit) V Fixed plate - x o - x-tl'tr^ •^Tjcir—' FIGURE  1.22 Capacitor  microphone When  displacements  are small the expression  for the capacitance is given ap 85 CHAPTER  1: Mathematical Descriptions of Systems proximately  by XQ  —  X with  Co  =  eA/xo where e  >  0 is the dielectric constant for air and A is the area of the plate. Use the Lagrange formulation of dynamical systems to derive two second-order ordinary differential  equations that characterize this system. Transform  these equa tions into a system of first-order ordinary differential  equations. Verify  your  answer by  using  Newton's  laws  of  motion  and  Kirchhoff's  voltage/current  laws.  Assume that x(0) i(0) ^(0) and q{0) are given. Use the Lagrange formulation  to derive a system of first-order differential  equations for the system given in Example 4.3. (c) 1.3.  Find examples  of initial-value problems  for  which  (a) no solutions exist;  (b) more than one solution exists; (c) one or more solutions exist but cannot be continued for all r G  R\ and (d) unique solutions exist for all r G /?. 1.4.  (Numerical  solution  of  ordinary  differential  equations—Euler's  method)  In  Sub section  1.6B  it is shown that an approximation  to the solution of the scalar  initial-value problem y  =  jyty) fit  y) j(ro)  =  JO y{to)  =  Jo is given by Euler's  method  [see Eq.  (6.6)] yk+i  =  yk  + hf{tkyk\ k  =  Qh2. (20.7) (20.8) where h  =  tk+\—  tk is the (constant) integration  step. The interpretation  of this method is that the area below the solution curve  [see Eq. (V) in Section  1.6]  is approximated  by a sequence of sums of rectangular  areas. This method is also called \hQ forward  rectan gular  rule (of integration) (a)  Use Euler's  method to determine the solution of the initial-value  problem j  =  3 j y{tQ) tQ =  0 tQ^  t ^  10. (b)  Use Euler's method to determine the solution of the initial-value  problem y  =  t{yf - / yit^)  =  1 y(to)  =  0 to  =  0 to ^  t  ^  10. Hint:  In  both  cases  use  h  =  0.2.  For  part  (b)  let  y  =  xi xi  =  X2 X2 =  tx^  -  x\ and  apply  (20.8)  appropriately  adjusted  to  the  vector  case.  In  both  cases  plot  yk  vs. ^^ y^ =  01 2  . . .. Remark:  Euler's  method yields  arbitrarily  close approximations  to the solutions of (20.7)  by  making  h  sufficiently  small  assuming  infinite  {computer)  word  length.  In practice  however  where  truncation  errors  (quantization)  and  round-off  errors  (finite precision  operations)  are  a reality  extremely  small  values  of  h may  lead  to  numerical instabilities. Therefore  Euler's method is of limited value as a means of solving initial-value problems  numerically. 1.5.  (Numerical  solution  of  ordinary  differential  equations—Runge-Kutta  methods) The Runge-Kutta  family  of integration  methods  are among the most widely used  tech niques to solve initial-value problems  (20.7). A simple version is given by where k  =  Uki  -h 2y^2 +  2^3  +  ^4) yi+i  =  yi  +  K 86 Linear Systems with fe k\  =  hf(ti  yt) =  hfiu  +  ^hyi  +  ^ki) h  =  hf(ti  +  i/i};/  +  ife) k4  =  hfiti  +  h ji  +  h) andf/+i  =  ti +  Kyito)  =  y^. The idea of this method is to probe ahead (in time) by one-half or by a whole step h to determine the values of the derivative  at several points and then to form  a weighted average. Runge-Kutta methods can also be applied to higher order ordinary differential  equa tions. For example after  a change of variables suppose that a second-order  differential equation has been changed to a system of two  first-order  differential  equations say x\  =  fi(txiX2) Xl  =  flit  Xi X2X xiito)  =  xio X2(to) = X20. ^20 9) In solving (20.9) a simple version of the Runge-Kutta  method is given by yt+i  =  Ji  +  k yi  =  {xu  xiiY and k  =  (k  lY k  =  ^('^i  +  ^h  +  2fe  +  h) I  =  l(l\+  2/2  +  2/3 +  U) where with and h  =  hfi{tuXiiX2i) h  =  hfiiU  Xu X2i) h  =  hfiiti  +  i/z Xu  +  ^ki  X2i  +  5/1) h  =  hf2(ti  +  ^h  xu  +  ^h  X2i  +  5/1) h  =  hfiiU  +  ^h  Xu  +  ^k2 X2i  +  5/2) h  =  hf2(ti  +  \K  Xu  +  {h  X2i  +  ^2) h  =  hfi(ti  +  h Xu  +  ks X2i  +  h) k  =  hf2(ti  +  h Xu  + h  X2i  +  h\ Use the Runge-Kutta  method described  above to obtain numerical  solutions to the initial value problems given in parts (a) and (b) of Exercise  1.4. As there plot your data. Remark:  Since  Runge-Kutta  methods  do  not  use  past  information  they  consti tute  attractive  starting  methods  for  more efficient  numerical  integration  schemes  (e.g. predictor-corrector methods). We note that since there are no built-in accuracy  measures in the Runge-Kutta  methods significant  computational  efforts  are frequently  expended to achieve a desired  accuracy. 1.6.  (Numerical  solution  of  ordinary  differential  equations—Predictor-Corrector methods)  A  common  predictor-corrector  technique  for  solving  initial-value  problems determined  by  ordinary  differential  equations  such  as  (20.7)  is  the  Milne  method which we now  summarize. In this method  yt-i  denotes the value of the first derivative at time ^/_i where  ti is the time for  the  /th iteration  step j/-2 is  similarly  defined  and yt+i represents the value of j  to be determined. The details of the Milne method are: 1-  yi+ip  =  yi-3  +  —(2J/-2  -  yt-i  +  2j/) (predictor) Ah T 2.  yi+ip  = fiti+uyt+ip) 3.  yt+ic  =  yt-i  +  3 (i^z-i  +  4j/  + yt+ip) h (corrector) 4.  yi+ic = f(ti+uyi+ic) 5.  yt+u  =  yi-i  +  3 (>'/-i  +  4j/  +  yt+ic) h (iterating  corrector) The  first  step  is to  obtain  a predicted  value  of  yt+i  and  then  substitute  yi+\p into the  given  differential  equation  to  obtain  a predicted  value  of  yi+i  as  indicated  in  the second equation above. This predicted value yi+ip is then used in the second equation the corrector  equation  to obtain  a corrected  value  of  yi+\.  The corrected  value  yt+ic is next  substituted  into  the  differential  equation  to  obtain  an  improved  value  of  y/+i and  so  on.  If  necessary  an  iteration  process  involving  the  fourth  and  fifth  equations continues until successive values of yi+i differ  by less than the value of some desirable tolerance. With yt+i determined to the desired accuracy the method steps forward  one h  increment. A  more  complicated  predictor-corrector  method  that  is  more  reliable  than  the Milne  method  is  the  Adams-Bashforth-Moulton  method  the  essential  equations  of which  are 87 CHAPTER  1: Mathematical Descriptions of Systems yi+ip  =  yt  + ^ 5 5 ^  -  59y-i  +  31yi-2  -  9^-3) 24' Ji+U J/  + - ( 9 j  +i  +  19j-5 j / -i  +  yi-2) where in the corrector equation yi+\ denotes the predicted value. The  application  of predictor-corrector  methods  to  systems  of  first-order  ordinary differential  equations  is  straightforward.  For  example  the  application  of  the  Milne method to the second-order  system in (20.9) yields from  the predictor  step ^ki  + \p  =  Xki-3  +  ^(2Xki-2 4/z. -  Xki^i  +  2Xkil k  =  1  2. T h en ^ki+ip  —  fk{U+\ ^ii+\py  X2i+\p)> k=  12 and the corrector  step assumes the  form ^ki+\c  —  ^ki-l + l^{^ki-\ +  ^^ki  +  ^ki  + \) k  —  12 and Xki+\c  =  fk(ti+h  xij+ic  X2i+ic\ k  =  12. Use  the  Milne  method  and  the  Adams-Bashforth-Moulton  method  described above to obtain numerical  solutions to the initial-value problems given in parts (a) and (b) of Exercise  1.4.  To initiate the algorithm refer  to the Remark in Exercise  1.5. Remark.  Derivations and convergence properties of numerical integration schemes such as those discussed here and in Exercises  1.4  and  1.5 can be found in many of the standard texts on numerical  analysis (see e.g. D. Kincaid  and W. Cheney  Numerical Analysis  Brooks-Cole Belmont CA  1991). 1.7.  (a)  Prove Theorem 6.2 for the interval  [to  -  c to]. (b)  Prove Theorem 7.2 for the endpoint  a. (c)  Prove Theorem  8.2 for the interval  [to  -  d to]. (d)  Prove Theorem  8.4 for  t  <  to. (e)  Prove Theorem  8.5 for the interval  [to  -  cto]. (f)  Prove Theorem  10.1 through  and including Theorem  10.10. 1.8.  (a)  Prove that the function  f(x)  =  x^^^ is continuous but not Lipschitz  continuous. (b)  Show that the initial-value problem i;  =  x^^^ ent solutions. (Remember to consider t  <  0.) x(0)  =  0 has infinitely  many  differ-1.9.  Use Theorem  8.5  to solve the initial-value problem  x  =  ax  + t x(0)  =  xo for  ^ >  0. Here a  E  R. 1.10.  Consider the initial-value  problem X =  Ax x(0)  -  Xo (20.10) 88 Linear Systems where  x  ^  R^  and  A  E  R^^^.  Let  Ai A2 denote  the  eigenvalues  of A i.e.  Ai  and A2 ^^^ ^^^ ^^^^^ ^^ ^^^ equation  det (A  -  XI)  =  0 where  det  denotes  determinant  A is a scalar and /  denotes the 2 X 2 identity matrix. Make specific  choices of A to obtain the following  cases: rollowing cases: L  Ai  >  0 A2 >  0 and Ai 7^  A2; 2.  Ai  <  0 A2 <  0 and Ai 7^  A2; ^ \.  ^  0-3.  Ai  =  A2 >  0; 4.  Ai  =  A2 <  0; 5.  Ai  >  0 A2 <  0; 6.  Ai  =  a  +  /j8 A2 =  a  -  /jS /  =  v ^  a  >  0; 7.  Ai  =  a  +  //3  A2  =  a  -  //3 a  <  0; 8.  Ai  =  /jSA2  =  - / / 3. \.  = Using  r as  a parameter  plot  (l)2(t 0 xo)  vs.  (t)i(t 0 XQ) for  0  <  ^ <  r/  for  every case enumerated  above. Here  [(/>i(t to XQ) (f)2{t to xo)V  =  <P(t to xo) denotes the so lution  of  (20.10). On your plots indicate  increasing  time  t by means  of  arrows.  Plots of this type  are called  trajectories  for  (20.10) and  sufficiently  many  plots  (using  dif ferent initial conditions and sufficiently  large tf)  make up a phase  portrait  for (20.10). Generate a phase portrait for each case given  above. 1.11.  Write two first-order ordinary differential  equations for the van der Pol equation  (4.10) by choosing xi  =  x and X2 =  i i. Determine by simulation/?/z<35^p6>rrra/r5 (see Exer cise  LIO) for this example for the cases e  =  0.05  and e  =  10 (refer  also to Exercises L5  and  L6  for  numerical  methods  for  solving  differential  equations).  The  periodic function  to which the trajectories  of (4.10) tend is an example of a limit  cycle. 1.12.  For (4.11) consider the hard linear and soft spring  models  given by g(x)  =  k(l  +  a^x^)x g{x)  =  kx g(x)  =  k(l  -  a^x^)x respectively  where  ^  >  0 and a  7^ 0. Write two  first-order  ordinary  differential  equa tions  for  (4.11)  by  choosing  x\  =  x  and  X2 =  x.  Pick  specific  values  for  k  and  a^. Determine by  simulation/7/i(2>y^ portraits  (see Exercise  1.10)  for  this example  for  the above three cases. 1.13.  Verify  that the spaces in Examples  10.1 to  10.6 satisfy  the axioms of vector  space. 1.14.  Let F  =  {0 1 2 3}. Determine operations  " +"  and "•" so that [F -H •} is a field. 1.15.  (a)  Verify  that the transformation  given in Example  10.8 is linear (b)  Verify  that the transformation  given in Example  10.9 is linear. 1.16.  (a)  Prove (10.13) (10.14) (10.15) and  (10.16). (b)  Verify  that  the  functions  ||  • ||i ||  • lb  and  ||  • ||oo defined  in  (10.11)  (10.12)  and (10.10) respectively each  satisfy  the axioms of a norm. (c)  Prove the relations (D-i) (D-ii) (D-iii) given at the beginning of Subsection  1 .IOC. 1.17.  Let  g{t)  =  [g\(t)...  gn(t)V  be  defined  on  some  interval  J  C  R  and  assume  that gi  : J  ^  R  is  integrable  over  Ji  =  I..  .n.  Prove  that  for  b  >  a \\\^  g(t)dt\\  < J  11^(011 dt  where ||  • || denotes a norm on 7?". 1.18.  (a)  Show that x^  =  (0 0) is a solution of the system of  equations Xi  =  x\  +  x\  +  X2 cos  X\ X2 =  {\  + X\)x\  +  (1 +  X2)x2 +  xi  sinx2. Linearize this system  about the point  x^  =  (0 0). By means  of computer  simula tions compare solutions corresponding to different  initial conditions in the vicinity of the origin of the above system of equations  and its  linearization. (b)  Linearize the (bilinear control)  system jc +  (3 +  x^)x  +  (1 +  jc +  x^)u  =  0 about the solution ;c  =  0 i  =  0 and the input u{t)  =  0. As in part (a) compare (by means of computer simulations) solutions of the above equation with corresponding solutions of its  linearization. (c)  In the  circuit  given  in Fig.  1.23  v/(0  is a voltage  source  and the nonlinear  resis tor obeys the relation  IR  =  1.5v|  [Vi(t)  is the circuit  input  and  v/?(0  is the  circuit output]. Derive the differential  equation  for this circuit. Linearize this  differential equation for the case when the circuit operates about the point v/  =  14. CHAPTER  1: Mathematical Descriptions of Systems 1 Q -MAr 'fl(0 1  F. VR(t) .(.) Q FIGURE  1.23 Nonlinear  circuit 1.19.  Consider a system whose state-space description is given by X =  —k\k2j^+ k2u(t) y  =  ki  Jx. Linearize this system about the nominal  solution Wo  =  0 2 Vxo(0  =  2 Jk  —  ki k2t where xo(0)  =  k. 1.20.  (Inverted  pendulum)  The  inverted  pendulum  on  a moving  carriage  subjected  to  an external force  ix{t) is depicted in Fig.  1.24. The moment of inertia with respect to the center of gravity is /  and the  coefficient of friction  of the carriage  (see Fig.  1.24)  is F. From Fig.  1.25  we obtain the  following equations for the dynamics of this  system: m—r(5'  +  Lsin(/>)  =  H (20.11a) 90 Linear  Systems (Center of gravity) FIGURE  1.24 Inverted  pendulum m-r^{Lcos^) dfi =  Y  -  mg J—p2  ^  LY  sin (j)  -  LH  COS (I) Assuming that m  «  M  (20.lid)  reduces to ^d^S  dS (20.11b) (20.11c) (20. lid) (20. lie) Eliminating H  and  Y from  (20.11a) to (20.11c) we obtain (/  +  mL^)cf>  =  mgL  sin (/) -  mLS  cos (/>. (20.1If) Thus the system of Fig.  1.24  is described by the  equations 4>  -  (77 )sin(^ +  (^]scos(t>  =  0 (20.11g) MS  +  F5  =  fji(t) FIGURE  1.25 where L'  = J  +  mL'^ mL denotes the effective  pendulum  length. Linearize  system (20.11g)  about cj) 0. 91 CHAPTER 1: Mathematical Descriptions of Systems 1.21.  (Magnetic ball suspension system) Figure  1.26  depicts a schematic diagram of a ball suspension  control  system.  The steel  ball  is suspended  in air by the electromagnetic force  generated by the electromagnet.  The objective of the control is to keep the steel ball  suspended  at a desired  equilibrium  position by controlling the current  i(t) in the magnet coil by means of the applied voltage v(0 where r >  0 denotes time. The resis tance and inductance of the coil are R and L(s(t))  =  L/s(t)  respectively where L > 0 is a constant and s(t)  denotes the distance between the center of the ball and the magnet at time t. The force  produced by the magnet is Kfl(t)/s^(t)  where  jfiT >  0 is a propor tional constant and g denotes acceleration  due to gravity. (a)  Determine the differential  equations governing the dynamics of this  system. (b)  Let v(t) = Veq2i nominal  (desired) value of v{t).  Determine  the resulting  equilib rium of this  system. (c)  Linearize the equation  obtained  in (a) about the equilibrium  solution  determined in (b). y//////////^ NsN\^  ns^S^—^*--o"^ R L v{t) Electromagnet Steel ball FIGURE 1.26 Magnetic ball suspension  system 1.22.  (a)  For the mechanical  system  given in Exercise  1.2a we view  /i  and /i  as making up the system  input  vector and y\  and y2 the system  output  vector.  Determine a state-space description for this  system. (b)  For the same mechanical  system we view  (/i 5/2)^  as the system  input and we view  83^1 +  10^2 as the (scalar-valued)  system  output.  Determine  a  state-space description for this  system. (c)  For part (a) determine the input-output  description of the  system. (d)  For part (b) determine the input-output  description of the  system. 92 1.23.  For  the  magnetic  ball  suspension  system  given  in  Exercise  1.21  we  view  v and  s  as Linear  Systems the system input and output  respectively. (a)  Determine  a state-space representation  for this  system. (b)  Using the linearized equation obtained in part (c) of Exercise 1.21 obtain the input-output description of this  system. 1.24.  In Example 4.3 we view  Ca  and 9 as the system input and output  respectively. (a)  Determine a state-space representation  for this  system. (b)  Determine the input-output  description  of this  system. 1.25.  For the second-order  section digital filter in direct form  given in Fig.  1.13  determine the input-output description where xi(k)  and u(k)  denote the output and input respec tively. 1.26.  In  the  circuit  of Fig.  1.27  Vi{t)  and  Vo(t)  are voltages  (at time  t)  and  Ri  and  R2  are resistors.  There  is  also  an  ideal  diode  that  acts  as  a  short  circuit  when  V/  is  positive and as an open circuit when Vi is negative. We view  Vi and  VQ as the system input and output  respectively. (a)  Determine an input-output  description  of this  system. (b)  Is  this  system  Hnear?  Is  it  time-varying  or  time-invariant?  Is  it  causal?  Explain your answers. Diode i{t) ^o(0 ^•(0  O FIGURE  1.27 Diode circuit 1.27.  We consider the truncation  operator  given by y{t)  =  Tr(u(t)) as a system where r  E  7? is fixed u and y denote system input and output respectively t denotes time and Tr(  • ) is specified  by Tr(u(t)) u{t) 0 t  <  T t  >  T. Is  this  system  causal?  Is  it  linear?  Is  it  time-invariant?  What  is  its  impulse  re sponse? 1.28.  We consider the shift  operator  given by y{t)  =  Qr(u(t))  =  u(t  -  T) as  a  system  where  r  ^  R  is  fixed  u  and  y  denote  system  input  and  system  output respectively and t den  tes time. Is this system causal? Is it linear? Is it time-invariant? What is its impulse  response? 1.29.  Consider the system whose input-output  description is given by y(t)  =  min{wi (0^2(01 where  u(t)  =  [ui(t)  U2(t)]^  denotes the system input and y(t)  is the system output. Is this system  linear? 1.30.  Suppose  it  is  known  that  a  linear  system  has  impulse  response  given  by  h(t r)  = exp(-|r  -  T|).  IS this system causal? Is it  time-invariant? 93 CHAPTER  1: Mathematical Descriptions of Systems 1.31.  Consider a system with input-output  description  given by y(k)  =  3u(k  +  1) +  1 kGZ where y and u denote the output and input respectively  (recall that Z denotes the inte gers). Is this system causal? Is it linear? 1.32.  Use expression  (16.8) x(n)  =  ^ x(k)8(n  -  k) and 8(n)  =  p(n)  -  p(n  -  1) to express the system response y(n)  due to any input  u{k) as a function  of the unit step response of the system  [i.e. due to u{k)  =  p(k)]. 1.33.  (Simple  pendulum)  A  system  of first-order  ordinary  differential  equations  that  char acterize the simple pendulum considered in Exercise  1.1b  is given by X2 — J  smjJCi where xi  =  6 and X2 =  6 with xi(0)  =  0(0)  and ^2(0)  =  6(0)  specified.  A linearized model of this system about the solution x  =  [0 0]"^ is given by Xi X2\ 0 11 1  ° Xi X2\ Let ^  =  10 (m/sec^) and /  =  1 (m). (a)  For the case when  x(0)  =  [OQ Of  with  ^o  =  ^/18 77/12 7r/6 and  7r/3 plot  the states for t  >  0 for the nonlinear model. (b)  Repeat  (a) for the linear model. (c)  Compare the results in (a) and (b). CHAPTER  2 Response of Linear  Systems In system theory it is important to clearly understand how inputs and initial condi tions affect the response of a system. There are many reasons for this. For example in control theory it is important to be able to select an input that will cause the sys tem output to satisfy certain properties [e.g. to remain bounded (stability) to follow a given trajectory  (tracking) and the like]. This is in stark contrast to the study of ordinary differential  equations where it is usually assumed that the forcing  function (input) is given. 2.1 INTRODUCTION The goal of this chapter is to study the response of linear systems in greater detail than  was done in Chapter  1. To this  end  solutions  of linear  ordinary  differential equations  are reexamined  this  time with  an emphasis  on characterizing  all solu tions using bases (of the solution vector space) and on determining  such solutions. For convenience certain results from Chapter 1  are repeated and time-varying and time-invariant cases are treated separately as are continuous-time and discrete-time cases. Whereas in Chapter  1 certain fundamental  issues that include  input-output system  descriptions  causality  linearity  and  time-invariance  are emphasized  we will here address in greater detail impulse (and pulse) response and transfer  func tions for continuous-time systems and discrete-time systems. A.  Chapter Description As  in  Chapter  1 we  will  concern  ourselves  with  linear  continuous-time finite-dimensional systems represented by the state and output equations (internal descrip tion) 94 X =  A(t)x  +  B(t)u y  =  C(t)x  +  D(t)u and the corresponding  input-output  description  (external  description) y(t) H{tT)u{T)dT (1.1) (1.2) 95 CHAPTER 2: Response of Linear Systems where H{t r)  denotes the impulse response matrix. As pointed out in Chapter  1 in the  special  case  when  A{t)  =  A B(t)  =  B  C{t)  =  C  and  D(t)  =  D  with  initial time to  =  0 the external description  (1.2) can be represented  equivalently  in terms of Laplace transform  variables y(s)  =  H(s)u(s) (1.3) where H(s)  denotes the transfer  function  matrix of the system. We will examine  in greater detail the relationship between the internal description (1.1) and the external description  (1.2) here and in subsequent  chapters. Again as in Chapter  1 we will also concern ourselves with linear discrete-time finite-dimensional  systems  represented  by  the  state  and  output  equations  (internal description) x(k  +  1)  =  A(k)x(k)  +  B(kMk) y(k)  -  C(k)x(k)  +  D(k)u(k) (1.4) and the corresponding input-output description  (external  description) n y(n)  =  ^H{nk)u{k\ k =  h (1.5) where H{n  k) denotes the unit pulse response. In the time-invariant case with k^  = 0 (1.5) can be represented  equivalently  (in terms of z-transform  variables)  as y(z)  =  H(z)u(zl (1.6) where H(z)  denotes the system transfer  function  matrix. In the following  we provide a brief outline of the contents of this  chapter. In  the  second  section  we  provide  some  mathematical  background  material on  linear  algebra  and  matrix  theory.  In  the  third  section  we  further  study  sys tems  of  linear  homogeneous  and  nonhomogeneous  ordinary  differential  equations. Specifically  in  this  section  we  develop  a  general  characterization  of  the  solu tions  of  such  equations  and  we  study  the  properties  of  the  solutions  by  inves tigating  the  properties  of  fundamental  matrices  and  state  transition  matrices.  In section  four  we  further  investigate  systems  of  linear  autonomous  homogeneous ordinary  differential  equations.  In  particular  in  this  section  we  emphasize  sev eral  methods  of  determining  the  state  transition  matrix  of  such  systems  and  we study  the  asymptotic  behavior  of  the  solutions  of  such  systems.  In  the  fifth  sec tion  we  study  systems  of  linear  periodic  ordinary  differential  equations  (Floquet theory).  In  the  sixth  and  seventh  sections  we  further  investigate  the  properties  of the  state  representations  and  the  input-output  representations  of  continuous-time and  discrete-time  finite-dimensional  systems.  Specifically  in  these  sections  we study  equivalent  representations  of  such  systems  we  investigate  the  properties  of transfer  function  matrices and for  the discrete-time  case we also address  sampled-data  systems  and the asymptotic behavior  of the  system response  of  time-invariant systems. 96 Linear Systems B.  Guidelines  for the  Reader In  a first reading  the background  material  on linear  algebra  given  in  Section  2.2 can be reviewed rather quickly. In the study of the subsequent material of this book selective detailed coverage of topics in Section 2.2 may  also be desirable. A typical beginning graduate course in linear systems will include Theorem  3.1 in Subsection 2.3 A which shows that the set of solutions of the linear homogeneous equation x  =  A(t)x  forms an n-dimensional vector space. This theorem provides the basis for the definitions  of the fundamental  and the state transition matrix  (Subsec tions  2.3A  2.3B  and  2.3D). These  results  enable  us  to  determine  solutions  of  the nonhomogeneous  equation  x  =  A(t)x  +  g(t)  in  Subsection  2.3C.  Background  re quired for the above topics includes material on vector spaces linear  independence of vectors bases for  vector  spaces and linear transformations  (Subsections  2.2A to 2.2E). A typical beginning graduate course in linear systems will also address the ma terial on time-invariant systems given in Section 2.4 including solutions of the equa tion X  =  Ax-\-  g(t)  various methods of determining the matrix exponential e^\  and asymptotic behavior of time-invariant systems x  =  Ax  (including system modes and stability properties of an equilibrium). Background required for these topics includes material  on equivalence  and  similarity  eigenvalues  and  eigenvectors  (Subsections 2.21 and 2.2J). In  addition  to  the  above  a  beginning  graduate  course  on  linear  systems  will also  treat  the  input-output  description  of  continuous-time  systems  and  its  relation to state-space representations including  the impulse response for time-varying  and time-invariant  systems the transfer  function  matrix for time-invariant  systems and the equivalence of state-space representations  (Section 2.6). Finally  such a course will also cover state-space  and input-output  descriptions of discrete-time  systems  (Section 2.7). 2.2 BACKGROUND  MATERIAL In this  section we consider material  from  linear  algebra  and matrix  theory.  We as sume that the reader has some background in these areas and therefore  our presen tation will constitute a summary  rather than  a development  of the subject  matter on hand. This section consists of fifteen subsections. In the first three subsections we con sider Hnear subspaces of vector spaces linear independence of a set of vectors and bases of vector spaces respectively. In the next five subsections we address general linear transformations  defined  on vector spaces the representation  of such  transfor mations by matrices some of the properties  of matrices  and determinants  of matri ces and  solutions of linear  algebraic  equations respectively.  In the ninth  and  tenth subsections we address equivalence and similarity of matrices and eigenvalues  and eigenvectors respectively. In the eleventh  subsection  we digress by considering di rect sums of linear  subspaces. In the last four  subsections  we address  respectively certain canonical forms  of matrices minimal polynomials of matrices nilpotent op erators and the Jordan canonical  form. 97 CHAPTER  2: Response of Linear Systems A.  Linear  Subspaces In Section  1.10 we gave the formal definition of vector space over afield  say (K F) where V denotes the set of vectors and F denotes the set of scalars. When F (the field) is clear from context we usually speak of a vector space V (or a linear space  V) rather than {V F). A nonempty subset W of a vector space V is called a linear subspace  (or a linear manifold)  in  V if (i) w\  +  W2 is in W whenever w\  and W2 are in W and (ii) aw  is in W  whenever a  G F  and w  ^  W.li  is an easy matter to verify  that a linear  subspace W  satisfies  all the axioms of a vector space and may  as such be regarded  as a linear space itself. Two  trivial  examples  of  linear  subspaces  include  the  null  vector  (i.e.  the  set W  =  {0} is a linear  subspace  of  V) and the vector  space  V itself. Another  example of  a linear  subspace  is  the  set  of  all  real-valued  polynomials  defined  on  the  inter val  [a b] that is a linear  subspace  of the vector  space consisting  of  all  real-valued continuous functions  defined  on the interval  [a b] (refer to Example  10.4 in Chapter 1). As another example of a linear subspace (of R^)  we cite the set of all points on a straight line passing through the origin. On the other hand a straight line that does not pass through the origin is not a linear subspace of R^  (why?). It is an easy  matter  to show that if  Wi  and  W2 are linear  subspaces  of a vector space  V then  Vl^i Pi 1^2. the intersection  of  Wi  and  W2 is also a linear  subspace of V. A  similar statement cannot be made however for the union of Wi  and W2 (prove this). Note that to show that a set  V is a vector  space it suffices  to show that it is a linear subspace of some vector  space. B.  Linear  Independence Throughout  the  remainder  of  this  section  we  let  { a i  . . .  a„} at  G  F  denote  an indexed  set  of  scalars  and  we  let  {v\  ...  v"} v'  E  V  denote  an  indexed  set  of vectors. Now let  W be a set in a linear space  V (W may be a finite set or an infinite  set). We say that a vector v E  V is 3. finite linear  combination  of vectors  in  W if there is a finite set of elements {w^...  w^] in W and a finite set of scalars { a i  . . . an)  in F such that V =  a\w^  +  •• • +  a^w^. Now let Ty be a nonempty subset of a linear space V and let S{W)  be the set of all finite linear combinations  of the vectors from  W i.e. w  G S{W)  if and only if there is  some  set  of  scalars  {a\... a^}  and  some  finite  subset  {w^...  w^}  of  W  such that w  =  aiw^  ^ h a^v^^ where m may be any positive integer. Then it is easily shown that S(W)  is a linear subspace of  V called the linear  subspace  generated  by the set  W. Now  if  t/  is  a linear  subspace  of  a vector  space  V  and  if  there  exists  a  set of vectors  W  CV  such that the linear  space S{W)  generated  by  W is  U then  we  say that  W  spans  U. It  is  easily  shown  that  S(W)  is  the  smallest  Hnear  subspace  of  a vector  space  V containing  the subset  W of  V. Specifically  if  t/  is a linear  subspace of  V and if  U contains  W then  U also contains  S(W). 98 Linear  Systems As  an  example  in  the  space  (R^  R)  the  set  Si  =  {e^}  =  {(1 0)^} spans  the  set consisting  of all vectors  of the form  (a  0)^ a  EL R  while  the  set ^2  =  {e^  e^}  e^  = (01)^  spans  all  of 7^2. We  are  now  in  a position  to  introduce  the  notion  of  linear  dependence. DEFINITION  2.1.  Let 5  =  {v\  ...  v'"} be a finite nonempty  set in a linear space V. If there exist scalars ai... a^  not all zero such that aiv^  +  •••  + a ^ v^  =  0 (2.1) then the set S is said to be linearly  dependent  (over F)- If a set is not linearly  dependent then  it  is  said  to be  linearly  independent.  In  this  case relation  (2.1)  implies  that  a\  = • • •  =  am  =  0.  An  infinite  set  of  vectors  T^ in  V is  said  to  be  linearly  independent  if every finite subset of  W is linearly independent. • EXAMPLE  2.1.  Consider  the  linear  space  (R^R) (see  Example  10.1  in  Chapter 1)  and  let  e^  =  (1 0  . . .  0)^ e^  =  (0 1 0  . . .  0 ) ^  . . .  ^"  =  ( 0  . . .  01)^.  Clearly S"^i  <^/^'  =  0 implies that at  =  0 /  =  I..  .n.  Therefore  the set 5" =  {e^...  ^"} is a linearly  independent  set of vectors in R^ over the field of real numbers R. • EXAMPLE  2.2.  Let  V be the set of 2-tuples  whose entries are complex-valued  rational functions  over the field of complex-valued  rational functions.  Let 1 s-h  1 1 ^s  + 2 s + 2 (s  +  1)(^ +  3) 1 s + 3 and  let «!  ==  - 1  0^2  =  (s  + 3)/(s  + 2). Then  aiv^  + a^v^  =  0  and  therefore  the  set S  =  {v\ v^} is linearly  dependent  over the field of rational functions.  On the other hand since aiv^  +0:2 v^  =  0 when ai 0:2  ^  ^  is true if and only if ai  =  0:2  =  0 it  follows that  S  is  linearly  independent  over  the  field  of  real  numbers  (which  is  a  subset  of  the field of rational  functions).  This  shows  that  linear  dependence  of  a set  of  vectors  in  V depends on the field F. m Linear  independence  of  functions  of  time EXAMPLE  2.3.  Let  V  =  C{{a b) R"") let  F  =  R  and  for  xy  EV  mda  E  F  de fine addition  of elements  in  V and multiplication  of elements  in  V by  elements  in F by (x  +  y)(t)  =  x(t)  +  y(t)  for all t  G (a b) and iax){t)  =  ax(t)  for all t  E  (a b). Then as in Example  10.4 of Chapter  1 we can easily show that (V F)  is a vector space. An inter esting question that arises is whether for this space linear dependence  (and linear inde pendence)  of  a set of vectors  can be phrased  in some testable form.  The  answer  is yes. Indeed  it can readily be verified  that for  the present vector  space  (K F)  linear  depen dence  of a set of vectors S  =  {(f)i...  (f)k} in V  =  C{{a b\  7?") over F  =  Ris  equivalent to the requirement that there exist scalars a/  G F /  =  1...  ^ not all zero such that ai4>i(t)  +  • • • +  ak(i)k(t)  =  0 for  all t  G (a b). Otherwise S is linearly  independent. To  see  how  the  above  example  applies  to  specific  cases  let  V  =  C((-^  00) R^) and  consider  the  vectors  (/)i(0  =  [1 tY  <p2(t)  =  [L t^]^.  To  show  that  the  set  S  = {</>b (f>2} is linearly independent (over F  =  R) assume for purposes of contradiction that S  is  linearly  dependent.  Then  there  must  exist  scalars  ai  and  a2  not  both  zero  such that Q!i[l tf  +  a2[l t^f  =  [0 0]^ for  all t  G  ( - ^  00). But in particular for t  =  2 the above equation is satisfied if and only if a 1 =  a2  =  0 which contradicts the assumption. Therefore  5" =  {(^1 (/)2} is linearly  independent. As another specific case of the above example let V  =  C((-<^ ^) R^) and consider 99 the set S  = {</>! ^2 h> </>4} where cj^iit)  =  [1 tf  (j)2{t)  =  [1 fl  (jy^it)  =  [0 1]^ and CHAPTER  2: 04(0  =  [e~^ 0].  The  set S is  clearly  independent  over R  since  aiipiit)  + 0L2(t>2{t)  + Response of 0^303(0 + 0L4r4>A{t)  = 0 for all t G (-00 00) if and only if ai  =  0:2  =  0:3  =  0:4  =  0. Linear Systems • hy^~^v'~^ +y^+iyi+i -^ Next  let 5  =  {v^ . . .  v^} be a linearly  independent  set in a vector  space  V. If Xr= 1 ^^^^  ^  2 r=  1 i^i^^ t^^^ it is readily  shown that at  =  j8/ for all /  =  1...  m. Also  it  is  easily  shown  that  the  set  S  is  linearly  dependent  if  and  only  if  for some index  /  1 <  /  <  m we  can  find  scalars  y i  . . . 7/-1 7/+1...  ym  such  that v'  =  yiv^ H hy^v'^. Furthermore it is not hard to ver ify  that a finite nonempty  set  V^ in a linear space is linearly independent if and only if for each v G 5(W) v 7^ 0 there is a unique finite subset of W say {v^ v^...  v^} and  a unique  set of nonzero  scalars {81...  S^} such that v  =  Si v^  +  • • • +  8mV^. Finally  if  t/  is  a  finite  set  in  a  linear  space  V  then  it  is  easily  shown  that  U is  linearly  independent  if  and  only  if  there  is  no  proper  subset  Z  of  U  such  that S(U)  =  S(Z).  (Recall  that Z is  a proper  subset  of  U if  there  is  a  w  G  6^ such  that u  ^ Z .) C.  Bases We are now in a position to introduce another important  concept. DEFINITION 2.2.  A set W in a linear space V is called a basis for V if (i)  W is linearly independent (ii)  The span of W is the linear space V itself i.e. S(W)  = V. • An immediate  consequence  of the above definition  is that if  W is a linearly  in dependent  set in a vector space  V then  T^ is a basis for  S(W). To introduce the notion of dimension of a vector space it is shown that if a linear space  V is generated by  a finite number of linearly  independent  elements then  this number of elements must be unique. The following  results lead up to this. Let {v^ . . .  v"} be a basis for  a linear  space  V. Then it is easily  shown that  for each vector v  EV there exist unique  scalars a\.  ..an  such that any linearly independent  set of vectors in V then m  < Furthermore ifu^...u^is n. Moreover  any other basis of  V consists of exactly  n elements. These facts  allow the definitions  given in the  following. If  a  linear  space  V  has  a  basis  consisting  of  a  finite  number  of  vectors  say {v^ ...  v'^} then  V is  said  to be  di finite-dimensional vector  space  and  the  dimen sion  of  V is n  abbreviated  dim V  =  n.ln this  case  we  speak  of  an  n-dimensional vector  space.  If  V is not a  finite-dimensional  vector space it is said to be an infinite-dimensional  vector  space. By convention the linear space consisting of the null vector is  finite-dimensional with dimension  equal to zero. An alternative to the above definition of dimension of a (finite-dimensional)  vec tor space is given by the following  result which is easily verified:  let  V be a vector space that contains n linearly independent vectors. If every set of n +  1 vectors in V is linearly dependent then  V is  finite-dimensional  and dim V  =  n. 100 Linear Systems The preceding results enable us now to introduce the concept of coordinates of a vector. We let {v^ . . .  v'^} be a basis of a vector space V and let v E  V be represented by The unique  scalars ^ i  . . .  ^^^  are called the coordinates  ofv  with  respect to the  basis EXAMPLE  2.4.  For  the  linear  space  (/?^/?)  let  S  =  {e\...e''}  where  the  e' G R^ i  =  1... n were defined earlier (following Defintion 2.1). Then S is clearly a basis for (/?" R) since it is linearly independent and since given any v G R^ there exist unique real scalars at i  =  1... n such that v =  X"=i ocie^  = (ai...  a:„)^ i.e. S spans /?". It follows that with every vector v E  /?^ we can associate a unique n-tuple of scalars ai or (ai  ...a„) relative to the basis {e^...  e^} the coordinate representation of the vector v E  /?" with respect  to the basis  5" =  {e\  ...  ^"}.  Henceforth  we will refer  to the basis S of this example as the natural basis for  R^. • EXAMPLE2.5.  We note that the vector space of all (complex-valued) polynomials with real coefficients  of degree less than n is an ^-dimensional  vector  space over the  field of real numbers. A basis  for  this  space is given by S  = {I s...  s'^~^}  where ^ is a complex variable. Associated  with  a given  element  of this  vector space say p(s)  = ao + ais  + ' • • + an-\s^~^  we have the unique fz-tuple given by (ao OL\ ...  ocn-iY which constitutes the coordinate representation of p{s) with respect to the basis S given above. • EXAMPLE 2.6.  We note that the space (V /?) where V  =  C([a b] R) given in Exam ple 10.4 of Chapter 1 is an infinite-dimensional  vector space (why?). • D.  Linear  Transformations In  Subsection  1.IDA  we  introduced  the  notion  of  linear  transformation  ST from  a vector  space  V  (over  the fiield F)  into  a  vector  space  W  (over  the  same fiield F). Henceforth  we  will  write  ST E  L(V W)  to  express  this.  It  is  our  objective  in  this subsection to identify  some of the important properties of linear  transformations. Linear  equations With  Sr E  L(V; ^)  we define  the null space  of 3'  as the set J<(^)  =  {v E  y  : STv =  0} and the range space  of?)  as the set 91(9")  =  {w E  W : w  =  STv V E  y}. Note  that  since  STO =  0 }({^)  and  2/1(9")  are  never  empty.  It  is  easily  verified that M{^)  is a linear  subspace of  V and that 91(9") is a linear subspace of W. If  V is finite-dimensional  (of dimension n) then it is easily shown that dim 91(9") <  n. Also if  V is  finite-dimensional  and  if  {w^ ...  w^} is  a basis  for  91(9") and  v^ is  defined by  ^V  w  i  =  I..  .n  then  it is readily  proved  that  the  vectors  v^  ...  v^  are Hnearly  independent. One of the important  results  of linear  algebra  called  iht  fundamental theorem of linear  equations  states that for  ^  G L{V W)  with  V  finite-dimensional  we have 101 CHAPTER  2: Response of Linear Systems dim>r(2r)  +  dimSlCST)  =  dim V. For the proof  of this result refer  to any  of the references  on linear  algebra  cited  at the end of this  chapter. The above result gives rise to the notions of the rank  p{^)  of a linear  transfor mation  ST of  a  finite-dimensional  vector  space  V into  a vector  space  W  which  we define  as the dimension of the range space 9l(2r) and the nullity  v{?F) of ST which we define  as the dimension of the null space }^{^). With the above machinery in place it is now easy to establish the following  im portant results concerning  linear  equations.  We let ST E  L{V W)  where  V is finite-dimensional  let s  =  dim>r(2r)  and  let {v^ ...  v^} be  a basis  for  >r(2r). Then  it is easily  verified  that  (i)  a vector  v G  V satisfies  the  equation  STv  =  0 if  and  only  if V =  XJ= 1 <^/v^ for some set of scalars {ai...  as} and furthermore  for each v G V such that  STv  =  0 is true the set of scalars {ai...  a^} is unique; (ii) ifw^GW is a  fixed  vector  then  STv  =  w^ holds  for  at least  one  vector  v G  V (called  the  solu tion of the equation  STv =  w^) if and only if w^  G 5/1(9"); and (iii) if w^ is any fixed vector in  W and if  v^ is some vector in  V such that  STv^  =  w^  (i.e. v^ is a  solution of  the  equation  STv^  =  w^) then  a vector  v G  V satisfies  STv  =  w^  if  and  only  if V =  v^ +  X/ = i PiV^  for  some set of scalars {/3i... Ps} and furthermore  for  each V  G  V such that  STv  =  WQ the set of scalars {j8i... jS^} is unique. General properties of linear  transformations Before  proceeding  further  we  briefly  digress  by  recalling  certain  elementary properties  of a function/  from  a set X to a set  7 written  f  : X  ^  Y. Letting  2/l(/) denote the range of/  we can classify/  in the following  manner: if 9l(/)  =  F then /  is  said  to be  surjective  or  a surjection  and  we  say  t h a t/  maps  X  onto  Y; iff is such that for  every  xi  X2 G X f(xi)  =  f(x2)  implies  that  xi  =  X2 then/  is  said to be injective  or an injection  or a one-to-one  mapping;  and if/  is both injective  and surjective  we say that/ is bijective  or a one-to-one  and onto mapping  or a bijection. When/  is injective  its inverse  f'^ =  x  for all  X G X  and  f{f~^(y))  =  y  for  all  y  G 9l(/).  Note that  w h e n/  is bijective  we h a v e / -I  :Y^ : 2/l(/)  -^  X  exists so that  f~^(f(x)) X. Returning now to the subject on hand we note that since a linear  transformation ST of a linear space V into a linear space W is a mapping we distinguish in particular among  linear  transformations  that  are  surjective  injective  and  bijective.  We  will often  be particularly  interested  in knowing  when  a linear  transformation  ST  has  an inverse  2r~^  When  this is the case we  say that  9" is  invertible  that  ST"^ exists  or that  9" is nonsingular.  A  linear transformation  that  is not nonsingular  is  said  to be singular. Concerning  the  inverse  of  a  linear  transformation  ST G  L(V W)  it  is  eas ily  shown  that  ST"^  exists  if  and  only  if  STv  =  0  implies  v  =  0  and  further more  if  2r~^  exists  then  ST"^  is  a  linear  transformation  from  9l(2r)  onto  V  [i.e. S''^  G L(2ft(2r)  V)]. Moreover  if  V is  finite-dimensional  then  S" has  an inverse if and  only  if  91(9") has  the  same  dimension  as  V  i.e.  p(9')  =  dim V.  Also if  both 102 Linear Systems V and  W are  finite-dimensional  and of the same dimension then 9l(2r)  =  W  if  and only if ST has an inverse. In the next few results which are phrased in terms of equivalent statements we summarize  some of the important properties of linear  transformations. (Injective  linear  transformations)  For 9" G L{V W)  the following  statements are equivalent:  (i) ST is injective;  (ii) ST has an inverse; (iii)  STv  =  0 implies V =  0; (iv) for each w  G ?k('3') there is a unique v G  V such that STv  =  w; (v) if  gTv^  =  STv^ then v^  -  v^; (vi) if v^  T^  V^  then  STv^  T^ STV^. If in addition  V is  finite-dimensional  then the following  are equivalent:  (i) ST  is injective;  (ii) p(2r)  =  dim  V. (Surjective  linear  transformations)  For ST G L{V W)  the following  statements are equivalent:  (i) ST is surjective;  (ii) for each w  G  W there is a v  G V such that STv  =  w. If in addition  V and  W are  finite-dimensional  then the following  are equivalent:  (i) ST is surjective;  (ii) dim W  =  pi^f). (Bijective linear transformations)¥or3'  G L(V W) the following are equivalent: (i)  9" is bijective;  (ii) for every w  G  W there is a unique v  E  V  such that 9'v  =  w. If in addition y and Ware  finite-dimensional  then the following  are equivalent: (i) ST is bijective; (ii) dim V  =  dim W  = p(?J'). (Injective  surjective  and  bijective  linear  transformations)  For S'  E  L(V  W) with  V and  W  finite-dimensional  and with dim V  =  dim W  the  following are equivalent:  (i) ST is injective;  (ii) ST is surjective;  (iii) ST is bijective; (iv) ST has an inverse. Next we examine  some of the properties  of L(V W)  the set of all linear trans formations  from  a vector  space  V into a vector space  W. As before  we assume that V  and  W are linear spaces over the same field F. We let ^  3" G L(V W)  and we define the sum  of^  and  ST by (^  +  2r)v  =  SPv +  STv (2.2) for all V  G  y. Also with a  G F  and ST G L(V; W) we define multiplication  of ST by a scalar  a  as {a^)v  =  a^v (2.3) for all V  G  y. It is easily shown that (&'-\-^)G  L(V W) and also that aST G L(V  W). We further  note that there exists a zero element in L(V W)  called the zero  transfor mation  denoted by 0  and defined  by Cv  =  0 (2.4) for  all V  G  y.  Furthermore  we note that to each  ST G L(V W)  there corresponds  a unique linear transformation  - 9"  G L(V W)  defined  by (-oj-)v  ^  -STv (2.5) for  all V  G  y.  In this case it follows  trivially that  -ST  +  ST =  0. With these definitions  in place it is easily proved that L(V W)  is a linear  space over F  called  the space  of linear  transformations  [with vector addition  defined  by (2.2) and multiplication  of vectors by scalars defined  by  (2.3)]. To explore the properties of the space of linear transformations further we briefly digress to recall the definition of an algebra. Specifically  a set V is called an  algebra 103 CHAPTER  2: Response of Linear Systems if it is a linear space and if in addition to each vw  GV  there corresponds an element in  V denoted by v •  w and called  the product  ofv  times  w satisfying  the  following axioms: 1.  V •  (w +  w)  =  V •  w +  V •  w for  all V w w  G  y. 2.  (v + w)  • u  =  V ' u -\-  w ' u for dill V w u  E: V. 3.  (av)'  (f3w)  =  (cj^/3)(v •  w) for all v w E  V and for all a  p  G  F. If in addition to the above 4.  (v'w)'  u  =  V'(w-  u) for all vwuG  V then  V is called an associative  algebra. If there exists an element /  G  V such that i-v  =  v  i  =  v for every v G  V then / is  called  the  identity  of  the  algebra.  It  can  readily  be  shown  that  if  / exists  then it  is  unique.  Furthermore  if  v • w  =  w •  v for  all  v w  G  V then  V is  said  to  be  a commutative  algebra. Returning  to the  subject  on hand  let V  W  and  U be linear  spaces  over F  and consider the vector spaces L(V W)  and L(W U). If S/^ G L{W U) and ST G LiV  W) then we define  the product  5f ST  as the mapping of  V into  U by the relation (^2r)v  =  ^(STv) (2.6) for all V  G  y.  It is easily verified  that ^ST  G L{V U). Next  let  y  =  \y  =  f/.  If  ^^^E LiV  V)  and  if a  jS  G F  then  it is  easily shown that and ^(sra)  =  (SP2r)a (^  + 2r)a  =  jf a  +  gra (aSf)(/32r)  =  (af3)&'^. (2.7) (2.8) (2.9) (2.10) We  emphasize  that  in  general  commutativity  of  linear  transformations  does  not hold i.e. in general spsr 7^  srsp. (2.11) There  is  a  special  mapping  from  a  linear  space  V  into  V  called  the  identity transformation  defined  by 3v  =  V (2.12) for all V  G  y.  We note that J^ G L(V V)  that ^  T^ 0 if and only if  V T^ {0} that 3  is unique and that aj-^  =  ^Gj-  ^  aj- (2.13) for  all ST G L(V V).  Also we can readily verify  that the transformation  a J^ a  G  F defined  by (a^)v  =  aS>v =  av (2.14) is also a linear  transformation. Relations  (2.7)  to  (2.14)  now  give  rise  to  the  following  result:  L(VV) is an  associative  algebra  with  identity  ^.  This  algebra  is  in  general  not  commu tative. 104 Linear Systems Concerning  invertible linear transformations  we note that if  J  G L(V; V) is bi-jective then  ST"^  G L(V V) and  furthermore of-^oj-  =  g-gj-1  ^  ^^ (2.15) where ^  denotes the identity transformation  defined  in (2.12). Next  if  V is  a  finite-dimensional  vector  space  and  ST G  L(V V)  then  we  can readily show that the following are equivalent: (i) ST is invertible; (ii) pCJ)  =  dim V; (iii)  ST is one-to-one; (iv)  ST is onto; and (v) STv  =  0 implies that v  =  0. For  bijective  linear  transformations  we  can  easily  verify  the  following  char acterizations.  Let  £/" ST a  G  L(V V)  and  let  ^  denote  the  identity  transformation. Then  (i)  if  SPST =  aSP  -  J^ then  £P is  bijective  and  Sf ~i  =  ST =  1;  (ii)  if  ^  and Sr  are  bijective  then  if?r  is  bijective  and  (S^^y^  =  ^T'^if-^;  (iii)  if  ^  is  bi jective  then  (S^'^y^  =  ^;  and  (iv)  if  ^  is  bijective  then  a^  is  bijective  and (aSP)-i  =  ( l / a ) y -i  for  all a  G F a  T^ 0. With  the  aid  of  the  above  concepts  and  results  we  can  now  construct  certain classes  of functions  of  linear  transformations.  Since  (2.7)  allows  us  to  write  the product  of three  or more linear  transformations  without  the use  of parentheses  we can define  3'^  where  9" G L(V V)  and n is a positive integer  as aj-n  A  Of  0}-  .  .. .oj-^^ (2.16) n times Similarly if ST  Ms the inverse of ST then we can define  9"  ^ where m is a positive integer  as aj-m A (2r-i)^  =.  ^-^  ^or-\' '"  -gr-y (2.17) Using these definitions  the usual laws of exponents  can be verified.  Thus m  times aj-m  aj-n  ^  oj-m+n  ^  aj-n   aj-m /(jrm\n  __  oj-mn  __  arnm  _  rarnyn and where m and n are positive integers. Consistent with the above we also have ST"' • 9"-"  =  gT'"-" (2.I8) /o  1 Q\ (2.20) and (2.21) (2.22) We are now in a position to consider polynomials  of linear  transformations.  For 3"!  =  Sr Sro  =  3>. example if /(A)  is a polynomial i.e. /(A)  =  ao  +  aiA  +  ---+a„A" (2.23) where oio...  Q!^  G i^ then by  f(^)  we mean /(ST)  =  a oi  +  aiSr  +  • • • +  A^gr^ (2.24) The  reader  is  cautioned  that  in  general  the  above  concept  cannot  be  extended  to functions  of  two  or more  linear  transformations  because  linear  transformations  in general do not commute. E.  Representation  of Linear  Transformations  by  Matrices In the following  we let  (V F)  and  (W F)  be vector  spaces  over the same  field  and we let ^  : y  -^  W denote a linear mapping. We let {v^ ...  v"} be a basis for  V and we set v^  =  ^ v^  . . .  v"  =  siv^.  Then  it is an easy  matter  to  show that  if  v is  any vector in  V and if  ( a i  . . . a^)  are the coordinates  of v with respect  to {v^ . . .  v"} then  ^v  =  aiv^  +  • • •  +  a„v".  Indeed  we  have  siv  =  si(aiv^  +  • • • +  a^v")  = ai^v^  4- • • • +  a„64v"  =  aiv^  +  • • • +  a„v". Next  we  let  {v^ ...  v'^} be  any  set  of  vectors  in  W.  Then  it  can  be  shown that there  exists  a unique  Hnear transformation  si  from  V into  W  such that  ^v^  = v^  . . .  ^^v'^  =  v". To show this we first observe that for each v E  V we have unique scalars ai.. .an  such that 105 CHAPTER  2: Response of Linear Systems Now define  a mapping  M : V ^  W as V =  aiv^  +  •••  +  a„v". 1..  .n.  We first must show that d- is Hnear and then that aiv^  +  • • • +  a„v"  and  w  =  /3iv^  +  • • • +  jS^v'^ we  have Clearly ^ ( v^  =  v^ / ^  is unique.  Given v ^(v + w)  =  6 4 [ ( a i + ) 8 iy  +  --- + (a^ + i 8 > "]  =  (aj+/3i)v^  +  •••+ (a„ + iS^)v^ On the other hand  d(v)  =  aiv^  +  • • • +  a^v'^ d(w)  =  /3iv^  +  • • • +  j8„v". Thus ^(v)  +  ^(w)  =  (aiyi  +  • • • + a^v^)  +  (jSiv^  +  • • • +  ^^v^)  =  (^i  +  f3i)v^  +  • • • + (a„  +  i8„)v"  =  62i(v +  w). In a similar manner it is easily established that ad(v)  = d(av)  for  all  Q: G F  and  v G  V. Therefore  ^  is linear.  Finally  to  show  that  ^  is unique  suppose  there  exists  a linear  transformation  ^  : V ^  W  such  that  ^v^  = v^ /  =  1...  /2. It  follows  that  (d  -  ^)v^  =  0 /  =  1...  n  and  therefore  that These  results  show  that  a  linear  transformation  is  completely  determined  by knowing  how it transforms  the basis vectors  in its domain  and that this linear  trans formation  is uniquely  determined  in this  way. These  results  enable  us to  represent linear transformations  defined  on  finite-dimensional  spaces in an unambiguous  way by means of matrices. We will use this fact  in the  following. Let  (V F)  and  {W F)  denote  n-dimensional  and  m-dimensional  vector  spaces respectively  and  let  {v^ ...  v^} and  {w\  ...  w^} be  bases  for  V  and  W  respec tively.  Let Since {w^ 1...  n such that V  ^  W be  a  linear  transformation  and  let  v'  =  siv\  /  =  1 w^} is a basis for  W there are unique scalars {aij} i  =  I..  .m  j n. siv  =  V  =  aiiw  +  a2iw  +  • • • +  a^iw^ siv^  =  Sp'  =  a\2W^ 4- a22>v^ +  • • • +  a^2>^^ (2.25) ^v^  =  y"  =  aiyiW^  +  a2nW^ +  • • • + amn"^^' Next let V  G  V. Then v has the unique representation v  =  aiv^  + 0:2v^ H h a^v" with respect to the basis {v^ ...  v"}. In view of the result given at the beginning of this subsection we now have ^v  =  aiv^  +  • • • +  anv"". (2.26) Since ^^v G W d-v has a unique representation with respect to the basis {w^ . . . w^} say ^v  =  jiw^  +  72W^ +  • • • +  7mw'^. (2.27) 106 Linear Systems Combining  (2.25) and (2.26) we have ^^  -  rv (n ^A^  -u 4. ^  ^^^^\ +  ^2(^121^^  +  • • • + Clm2^^) +  aniainW^ +  • • • + amn^"^)' Rearranging this expression we have ^v  =  (cin^i  +  ^i2<^2 +  • • • +  ai^a„)w^ +  (^21 a; 1 +  a22<^2 +  • * *  +  a2nOin)'^^ In view of the uniqueness  of the representation  in (2.27) we have +  (amiai +  am20^2  +  • • • +  amnO^n)'^'^' 71  =  a\\a\  +  ^120:2 +  • • • +  ainOLn 72  ==  Cl2\0i-\ +  <322«2  +  • • •  +  a2^Q^Az (2.28) Tm  =  ^ m l «l  +  Clm20^2  +  * *  •  +  ^m^Q^m where ( a i  . . . anf  and  ( 7 1  . . .  7m)^ are coordinate representations  of v E  V and siv  ^  W  with  respect  to  the  bases  {v^ ...  v"} of  V and  {w^...  w^}  of  W  re spectively.  This  set  of  equations  enables  us  to  represent  the  linear  transformation si  from  the linear  space  V into the  linear  space  W by  the unique  scalars  {atj} i  = I..  .m  j  =  !.../!.  For convenience we let an CI21 ai2 ^ 22 ... •  • • ain ^2n [atj] (2.29) ^m\ ^m2 ' •  • ^mn We see that once the bases {v^ . . .  v^} {w^ . . .  w^}  are fixed we can represent  the linear transformation  d- by the array of scalars in (2.29) that are uniquely  determined by (2.25). Note  that thejth  column  of A is the coordinate  representation  of the  vector Av^  G  W  with  respect  to the basis  [w^...  w^}. In  view  of  the  results  given  at  the  beginning  of  this  subsection  the  converse to the preceding  statement  also holds. Specifically  with the bases for  V and  W  still fixed the array given in (2.29) is uniquely  associated with the linear  transformation ^ of  Vinto  W. The above discussion gives rise to the following  important  definition. DEFINITION 2.3.  The array given in (2.29) is called the matrix A of the linear trans formation si from a linear space V into a linear space W (over F) with respect to the basis {v^ ... v"} of V and the basis {w\ ... w^} of W. • If in Definition  2.3 V  =  W and if for both  V and  W the same basis {v^ ... v'^} is used  then  we  simply  speak  of the matrix  A  of  the  linear  transformation  si  with respect  to the basis  {v^ ... v"}. In  (2.29)  the  scalars  (an  ai2...  atn)  form  the  /th  row  of  A  and  the  scalars (aij  a2j...  amj)^  form thejth  column  of A.  The scalar atj refers to that element of 107 CHAPTER  2: Response of Linear Systems matrix A  that can be found  in the ith row and jth  column of A. The array in (2.29) is said to be an m X n matrix.  If  m  =  n we speak of a square  matrix.  Consistent  with the above an ^ X 1  matrix is called a column  vector column  matrix  or n-vector  and a  1 X n matrix  is  called  a row vector.  Finally  if  A  === [a/y]  and  B  =  [btj] are  two mX  n matrices then A  =  B  i.e. A and B are equal  if and only if aij  =  btj  for  all /  =  1...  m  and  for  all  j  =  1...  n.  Furthermore  we  call  A^  =  [ciijV  =  l^ji] the transpose  of A. The  preceding  discussion  shows  in  particular  that  if  ^  is  a  linear  transfor mation  of  an  n-dimensional  vector  space  V  into  an  m-dimensional  vector  space W W  ==  SiVy (2.30) if 7  =  ( y i  . . . ymV  denotes the coordinate representation  of w with respect to the basis {w^...  w^}  if  a  =  ( ^ i  . . .  a„)^  denotes the coordinate representation  of v with respect  to the basis {v\  ...  v"} and if A denotes the matrix  of ^  with  respect to the bases {v^ . . .  v"} {w^ . . .  w"^} then or equivalently. 7  =  Aa =  1...   m (2.31) (2.32) which are alternative ways to write (2.28). Some important  remarks 1.  Throughout  this  section  we  use  in  the  interests  of  clarity  of  presentation  low ercase  Greek  letters  to  denote  the  coordinate  representations  of  vectors  [see (2.31)]. In the interests of simplicity however we will use common (Latin) low ercase  letters  to  denote  vectors  throughout  the remainder  of  this  book  whether they  are  coordinate  representations  of  vectors  or  underlying  objects  (elements ofV). 2.  We note that  if  in particular  V  =  R^  then  v  E.V  and  its  coordinate  represen tation  7]  with  respect  to  the  natural  basis  {^^  ...  ^^} of  V will  have  the  same form. *F.  Some Properties  of  Matrices The rank of a matrix We  first  consider  the  characterization  of  the  rank  of  a  hnear  transformation in  terms  of  its  matrix  representation.  To  this  end  let  ^  be  a  linear  transforma tion  from  a  vector  space  V  into  a  vector  space  W.  It  is  easily  shown  that  d-  has rank  r if  and  only  if  it  is  possible  to  choose  a  basis  {v^ ...  v"} for  V  and  a  basis {w^ . . .  w^}  for  W  such  that  the  matrix  A  of  ^  with  respect  to  these  bases  is  of the  form 108 Linear Systems rioo  •• 010 •• 000 • 000  • ••  oT ••  0 A  = 000 000 •• •• 100  • 000 • •• •• ° 0 > m  =  dim  W. 000 •• 000 • ••  0  J Y n = dim  V More directly if A is the matrix representation  of ^  E  L(V W)  with respect to some arbitrary bases {v^ . . .  v"} and {w^...  w'"} then (i) the rank of ^  is the num ber of vectors  in the largest possible  linearly  independent  set of columns  of A; and (ii) the rank of ^  is the number of vectors in the smallest possible set of columns of A that has the property  that  all columns  not in it can be expressed  as linear  combi nations of the columns in it. The  above result  enables  us now  to make  the following  definition:  the  rank  of an mX  n matrix A  is the largest number of linearly independent  columns of A. General properties of  matrices Since matrices are representations of linear transformations on  finite-dimensional vector  spaces  (in the sense defined  in Subsection  E of this  section) it is  reasonable to suspect that matrices inherit the properties  of the transformations  they  represent. In the following  we address this issue. Let V and W be ^-dimensional  and m-dimensional vector spaces over F respec tively  and  let  ^  and  ^  be  linear  transformations  of  V  into  W.  Let  A  =  [atj]  and B  =  [bij] be the matrix representation  of si  and 2^ respectively with respect to the bases  {v^ ...  v"} in  V and  {w^ . . .  w^}  in  W.  Using  (2.2)  and  Definition  2.3 the reader  can  readily  verify  that  the matrix  of the linear  transformation  ^  +  2S (with respect to the above bases) is given by A  + B  =  [atj] +  [bij]  =  [atj  +  btj]  =  [dj]  =  C. (2.33) Also  using  (2.3)  and  Definition  2.3 the reader  can  easily  show  that  the matrix  of a A  denoted by D = a A is given as a A  -=  a[aij]  =  [aatj]  =  [dfj]  =  D. (2.34) From (2.33) we note that for two matrices A and B to be added they must have the same number  of rows and  columns. When  this is the case we say that A and B  are comparable  matrices. Clearly if A is an m X n matrix then  so is  aA. Next  let  U be  an  r-dimensional  vector  space  let  si  E  L(K  W)  and  let  2^ G L{W U).  Let A be  the  matrix  of  d^ with  respect  to  the  basis  {v^ . . .  v"} in  V and with respect  to the basis  {w^...  w^}  in  W. Let  SS be the matrix  of B  with  respect to the basis  {w^ ...  w^}  in  W  and  with  respect  to the basis  {u^..  .u^}m  U. The product mapping  SS^ as defined  by (2.6) is a linear transformation  of  V into  U. By applying  definitions  it is readily  verified  that the matrix  of ^^  with respect  to the bases {v^ ..  .v^}ofVand {u^..  .u^}of  Uis  given by where C  =  [Cij] =  BA Cij  =  ^ bikakj k=i 109 CHAPTER  2: Response of Linear Systems (2.35) (2.36) for /  =  1...  r and j  =  I..  .n.  Clearly two matrices A and B can be multiplied to form the product BA  if and only if the number of columns of B is equal to the number of  rows  of A.  When  this  is  true  we  say  that  the  matrices  B  and A  are  conformal matrices. As  mentioned  earlier  the  properties  of  general  transformations  established  in Subsection  D hold of course in the case of their matrix representations  as well. We summarize  some of these in the  following: 1.  Let A  and BhemX n matrices and let C be an n X r matrix; then 2.  Let A be an m X n matrix and let B and  ChtnXr  matrices; then (A +  B)C  =  AC  + BC. A{B  + C)  =  AB  + AC. (2.37) (2.38) 3.  Let A be an m X n matrix let 5  be an ^  X r matrix and let C be an r  X 5" matrix; then A{BC)  =  (AB)C 4.  Let a  (3 E  F  and let A be an m X n matrix; then 5.  Let a  G F  and let A and BhQ  mX  n matrices; then (a  +  /3)A  =  aA  +  j8A. a(A  +  B)  =  aA  +  aB. (2.39) (2.40) (2.41) 6.  Let a  (3 E:  F  let A be an m X n matrix and let 5  be an n X r matrix; then (aAXfiB)  =  (aPXAB). 7.  Let A and Bbe  mX  n matrices; then 8.  Let A B  and  ChtmXn  matrices; then A  + B  =  B  + A. {A + B)  + C  =  A  + {B-^C). (2.42) (2.43) (2.44) Next let 0  G L{V W) be the zero transformation  defined  by (2.4). Then for any bases {v^ . . .  v"} and {w^ . . .  w^} for V and W respectively the zero  transformation is represented by the m X n matrix O  = Too 00 •••  0 •••  0 00 0 (2.45) called  the  null  matrix.  Further  let  3  E:  L(V V)  be  the  identity  transformation  de fined  by  (2.12)  and  let  {v\  ...  v'^} be  an  arbitrary  basis  for  V.  Then  the  matrix 110 Linear Systems representation  of the linear transformation  3"  from  V into  V with respect to the basis {v\  • • • v«} is given by ' 1 00 /  =  0 10 0 00 ••• ••• ••• 0 0 1 called the n X n identity  matrix. For any mX  n matrix A  we have that A^O  =  0  + A  =  A and for  any nX  n matrix B we have where /  is the n  X n identity  matrix. BI  =  IB  =  B (2.46) (2.47) (2.48) If  A  =  [uij] is  a matrix  representation  of  a linear  transformation  ^4 then  it  is easily  verified  that the matrix  -A  ==  ( - l )A  =  [-a/y]  is the corresponding  matrix representation  of the linear transformation  -s^.  In this case it follows  immediately that A  -\-  (-A)  =  O where  O denotes the null matrix. By convention we write that A + (-A)  =  A-A. Next let A  and Bhe  nX  n matrices. Then we have in general that AB  7^ BA (2.49) as was the case in (2.11). Further  let ^  G L(V V)  and  assume  that  si  is nonsingular  with inverse  ^ ~^ so that MM~^  =  si~^si  =  ^.  Now if A is the n X n matrix of si  with respect to the basis  {v^ ...   v"} in  V then  there  is  an  n  X ^  matrix  B  of  M~^  with respect  to  the basis {v\  ...  v'^} in  y  such that BA  =  AB  =  I. (2.50) We call B the inverse of A and we denote it by A~ ^  Under the present  circumstances we say that A~^  exists  or A has an inverse  or A is invertible  or A is nonsingular.  If A"^  does not exist we say that A is  singular From corresponding properties given in Subsection D for  arbitrary linear trans formations  several  properties  of  matrices  are  evident.  In  particular  for  an  n  X n matrix  the following  are equivalent:  (i)  rank  A  — n\  (ii) Aa  =  0 implies  a  =  0; (iii)  for  every  yo  ^  F^  there  is  a  unique  ao  E  F^  such  that  yo  =  Aao;  (iv)  the columns of A are linearly independent;  and (v) A~^  exists. In  Subsection  E  it  was  shown  that  we  can  represent  n  linear  equations  by  the matrix equation  (2.32) y  =  Aa. (2.51) Now  assume that A is nonsingular. If we premultiply  both  sides of this equation by A~^  we obtain a  =  A-^y (2.52) the  solution  to Eq.  (2.51). Thus knowledge  of the inverse  of A enables  us to  solve the system of linear equations  (2.51). Concerning  inverses  of  matrices  the following  facts  are easily  verified:  (i)  an nXn  nonsingular matrix has one and only one inverse; (ii) if A and B are nonsingular nX  n matrices then (AB)~^  =  B~^A~^'  and (iii) if A and B diVQnX  n matrices  and if AB  is nonsingular  then so are A  and  B. Next  we  consider  the principal  properties  of the transpose  of matrices  which follow  readily from  definitions:  (i) for  any matrix A  (A^)^  =  A; (ii) if A and B  are conformal  matrices  then  (ABY  =  B^A^;  (iii)  if A  is  a  nonsingular  matrix  then (A^)"^  =  (A~^)^'  (iv) if A is an n X n matrix then A^ is nonsingular if and only if A is nonsingular; (v) if A and B are comparable matrices then (A + B)^  =  A^  +  B^; and (vi) if a  G F  and A is a matrix then (aA)^  =  aA^. Next  we  let A  he  an n  X  n  matrix  and  we  let  m be  a positive  integer.  As  in (2.16) we define  the  nX  n matrix A^  by A^  =  /{'A 4 "Y" m  times (2.53) and if A  ^ exists then as in (2.17) we define  the  nX  n matrix A  '^ as 111 CHAPTER  2: Response of Linear Systems A-^  = (A-'y l\m  A ••  -A" (2.54) V m  times As in the case of Eqs.  (2.18) to (2.20) the usual laws of exponents follow  from  the above  definitions.  Specifically  if A is  an  n  X n  matrix  and  if  r  and  s  are  positive integers then A'--A'  =  A'^'  =  A'^'  =  A' • (Ay  =  A"  =  A''  =  (A'Y A' and if A"'  exists then Consistent with this notation we have A'  • A-'  =  A'-'. (2.55) (2.56) (2.57) A'  =  A lO (2.58) and (2.59) We are now once more in a position to consider functions  of linear  transformations where in the present case the linear transformations  are represented by matrices. For example if /(A) is the polynomial in A given in (2.23) and if A is any nXn  matrix then by /(A)  we mean ^ /(A)  =  ao/  +  aiA  +  • • • +  a^A^. (2.60) Finally we noted earlier that in general linear transformations  (and in particu lar matrices) do not commute [see (2.11) and (2.49)]. However in the case of square matrices the following  facts  are easily  verified:  let A B C  denote  nX  n matrices; let O denote the  nX  n null matrix; and let /  denote the n X n identity  matrix. Then (i)  O commutes  with  any A; (ii) A^ commutes  with A^ where/?  and q are positive integers; (iii) a I  commutes with any A where a  E: F' and (iv) if A commutes  with B and if A commutes with  C then A commutes with aB  -\-  pC  where a  (3 E  F. *G.  Determinants  of  Matrices Definition  of  determinant In  this  section  we  recall  and  summarize  some  of  the  important  properties  of .n}  and  recall  that  a determinants  of  a  matrix.  To  this  end  we  let  N  =  {12.. 112 Linear  Systems permutation  on A/^ is a one-to-one mapping  ofN  onto itself. For example if  O" denotes a  permutation  on N  then  we  can  represent  it  symbolically  as 1  2---  n (2.61) where  ji  G A/^ for  / =  1 (7 more  compactly  as  n  and  jr  ^  jk  whenever  r  ^  k.  Henceforth  we  represent (y  = JlJ2---jn-Clearly  there  are  nl  possible  permutations  on  N.  We  let  P{N)  denote  the  set  of  all permutations  on N  and  we  distinguish  between  odd  and  even  permutations.  Specif ically  if  there  is  an  even  number  of  pairs  (/ k)  such  that  / >  k  but  / precedes  k  in O" then  we  say  that  a  is  even.  Otherwise  a  is  said  to  be  odd.  Finally  we  define  the function  sgn  from  P{N)  into  F  by sgn  (cj) +1 if  (7 is  even if  a  is  odd forallcJGP(A^). As  a  specific  example  foxN= {123}  there  are  six  different  permutations even  and  odd  on N  given  in  the  following  table: G 123 132 213 231 312 321 Uuh) (12) (13) (21) (23) (31) (32) Uuh) (13) (12) (23) (21) (32) (31) G  is U2J3) odd  or  even sgn(G) (23) (32) (13) (31) (12) (21) Even Odd Odd Even Even Odd -1 Now  let A  denote  the  nxn  matrix  given  by 'an ail an ail \_afii a^ii a\n ain ann. We  form  the  product  of  n  elements  from  A  by  taking  one  and  only  one  element  from each  row  and  one  and  only  one element  from  each  column.  We represent this  product as ^Ijl  '^2J2 anj^^ where  {jiji • • • jn)  ^  ^ ( ^ )-  H is  possible  to  find  n\  such  products  one  for  each  a  G P{N).  We  are  now  in  a  position  to  define  the  determinant  of  A  denoted  by  det{A) by  the  sum det{A)  = ^ sgn{G)  • aij^  • ay^ oePiN) ^njn^ (2.62) where a  =  jvin-  We frequently  denote the determinant  of A by det{A)  = Cl\n Clin  =  \A[ 113 CHAPTER  2: Response of Linear Systems (2.63) Properties of  determinants We now enumerate some of the common properties of determinants. The proofs of these follow  mostly from  definitions. Let A and BhonXn  matrices. Then (i) det (A^)  =  det (A); (ii) if all elements of a column  (or row) of A are zero then det (A)  =  0; (iii) if the matrix B is the matrix obtained  by  multiplying  every  element  in  a column  (or row)  of A by  a constant  a while  all  other  columns  of B  are the  same  as those  of A then  det(B)  =  a  det{A)\ (iv) if JB is the same as A except that two columns  (or rows) are interchanged  then det(B)  ==  -J^f  (A); (v) if two columns (or rows) of A are identical then J^^( A)  =  0; and (vi) if the columns  (or rows) of A are linearly  dependent then det (A)  =  0. We now  introduce  some  additional  concepts  for  determinants.  To this  end  let A  =  [aij] be  an  /I X n  matrix.  If  the  /th row  andjth  column  of A are  deleted  the remaining (n -  1) rows and (n -  1) columns can be used to form another matrix  Mij whose  determinant  is det (Mij).  We call det {Mij)  the minor  of  aij.  If  the  diagonal elementsofM/y  are diagonal elements of A i.e. if /  = j  then we speak of aprmc/pa/ minor  of A.  The cofactor  of aij  is defined  as (-  ly^^det As a specific  example if A is a 3 X 3 matrix then (Mij). the minor of element ^23 is and the cofactor  of ^23 is det (A)  = an ^21 ^31 ai2 ^22 au Cl23 ^ 32 <^33 det(M23)  =  an «31 ai2 CI31 CTh (-1)  a\\ <23i a\2 (332 Next  for  an  arbitrary  nXn  matrix A let  c/y  denote the  cofactor  of  aij  i j  = I..  .n.lt  can be  shown  from  definitions  that the  determinant  of A is equal  to  the sum of the products of the elements of any column (or row) of A each by its cofactor. Specifically det (A)  =  ^ ^ijCij j  =  \.. .n i = \ or det (A)  =  ^^aijCij i  =  1.. For example if A is a 2 X 2 matrix we have det  (A) an <^2i a\2 ^ 22 -  <2ll^22  " ^ 1 2 ^ 2b (2.64) (2.65) 114 Linear  Systems and if A is a 3 X 3 matrix we have det{A) = k ll \a2i 1^31 ^12 ^13 ^22  <^23 ^32  ^33! =  an Cl22 ^ 32 <^23 (233 - ^ 12 (221 ^31 =  ancn  +  ^21^21 -^ cisic^i. ^ 23 <^33  +  a\2> Cl2\ (331 '^22 ^ 32 We now consider a few additional useful properties of determinants. In particular from basic definitions it can be shown that if the ith row of an n X ^ matrix A  consists of elements of the form  an  +  a'-p ai2 +  a[2..  .ain  + a\^ i.e. if an <221 (212 <322 ^ 1« ^2n A  = (an  + a'.^) ((2-2 +  ^;2) (atn  +  (2;^) then det(A)  = an a\2 <321 <322 <3/l (2/2 <^/22 ^2n an Cl2\ a\^ a\2 C122 ^a (^n\ Cln2 (^2n <2«1 ^ n2 Next  if A  and B  axe n  X  n matrices  and  if B  is  obtained  from  A  by  adding  a constant  a  times  any  column  (or row)  to any other column  (or row)  of A then  it is easily  shown that (i^r (A)  =  det(B). Further if ctj  denotes the cofactor  of atj  i j  =  1...  n for annX  n matrix A then it is easily  shown that and ^atjCik i = \ ^^CLijCkj 7 = 1 =- 0 foxJT^k =  0 for  /  7^  k. (2.66) (2.67) We can combine (2.64) with (2.66) and (2.65) with (2.67) to obtain the relations n ^atjCij^  =  det(A)Sjk / =  i ^atjCkj  =  det(A)81^ 7 = 1 j k  =  \...n (2.68) i k  =  \.. .n (2.69) and respectively where 8mn denotes the Kronecker delta (i.e. 8mn =  1  when m  =  n and ^mn =  0 Otherwise). An  extremely  useful  result  concerning  determinants  (which  can  be  proved  by using  the  definition  of  determinant  and  some  of the  properties  enumerated  above) states that the determinant  of the product  of two matrices  is equal to the product of the determinants  of the matrices. Thus if A and B are nX  n matrices then det(AB)  = det(A)det(B). (2.70) It is  easily  verified  that  for  the  n  X n identity  matrix  /  and  for  the  n  X  n  zero matrix  O we have det (I)  =  1 and det (O)  =  0. Finally we can readily  show that  annX  n matrix A  is nonsingular  if  and  only if det (A)  ¥=0. We conclude this discussion by considering a means of determining the inverse A~^  of a nonsingular  nX  n matrix. To this end let c/y be the cofactor  of atj  i j  = 1...  n for the matrix A and let C be the matrix formed  by the cofactors  of A i.e. C  =  [cij]. The  matrix  C^  is  called  the  (classical)  adjoint  of A and  is denoted  by adj (A). It is easily verified  that A •  [adj (A)]  =  [adj (A)]  • A  -  [det (A)]  • / (2.71) 115 CHAPTER  2: Response of Linear Systems from  which it follows  that 1 adj  (A). det  (A) (2.72) As a specific  case consider A  = 0 1 1 1  1 2  2 0 -1 Then  det (A)  -  - 1 adj (A)  = and A" 2 2 3 2 2 3 -1 -1 1 1 1 -1 0' 1 -1 0" -1 1 H.  Solving Linear Algebraic  Equations Now consider the linear system of equations given by Aa  =  y (2.73) where A  E  R^^^  and y  E. R^  are given  and a  G /?" is to be determined.  By  using the results  of the preceding  subsections  especially  Subsection  2.2D the  following important results can readily be  established. 1.  For a given y a solution a  of (2.73) exists (not necessarily unique) if and only if 7  G 2^(A) or equivalently if and only if 2.  A solution a  of (2.73) exists for any y  if and only if p{[Ay^)  =  p{Ar p(A)  =  m. (in  A) (2.75) If (2.75) is satisfied  a solution of (2.73) can be found  by using the relation a  =  A'^(AA^y^y. (2.76) 116 Linear Systems When  in  (2.73)  p(A)  =  m  =  n  then  A  G unique  solution of (2.76) is given by j^nxn  ^^^  jg  nonsingular  and  the a  =  A~^y 3.  Every  solution a  of (2.73) can be expressed  as a sum a  =  ap-\-  ah (2.77) (2.78) where  ap  is  a  specific  solution  of  (2.73)  and  ah  satisfies  Aah  =  0. This  result allows us to span the space of all solutions of (2.73). Note that there are dim>f(A)  =  n-  p{A) (2.79) linearly independent  solutions of the system of equations AjS  =  0. EXAMPLE 2.7.  Consider Aa  = 0 0 .0 0 0 0 0" 1 0. r-(2.80) It is easily verified  that {(0 1 0)^} is a basis for S/l(A). Since a solution of (2.80) exists if and only if y  G ^(A) y  must be of the form y  = (0 k0) k G R. Note that p(A)  =  I  = p([A y])  = rank 0 0 0 0 0 0 0 1 0 0 k 0 as expected. To determine all solutions of (2.80) we need to determine an a^  and an ah  [see (2.78)]. In particular  ap  =  (00 Z:)^ will  do. To determine  an we consider AjS  =  0. There are dim >r(A)  =  2 linearly independent solutions of A/3  =  0. In partic ular {(1 0 0)^ (0 1 0)^} is a basis for M(A). Therefore  any solution of (2.80) can be expressed as ap  -\- ah 0' 0 .k. + T  01 0  1 .0  oj \ci l_<^2. where ci C2 are appropriately chosen real numbers. • We conclude by noting that an extensive discussion of determining  solutions of the linear system of equations  (2.73) is provided in the  Appendix. I.  Equivalence  and  Similarity From  our previous  discussion  it  is  clear  that  a linear  transformation  ^4 of  a  finite-dimensional  vector  space  V into  a  finite-dimensional  vector  space  W can be repre sented by means of different  matrices depending on the particular choice of bases in V and W. The choice of bases may in different  cases result in matrices that are easy or hard to utilize. Many of the resulting "standard" forms of matrices called  canonical forms  arise because of practical considerations. Such canonical forms  often  exhibit inherent characteristics  of the underlying transformation  si. Throughout the present subsection V and W are finite-dimensional vector spaces over the same field F dim V  =  n and dim W  =  m. Change of bases: Vector case Our  first  aim  will  be  to  consider  the  change  of  bases  in  the  coordinate  repre- sentation  of vectors. Let {v^ . . .  v""} be a basis  for  V and let {v^ . . .  v'^} be  a set of vectors in  V given by 117 CHAPTER 2: Response of Linear Systems =  ^Pn i  =  \.. (2.81) where  pij  G F  for  all  i j  =  1...  n.  It  is  easily  verified  that  the  set  {v^ ...  v"} forms  a basis for  V if and only if the n X n matrix P  =  [pij] is nonsingular. We call P the matrix  of the basis  {v\  ...  v"} with  respect  to the basis  {v^ . . .  v"}. Note that the  /th  column  of P  is  the  coordinate  representation  of  v'  with  respect  to the  basis Continuing  the  above  discussion  let {v^ . . .  v"} and  {v\  ...  v'^} be  two  bases for  y  and  let  P  be  the  matrix  of  the  basis  {v^ ...  v"} with  respect  to  the  basis {v^ . . .  v"}. Then  it is easily  shown  that  P~^  is the matrix  of the basis  {v^ ... v'^} with respect to the basis {v^ ...  v"}. Next  let  the  sets  of  vectors  {v^ . . .  v"} {v^ . . .  v^}  and  {v^ ...  v^} be  bases for  V. If P is the matrix of the basis {v^ . . .  v"} with respect to the basis {v^ ...  v"} and if  Q is the matrix  of the basis {v V  • • v"} with respect to the basis {v^ . . .  v'^} then it is easily verified  that PQ is the matrix of the basis {v^ ...  v"} with respect to the basis {v^ ...  v^}. Continuing  further  let  {v^ ...  v"} and  {v^ ...  v"} be  two  bases  for  V  and  let P  be  the  matrix  of  the  basis  {v^ . . .  v"} with  respect  to  the  basis  {v^ . . .  v'^}.  Let a  ^  V  and  let  a^  =  ( a i  . . . a„)  denote  the  coordinate  representation  of  a  with respect to the basis {v^ ...  v"} (i.e. a  =  XJ^= i cav^)- Let d^  =  (di... dn)  denote the  coordinate  representation  of  a  with  respect  to  the  basis  {v^ ...  v"}.  Then  it  is readily verified  that Pd  =  a. R^  be given. Let EXAMPLE  2.8.  Let  V  =  R^ md  F  =  R and let a  =  (1 2 3)^  ( =  (10 Of  ^2  ^ {v\ v^ v-^} =  {e^ e^ e^} denote  the  natural  basis  for  R^  i.e. e^ (0 1 0)^ e^ =  (001)^. Clearly the coordinate representation  a  of a with respect to the natural basis is (1 2 3)^. Now let {v\ v^ v^} be another basis for R^ given by v^  =  (1 0  1)^ = (0 1 l)'^. From the relation  v2 = (0 1 0)^ (101)^  =  v^  = rii 0 0 =  Pii P\\V^  +  PllV^  +  P3lV^ roi 0 1 ro" 1 0 +  P21 +  P31 we conclude that pn  =  1 p2i  = 0 and psi  =  1. Similarly from (0  1 0)^  =  v'^  =  pnV^  +  P22V^  +  P32V^ roi 0 1 roi 1 0 \l~ 0 0 +  P32 +  Pll P\2 118 Linear Systems we conclude that pu  = 0 P22  =  1 and P32  = 0. Finally from the relation (011)^ r ii 0 0 Pl3 +  P23 +  P33 roi 1 0 roi 0 1 we obtain that pu  = 0 P23  =  1 and P33  = 1. The matrix P  =  [ptj] of the basis {v\ v^ v^} with respect to the basis {v^ v^ v^} is therefore determined to be ri  0  01 p  =  \o  1  1 ij [1  0 and the coordinate representation  of a with respect to the basis {v^ v^ v^} is given by d  = P~^a or ri  0  01 0  1  1 .1  0  1 r  1  0 1 1 -1 . -1  0 -1  r  11 r L: 3j rr 01 2 L3. oj = Ml 0 _2_ Change of bases: Matrix  case Having  addressed  the relationship  between  the coordinate  representations  of  a given  vector  with  respect  to different  bases  we  next  consider  the  relationship  be tween  the  matrix  representations  of  a  given  linear  transformation  relative  to  dif ferent  bases. To this  end  let  ^  G L(V W)  and  let {v^ . . .  v^} and  {w^ . . .  w"^} be bases for  V and  W respectively. Let A  be the matrix of si  with respect to the bases {v^ . . .  v"} and {w^...  w"^}. Let {v^ . . .  v^} be another basis for  V and let the ma trix of {v^ ...  v'^} with respect to {v^ ...  v'^} be P. Let {w^...  w^]  be another basis for  W  and  let  Q be the matrix  of {w^ ...  w^}  with respect  to {vP^ ...  w^}.  Let  A be the matrix of ^  with respect to the bases {v^ . . .  v'^} and {w^ . . .  w^}.  Then it is readily verified  that A  =  QAR (2.82) This result is depicted  schematically  in Fig. 2.1. V-^  W A —* V  =  PV Pt {v\  • • •. v"} A V 0)  =  Av \  Q cb = Qo) FIGURE 2.1 Schematic diagram of the equivalence of two matrices Equivalence of matrices The preceding discussion motivates the following  definition. 119 CHAPTER  2: Response of Linear Systems DEFINITION 2.4.  Anm  X n matrix A is said to be equivalent to an m X ^ matrix A if there exists an m X m nonsingular matrix Q and an /i x ^ nonsingular matrix P such that • (2.82) is true. If A is equivalent to A we write A ~  A. Thus an m X ^ matrix A is equivalent to an m X n matrix A if and only if A and A can be interpreted  as both being matrices  of the same linear transformation  ^4 of a linear space V into a linear space W but v^ith respect to possibly different  choices of bases. It is clear that  a matrix A is always  equivalent  to itself  (i.e. A ~  A). Also if a matrix A is equivalent to a matrix 5 then clearly B is equivalent to A (i.e. if A ~  5 then 5  —  A). Furthermore if A is equivalent to B and B is equivalent to a matrix C then it is evident that A is equivalent  to C (i.e. if A ~  5  and B  — C then A ~  C). This shows that  ~  is an equivalence  relation. The reader can easily verify  that every m X n matrix is equivalent to a matrix of the  form "100 010 000 000 •• •• •• •• ••  0  " ••  0 }  r  =  rank  A 100 000 ••  0 ••  0 000 •• 000 •• 0-From this it follows  that two  mX  n matrices A  and B  are equivalent  if  and  only if they have the same rank and furthermore  that A  and A^  have the same rank. The definition of rank of a matrix that we used in Section 2.2 is sometimes called the column  rank  of a matrix.  Sometimes an analogous  definition  for  row rank  of a matrix  is also used. The result given in the above paragraph  shows that row rank of a matrix  is equal  to its column  rank. Similarity of matrices Next let  V  =  W let 5i  e  L{V V) let {v^... v"} be a basis for  V and let A be the matrix  of d.  with  respect  to {v^ ...  v"}. Let {v'... v"} be  another basis  for  V whose matrix with respect to {v^ . . .  v"} is P. Let A be the matrix of ai with respect to {v^ . . .  v"}. Then it follows  immediately  from  (2.82) that A  =  P'^AP. (2.83) The meaning of this result is depicted  schematically  in Fig. 2.2. This discussion  motivates the following  definition. {v\...v"} t  P V {v\...v"} IP-' {v   . . .  v"} A -^ {v  ...  v"} FIGURE 2.2 Schematic diagram of the similarity of two matrices 120 Linear Systems DEFINITION 2.5.  knnXn  matrix A is said to be similar to em nXn matrix A if there exists SinnX  n nonsingular matrix P such that A  =  P-^AP. If A is similar to A we write A -- A. We call P a similarity transformation. • It is easily verified  that if A is similar to A  [i.e. (2.83) is true] then A is similar to A i.e. A = PAp-\ (2.84) In view  of this there is no ambiguity  in  saying  "two matrices  are similar"  and  we could just as well have used (2.84) [in place of (2.83)] to define similarity of matrices. To  sum  up if two  matrices A and  A represent  the  same  linear  transformation d^ G L{V y)  possibly  with  respect  to two  different  bases  for  V then A and  A are similar matrices. Since the similarity of two matrices is a special case of the equivalence of matri ces it follows that (i) a matrix A is similar to A; (ii) if A is similar to a matrix B then B is similar to A; and (iii) if A is similar to B and B is similar to a matrix C then A is similar to C. Therefore the similarity relation of matrices is an equivalence relation. Now let A be an n X fz matrix that is similar to a matrix B. Then it is easily shown that A^ is similar to B^ where k denotes a positive integer i.e. B^  = P~^A^P.  This can be extended further  by  letting and by verifying  that m i = 0 ao + aiX + • • • + a^A" (2.85) f(p-'AP)  = p-'f{A)P (2.86) where  a o  . . . a^ E F. This  shows  that if B is similar  to A  then  f{B) is similar to /(A)  where in fact  the same  similarity  transformation  P is involved. Further if A is similar  to A  and if /(A)  is as given in (2.85)  then  /(A)  =  O if and  only if /(A)  ==  O. Next  let ^4 G L(K  V)  and  let  A be the  matrix  of ^4 with  respect  to a basis {v\  . . .  v"} in y. Let  /(A)  denote  the polynomial  given  in  (2.85)  and  let A be  any matrix of si.  Then it is readily verified  that f{d)  = 0 if and only if /(A)  = O We can use results such as the preceding to good advantage. For example let A denote the diagonal  matrix  given by 'Ai  0  0  •••  0 0  A2  0  ••• 0 0 0 A  = ' 0 0 0  0  •• 0  0  •• •  A„-i Then {Af  = \x\ 0 0  0  • A^  0  • 0 0  0  • 0  0  • 0 An J 0 0 0 AS 0 0 0 0 121 CHAPTER  2: Response of Linear Systems Now let /(A) be given by (2.85). Then [l  0  ••• 0  1 ••• ...  0 ...  0 Ai 0 0 A2 • • f{A)  =  ao +  ai 0  0 |_0  0 ••• ••• 1 0 0 1 0 [0 0-0 ••  A „ -i 0 • 0 0 0 An + ••• +  a„ "AY* 0 0  A-0 .0 0 0 01 0 0 A"* 0 7(Ai) 0 0 /(Ai) • 0 0 0 0 0 0 0 /(A„-i) 0 /(A„) Next let ^  E  L{V V) let A be the matrix of si  with respect to a basis {v^ . . .  v"} in y and let A be the matrix of si  with respect to another basis {v^ . . .  v'^} in V. Then it is easily verified that det (A)  =  del (A). From this it follows that for any two similar matrices A and B we have det (A)  =  det  (B). In view  of these results there is no ambiguity  in defining  the determinant  of a linear  transformation  6^^ of a  finite-dimensional  vector  space  V into  V as the  deter minant of any matrix A representing  it i.e. det(^)  =  det  (A). J.  Eigenvalues  and  Eigenvectors Definitions Throughout  this  subsection  V  denotes  an  n-dimensional  vector  space  over  a field  F. Let  ^  E  L(V V)  and  let us  assume  that  there  exist  sets  of  vectors  {v^ . . .  v"} and {v^ . . .  v"} that are bases for  V such that V = v2  = =  Aiv' = W v«  =  siv"  =  A„v" where  Xi ^  Fi  =  I...  n. If this is the case then the matrix  Aof  d.  with  respect to the given bases is A  = [Ai 0 0 A„ This motivates the following  result that is easily verified:  for M G LiV V)  and A  G F  the set of all v G  V such that siv  = Av (2.87) 122 Linear Systems is  a  linear  subspace  of  V.  In  fact  it  is  the  null  space  of  the  linear  transformation (^  ~~  ^^)^  where ^  is the identity element of L(V V).  Henceforth  we let JVTA =  {v  G  y  : (^  -  Ai)v  =  0}. (2.88) The above gives rise to several important concepts that we introduce in the fol lowing  definition. DEFINITION 2.6.  A scalar A such that Kx [given in (2.88)] contains more than just the zero vector is called an eigenvalue of ^4 (i.e. if there is a v T^ 0 such that siv  = Av then A is called an eigenvalue of si). When A is an eigenvalue of si then each v T^ 0 in J^TA is called an eigenvector of ^  corresponding to the eigenvalue A. The dimension of the linear subspace JVA is called the (geometric) multiplicity of the eigenvalue A. If JVA is of dimension one then A is called a simple eigenvalue.  The set of all eigenvalues of d^ is called the spectrum of si. • Other  names  for  eigenvalue  that  are in use include proper  value  characteris tic value  latent  value  or secular  value.  Similarly other names for eigenvectors  are proper  vector  or characteristic  vector.  The  space Mx is  called  the  Ath proper  sub-space  of  V. For matrices we give the following  corresponding  definition. DEFINITION 2.7.  Let A be an « X ^ matrix whose elements belong to the field F. If there exists A E F  and a nonzero vector a  E F"^ such that Aa  = Xa (2.89) then A is called an eigenvalue of A and a is called an eigenvector of A corresponding to the eigenvalue A. • The connection between Definitions  2.6 and 2.7 is given in the following  result that the reader can verify  easily: let d^ E:  L{VV)  and let A  be the matrix of ^  with respect to the basis {v^ . . .  v"}. Then  A is an eigenvalue  of d^ if and only if A is an eigenvalue of A. Also a  G  V is an eigenvector of si  corresponding  to A if and only if  the  coordinate  representation  of  a  with  respect  to the basis  {v^ . . .  v"}  a  is  an eigenvector of A corresponding to A. We note that if a (or a)  is an eigenvector of si  (of A) then any nonzero  multiple of a  (of a)  is also an eigenvector of si  (of A). In the case of matrices in place of (2.89) one can also consider the relationship aA  =  Xa (2.90) where a  denotes a  1 X n row vector. In this context a. in (2.89) and a  in (2.90)  are referred  to as a right eigenvector  and a left eigenvector  respectively. Unless explic itly stated we will have in mind a right eigenvector when using the term eigenvector of a matrix. Now let si  G L{V V)  and let A denote the matrix of si  with respect to the basis {v^ ...  v"} in y. Then it is easily shown that A E  F  is an eigenvalue of d^ (and hence of A) if diViA only if det {si-X3>)  =  0 orequivalently if andonly if J^r(A-A/)  =  0. Characteristic  polynomial The above result enables  us to determine the eigenvalues  of d  (or A) in a sys tematic manner. So let us examine the  equation det{si-  X3)  =  0 (2.91) or equivalently the equation in terms of the parameter  A. We first rewrite (2.92) as det{A- \I)  = 0 1(^11  -  A) (212 Clll (Cl22  -  A) Clin ^2n det(A-\I) = 123 CHAPTER  2: Response of Linear Systems (2.92) (2.93) ^nl ^n2 '" (^nn  ~  A)| It is clear from  (2.62) that the expansion  of the determinant  (2.93) yields a polyno mial  in  A  of  degree  n. For  A  to be  an eigenvalue  of si  (or A)  it must  satisfy  (2.91) [or (2.92)]  and it must belong to F. Note that in general  we have no assurance  that the fzth-degree  polynomial  given by  (2.92) has any roots in F. There is however  a special class of fields for which this requirement is automatically  satisfied:  a field F is  said  to be algebraically  closed  if  for  every polynomial  p(X)  there is at least  one A G F  such that p{X)  =  0. (2.94) Any A that satisfies  (2.94) is said to be a root of the polynomial equation (2.94). In particular  the field of complex  numbers  is algebraically  closed  whereas  the field of real numbers is not (e.g. consider the equation A^ +  1  =  0). There are other fields besides the field of complex numbers that are algebraically  closed.  However since  we  will  not  require  these  we  will  restrict  ourselves  to  the  field  of  complex numbers C whenever the algebraic closure property is required. When  considering results  that  are valid  for  a vector  space  over  an  arbitrary  field  we  will  as  before make use of the symbol F or make no reference  to F at all. Summarizing  the  above  discussion  we  have  the  following  result.  Let  ^  G L(V V) and let A be the matrix of ^  with respect to the basis {v^ . . .  v"} in V. Then (i) det (^  -  \3)  =  det (A -  A/) is a polynomial of degree n in the parameter A i.e. there exist scalars ao a i  . . . a„  depending on d^ (and therefore  on A)  such that det{d^ -  X3)  =  det (A  -  A/)  =  ao  +  o^i^  +  «2A^ +  •••  +  a^A'^ (2.95) [note that ao  =  det (si)  and an  =  (-l)'^]; (ii) the eigenvalues of ^  are precisely the roots of the equation det(M  -  A^)  =  det (A  -  A/)  =  ao  +  aiA  +  a2A^  +  • • • +  a^A'^  =  0; (2.96) and (iii) M^ has at most n distinct  eigenvalues. We call  (2.95) the characteristic  polynomial  of si  (or of A) and  call  (2.96)  the characteristic  equation  ofd (or of A). An important remark concerning  notation The above definition of characteristic polynomial is the one usually used in texts on linear algebra and matrix theory  (refer  e.g. to some of the books on this  subject cited  at the  end  of this  chapter). An  alternative  to the  above  definition  is given  by the  expression a(A)  -  det(X3  -A)  =  det{XI  -  A). One of the reasons for using this convention is that this polynomial arises in a natural manner when solving systems of ordinary differential  equations by operator methods 124 Linear Systems [e.g. system  (LH)].  Since the reader  may  have many  occasions to consult texts on linear algebra and matrix theory we will employ in this section  the definition  given in (2.95). Throughout  the remainder  of this book however  we will follow  the con vention used in linear  systems texts by utilizing the expression det (A/  -  A) for  the characteristic polynomial. We note that because of the relationship det (A  -  XI)  =  {-If det {XI -  AX either  definition  can  be  used  to  develop  the  results  considered  herein.  Note  that det (XI  -  A) is a monic polynomial i.e. its leading coefficient  equals  1. From the fundamental  properties of polynomials over the field of complex num bers  there  now  follows  the  next  important  result:  if  V  is  an  n-dimensional  vector space over C and if ^^i E  LiV V)  then it is possible to write the characteristic  poly nomial of d^ in the  form det  (d  -  X3)  =  (Ai  -  A)^i(A2  -  A)^^.. .(^^  _  x)'^p (2.97) where  Xu i  =  I...  p  are  the  distinct  roots  of  (2.96)  (i.e..  A/ T^ XJ if  /  T^ j).  In (2.97)  Mi is  called  the  algebraic  multiplicity  of  the  root  A/. The  m/  are  positive integers and S f =i  ^/  =  ^• The reader should make note of the distinction between the concept of algebraic multiplicity of A^ given above and the (geometric) multiplicity of an eigenvalue A/ given earlier. In general these need not be the same as will be seen later. The Cayley-Hamilton  Theorem and  appUcations We now state and prove a result that is very important in linear systems theory. THEOREM  2.1. (CAYLEY-HAMILTON  THEOREM)  Every  square matrix  satisfies its characteristic equation. More specifically if A is an n X n matrix and p(X)  = det (A  -XI) is the characteristic polynomial of A then p(A)  = O. Proof. Let the characteristic polynomial for A be p{X) = ao + aiX-\ h a„A" and let B{X) =  [bij(X)] be the classical adjoint of (A -  XI) (refer to Subsection 2.2G). Since the bij(X) are cofactors of the matrix A -  A/ they are polynomials in A of degree not more than n-l.  Thus bij(X) =  Ptjo  + ptjiX  +  ••• + Ptj^n-DX^'-K  Letting Bj =  Wtjk] for A:  =  0 1... n -  1 we have B(X) =  BQ + XBi +  "  + A^~^5„-i. By (2.71) we have (A~XI)B(X)  =  [det(A-XI)]LThus(A-XI)[Bo  + XBi-h"'  + X''-^Bn-i]  =  (ao+Q:iA+ • • •  + anX^)I. Expanding the left-hand  side of this equation and equating like powers of A we have -Bni  = oLnL AB^i  -  Bn-2 = 0Ln-\I... AB\  -  Bo = ail  ABQ  =  a^I. Premultiplying the above matrix equations by A" A"~\ ... A / respectively we have -A^Bn-i  -  Qf„A«A«5„-i-A"-i5„-2  =  a„-iA"-i  . . .  A 2 B I - A 5O  =  a^AAB^  = aol.  Adding these matrix equations we obtain O = a^I  + aiA  +  • • •  + anA"^  = p(A) which was to be shown. • As  an immediate  consequence  of  the  Cayley-Hamilton  Theorem  we have  the following  results: let A be an n  X n matrix  with characteristic  polynomial  given by (2.96). Then (i) A^  -  (-l)"+i[«o/  + ^lA  +  • • • + an-iA""-^];  and (ii) if/(A)  is any polynomial in A then there exist  JSQ jSi... /3„_i  G F  such that f(A)  =  /3o/ +  iSiA  +  • • • +  /3n-iA'~\ (2.98) Part  (i)  follows  froir  the  Cayley-Hamilton  Theorem  and  from  the  fact  that an  =  (-1)".  To prove part (ii) let /(A) be any polynomial in A and let p(X)  denote the characteristic polynomial of A. From a result for polynomials (called the  division algorithm)  we know that there exist two unique polynomials g{X)  and r(A) such that fil)  = p{l)gil)  + r{l) (2.99) where the degree  of  r(A)  <n—l.  Now  since p(A)  =  O we have that /(A)  =  r(A) and the result  follows. Finally  we also note that if  ^  G L(VV)  and if p(A)  denotes the  characteristic polynomial of ^  then p ( ^)  =  ^. As  a  specific  application  of  the  Cayley-Hamilton  Theorem  we  evaluate  the 125 CHAPTER  2: Response  of Linear  Systems q7 [l  Ol matrix  A ^\  where  A  =  L ^  .  Since  n  =  2  we  assume  in  view  of  (2.98) that  A^^  is  of  the  form  A^^  =  JSQ/ +  /3iA.  The characteristic  polynomial  of A  is p{X)  =  (1 — A) (2 — A)  and  the  eigenvalues  of A  are  Ai  =  1 and  X2 =  2. In the  present case  / ( A)  =  A^^  and r(A)  in (2.99)  is r(A)  =  /3o +/3iA.  To determine  /3o  and  /3i we use the fact  that p(Ai)  =  p{^2)  =  0 to conclude  that / ( A i)  =  r(Ai)  and  /(A2)  =  r(A2). Therefore  we  have  that  /3o + / 3i  =  1^^ =  1  and  j8o +  2/3i  =  2^^.  Hence  j8i  = 2^^  -  1  and /3o =  2 -  2^^.  Therefore  A^^ =  (2 -  2^^)/ +  (2^^ -  1)A  or A^^  = 1 237-1 0] 237j-The Cayley-Hamilton Theorem can also be used to express matrix-valued power series  (as  well  as  other  kinds  of  functions)  as  matrix  polynomials  of  degree  n— 1. Consider in particular the matrix exponential  e^^  defined  by ^At  1 k=0 A^ t  G  {—aa). (2.100) In view of the Cayley-Hamilton  Theorem we can write /(A) ^At i=0 (2.101) In the following  we present  a method  to determine the coefficients  ai{t)  in  (2.101) [or/3/in (2.98)]. In  accordance  with  (2.97)  let  p(A)  =  det{A -  XI)  =  Uf=i{^i  -  ^T'  be  the characteristic  polynomial  of A.  Also  let /(A)  and  g(A)  be  two  analytic  functions. Now if /«(A0=g«(A0 / =  0  . . .  m  - - l  / = l  . . .  p (2.102) where f^^\Xi)  =  ((i7/^^0(^)U=Aplf=i  m  = n then /(A)  =  g{A).  To see this we note that condition (2.102) written as (/  -  g)^^^ (A/)  =  0 impUes that /(A)  -  g{X)  has p(A)  as  a  factor  i.e.  /(A)  — g(A)  =  w(A)p(A)  for  some  analytic  function  w(A). From  the  Cayley-Hamilton  Theorem  we have  that  p{A)  =  O and  therefore  /(A) — g{A)  =  0. E X A M P LE  2.9.  Let  A and let /(A)  =  e^\f{X)  = e^\  and g{X) =  aiA+ OQ. The matrix A has an eigenvalue  A =  Ai =  A2 =  0 with multiplicity m\  =2.  Con ditions (2.102) are given by /(Ai)  = ^(Ai) =  1 and f^^\X\)  = g^^\h)  and imply that -1  1 -1  1 126 Linear Systems ao  =  1  andai  =  f. Therefore (^)  ^  ^^  + ^A. ^  f^^^  ^ j  ^  [-«!  + «o a^ ] l l ~t t In the final result of this  section we let F  =  C we let A be an n X n matrix of A  G L(V; y) and we let det{A  -  A/)  -  det(A  -  \3)  be given by (2.97). It is read ily  verified  that  (i) det(A)  =  [ l ^ .i  ^J\ (ii) trace  (A)  =  X^^iau  =  Zy = i m^Ay (iii)  if  B  is  any  matrix  similar  to A  then  trace  (B)  =  trace  (A)  and  (iv)  if  /(A) denotes the polynomial /(A)  =  7o + 7i A H \- 7mA^ then the roots of the charac teristic polynomial  of /(A)  are / ( A i )  . . . /(A^) and J^^ [/(A)  -  A/]  =  [/(Ai)  -Ar---[/(A;)-An. K.  Direct  Sums  of Linear  Subspaces One of the important topics in matrix theory is the development of canonical  forms including the lower (upper) triangular form the block diagonal form and the Jordan canonical  form  of matrices. Before  presenting  these we need  to address  additional topics  which  are  of  interest  and  importance  to us  in  their  own  right.  One  of  these concerns direct sums of linear subspaces and linear transformations  defined  on such sums. Let  y  be a linear  space and let  W and  U be arbitrary  subsets  of  V. The sum  of sets  W and  U denoted by  W  -\-  U is the  set of  all vectors in  V that are of the  form w +  w where  w  E. W  and  u  E  U.  If  in  particular  W  and  U are  linear  subspaces of  y  then  it is easily  shown  that  W  -\-  U is  also  a linear  subspace. If  W  and  U are linear  subspaces  of  V and ifWnU =  {0} (the singleton  set consisting  of the null vector of V) then we say that W and  U are disjoint.  Note that this terminology is not consistent with that used in connection with sets. If W and  U are linear subspaces of y  then  it is easily  verified  that  for  every  v  E  U  +  W  there  exist  unique  elements w  ELW and  u E  U such that v  =  u -\-  wif  and only ifUHW = {0}. The preceding discussion is readily extended to any number of linear  subspaces of  y  and  gives rise to the following  concept.  Let  y i  . . .  y^ be linear  subspaces of a vector  space  V. The  sum  yi  +  • • • +  y^ is said to be  a direct  sum  if for  each v G + Vr there is a unique set v^ G  Vi i  =  1...  r such that v  =  v^ +  • • •  + v'*. Vi  -\ Henceforth  we will denote the direct sum of  y i  . . .  y^ by  yi  ©  • • • ©  y^. Now let y  be the direct sum of linear spaces  Vi  and  V2 i.e.  y  =^ yi  © V2 and let V =  v^ -h v^ be the unique representation of v G  y  where v^  G  yi  and v^  G  y2. We say that the projection  on  Vi  along  V2 is the transformation  defined  by 2^(v)  =  v^ (2.103) We can easily verify  that (i) ^  G L(y  y) (ii) 31(2^)  -  Vu  and  (ii) J{{^)  =  V2. More generally we can show that if 2P G L(V V) then 2^ is a projection on 9l(SP) along X(&)  if and only if 2^2P  =  ?P^  =  ?P.  This gives rise to the following  concept: ^  G L(V; V) is said to be idempotent  if 2P^  =  9^. We can also verify  easily that 2^ is a projection  on a linear subspace if and only if  (J^ -  2P) is  a projection.  If  in particular  2P is the projection  on  Vi  along  V2 then (3  -  ^)  is the projection  on V2 along  Vi. In view of the preceding results there is no ambiguity in simply saying a trans formation  9^ is a projection  (rather than 9^ is a projection  on  V\  along  V2). We em-phasize that if 2?^ is a projection  then This is not necessarily the case for arbitrary linear transformations ST in general 9l(2r) and J^CJ) need not be  disjoint. (2.104) L(K  V)  for 127 CHAPTER 2: Response of Linear Systems Next  let  ST G L(V; y). A linear  subspace  W of  V is  said to be  invariant  under the  linear  transformation  ST  if  w G  W implies  that  ^w  G  W.  From  this  definition it follows  trivially  that  (i)  V is invariant  under  ST (ii) {0} is invariant under  ST (iii) 2/1(9") is invariant under  ST and (iv) ^{'J) is invariant under ST. Next  let  y  be  a linear  space  that  is the  direct  sum  of two  linear  subspaces  W and  U. If W and  U are both invariant under a linear transformation  ST then ST is said to be reduced by  W and  U. It is readily verified  using definitions that ST G L{V V) is reduced  by  W  and  U if  and  only  if  S^ST  =  ST9^ where  9^ is the projection  on W along  U. Next we consider briefly the matrix representation of projections. To this end let V  be an ^-dimensional  vector  space and let 9^ G L( V V). It is easily verified  [using (2.104)] that if 9^ is a projection then there exists a basis {v^ . . .  v'^} for  V such that the matrix P  of 9^ with respect to this basis is of the  form 1 0 0 1 0 0 0 0 1 (2.105) > r 0 0 0 0 0 0 where r  =  dim 91(9^). We conclude with the following  interesting result. Let V be a  finite-dimensional vector  space  and let ^  G L{V V).  If  W is a/^-dimensional  invariant  subspace  of V and ifV  =  W®U then there exists a basis for  V such that the matrix A of ^^i with respect to this basis is of the  form A  = 0 M2 122 (2.106) where  An  is  a. p  X  p  matrix  and  the remaining  submatrices  are of  appropriate  di mension. The  canonical  form  (2.106)  will  be  used  in  Chapter  3 in  developing  standard forms  for uncontrollable  and unobservable  systems. L.  Some  Canonical  Forms  of  Matrices In this  subsection  we investigate under  which conditions  a linear transformation  of a  vector  space  into  itself  can  be  represented  by  (i)  a  diagonal  matrix  (ii)  a  block 128 Linear Systems diagonal matrix (iii) a triangular matrix and (iv) a companion matrix. We will also investigate when a linear transformation  cannot be represented by a diagonal matrix. Throughout  this  subsection  V  denotes  an  fz-dimensional  vector  space  over  a field  F. Diagonal  form We begin  with the following  fundamental  result. Let  Ai... A^ be the  distinct eigenvalues  of  a linear  transformation  ^  G  L(V V)  and  let  v^  7^ 0  . . .  v^  7^ 0  be corresponding  eigenvectors  of si.  Then it is easily  shown that the set {v^ . . .  v^} is linearly independent. We note that if in particular ^  has n distinct eigenvalues then the  corresponding  n  eigenvectors  span  the  linear  space  and  as  such  form  a  basis forV. The  above  gives  immediate  rise to the next  important  result.  Let  si  E  L(V  V) and assume that the characteristic polynomial of si  has n distinct roots so that det  (si  -  X3)  =  (Ai  -  A)(A2 -  A)- • -(A^ -  A) (2.107) where Ai... A„ are distinct eigenvalues. Then there exists a basis {v^ ...  v'^} of V such that v^ is an eigenvector corresponding  to A/ for  /  =  1...  n. The matrix A  of ^  with respect to the basis {v^ . . .  v"} is A2 A  = [Ai 0 0 Xn =  diag{Ki...\n\ (2.108) In  the  same  spirit  as  above  we  can  also  easily  establish  the  next  result.  Let si  G L{V V) and let A be the matrix of 6?^ with respect to the basis {v^ ...  v"}. If the characteristic polynomial det {si — \3)  =  ao + ^i A H h a^A" has n distinct roots Ai...  \n  then A is similar to the matrix A of ^4 with respect to the basis {v^ ...  v^} where A is given in (2.108). In this case there exists a nonsingular matrix P such that A  =  P'^AP. The  matrix  P  is  the  matrix  of  the  basis  {v^...v"}  with  respect  to  the  basis {v^ . . .  v"} and  P~^  is the matrix  of the basis {v^ . . .  v^} with respect to the basis {v^ . . .  v"}. The matrix P can be constructed by letting its columns be  eigenvectors of A corresponding  to Ai... A„ respectively; that is P  =  [^1  ^2^^^^«]^ (2.109) where TT^ . . .  77^ are eigenvectors of A corresponding to the eigenvalues Ai... A„ respectively  (verify  this). The similarity transformation  P given in (2.109) is called a modal  matrix.  If the conditions  of the  above result  are  satisfied  and if  in particular  (2.108)  holds  then we say that the matrix A  has been  diagonalized. As  a specific  case let  V be a two-dimensional  vector  space over the real  num bers  let  si  G L{V V) and  let {v^ v^} be  a basis  for  V.  Suppose  the matrix A of ^4 with respect  to this basis is given by  A  = \-2 . 4l .  . The characteristic  polynomial of ^  is p{X)  =  det(si  -  A^)  =  det{A  -  \I)  =  A^ + X-6  =  (\-  2)(A +  3) and the eigenvalues  of 62^ are A1 =  2 and A2 =  - 3. Let  77 =  (r/i 772)^ denote  the  coordinate  representation  of  v  G V witli  respect  to tlie  basis  {v  v  }.  To  find  eigenvectors  corresponding  to  A//  = 1  2 we  solve  the =  (11)^  and system  of  equations  (A — A/) 77 =  O.  An  easy  computation  yields 77^ 77^ =  (4 — 1)  as  eigenvectors  corresponding  to  Ai  and  A2 respectively.  The  diagonal 129 CHAPTER  2: Response of Linear  Systems matrix  A  given  in  (2.108)  is  A (2.109)  and  its inverse  P~^  are 0 0 The  matrix  P  given  in [n\n" '1 1 4" -1 5 1  _ P-".2 .2 .8" - .2 As  expected  we  have  P  AP "2 0 0" 3 -= "Ai 0 0' X2_ •  By By  (2.81)  the  basis { v\  v^} C y  with respect to which A  represents sz/ is given by  v^  =  Xj=i Pji ^^  = 772)^ is the coordinate  representa-(r]i P~^77  is the  coordinate  representation tion  of  V with  respect  to  { v\  v^}  then 77 of  V with  respect  to  { v\  v^}.  The  vectors  v\  v^  are  of  course  eigenvectors  of  ^ corresponding to Ai and A2 respectively. When  the  algebraic  multiplicity  of  one  or  more  of  the  eigenvalues  of  a  linear transformation  is  greater  than  one  then  the  linear  transformation  is  said  to  have repeated  eigenvalues.  In  this  case  it  is  not  always  possible  to  represent  the  lin ear  transformation  by  a  diagonal  matrix.  However  from  the  preceding  results  of this  section  it  should  be  clear  that  a  linear  transformation  with  repeated  eigenval ues  can  be  represented  by  a diagonal  matrix  if  the  number  of  linearly  independent eigenvectors  corresponding  to  any  eigenvalue  is  the  same  as  the  algebraic  multi plicity  of  the  eigenvalue.  We  consider  two  specific  cases  to  shed  additional  light on this. First we consider the matrix 1 0 0 3 4 3 -2 -2 -1 with  characteristic  equation  det{A  — XI)  =  (1—A)^(2  — A ) =0  and  eigenvalues Ai  =  1 and  A2 =  2.  The  algebraic  multiplicity  of  Ai  is  two.  Corresponding  to  Ai we can find two linearly  independent  eigenvectors  (123)^  and  (100)^  and cor responding to A2 we have an eigenvector  (111)^. Letting P denote a modal matrix we obtain 1 2 3 1 0 0 1 1 1  P-'  = 0 1 0 — 1 -2 3 1 1 -2 and A = P-^AP  = "1  0 0 1 0  0 0" 0 2 In this example dim ^^  =  2 which happens to be the same as the algebraic multi plicity of Ai. For this reason we were able to diagonalize A. 130 Linear Systems As a second example consider the matrix 2 0 0 1 2 0 -2 -1 1 with  characteristic  equation  det(A  — XI)  =  (1—A)(2  — A)^  =  0  and  eigenvalues Ai  =  1 and  X2 =  2. The  algebraic  multiplicity  of  X2 is two  and  an eigenvector  cor responding to Ai is  (111)^. It is easily verified  that any eigenvector  corresponding to A2 must be of the form  (vi 00) Vi 7^ 0. We see that dim ^^  =  1 and thus we are  not  able  to  determine  a basis  for  the  three-dimensional  vector  space  V  which consists of eigenvectors. Consequently we are unable to diagonalize A. Block diagonal  form When  a matrix  cannot  be  diagonalized  we  seek  for  practical  reasons  to rep resent  a  linear  trasformation  by  a  matrix  that  is  as  nearly  diagonal  as  possible. The  next  result  provides  the  basis  of  representing  linear  transformations  by  such matrices  called  block  diagonal  matrices.  In  Subsection  O  of  this  section  we  will consider  the  "simplest"  type  of block  diagonal  matrix  called  the Jordan  canonical form. We let V  be an n-dimensional  vector  space and we let ^  G L(y V).  IfV  is the direct sum of p linear subspaces Vi... Vp which are invariant under ^  then it can be readily shown that there exists a basis for V such that the matrix representation  for ^ is in the block diagonal form given by Ui  I 0 (2.110) A  = Ai 0 Moreover A/ is a matrix representation  of s^i the restriction  of ^ Also to Vi / =  1... p. p det[A)=Wdet{A^). i=\ (2.111) From the above it is clear that to carry out a block diagonalization  of a matrix A we need to find an appropriate  set of invariant  subspaces  of V and furthermore  we need to find a simple matrix representation  on each of these subspaces. As a specific  case for the above let V be an n-dimensional  vector space. If ^  G L(y y)  has n distinct eigenvalues Ai... A^ and if we let ^-  =  { v : (^  — Aj J^) v  = 0} 7 =  1... n then jVj is an invariant linear subspace under ^  and V =  ^  0  • • • 0 JVn. For any v G  JVJ we have s^v  =  AyV and hence s^jV  =  A v for  v G  JVJ. A  basis for  ^j is  represented by the matrix  Xj  (in this case  simply  a scalar). With respect to a basis of n linearly independent eigenvectors { v \ . . .  v^}  ^ is  any  nonzero  Vj G ^j.  Thus  with  respect  to this basis  ^j is represented by (2.108). Triangular  form In addition to the diagonal form and the  block diagonal form there are many other useful  forms  of  matrices  to  represent  linear  transformations  on  finite-dimensional vector spaces. One of these canonical forms  involves triangular matrices with one of 131 CHAPTER  2: Response of Linear Systems the two forms given by an 0 <3l2 Cl22 ai3 (323 0 0 0 0 0 0 . • . . ain Clin ^n-ln ^nn or an Cl2\ I <^n-ll ^nl 0 a22 0 0 Cln-12 ^n2 Cln-13 UnS '  • ' 0  " 0 0 0 (2.112) We call the matrix on the left  an upper  triangular  matrix  and the matrix on the right a lower  triangular  matrix. Now let V be an n-dimensional vector space over C and let ^  G L(VV)At  can be  shown that there exists  a basis  for  V such that ^  is represented  by  an upper  (or by a lower) triangular matrix with respect to that basis. We note that if A is in triangular form  then det(A  -  A/)  =  (an  -  A)(a22 -  A)- • -(ann  -  A) (2.113) i.e. the diagonal elements of A are in this case the eigenvalues of A. Companion  form We  conclude  this  subsection  by  considering  a  canonical  form  for  real  square matrices that arises frequently  in systems theory called companion form.  As a mat ter of fact  a given  matrix  A  ^  R^^^  can be transformed  via  appropriate  similarity transformations  into four  different  forms  of this type given by "0 / x' 0  ' / 0  ' "x / "x X "0 X x' X / X ' -an-i  determined  by  (-iydet(A where  the  rows  or  columns  denoted  by  (XX)  are  made  up  of  the  coefficients -  A/)  =  det(XI  -  A)  =  A'^  + -ao  —ai... an-iX""-^ +  • • • +  ao where /  G 7^("-i)x("-i)  and where the 0 denotes  an  (n  -  1)-dimensional  column  or  row  vector.  For  example  the  matrix  on  the  left  which  is perhaps  the  most  commonly  used  companion  form  and  which  henceforth  we  will identify  as A^ is given by Ac  = 0 0 1 0 .. 0 1 0 -ao 0 — Ui 0 - ( 32 .. • •  • 0 0 1 " f l n -l In the following  we confine  our discussion to this matrix. Similar treatments  apply to the other three companion  forms. Given  A  G  R"-^"-  it  can  be  shown  that  there  exists  a similarity  transformation P that transforms  A to the companion  form  Ac  given  above if  and only  if there  ex ists  a vector  b  EL  R^  such that the matrix %  =  [b Ab...  A^~^b]  is of full  rank  n. Furthermore P is given by n -l qA P  = [qA n-\ ^^^ Linear Systems where q is the nth row of ^ -^  and >.  _  p - i / ip A  matrix A  for  which  such  a vector  Z?  exists  is  called  cyclic.  It  can  be  shown  that A  is  cyclic  if  and  only  if  the  geometric  multiplicity  of  each  of  its  n eigenvalues  is one  or  equivalently  if  and  only  if  the  n  eigenvalues  of A  are  exactly  the  roots  of its minimal polynomial  (refer  to Subsections M and O which follow). Finally if A/ is  an  eigenvalue  of  Ac  (or  of A)  then  it  can  be  verified  that  (1 A/...  Ap^)^  is  a corresponding  eigenvector. M.  Minimal  Polynomials One of our goals in this  section is to develop the Jordan canonical form.  To accom plish this we first need to introduce  and  study minimal polynomials  (which  will be accomplished  in  the  present  subsection)  and  nilpotent  operators  (to be  considered in  Subsection  N).  Throughout  this  subsection  V  denotes  an  n-dimensional  vector space. For purposes  of motivation consider the matrix A  = 1 0 0 3 4 3 -2 -2 -1 The  characteristic  polynomial  of A  is p(\)  =  (1  -  A)^(2 -  A) and  we know  from the Cayley-Hamilton  Theorem  that p(A)  =  O. (2.114) Now let us consider the polynomial  m(A)  =  (1 -  A)(2 -  A)  =  2 -  3A +  A^. Then m(A)  =  2/  -  3A +  A^  -  O. (2.115) Thus matrix A satisfies  (2.115) which is of lower degree than  (2.114) the charac teristic equation of A. More generally it can be shown that for annX  n matrix A there exists a unique polynomial  m(A)  such  that  (i)  m(A)  =  O  (ii)  m(A)  is  monic  (i.e.  if  m  is  an  nth-degree polynomial  in A then the coefficient  of A" is unity) and  (iii) if  m\X)  is any other polynomial  such  that  m\A)  =  O then  the degree  of  m(A) is less  or equal  to the  degree  of  m'(A)  [i.e.  m(A)  is  of  the  lowest  degree  such  that  m(A)  =  O].  The polynomial  m(A) is called the minimal  polynomial  of  A. In the remainder of this section we let p(X)  denote the characteristic  polynomial of an n X n matrix A and we let m(A) denote the minimal polynomial  of A. In  what follows  we develop an explicit form  for the minimal polynomial  of A that makes it possible to determine it  systematically. Let  /(A)  be any polynomial  such that  /(A)  =  O (e.g. the characteristic  poly nomial). Then it is easily  shown  that  m(A) divides  /(A)  [i.e. there is a polynomial ^(A) such that /(A)  =  ^(A)m(A)]. In particular the minimal polynomial of A m(A) divides  the  characteristic  polynomial  of  A p(X).  Also  it  can  be  shown  that  p(X) divides [m(A)]^ 133 CHAPTER  2: Response of Linear Systems Next let p(X)  be given by p(X)  =  (Ai  -  ArHA2  -  Ar  • • -(A^ -  A r^ (2.116) where m i  . . .  m^^ are the algebraic multiplicities of the distinct eigenvalues  Ai... \p  of A  respectively. It can be shown that m(A)  =  (A -  AO^KA -  A2)^^---(A  -  A . r^ (2.117) where  1 <  /i/  <  mt i  =  \... p. The only unknowns  left  to determine the minimal polynomial  of A are  )Lti... fjip in (2.117). These can be determined in several ways. The  next  result  is  a  direct  consequence  of  (2.86).  Let  A  be  similar  to A  and let  m(A) be the minimal  polynomial  of A. Then  m(A)  =  m(A). This result  in  turn justifies  the following  definition.  Let d^  G L{V V).  The minimal polynomial  of si  is the minimal polynomial of any matrix A that represents  M. To develop the Jordan canonical form  (for  linear transformations  with  repeated eigenvalues) we need to consider several additional preliminary results that are im portant in their own right. Let ^  G L(V; V)  and let /(A)  be any polynomial in A. Let M'f  =  {v : f(A)v  = 0}. It can be shown that Xf  is an invariant linear subspace of V under ^.  In particular let Ai... Ap be the distinct eigenvalues  of ^4 G L(V V)  and for  j  =  1... p  and for any positive integer q let Xj  =  {v : (^  -  Xj^)^v  = 0}. (2.118) In view  of the  above result  it follows  that Xj  is  an invariant  linear  subspace  of V under  ^. Next  let  ^  G  L(V V)  let  Vi  and  V2 be  Hnear  subspaces  of  V  such  that  V  = Vi  0  V2 and let ^1  be the restriction  of 6?^ to Vi. Let /(A)  be any polynomial in A. It can be shown that if si  is reduced by  Vi  and  V2 then for all v^  G  V\  f(sii)v^  = f(d)vK Next  let  y  be  a  vector  space  over  ^  and  let  M  G  L(VV).  Let  m(A)  be  the minimal  polynomial  of  ^  as  given  in  (2.117). Let  ^(A)  =  (A -  Ai)^i  let  /z(A)  = (A -  A2)^2.. .(A -  Ap)^/' if/?  >  2 and let /z(A)  =  1 if p  =  1. Let 6^1  be the restric tion of si  to jNff S i.e. div  =  siv  for  all v G >f/'\  Let J/t  =  {v G  V : h(si)v  = 0}. Using the preceding results we can show that (i) V  =  X{^^  ®M  and (ii) (A -  AO^^ is the minimal polynomial of  ^ 1. These  make  it  possible  to  prove  the  next  result  which  is  called  the  primary decomposition  theorem for  linear  transformations  and which we state next. Let  V be  an  n-dimensional  vector  space  over  C  let  Ai... A^ be  the  distinct L{V V) let the characteristic  and minimal polynomials of ^  be eigenvalues ofsi^ k^p and m(A)  =  (A -  AO^^ • • -(A -  A.)^^ respectively.  Let Vi  =  {v : A^-i)A^'v  =  0} i  = l...p. (2.119) Then  (i)  Vi i  =  1.../?  are  invariant  linear  subspaces  of  V  under  si  (ii)  V  = Vi  ©  • • •  0  Vp  (iii)  (A -  A/)^'  is  the  minimal  polynomial  of  sit  where  ^i  is  the restriction  of si  to Vi and (iv) dim Vi  =  mi i  =  \... p. 134 Linear Systems The above result  shows that we can  always present  ^  G L(y V)  by  a matrix in block  diagonal  form  where  the  number  of  diagonal  blocks  is  equal  to  the  number of distinct  eigenvalues  of  ^.  We will next consider  a convenient  representation  for each of the diagonal  submatrices A/. It may turn out that one or more of the  subma-trices A/ will be diagonal.  The next result tells us  specifically  when  ^  G L(y V)  is representable by a diagonal matrix. Let  V  be  an  n-dimensional  vector  space  over  C  and  let  ^  G L(VV).  Let Ai...App  <  n  be  the  distinct  eigenvalues  of  ^.  Then  there  exists  a  basis  for V  such that the matrix A of ^  with respect to this basis is diagonal if and only if the minimal polynomial for ^ is of the  form m(A)  =  (A-Ai)(A-A2)---(A-Ap). (2.120) N.  Nilpotent Operators Let us now proceed by considering  a representation  for  each  of the ^-  G L(Vi- Vi) in the primary  decomposition  theorem presented in Subsection M so that the block diag onal  matrix  representation  of  ^  G L(y V)  is  as  simple  as possible.  To  accomplish this we need to define  and examine nilpotent operators. Let ^  G L(y V).  Then ^ is said to be nilpotent  if there exists an integer  q>Q such  that  ^^  =  ^.  A  nilpotent  operator  is  said  to  be  of  index  q  if  ^^  =  ^  but Recall that the primary decomposition theorem enables us to write V =  Vi 0  • • • 0 is nilpotent  on  Vi. If  we  let then ^-  =  A /^  + JVi. Now A /^  is clearly represented by a diagonal Vp. Furthermore  the linear  transformation  (^-  — Xi/) JVi = £^i — Xi/ matrix. However the transformation  jVi forces  the matrix representation  of ^-  to be in general nondiagonal. Therefore the next task is to seek a simple representation of the nilpotent operator jVi. In the next few  results which are concerned  with properties  of nilpotent  opera tors we drop the subscript / for  convenience. Let JV  G L(W W )  where W is an m-dimensional  vector  space. It can be  shown is  a nilpotent  linear  transformation  of  index  q and  if  w G W is  such  that that  if  ^ JV^~^W  7^ 0 then the vectors w JVw^...  JV^~^w  in W are linearly  independent. Next we examine the matrix representation  of nilpotent  transformations. Let  W be  a  ^-dimensional  vector  space  and  let  JV  G L(WW)  be  nilpotent  of index q. Let w^ G W be such that JV^~^w^  ^  0. It can be shown that the matrix N  of ^  with respect to the basis .Mp}^m^^ A'l-^w^. is  [J^i-{^^ •VO N-0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 ... ... ... ... 0 0 0 0 0 0 1 0 (2.121) The  above result  characterizes  the matrix  representation  of  a nilpotent  linear trans formation  of index ^ on a ^-dimensional  vector  space. The next task is to  determine the representation  of a nilpotent  operator of index  7 on a vector  space of  dimension m  where  y  <  m.  It  is  easily  shown  that  we  can  dismiss  the  case  y  >  m  i.e.  if ^  G L(W W)  is nilpotent of index  7 where dim W  = m then  y<m. Now  let  W be  an  m-dimensional  vector  space  let J{  G  L(WW)  let  y  be  any positive integer and let W^  =  {w:  J^w  =  0} dim Wi  =  h W2  =  {w\  M^w  =  0} dim W2  =  h Wy  =  {w:  X^w  =  0} dim W^  =  L. (2.122) 135 CHAPTER  2: Response of Linear Systems Also  for  any  /  such  that  1 <  /  <  7  let  {w^...  w^}  be  a  basis  for  W  such  that {w^...  w^'}  is  a basis  for  Wi.  It  can  be  shown  that  (i)  Wi  C  W2 C  ---  C  Wy  and (ii) {w^...  w^'-i Xw^^^^...  Xw^^+^} is a linearly independent  set of vectors in  Wi. The next result which is the principal result of this subsection is a consequence of the above results. Let W be an m-dimensional vector space over C and let X  ^  L(W W) be nilpo-tent  of  index  7.  Let  Wi  =  {w  : J^w  =  0}..  .Wy  =  {w  : Ji^w  =  0} and  let  //  = dim Wi i  =  1... 7. Then there exists  a basis in  W such that the matrix A^ of >r is of block diagonal  form. A^i A^  -0 Nr 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 . . . . .  0 .  0 .  0 .  0 0 0 1 0 (2.123) (2.124) where Ni  = /  =  1...  r where r  == /i Ni is a ki X ki matrix  1 < fc^ <  7 and  ki is determined in the following  manner: there are ly  -  ly_i 2li  -  li+i -  li-i 2 / 1 - /2 7 X7  matrices i X / matrices 1 x1  matrices. /  =  2  . . .  7-  \ (2.125) The basis for W consists of strings of vectors of the form M^^ ^w^...  w^ J^^^  iy^;2^ ..  .w' \  . . .K^'  ^W^ ...W\ O.  The Jordan  Canonical  Form The results of the preceding  three  subsections  can be used to prove the next  result which yields the Jordan canonical form  of matrices. Let  Y  be  an  /i-dimensional  vector  space  over  C  and  let  ^  G  L(V; V). Let  the characteristic polynomial of ^4 be p{K)  =  (Ai -  X)^^ • • -(A^ -  A)^^ and let the mini mal polynomial of d  be m(A)  == (A -  Ai)^i • • -(A -  A^)^^ where Ai... Ap are the distinct eigenvalues of 6^. Let Vi  =  {v G  V  : (si-Ai3)^^v  =  0}. Then (i)  Vu...Vp are invariant subspaces of y under 5i; (ii) y  == y i © - - - © ^ ^; (iii)dim V/  =  m/ /  = \...  p\  and (iv) there exists a basis for  V such that the matrix Aof  d^ with respect 136 Linear Systems to this basis is of the  form "Ai 0 0  A2 • • • • 0 0 A  = 0 0 • •  Ap (2.126) where A- is an m  X m- matrix of the  form Ai  -  A//  +  Ni (2.127) and  where Ni  is the  matrix  of  the nilpotent  operator  {sit  -  A/^)  of index  [xi on V/ given by (2.123) and (2.124). Parts (i) to (iii) of the above result are restatements of the primary  decomposition theorem. From this theorem we also know that (A -  A/)^' is the minimal  polynomial of sii  the restriction of d^ to Vi. Hence if we let Mi =  sii-Xi3 operator of index  />t/ on Vi. We are thus able to represent Xi  as shown in (2.124). then J^T/ is a nilpotent A little extra work  shows that the representation  of ^  G L(V; V) by a matrix  A of the form  given in (2.126)  and (2.127) is unique except for the order in which  the block diagonals  Ai. The matrix A  of ^ canonical form  ofd. Ap  appear in A. G  L(K  V)  given by (2.126) and (2.127) is called the  Jordan An  example We conclude this section by considering  a specific  case. We let  V  =  R^  we  let {e^...  ^^} be the natural  basis  for  V  and  we  let L(V V) be represented by the matrix A  -1 0 2 2 0 0 1 0 1 1 0 0 0 -1 -1 0 2 -1 0 0 0 1 0 -1 2 0 0 1 1 0 -1 1 1 0 2 3 0 -6 3 0 1 4 0 0 0 0 0 0 1 with respect to {e^...  e^}. We wish to determine the matrix A that represents si  in the Jordan canonical  form. We  first  determine  that  the  characteristic  polynomial  of  si  is  det (A  -  A/)  = det(si  -  \3)  =  (1 -  A)^. This indicates that Ai  =  1 is the only distinct  eigenvalue of d  having  algebraic  multiplicity  m\  =  7. To find the minimal polynomial  of  si we let >r  =  ^  -  Ai^ where 3  is the identity operator in L{VV).  The representation of Ji  with respect to the natural basis is N  =  A-I = -2 0 2 -2 0 0 -1 0 0 1 0 0 0 -1 -1 0 1 -1 0 0 0 1 0 -1 1 0 0 1 1 0 -1 1 0 0 2 3 0 -6 3 0 0 4 0 0 0 0 0 0 0 137 CHAPTER 2: Response of Linear Systems The minimal polynomial will be of the form m(A)  =  (X-iyK  We need to determine the smallest  vi  such that m(A  —  A/)  =  m(N)  =  O. We first obtain '2  _ "0 0 0 0 0 0 0 -1 0 1 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 -3 3 0 0 0 0 0 0 0 0 0 0 Next  we  obtain  N^  =  0  and  therefore  z^i  =  3  and  Jvf  is  a  nilpotent  operator of  index  3.  We  see  that  V  =  J{f  [refer  to  (2.118)  for  the  notation  >ff].  We  will now  apply  the  results  of  Subsection  F  to  obtain  a  representation  for  M  in  this space. We  let  Wi  =  {v  : Xv  =  0} W2  =  {v  : J^^v  =  0} and  W3  =  {v  : J{\  = 0} and  we  observe  that  A^  has  three  linearly  independent  rows.  This  means  that  the rank  of  jf  is  3  and  therefore  dim(Wi)  =  h  =  4.  Similarly  the  rank  of  X^  is  1 (since  N^  has  one  linearly  independent  row)  and  so dim (^2)  =  h  =  6.  Clearly dim(W3)  =  h  =  1. We conclude  that }(  will have  a representation  N  of the  form (2.123) with r  =  A. Each of the Nt will be of the form (2124). There will be h-h  = 1 (3 X 3) matrix Ik  - h -h  =  1 (2 X 2) matrix  and 2/i  -  fc  -  2 (1 X 1) matri ces  [see  (2.125)]. Hence there is a basis  for  V such that >f  may be represented  by the matrix 0  0  0  0 0  10 0  0  10 0  0  0 0  0  0  0  0  0  0 7 V = |0 0 00 1 00 0  0  0  0  0  0  0 0  0  0  0  0  0  0 0  0  0  0  0  0  0 The corresponding basis will consist of strings of vectors of the form X'^v^  Xv^  v^ >fv^ v^ v^ v^. We  will  represent  the  vectors  v^v^ v-^  and  v^  by  17^17^17^  and  iq^  their coordinate  representations  respectively  with  respect  to  the  natural  basis  {e^  e^ e^  e^  e^  e^  e^}  in  V.  We  begin  by  choosing  v^  G  W3  such  that  v^  ^  W2 i.e.  we  determine  a  iq^  such  that  N^rj^  =  0  but  N^r]^  9^ 0.  The  vector  TJ^ = (0 1 0 0 0 0 0)^  will  do. We  see  that  Nr]^  =  (0 0 1 0 0 0 - 1 )^  and  A^^^^  = ( - 1  0 1 - 1  0 0 0)^. Hence Jiv^  E  W2 but Xv^  ^  Wi  and  X^v^  G  Wj. We  see there  will  be  only  one  string  of  length  3  and  therefore  we  choose  next  v^  E  W2 such that v^  ^W\.  Also the pair {Jfv^  v^} must be linearly independent. The vector T/2  - (1 0 0 0 0 0 0)^  will  do.  Next  we  have  Nr]^  -  (-2  0 2 - 2 0 0 - 1 ) ^ and therefore  Xv'^  E  Wi.  We complete the basis for  V by  selecting  two more vec tors v^ v"^  E  Wi  such that {Ji^v^ M'v^ v^ v^} are linearly independent. The vectors 7]^ =  (0 0 - 1  - 2  1 0 0)^ and  ry^  =  (1 3 1 0 0 1 0)^ will do. It now follows  that the matrix P  =  [N^rj^ Nrj^  17^ Nrj'^ 17^ 17^ r]"^] is the ma trix of the new basis with respect to the natural basis. The reader can readily  verify 138 Linear Systems thatA^  =  P~^NPwhQYQ P  = 1 0 1 1 0 0 0 0 0 1 0 0 0 -1 0 1 0 0 0 0 0 -2 0 2 -2 0 0 -1 1 0 0 0 0 0 0 0 0 -1 -2 1 0 0 1 3 1 0 0 1 0 p-'  = 0 0 0 0 1 0 0 0 0 1 0 0 0 0 2 1 0 -1 0 0 0 1 1 0 -1 -1 0 0 4 3 0 -3 -2 1 0 -2 -1 -3 1 -1 0 1 2 0 0 -1 0 0 0 Finally the Jordan canonical form  for A  is now given by A  =  A^  +  /  (recalling that  the matrix  representation  for  ^  is the  same  for  any basis  in  V). Therefore  we have A  = 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 :0 1  i  0 1  1  0 o! 1 O 'O 0  0 0  0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 It is easily verified  that A  =  P  ^AP.  In general it is more convenient  as a check to show that PA  =  AP. 2.3 LINEAR HOMOGENEOUS AND NONHOMOGENEOUS  EQUATIONS In this section we consider systems of linear homogeneous ordinary differential  equa tions X =  A(t)x and linear nonhomogeneous  ordinary differential  equations X =  A(t)x  +  ^(0. (LH) (LN) In Theorem 12.1 of Chapter 1 it was shown that these systems of equations subject to initial conditions x(to)  =  XQ possess unique solutions for every (to XQ) E  D  where D  =  {(t x):t  GJ  =  (ab)xE  R""} and where it is assumed that A  G  C(/ i?"^'") and g  E  C(J R^)  These solutions exist over the entire interval /  =  (a b) and they depend  continuously  on the initial  conditions.  Typically  we  will  assume  that  /  = (-00 00). We note that (/)(0  =  0 for  all r E  /  is a solution of (LH)  with (l)(to)  =  0. We call this the trivial solution.  As in Chapter  1  (refer to Section 1.13) we recall that the preceding  statements  are also true when A(t)  and g(t)  are piecewise  continuous o n /. In the sequel we sometimes will encounter the case where A(t)  =  A is in Jordan canonical  form  that  may  have  entries  in  the  complex  plane  C. For  this  reason  we will  allow  D  =  {(t x):t  E  J  =  (ab\xE /?"(or  x  E  C")}  and  A  E  C(J  JR"^^'^) [or A  E  C(J C^^^)]  as needed. For the case of real vectors the field of  scalars  for the X-space  will  be  the  field  of  real  numbers  {F  =  R)  while  for  the  case  of  complex vectors  the  field  of  scalars  for  the  x-space  will  be  the  field  of  complex  numbers (F  =  C).  For  the  latter  case  the  theory  concerning  the  existence  and  uniqueness  of solutions  for  (LH)  as presented  in Chapter  1 carries  over  and  can be modified  in the obvious  way. 139 CHAPTER 2: Response of Linear  Systems A.  The  Fundamental  Matrix Solution  space We  will  require  the  following  result. THEOREM  3.1.  The set of solutions of  (LH)  on the interval /  forms  an  ^-dimensional vector  space. Proof  Let  V  denote  the  set  of  all  solutions  of  (LH)  on  /.  Let  a i  a2  G F  and let  0102  G V.  Then  ai0i  +  0(202  ^  V  since  {d/dt)[ai(l)i  +  0(202]  =  a\{d/dt)(\)\{t)  + a2{d/dt)(^2{t)  =  aiA(r)0i(r)  +  a2A(r)02(O  =  A(r)[ai0i(r)  +  0(202(0]  for  ^^  ^ ^  J-This shows that V is a linear subspace of/?". Hence V is a vector  space. To complete the proof  of the theorem  we must  show that V  is of dimension  n. To accomplish this we must find n linearly independent solutions 0 i  . . . 0„ that span V. To this  end  we  choose  a  set  of  n  linearly  independent  vectors  XQ...XQ in  the ^-dimensional x-space (i.e. in R"^ or C"). By the existence results in Chapter  1 if ^o  ^J^ then there exist n solutions 0 i  . . . 0„ of  (LH)  such that 0i (^o) =  -^O' • • •' ^«(^o) =  -^0-  ^^ first  show  that  these  solutions  are  linearly  independent.  If  on  the  contrary  these  solu tions  are  linearly  dependent  there  exist  scalars  a i  . . . a„  G F  not  all  zero  such  that E Li  ^?0KO  = 0 for all t  G /.  This implies in particular that ^4=1  oci^i{to)  = E Li  ^i^o  ^ 0.  But  this  contradicts  the  assumption  that  {XQ ...  XQ}  is  a  linearly  independent  set. Therefore  the solutions  0 i  . . . 0„ are linearly  independent. To conclude  the  proof  we  must  show  that  the  solutions  0 i  . . . 0„  span  V.  Let  0 be any  solution  of  (LH)  on the interval /  such that  0(^o)  =  -^o-  Then there exist  unique scalars  a i  . . . a„  G F  such that xo  •• Z^^i^Qy since by assumption the vectors XQ ...  XQ form  a basis for the x-space. Now n is  a  solution  of  (LH)  on  /  such  that  \j/{^o) =  ^o-  ^^t  by  the  uniqueness  results  of Chapter  1 we have that „ Since 0  was chosen arbitrarily it follows  that the solutions  0 i  . . . 0„ span V. • EXAMPLE  3.1.  Let A (t)  for  (LH)  be given by Ait): -1 0 e^n -1 (3.122) It  is  easily  verified  by  direct  substitution  that  0i(O  =  (e  \0) are  solutions  oi  {LH)  onJ= ^) (—00^00)  for  the  present case.  Furthermore  it  is  easily and  (1)2  =  {\e\e 140 Linear  Systems shown  that  (pi and (p2  [defined  on /  =  (—^^)]  are hnearly  independent  (using  e.g. the  method  in Subsection  2.2B).  In view  of Theorem  3.1 the solutions  of  (LH)  with A{t)  specified  by (3.1) form  a two-dimensional  vector  space and {0i ^2} is a basis for this  solution  space.  Since  {0i02}  spans  this  vector  space  all  solutions  of  (LH) with  A{t)  specified  by  (3.1)  are  of  the  form  ^(t)  =  ai^i{t)  +  ^202(0  =  {oc\e~^ + 0^2 '].\J e^ 0^2^  0  where  a i 0^2 G R. Fundamental  matrix  a nd  properties Theorem  3.1 enables  us to make  the following  definition. DEFINITION  3.1. A set of n linearly independent solutions of {LH) on / {0i... 0„} is called di fundamental  set of solutions  of {LH) and ihQnxn  matrix O=[0i02...0n] "011 021 012 022 ••• ••• 01n 02n is called a. fundamental  matrix  of  {LH). Pnl (t>n2 We  note  that  there  are infinitely  many  different  fundamental  sets  of  solutions  of {LH)  and hence  infinitely  many  different  fundamental  matrices  for  {LH).  We now study  some  of the basic  properties  of fundamental  matrix. In the next  result X  =  [xij] denotes  an n x n matrix  and the derivative  ofX  with respect  to  t  is  defined  as X  =  [xij]. Let A{t)  be  the nx  n  matrix  given  in  {LH).  We call the system  of n^  equations X=A{t)X (3.2) a  matrix  differential equation. THEOREM  3.2.  A fundamental  matrix  O of  {LH)  satisfies  the matrix  equation (3.2) on the interval  /. Proof  WehaveO=[0i02...0n]  =  [A(O0iA(O02...A(O0n]=A(O[0i02...0n] =  A{t)^. • The  next  result is cailcd  Abel's formula. THEOREM  3.3.  If O is a solution of the matrix  equation  (3.2) on an interval /  and T is any point of /  then det 0{t)  = det 0 ( T) exp / trA{s)ds for  every  t  e  J.  [tr  A{s)  =  tr  [aij{s)]  denotes  the  trace  of  A{s)  i.e.  tr  A{s) I"=i«;7(*)-] Proof Let ^  =  [(j^ijl Then ^tj  = Zl=i  atkit^kj^  Now [det<^(t)]  = dt </>ii 021 <P 12 022 • . .. .. 01« 02n 011 021 012 022 . • . •  01n •• 02n + </>nl 0n2 • ..  0«n 0nl 0n2 • •  <Pnn 141 CHAPTER  2: Response of Linear Systems 011 021 012 022 . -. 01« • •  •  02n f  ••• + 0nl 0«2  • 0nn 021 0nl 022 0-•n2 011 012 T.l^iaik{t)(j>kn{t) 02n (t>nn 01« Y.l=iCl2k{t)4>k\ 2 ^ =1  a2k(t)<Pk2 • Y.l=iCl2k{t)4>kn 032 0 ril 4>'in 4>n 011 021 012 022 01n 02n 031 0nl + 0n-ll 0«-l2 \Y.l=\Clnk(t)4>kl Y.l=\Clnk{t)(f)k2 . .. • •. 0n-ln YJ[=iClnk{t)(j>kn The first term in the above sum of determinants is unchanged if we subtract from the \-{ain first  row the quantity {an times the second row) + (^13 times the third row) + times the nth row).  This yields 011 021 012 022 . . . . . . 01n 02n (2ii(O011 ai 021 zzz l(O012 022 0nl 0n2 . ..  0«n (knl 0n2 = 2n(t)det<^(t). i . . a\ l(O01« 02n (f>nn Repeating the above procedure for the remaining terms in the above sum of determinants we have (d/dt)[det ^(t)]  =  an(t)det  ^(t)  + a22(t)det ^(t)  +  • • • + UnniOdet ^(0  = [tr A(t)] det ^{t).  This imphes that det <l>(0 =  det ^(r) exp [{/ tr A{s)  ds]. m Since  in  Theorem  3.3  r  is  arbitrary  it  follows  that  either  det  0 (0  7^ 0  for  all t  E  J  or det  0 (0  =  0 for each t  ^  J.  The next result provides  a test on whether  an nX  n matrix 0 (0  is a fundamental  matrix of  {LH) THEOREM 3.4.  A solution O of the matrix equation (3.2) is a fundamental  matrix of {LH) if and only if its  determinant is nonzero for alH E  /. 142 Linear Systems Proof  If <l> =  [(/>i (/)2... (/>n] is a fundamental  matrix for (L//) then the columns of $ (/)i...  (/>„ form  a  hnearly  independent  set. Now let (/> be a nontrivial  solution of {LH).  Then by Theorem 3.1 there exist unique scalars a i  . . . a„  E  F not all zero such that  (f) =  2 ; =! oij4'j  =  ^^» where  a^  =  (ai...  an). Let t  =  T G J.  Then  ^ ( T)  = <I>(T)(2 which is a system of n linear algebraic equations. By construction this system of equations has a unique solution for any choice of (^(r). Therefore  det 0 ( T)  T^ 0. It now follows  from  Theorem 3.3 that det ^(t)  ^  0 for any t G  /. Conversely let ^  be a solution of (3.2) and assume that det ^{f)  T^ 0 for all ^ G  /. Then the columns of <I> are linearly independent for all ? G /. Hence ^  is a fundamental matrix of {LH). • It is emphasized  that  a matrix  may have  identically  zero  determinant  over  some interval even though its columns are linearly independent. For example the columns of the  matrix 0(0  = '1 t 0  1 0  0 f t 0 are  linearly  independent  yet det  4>(0  =  0  for  all t  G  (-oo oo). In  accordance  with Theorem  3.4 the above matrix  cannot be a fundamental  solution of the matrix  equa tion  (3.2) for any continuous  matrix  A{f). EXAMPLE 3.2.  Using the linearly independent  solutions <p 14*2 given in Example 3.1 we obtain 0(0 2" e~ 0 t  G  ( - 0 0 00) as  a  fundamental  matrix  of  {LH)  with  A{t)  given  by  (3.1).  It  is  easily  verified  that the  matrix  O  satisfies  the matrix  equation  O  =  AO. Furthermore  det 0 (0  =  e~^^  ¥^ 0t  G  (-0000)  and det  0 (0  =  det  0(T)exp[|/rr  A{s)ds]  =  ^ ~ ^ ^ e x p [ | / -2 J^]  = ^-2r^-2(r-T)  ^  ^-2t^ f ^  ^_Qo^ 00^^ ^g expected. • THEOREMS.5.  If O is a fundamental  matrix of {LH) and if C is any nonsingular con stant nXn  matrix then OC is also a fundamental  matrix of {LH).  Moreover if ^  is any other fundamental  matrix of {LH) then there exists a constant nX  n nonsingular  matrix P such that ^  = OP. Proof.  For the matrix OC we have  {d/dt){^C)  =  OC  =  [A(0O]C  =  A(0(OC) and therefore OC is a solution of the matrix equation (3.2). Furthermore since det 0 (0  ^  0 for ? G /  and det C 7^ 0 it follows  that det [O(0C]  =  [det ^{t)]{det  C) 7^ 0 ^ G 7. By Theorem 3.4 OC is a fundamental  matrix. Next  let  '^  be  any  other  fundamental  matrix  of  {LH)  and  consider  the  prod uct  O'HO"^.  [Notice  that  since  det^{t)  7^ 0 for  all t  G 7  then  O"H0  exists  for all t  G J.]  Also  consider  0 0 ~^  =  /  where  /  denotes  the ^  X n identity  matrix.  Differ entiating  both  sides  we  obtain  [(<i/J0O]O"^  -H O[(J/^0O"M  =  0  or  {d/dt)^~^  = - 0 - ^ ( ^ / ^ 0 0 1 0 - 1.  Therefore  we  can  compute  {d/dt){^-^'¥)  =  <^~^[{d/dt)'i^] + [ ( J / J O O - ^^  = 0 - ^ ( 0 ^ - { O - i [ ( ^ / J 0 O ] O - i }^  =  O - i A ( 0 ^ - ( O - i A ( 0 O O - i )^ • =  0-1 A ( 0^  -  0-1 A ( 0^  =  0. Hence O ' ^^  =  P or ^  =  OP. EXAMPLES.3.  It is easily verified  that the system of equations XI  =  5x -  2x2 X2  =  4Xl  —  X2 (33) has two linearly independent solutions given by 4>i(t)  = {e^\ e^^Y <^2(0 ^  (^^ 2^0^. and therefore the matrix ^(0 2e' (3.4) is a fundamental  matrix of (3.3). Using Theorem 3.5 we can find the particular fundamental  matrix "^ of (3.3) that satisfies  the initial condition '^(0)  = / by using 0(0  given in (3.4). We have '^(0) = /  = O(0)C or C = 0-1(0) and therefore 143 CHAPTER 2: Response of Linear Systems II II C = and ^(0  = OC = {2e^' -e') i-e^'  -he') {2e^' -  2e')  {-e^'  + 2e') B.  The State Transition Matrix In Chapter  1 we used the Method  of Successive  Approximations  (Theorem  10.9) to prove that for every  {t^  XQ) G J X R^ X = A(t)x (LH) possesses  a unique solution of the  form (/)(/ to Xo)  =  ^(t to)Xo such  that  (l)(to to xo)  = xo  which  exists  for all t G J where  <E>(r ^o) is the  state transition  matrix  (see Section  1.13). We derived  an expression  for ^(t to) in  series form  called the Peano-Baker  series  [see Eq.  (13.3) of Chapter  1] and we  showed that 0(r ^o) is the unique solution of the matrix differential  equation ^-^(t to) =  A(t)^(t  to\ dt where ^{to  to) = I for all t G  / (3.5) (3.6) We provide an alternative  formulation  of state  transition  matrix  and  we  study some of the properties of such matrices. In the following definition we use the natural basis {ei  e2...  Cn) that was defined  in Section 2.2. DEFINITIONS.2.  A fundamental matrix O of (LH) whose columns are determined by the linearly independent solutions (^i... (/>„ with ^ l ( ^)  =  ei  . ..(f)n{tQ)  =  en. to E  / is called the state transition matrix O for (LH). Equivalently if ^  is any fundamental matrix of (LH) then the matrix O determined by <t>(t to) =  ^ ( 0 ^ " k ^) for all t to  G / is said to be the state transition matrix of(LH). • We  note  that  the  state  transition  matrix  of (LH)  is uniquely  determined by the  matrix  A(t) and  is  independent  of the  particular  choice  of the  fundamental 144 Linear  Systems matrix.  To  show  this  let  ^i  and  ^2  be  two  different  fundamental  matrices  of (LH).  Then  by  Theorem  3.5  there  exists  a  constant  n  X  n  nonsingular  matrix P  such  that  ^2  =  ^iP.  Now  by  the  definition  of  state  transition  matrix  we h a v e 3 ) ( U o)  =  ^ 2 ( 0 [ ^ 2 ( ^ ) ] -'  =  %(t)PP-'[%(to)]-' =  ^ i ( 0 [ ^ i a o ) ] "^  This shows  that  ^(t to) is  independent  of  the  fundamental  matrix  chosen. EXAMPLE  3.4.  In  Examples  3.1  and  3.2  we  showed  that  for  (LH)  with  A(t)  given by (3.1) ^(0 = 2^ 0 tG(--X is  a  fundamental  matrix.  For  this  case  the  state  transition  matrix  ^(t  to)  is  com puted  as ^(tto)  = ^(t)^-\to) 0 0 -t+3to The reader  should verify  that this matrix  satisfies  Eqs. (3.5) and (3.6). Properties  of  the  state  transition  matrix In the following  we  summarize  some of the properties  of  state transition  matrix. THEOREM  3.6.  Let  ^o ^  J  let  (l>(to)  =  XQ and let 0(^ ^o) denote  the  state  transition matrix for  (LH)  for all /  E  J.  Then the following  statements  are true: (i)  0(f  ^0) is  the  unique  solution  of  the  matrix  equation  (dldt)^(t to)  =  A(t)^(t  to) with 0 ( ^  to)  =  Ithe  nX  n identity  matrix (ii)  <l>(r ^0) is nonsingular for  all ^ E  /. (iii)  For any taTE (iv)  [<t>(t to)V^  =  ^-\t (v)  The unique  solution (/>(r to xo) of  (LH)  with  (/)(^ ^o xo) to)  =  <^(to t) for  all t to E  /. /  we have (^(t r)  =  ^(t  cr)^(cr r)  (semigroup property) Xo specified  is  given by Proof 4>(t to Xo) =  ^(t  to)xo for all t  E  /. (3.7) (i)  For  any  fundamental  matrix  of  (LH)  say  ^  we  have  by  definition  ^(t  to)  = = ^(toJo)  = the  choice  of  "¥.  Therefore  d^(tto)ldt =  A(t)^(tto)' Furthermore ^ ( 0 ^ ~ H ^ ) ^(t)'^-\to) ^(to)^-\to) independent  of =  A(t)^(t)^-\to) =  L (ii)  For any fundamental  matrix of (LH)  we have that det "^(t)  7^ 0 for all ^ E  7. There fore det ^(t  to)  =  det  [^(0^~H^o)]  =  det  'i^(t)det'i^~\to)  7^  0 for  all t to E  /. (iii)  For any fundamental  matrix "^ of (LH)  and for the state transition matrix O of  (LH) for =  ^(ta)<i>(ar) wehave^(rT)  =  - ^ ( 0 ^ " ^ ^)  =  'i^(t)^-\a)'¥(a)^~\T) any taTE. J. (iv)  Let '^  be any fundamental  matrix  of (LH)  and let O be the  state transition  matrix of  (LH).  Then  [^(tJo)]'^  =  WO^(^o)"M"^  =  ^ ( r o ) ^ " kO  =  0 ( ^  0  for  any t to E  7. (v)  By the results established  in Chapter  1 we know that for every  (^0. ^0)  ^  D  (LH) has  a unique  solution  0 (0  for  all  r E  7  with  </)(?o)  =  XQ. To verify  (3.7) we  note that 4)(t)  =  [d^(t  to)/dt]xo  =  A(t)<^(t to)xo  =  A(t)(l)(t). m EXAMPLE 3.5.  Let x(^o) =  (ai a2)^. In view of Example 3.4 we have for (L//) given in Example 3.1 (/)(r to Xo)  = 0 ^-(^-^o) and therefore 145 CHAPTER  2: Response of Linear Systems (t>2(t to Xo)  =  e-^'-'^^a2. (3.8) (3.9) We  can  verify  the  above  example  by  simple  integration.  In  doing  so we  first obtain  (^2(^> ^> -^o) by  integrating  both  sides  of  i:2  "=  ~-^2  and  by  using  the  initial condition X2(to) =  CLI- Next we obtain (piit to XQ) by integrating both sides of ii  = -x\  +  e^^cl)2(t to XQ)  and  using  the  initial  condition  (x\(to)  ^2(^0))^  =  (ai ^2)^. Note that this procedure for  solving an initial-value problem determined by (LH)  is valid for any triangular matrix  A(t). In  Chapter  1 we  pointed  out  that  the  state  transition  matrix  ^(t  to) maps  the solution (state) of (LH)  at time to to the solution (state) of (LH)  at time t. Since there is no restriction  on  t relative  to  to (i.e. we  may  have  t  <  tot  =  to or t  >  to) we can "move forward  or backward" in time. Indeed given the solution (state) of  (LH) at time  t we can  solve the  solution  (state)  of  (LH)  at time  to- Thus  x(to)  =  xo  = [^(t  to)V^(l>(t to Xo) =  0(^0 t)(l)(t to Xo). This  "reversibility  in  time" is  possible because  ^~^(t  to)  always  exists.  [In  the  case  of  discrete-time  systems  described by  difference  equations  this  reversibility  in  time  does  in  general  not  exist  (refer to Section  2.7).] C.  Nonhomogeneous  Equations In Section  1.13 we proved the following  result  [refer to Eqs. (13.8) to (13.10)]. THEOREM 3.7.  Let to E 7 let (to Xo)  E D and let ^(t  to) denote the state transition matrix for  (LH) for  all r E  /.  Then the unique solution (/)(r to xo) of (LN)  satisfying </>(^ to Xo)  = Xo is given by ^(^ ^0 ^o)  = ^(t  to)xo  + ^(t  v)8(v)dV' (3.10) As pointed out in Section  1.13 when  xo  =  0 (3.10) reduces to ct>(tto0) ^  cl>p(t)  =  f  ^(ts)g(s)ds. (3.11) Jto and when xo  #  0 but g(t)  =  0 (3.10) reduces to ct)(t to Xo) =  (l>h(t)  =  ^(t  to)xo. (3.12) and  the  solution  of  (LN)  may  be  viewed  as  consisting  of  a component  that  is  due to the initial data xo and another component that is due to the forcing  term g(t).  We recall  that  cffp  is called  a particular  solution  of the nonhomogeneous  system  (LN) while (f)h is called the homogeneous  solution. There are of course other methods of solving (LN).  For example in the common approach  to solving  linear  differential  equations  all  solutions  (^(0  of  i  -  A(t)x  = 146 Linear  Systems g{t)  are  assumed  to  be  of  the  form  0(f)  =  ^^{t)  +  ^p{t)  where  0/^(f)  is  the  solution of  the  homogeneous  equations  x  — A{t)x  =  0  and  ^p{t)  is  a  particular  solution.  It  is not difficult  to  see that  0/^(f)  =  0(f  to)oc where  a  eR^ is to be determined  [compare to  (3.12)].  Therefore  (p{t)  =  <^{tto)a-\- (pp{t).  The  vector  a  is  determined  using the  initial  conditions  on  the  solution  (p{t)  of  x  — A{t)x  =  g{t)  namely  (p{to)  =  XQ. Substituting  (j){to) =  O(fo^o)oj  +  0p(fo)  we obtain  a  =  (j){to)  -  0p(fo)  =  -^o - (t>p{to)' The  solution  to  (LN)  with  0(fo)  =  -^o is  therefore  given  by  0(f)  =  0/^(f)  +  ^p{t)  = O(ffo)[0(^o)  -  0p(^o)] +  0p(O'  or 0 ( O = O ( f  f o ) ^o  +  [0p(O-^(^^o)0p(^o)]. (3.13) As  expected  the  "zero-state  response  of  {LN)''  obtained  by  letting  XQ =  0  in  (3.10) =  g{t)  with the  property and given in  (3.11) is a particular  solution  ^p{t)  ofx—A{t)x that  0p(fo)  =  0-  Furthermore  the  "zero-input  response  of(LN)'' obtained  by  letting g(t)  =  0 in  (3.10) is the  solution  0/^(f)  ofx  — A{t)x  =  0.  It follows  that  the  variation of  constants  formula (3.10)  is  a  special  form  of  expression  (3.13)  corresponding  to the chosen  particular  solution  (pp{t)' for  a different  choice  of  (pp{t) a  correspondingly different  expression  for  the  solution  (p{t)  is  obtained. EXAMPLE3.6.  In  (LN)  \QtxeR^to  =  0 and A(t). g(t)-0 X(0): The state transition matrix for A(f)  has been determined in Example  3.4  as Using  (3.12) we  obtain (l)h{ttoxo)=^{t0)x{0) W-e-^) using  (3.11) we have (i>p{ttQXQ)  =  j ^(tri)g(ri)dri te and using  (3.10) we  finally  obtain (l)(ttoXo)  =  (l)h(ttoXo)  +  (l)p(ttoXo)  = 'e-'{t-\)  +  y D.  H ow  to  Determine  O(ffo) To  solve  {LN)  or  {LN)  in  closed  form  we  require  an  expression  for  the  state  tran sition  matrix  O(ffo)-  We  have  seen  in  Example  3.5  that  when  A{t) is  triangular we  can  solve  {LH)  [and  hence  O(ffo)]  by  sequential  integration  of  the  individual differential  equations.  Eor  the  general  case  however  closed-form  determination  of 0(f  fo)  is not  possible. In  the  following  we  identify  another  important  class  of  matrices A{t)  for  which a  closed-from  expression  of  0(f  t^)  exists. THEOREM  3.8.  If for every  T^ we have A(t) I  A{r\)dr\\  = JT \ j  A{n)dr\ \JT A(t) (3.14) Ul CHAPTER 2: Response of Linear  Systems then '^(tT)  =  eirMv)dv  A^  +  y  _L CO ^ A(r])d7] (3.15) Proof  We  recall  that  the  general  term  of  the  Peano-Baker  series  [see  Eq.  (13.3)  in Chapter  1] is given by rt rsx A{si)\ A{S2)"' AySm)  dSfn ClSfy 'ds]. (3.16) k=l We wish  to  show  that under  the present  assumptions  expression  (3.16)  is equal  to  the expression 1 m! A(s)  ds To verify  (3.17) we will apply the identity A(r) A(s)  ds  dr  = 1 m +  1 |W+1 A(s)  ds (3.17) (3.18) repeatedly to (3.16). First however we verify  the validity of (3.18). To accomplish this we first observe that (3.18) is true for any fixed t  =  r.  Differentiating  the left-hand  side of (3.18) we obtain [Air  I  A(s)ds m d_ Jt dr  =  A(t)  I  A(s)ds (3.19) Differentiating  the right-hand  side of (3.18) we have that -im+O ^\ dt 1 m  +  1 [  A{s)ds 1 m +  1 A{t) A{si)ds2' A{Sni^{)ds A{sx)dsAA{i) I  A{s^)ds^ A{Sm-^{)dSm^X  + + A{s\)ds\' A{Sm)dSm\A{t)\ =  A(t) A(s)  ds (3.20) where in the last  step of  (3.20) the assumption  (3.14) has been used repeatedly.  Using (3.19) and (3.20) we obtain (3.18). To complete the proof of the theorem we apply (3.18) repeatedly to (3.16) to obtain (3.17) the general term of the Peano-Baker series (13.13) in Chapter  1. Indeed we have A(si) A(s2y A(Sm-1) A(Sm)  dSm  dSm-1  dSm-2' A(Sm-2)^  r Sm-4 I A(Sm-3):^ "  dSi A(s)  ds  ds. 'm-2'-'dSi A(s)  ds  dStn-3'"dSi A(si)r A(s2y = ^j'A(si)pA(s2y' =  •••  =  -^W ml  [J^ A(s)ds which was to be shown. This completes the proof of the theorem. 148 Linear Systems For the scalar case i.e. when A(t)  =  a(t)  relation  (3.14) is always true. Also when  A(t)  =  diag[aii(t)]  [i.e.  A(t)  is  a  diagonal  matrix]  relation  (3.14)  is  true. Furthermore  for  A(t)  =  A a constant  matrix  (3.14)  will  always  hold. The  reader can readily verify  that A(t)  given in Example  3.1 also satisfies  relation  (3.14). We  conclude  by  pointing  out  that  for  A  G  C[R R''^'']  (3.14)  is  true  if  and only if for all t and r.  We ask the reader to verify  the validity of this  statement. A(t)A(T)  =  A ( T ) A (0 (3.21) 2.4 LINEAR  SYSTEMS  WITH  CONSTANT  COEFFICIENTS In  this  section  we  consider  systems  of  linear  autonomous  homogeneous  ordinary differential  equations X =^  Ax and systems of linear nonhomogeneous  ordinary  differential  equations X =  Ax  + g(tl (L) (4.1) where  ;c G  /?" A  G  i^^^"  and  g  G  C(R  R""). In  the  special  case  when  A(t)  =  A system  (LH)  reduces to system  (L) and  system  (LN)  reduces to system  (4.1). Con sequently the results  of  Section  2.3  are applicable  to (L) as well  as to (LH)  and to (4.1)  as  well  as  to  (LN).  However  because  of  the  special  nature  of  (L)  and  (4.1) more detailed information  can be  determined. A.  Some Properties  of e^^ Let  D  =  {(t x)  \ t  E. Rx  Ei R^}.  In  view  of  the results  of  Section  1.13  it  follows that for every  (to XQ) G D the unique solution of (L) is given by (l)(t to Xo) =  ^ + 2 A*(f  -  ^o)* . =1 ^' Xo =  *(f  to)xo  =  0(r  -  to)xo  =  e^^'-'^ho (4.2) where  ^(t  —  to)  =  e^^^~^^^  denotes  the  state  transition  matrix  for  (L).  [By  writing ^(ty  to)  =  ^(t  -  to) we are using a slight abuse of notation.] In  arriving  at  (4.2)  we invoked  Theorem  10.9 of  Chapter  1 in  Section  1.13  to show that the sequence {</)m} where (^m(r fo ^o)  = . =1 ^' ^0  -  Sm(t -  to)xo (4.3) converges  uniformly  and  absolutely  as m ^  oo to the unique  solution  ct)(t to xo) of (L)  given by  (4.2) on compact  subsets of R.  In the process of arriving  at this result we also proved the following  results. THEOREM4.1.  Let A be a constant nX n matrix  (which may be real or complex)  and let Sfn(t) denote the partial sum of matrices defined  by Sm(t) m . k=l  k\ (4.4) Then each element of the matrix Sm(t) converges absolutely and uniformly  on any finite t  interval  (-a a) a > 0  SLS  m ^  oo. Furthermore  Sm(t)  =  ASm-i(t)  = Sm-i(t)A  and thus the  limit  of Sm(t)  SLS t ^  oo is a C^  function  on R.  Moreover  this  limit  commutes with A. • In  view  of  the  above  result  the  following  definition  makes  sense  (see  also  Sec 149 CHAPTER  2: Response of Linear  Systems tion  1.13). DEFINITION  4.1.  Let A be a constant nX  n matrix  (which may  be real or complex). We define  e^^ to be the matrix °°  fk e^^  =  1 + ^  LA' k=l  k\ (4.5) for  any  -co < ^ < oo and we call e^^ a matrix  exponential. We  are  now  in a position  to provide  the  following  characterizations  of  e^^. THEOREM  4.2.  Let /  =  RJQ ^  J and let A be a given constant matrix for  (L). Then (i)  0 (0  -  e^^ is a fundamental  matrix for all t  < (ii)  The  state  transition  matrix  for  (L) is  given  by  ^(t to) <^(t -  tol  t G /. (iii)  ^^^i^^^2  =  ^A(fi+f2)  for  all tu  t2 G /. (iv)  Ae"^' = e^'A  for all t G /. (v)  {e^')-^  =  ^-^^ for alU  G 7. Proof.  By (4.5) and Theorem 4.1 we have that (d/dt)[e'^^]  = hm^^oo ASm(t) = hm^_^oo Sm(t)A  =  Ae^^  = e^^A. Therefore  0 (0  =  e^^ is a solution of the matrix  equa tion 6  = AO. Next observe that 0(0)  =  /. It follows from Theorem 3.3 that J^^[^^^  = ^tr(At)  -^ Q fQ^ ^Y[ t G R. Therefore  by Theorem 3.4 0 (0  =  e^^ is a fundamental  matrix for  (L). We have proved parts  (i) and (iv). To prove (iii) we note that in view of Theorem  3.6(iii) we have for  any  tit2  ^ R that 0(^1 t2) = 0(^1 0)0(0 t2). By Theorem  3.6(i)  we  see that 0(f  to) solves  (L)  with ^Oo  to)  = 1' It was  just  proved  that  ^ (0  = ^^(^~^o)  is also  a  solution.  By unique ness  it  follows  that  0(r  to)  = e^^^'^^K  For  t  =  ti to  =  —t2 we therefore  obtain eMti+t2)  = ^(^f^^  _^2)  =:  (^(t^)i^(-t2)-\ e^h  = ^{ti).  Also  for ^ =  0 ^  =  -^2  we obtain  0(0  -^2)  =  e^^^ = 0(-^2)~^. Therefore  ^^(^1+^2)  =  ^^^^^^2 for all ti t2 G R. and  for t  = tiJo  = 0 we  have  O(fi0)  = Finally to prove (ii) we note that by (iii) we have 0(^ to) = ^^(^~^o)  = /  + XI=i[(^  -  to)'/k\]A''  = 0(f -  to) is a fundamental  matrix  for  (L)  with  O(^o k)  = L Therefore  it is a state transition matrix for  (L). • We  conclude  this  section  by  stating  the  solution  of  (4.1) (f){t to xo)  =  0 (r  -  to)xo + 0(^  -  s)g(s) ds e^^'-'\g{s)ds to At e-^'g{s)ds (4.6) 150 Linear Systems for  all t  Ei R.ln  arriving  at (4.6) we have used  expression  (13.8) of Chapter  1 and the fact that in the present case 0(r  to)  =  ^^(^~^o) B.  How  to Determine  e At We begin by considering the specific  case A  = 0  a 0  0 From (4.5) it follows  immediately  that e"^' =  I  ^tA  =  1 0 at 1 As another example we consider Ai 0 0  A2 where Ai A2 E  R. Again from  (4.5) it follows  that k ^At  ^ 0 oht 0 0 e^^' k=l  k\ (4.7) (4.8) (4.9) (4.10) Unfortunately  in general it is much more difficult  to evaluate the matrix expo nential than the preceding  examples  suggest.  In the following  we consider  several methods of evaluating e^K The infinite  series method In this case we evaluate the partial  sum Sm{t) (see Theorem  4.1) m  ^k SM-1  +  Y.T^' k=l  k\ for  some  fixed  t say ti  and for  m  =  1 2  . ..  until no  significant  changes  occur  in succeeding  sums. This yields the matrix e^^^. This method works reasonably well if the smallest and largest eigenvalues  of A are not widely  separated. In the  same  spirit  as above we could use any  of the vector differential  solvers to  solve  X  =  Ax  using  the  natural  basis  for  R^  as  n  linearly  independent  initial conditions  [i.e.  using  as  initial  conditions  the  vectors  ei  =  (1 0  . . .  0)^ ^2  = (010... 0)^ ...en  =  (0...  01)^]  and  observing  that  in  view  of  (4.2)  the resulting  solutions are the columns of e^^ (with ^o =  0). EXAMPLE  4.1.  There  are cases  when  the  definition  of  e^^  (in  series form)  directly produces a closed-form  expression. This occurs for example when A^  =  0 for some k. In particular if all the eigenvalues of A are at the origin then A^ =  0 for some A: <  n. In this case only a finite number of terms in (4.5) will be nonzero and e^^ can be evaluated • in closed form. This was precisely the case in (4.7). 151 CHAPTER  2: Response of Linear Systems The similarity transformation  method Let us consider the initial-value  problem X =  Ax x(to)  =  xo (4.11) let P be a real nXn  nonsingular matrix and consider the transformation  x  =  Py  or equivalently y  =  P~^x.  Differentiating  both sides with respect to t we obtain y  = p-^x  =  P~~^APy  =  Jyy(to)  =  yo  =  P'^^o-  The  solution  of the  above  equation is given by il^(ttoyo)  =  e'^'-'^^p-^xo. Using (4.12) and x  =  Py  we obtain for the solution of (4.11) cl>(tJo.xo) =  Pe^^'-'^^p-^xo. (4.12) (4.13) Now suppose that the similarity transformation  P given above has been  chosen in such a manner  that /  =  p-^AP (4.14) is in Jordan canonical form (see Subsection 2.20). We first consider the case when A has n linearly  independent  eigenvectors  say v/ that correspond  to the  eigenvalues A/ (not necessarily  distinct)  /  =  1...  n.  (Necessary  and  sufficient  conditions  for this to be the case are given in Section 2.2. A sufficient  condition for the eigenvectors Vi i  =  1...  ^ to be linearly independent is that the eigenvalues of A Ai... A„ be distinct.) Then P can be chosen so that P  =  [vi...  v„] and the matrix /  =  P~^AP assumes the  form /  = 0 0 Afj Using the power series  representation we immediately  obtain the  expression 00 k=i Jt Ai« 0 ^A„r (4.15) (4.16) (4.17) Accordingly the solution of the initial-value problem (4.11) is now given by rgAi(/-/o) 0 (t)(t to Xo) =  P P~'xo. (4.18) 0 gA„(r-?o) In the general case when A has repeated eigenvalues it is no longer possible to diagonalize A (see Subsection 2.2L). However we can generate n linearly  indepen dent vectors v i  . . . v„ and an n X n similarity transformation  P  =  [vi... v„] that takes A into the Jordan canonical form /  =  P~^AP.  Here /  is in the block diagonal 152 Linear Systems form  given by /  = Ji Jo 0 (4.19) where  Jo  is  a  diagonal  matrix  with  diagonal  elements  Ai... A^ (not  necessarily distinct) and each 7 /  >  1 is an n  X m  matrix of the  form 'h+i 1 0  A k+i 0 1 ... 0 0 Ji  = (4.20) 0 0 0 0 ... • 0. 1 ^k+i where X^+i need not be different  from At+; if i ^  7» and where k + ni-\ h n^  =  n. Now since for any square block diagonal  matrix c = 0 with  Cii  =  I...  I square we have that cf 0 C'  = 0 it follows  from  the power series representation of e-^' that \gJot pJ\t e''  = t  E. R. As  shown earlier we have „Jot  = Alt 0 0 oJ st 0 For Ji i  =  I..  .swe  have /•  =  Xk+ili + Ni (4.21) (4.22) (4.23) where 7 denotes  the  n  X n  identity  matrix  and A^ is the  «  X n  nilpotent  matrix given by [O 1 ...  Ol Ni : 0 •• 1 0 (4.24) Since  \k+iU  and  Nt  commute  we  have  that ^Jit  ^ ^Xk+it^Nit^ (4.25) Repeated  multiplication  of  Nt  by  itself  results  in  Nf  =  0  for  all  k^ the  series  defining  e^^'  terminates  resulting  in ni.  Therefore 153 CHAPTER 2: Response of Linear  Systems \ t . JJi o^k+it  0  1 fUi-l («•  - 1 )! {ni  -  2)! /  =  1  . . .  5. (4.26) It now follows  that the solution of (4.11) is given  by 0  0 1 4>it to xo)  =  P 0 pJ\(t-to) 0 0 0 0 (fJs{t-to) P'^xo. (4.27) EXAMPLE 4.2.  In system (4.11) let A  = -1  2 0  1 . The eigenvalues of A are A i  =  -1 and  A2  =  1  and  corresponding  eigenvectors  for  A  are  given  by  vi  =  (10)^  and V2  =  (1 1)^  respectively.  Then  P  =  [vi V2]  = 1  1 .0  1 P~'  = 1 0 -1 1 and  /  = P-^AP  = -1  2 0  1 1  1 0  1 -1  0 0  1 0 Ai 0  A2 as  expected.  We  obtain ^At  =  Pe-^'P-^  = 1  1 0  1  0 0 1 0 -1 1 0 Suppose  next  that  in  (4.11)  the  matrix  A  is  either  in  companion form  or  that  it has  been  transformed  into  this  form  via  some  suitable  similarity  transformation  P so that  A  =  Ac  where Ar  = 0 0 1 0 0 -flO 0 -ai 0 1 0 - «2 0 0 1 (4.28) Since  in  this  case  we  have  xt+i  =  Xii  =  1  . . .  n  -  1 it  should  be  clear  that in  the calculation  of  e"^^  we  need  to  determine  via  some  method  only  the  first  row  of  e^^. We  demonstrate  this  by  means  of  a  specific  example. EXAMPLE 4.3.  In system (4.11) assume that A  =  Ac  = 0 L-2 11 -3J  which is in com-panion  form.  To demonstrate  the  above  observation  let us  compute  e^^ by  some  other method say diagonalization. The eigenvalues of A are Ai  =  -1  and A2 =  - 2 and a set of corresponding  eigenvectors is given by vi  =  (1 - 1 )^  and V2 =  (1 - 2 ) ^. We obtain 154 Linear Systems P  =  [viV2]  = 1 -1 P-'  = 1 -1 lip-^  ^  IM  ^ 2 -1 -2jL0 ^-^'JL-l  - 1. i-2e-'  + 2^-20 and J  =  p-^ArP  = -1 0 0 -2  e At  _ We note that the second row of the above matrix is the derivative of the first row  as expected. • The Cayley-Hamilton  Theorem  method If a(A)  =  det(\I -  A)  is the characteristic  polynomial  of an n X n matrix A then in viev^ of the Cayley-Hamilton  Theorem  we have that a(A)  =  0 i.e. every nXn  matrix satisfies its characteristic equation (refer to Subsection 2.2J). Using this result along with the series definition of the matrix exponential 6^^ it is easily shown that n-l e^'  = ^ai{t)A\ i = 0 [Refer to Subsection 2.2J for the details on how to determine the terms  adt).] The Laplace transform  method We assume that the reader is familiar  with the basics of the (one-sided)  Laplace transform.  If  / ( /)  =  [fi(tl... Ri  =  I...  n and if  each  fi  is  Laplace  transformable  then  we define  the Laplace  transform  of the vector/  componentwise i.e. f{s)  =  [fi{s\ . ..fn{s)Y  where fi{s)  =  X[Mt)]  = fn(t)f  where  ft  : [0^)-^ We  define  the Laplace  transform  of  a matrix  C(t)  =  [cij(t)]  similarly.  Thus if  each  ctj  : [0 ^)  —>  R and if each  c/y is Laplace  transformable  then the Laplace transform  of C(0 is defined  as C(5') =  iE[cij(t)]  =  [i£cij(t)]  =  [cij(s)]. Laplace transforms  of some of the common time signals are enumerated in Ta ble 4.1.  Also in Table 4.2 we summarize  some of the more important properties of the Laplace  transform.  In Table 4.1 8(t)  denotes  the Dirac  delta  distribution  (see Subsection  1.16C) and pit)  represents the unit step  function. Now consider once more the initial-value problem (4.11) letting to  =  0 i.e.. X =  Ax x(0)  = XQ. (4.29) TABLE  4.1 Laplace transforms ma  ^ 0) fis)  = Wit)] 8(t) Pit) t^lk\ ^-at fk^-at e~"^ sin bt e~''^ cos bt 1 1/s 1/^^+1 l/(^  +  a) Wis + af^^ bilis  +  af  + /?2] is  + a)l\_is  +  of-\-b'^] 155 CHAPTER 2: Response of Linear  Systems .+fik- i)(0)] TABLE4.2 Laplace transform  properties Time  differentiation Frequency  shift Time shift Scahng Convolution Initial value Final value df{t)/dt d^f{t)/dt^ e--^f{t) f{t  — a)p{t — a)a  > 0 f{t/a)a>0 Jl>f(T)g(t-T)dT  =  f{t)*g{t) lim^o+/W=/(0+) lim^^/(f) */(^)-/(o) */() _ [ / - i / ( 0)  + . /> + «) e-"'f(s) af(as) fim^) linis^^  sf{sy lims^o sf{s)^'' ^ If the limit exists. •'"'• If sf{s) has no singularities on the imaginary axis or in the right half s plane. Taking  the Laplace  transform  of  both  sides  of i  = Ax  and taking  into  account  the initial  condition x(0)  =  XQ we obtain  sx{s)  —xo=  Ax{s)  or {sI—A)x{s)  =  XQ or x{s)  =  {sI-A)-^xo. (4.30) It  can be  shown  by  analytic  continuation  that  {si — A)~^  exists  for  all s  except  at the  eigenvalues  of A.  Taking  the inverse  Laplace  transform  of  (4.30)  we obtain the solution 0 (0  =  ^~^[{sI-A)-^]xo = O(f0)jco  =  e^'xQ. (4.31) It follows  from  (4.29)  and (4.31)  that 6{s)  =  (sI-A)-^ and that O ( f  0 ) = O ( f - 0 ) = O ( 0 = ^ " M ( ^ / - A ) - ^ ] = /^ (4.32) Finally  note  that  when  to ^  0 we can immediately  compute  0(f  fo)  =  ^ (^ — ^o)  = EXAMPLE  4.4.  In (4.29) let A ^ -1  2] 0  1 . Then (sI-A)-5 +1 0 -2 s-l 1 5 +1 0 Using Table 4.1 we obtain if'^ [{si-A 2 ( 5 + l ) ( 5 - l) 1 5 +1 1 \S-l 1 5 +1 s-l 0 {e'-e-') 1 0 Before  concluding  this  subsection  we  briefly  consider  initial-value  problems determined  by (4.1) i.e. x=Ax^g{t) x{to)=xo. (4.33) We  wish  to apply  the Laplace  transform  method  discussed  above  in  solving  (4.33). To  this  end we  assume  ^o =  0  and we take  the Laplace  transform  of  both  sides  of (4.33) to obtain  sx{s)  —XQ =AX{S) -\-g{s)  or {si — A)x{s)  =xo-\-g{s)  or x{s)  = {sI-A)-\^{sI-A)-^g{s) = ^{s)xo^^{s)g{s) ^  ^h{s)^^p{s). (4.34) 156 Linear  Systems Taking  the  inverse  Laplace  transform  of  both  sides  of  (4.34)  and  using  (4.6) with  ^0  =  0  we  obtain  0 (0  =  0/z(O  +  </>p(0  =  i^~^[(sl -=  ^(t)xo  +  IQ  ^(t  ~  r])g(r])drj  where  0/^  denotes  the  homogeneous Ay^g(s)] solution  and  (l)p  is  the particular  solution  as  expected. -  A)~'^]xo  +  iE~^[(sI EXAMPLE4.5.  Consider the initial-value problem given by Xi  =  — Xi  +  X2 X2  = - 2 ^2  +  U(t) with xi(0)  =  -I  X2(0)  =  0 and u(t)  = for  t  >  0 for  t  <  0. It is easily verified  that in this case 1 /1 s + 1 s +I 1 s  + 2 0 0 e  ^ 0 1 5+  1 0 1 s  -h2 -It (e  ^  —  e -e 0 ^0' -1 0. 1 s  + 2 e-' /  1 5 +1 1 s + 2 2e- +  i^-2^-Ct>p{t)  = 1 '  2' 2 -2t _ 2 2' and 0 (0  =  0/(O  +  0p(O 2 i J '^  2\5 + 2 1  /  1 2  1^ +  2 2U s + 1 C.  M o d es  and  Asymptotic  Behavior  of  Time-Invariant  Systems In  this  subsection  we  study  the  qualitative  behavior  of  the  solutions  of  linear  au tonomous  homogeneous  ordinary  differential  equations  (L)  by  means  of  the  modes of  such  systems  to  be  introduced  shortly.  Although  we  will  not  address  the  stabil ity  of  systems  in  detail  until  Chapter  6  the  results  here  will  enable  us  to  give  some general  stability  characterizations  for  such  systems. Modes:  General  case We  begin  by  recalling  that  the  unique  solution  of i:  =  Ax (L) satisfying  x(0)  =  JCQ is  given  by (/>(r 0 jco)  =  $(r  O)x(O)  =  0(r  0)xo  =  e'^'xQ. (4.35) We also recall  that  det(sl -  A)  =  YlJ^i(s  -  KT\  where  Ai... A^- denote the  a distinct eigenvalues  of A where  A/ with  /  =  1... cr is assumed to be repeated  nt times (i.e. nt is the algebraic multiplicity  of A/) and Xf^im  =  n. To introduce the modes for  (L) we must show that 157 CHAPTER!: Response of Linear Systems ^At (T flj-l = 1  ^ = 0 =  J][Aioe^^'  +  Ante^^'  +  • • • + A-(„.--ix^t (4.36) where 1 Afk  =  -^-z 1 ^ j-r  lim{[(s  -  XiT^sI  -  A)'  i^im-i-k)  }. (4.37) In (4.37) [  • ]^^^ denotes the /th derivative with respect to s. Equation (4.36) shows that e"^^ can be expressed as the sum of terms of the form Aikt^e^'\  where A/^  E  R^^^  We call Ai^t^e^'^ a mode of system  (L). If an eigenvalue A/ is  repeated  nt  times  there  are  nt  modes  At^t^e^'^  k  =  0 I...  rii — \  in e^^ associated  with  A/. Accordingly  the  solution  (4.35)  of  (L)  is  determined  by  the  n modes of (L) corresponding to the n eigenvalues of A and by the initial condition x(0). We note that by selecting  x(0) appropriately modes can be combined or eliminated [Aikx{Q) ^  0] thus affecting  the behavior of <^(t 0 XQ). To verify  (4.36) we recall that  e^^  =  i£~^[(sl  -  A)"^]  and we make use of the partial fraction  expansion method to determine the inverse Laplace transform.  As in the scalar case it can be shown that {si  -  A)-'  =  X  ^(klAaXs -  Xir^'^'\ (4.38) i =  l  k^O where the (klAfk)  are the coefficients  of the partial fractions  (k\  is for  scaling). It is known that these coefficients  can be evaluated  for  each  / by multiplying both  sides of (4.38) by (s -  \iY\  differentiating  {nt -  \  -  k) times with respect to s and then evaluating the resulting expression dXs  — A/. This yields (4.37). Taking the inverse Laplace transform of (4.38) and using the fact that ie[^^^^^n  =  ^!(^-A/)"^^+^^  (refer to Table 4.1) results in (4.36). When  all n eigenvalues  kt  of A are distinct  then  cr  =  n n/  =  1 /  ^  1...  n and (4.36) reduces to the expression where l i m [ ( ^ - A / ) ( ^ / - A ) - ^ ]. ^At  =  Y.^ie M i =  l (4.39) (4.40) Expression  (4.40) can also be derived  directly using a partial fraction  expansion of (si  -  A)"^  given in (4.38) (verify  this). EXAMPLE 4.6.  For (L) we let A 1] 0 -4  -4J  for which the eigenvalue Ai -2 is repeated twice i.e. ni  =  2. Applying (4.36) and (4.37) we obtain Aioe^^^  ^  Ante ht - 1  0 0  1 e-^^  + te 158 Linear  Systems EXAMPLE  4.7.  For  (L)  we  let  A  = by  (the complex  conjugate  pair)  Ai  = (4.39) and (4.40) we obtain 0 -1 _i 2 for  which  the  eigenvalues  are  given +  j( V3/2) A2  =  -i  -  7( V3/2).  Applying A^  = 1 Ai  +  1  1 Ai  -  A2 A2  = A2 +  1  1 -1 _1 -J 73 2 1 .73 . ^J~3 -1 1 .73 -"2  +  ^-2 7^ ^~2 -1 1 2 7^ -^  2  J [i.e. Ai  =  A2 where (• )* denotes the complex conjugate  of (•)] and e^^  =  Aie^^'  +  A2e^^'  =  Aie^^'  + A\e^*i' =  2(Re  Ai)(Re  e^^')  - =  2e-^"^^'  2 0 2(ImAi)(Ime^'') 1 273 1 75 -COS -—t [  7^ 0 1 -: 2J 1 • ~7i 1 273. sinM.j / \ The  last  expression  involves  only  real  numbers  as  expected  since A  and  e^^ are  real • matrices. EXAMPLE  4.8.  For  (L)  we  let  A  = ri  01 [0 ij for  which  the  eigenvalue  Ai  =  1 is  re-peated twice i.e. n\  =  2. Applying  (4.36) and (4.37) we obtain Aioe^^^ +  Aute^^^ 1  0 0  1 e'  + 0  0 0  0 te'  =  Ie\ This  example  shows  that  not  all  modes  of  the  system  are  necessarily  present  in e^K What is present depends in fact  on the number  and dimensions  of the individual  blocks of the  Jordan  canonical  form  of A  corresponding  to identical  eigenvalues.  To  illustrate this  further  we  let  for  (L)  A  = 0  1   where  the  two  repeated  eigenvalues  Ai  =  1 belong to the same Jordan block. Then e^ 1  0 0  1 e'  + 0  1 0  0 te'. Stability  of  an  equilibrium In  Chapter  6  we  will  study  the  qualitative  properties  of  linear  dynamical  sys tems  including  systems  described  by  (L).  This  will  be  accomplished  by  studying the stability  properties  of  such  systems  or more  specifically  the stability properties of  an  equilibrium  of  such  systems. If  0(/ 0 Xe)  denotes  the  solution  of  system  (L)  with  x(0)  ==  Xe then  Xe is  said to  be  an  equilibrium  of  (L)  if  (j){t 0 Xe)  ^  Xe  for  all  f  >  0.  Clearly  jc^  =  0  is  an equilibrium  of  (L).  In  discussing  the  qualitative  properties  it  is  often  customary  to speak  somewhat  loosely  of  the  stability  properties  of  system  (L)  rather  than  the stability  properties  of  the  equilibrium  x^  =  0  of  system  (L). We  will  show  in  Chapter  6  that  the  following  qualitative  characterizations  of system (L) are actually equivalent to more fundamental  qualitative  characterizations of the equilibrium  Xe  =  0 of system (L): 1.  The  system  (L) is said to be stable  if all solutions of (L) are bounded  for  all t  > 159 CHAPTER  2: Response of Linear Systems 0  [i.e. for  any  solution  (f){t 0 XQ)  =  (4>i{t 0 XQ) ...  4>n{h 0. -^o))^  of  i^)^  there exist constants Mi i  =  I..  .n  (which in general will depend on the solution on hand)  such that  \(f)i{t 0 xo)| <  M/ for all t  >  0]. 2.  The  system  (L)  is  said  to  be  asymptotically  stable  if  it  is  stable  and  if  all  so lutions  of  (L)  tend  to  the  origin  as  t  tends  to  infinity  [i.e.  for  any  solution 4>{t 0 XQ)  =  {4>\{t 0 xo)... (l)n{t 0 xo)Y  of (L) we have lim^_^oo 4>i{t 0 xo)  = 0i  =  1...^]. 3.  The system (L) is said to be unstable  if it is not stable. By  inspecting  the  modes  of  (L)  given  by  (4.36)  (4.37)  and  (4.39)  (4.40)  the following  stability criteria for  system (L) are now  evident: 1.  The  system  (L) is asymptotically  stable  if  and  only  if  all eigenvalues  of A  have negative real parts (i.e.. Re Xj  <0  j  =  1...  a). 2.  The system (L) is stable if and only if Re A^ <  0 j  =  1... a  and for all eigen values with Re Xj  =  0 having multiplicity  HJ >  I  it is true that lim [(s k= I 1.  (4.41) 3.  System (L) is unstable  if and only if (2) is not true. We note  in particular  that  if  Re Xj  =  0  and  nj  >  1 then  there  will  be  modes -  1 that  will  yield  terms  in  (4.36)  whose  norm  will  tend  to ^jkl^y  k  =  0..  .nj infinity  as ^ ^  oo unless  their  coefficients  are zero. This  shows  why  the  necessary and sufficient  conditions for  stability of (L) include condition  (4.41). EXAMPLE  4.9.  The  systems  in  Examples  4.6  and 4.7  are  asymptotically  stable. A ro  n -1 [0 system  (L) with  A is  stable  since the eigenvalues  of A  above are Ai  = 0 A2 =  - 1.  A system (L) with A -1  01 0  1 is unstable since the eigenvalues of A are 1A2 -1. The system of Example 4.8 is also unstable. Modes: Distinct eigenvalue  case When the eigenvalues A/ of A are distinct there is an alternative way to (4.40) of computing the matrix coefficients  A/ expressed in terms of the corresponding  right and left  eigenvectors of A. This method offers  great insight in questions  concerning the presence or absence of modes in the response of a system. Specifically  if A has n distinct eigenvalues  A^ then where ^At  -X^i  M Ai ViVi (4.42) (4.43) where vi  G R^  and {viY  E  R^  are right and left  eigenvectors  of A corresponding to the eigenvalue  A/ respectively. 160 Linear Systems To prove the above assertions we recall that ( A / /- A)v/  =  0 and v ^ A / /-  A)  = O.lf  Q=  [vi... Vn] then the v/ are the rows of The matrix Q is of course nonsingular since the eigenvalues A/ /  =  1...  n are by  assumption  distinct  and  since the  corresponding  eigenvectors  are linearly  inde pendent.  Notice  that  Qdiag[k\.. .Xn\  =  ^Q  and  that diag[k\...  A„]P  =  PA. Also notice that v/Vj  =  5/y  where 8ij  = 1  when  /  =  j 0  when  /  T^ j. We now have (^/-A)-^  =  [si-Qdiag K]r'Q-' now take the inverse Laplace transform  of the above expression we obtain (4.42). ... =  Sf=iVvK^-AO-i.Ifwe -  Qdiag[(s-Xr\...(s-Xnr']Q-' [\u  ..  XnlQ'^T =  Q[sl-diag[Xu If we choose the initial value x(0)  for (L) to be cohnear with an eigenvector vj of A [i.e. x(0)  =  avj  for some real a  ¥=  0] then e^j^ is the only mode that will appear in the  solution  cf)  of  (L). This  can  easily  be  seen from  our preceding  discussion.  In particular if x(0)  =  aVj  then  (4.42) and  (4.43) yield (/>(^ 0 x(0))  =  e'^'xiO)  =  vivi40)^^^' +  • • • + VnVnX(0)e ^"'  -  avie^J' (4.44) since v/Vy  =  1 when  /  =  y and v/Vy  =  0 otherwise. r-1  1 0  1 1  1 0  2 EXAMPLE  4.10.  In  (L)  we  let  A  = .  The  eigenvalues  of A  are given  by M~ Ai  =  - 1 A2 =  1 and Q  =  [viV2] v\V\e^^^  + V2V2e^'^^  =  1  -i 0 (a 0)^ then (pit 0 x(0))  =  e^^x(0) = a(l O^e'^  which contains only the mode cor responding to the eigenvalue Ai  =  - 1.  Thus for this particular choice of initial vector the unstable behavior of the system is suppressed. • ^Mfin particular we choose x(0)  =  avi  = . Then  e"^' 0 1  -i 0 Remark We conclude  our discussion  of modes  and  asymptotic  behavior by briefly  con sidering systems of linear nonhomogeneous ordinary differential  equations (4.1) for the special case where g(t)  =  Bu{t) X =  Ax^  Bu(t\ (4.45) where B  G R^^^^  ^  - R^  Rm  ^^^  where it is assumed that the Laplace  transform of  u  exists.  Taking  the  Laplace  transform  of  both  sides  of  (4.45)  and  rearranging yields x{s)  =  (si  -  A)-'x(0)  +  (si  -  A)~'Bu(s). (4.46) By taking the inverse Laplace transform  of (4.46) we see that the solution  cf) is the sum of modes that correspond to the singularities or poles of (si  -  A)~ ^ x(0)  and (si  -A)~^Bu(s).  If  in  particular  (L)  is  asymptotically  stable  (i.e. for  x  =  Ax  Re Xi  < 0 /  =  1..  .n)  and if u in (4.45) is bounded (i.e. there is an M such that 1^^(01 <  M for  all  r >  0 /  =  1...  m)  then  it  is  easily  seen  that  the  solutions  of  (4.45)  are bounded as well. Thus the fact that the system (L) is asymptotically stable has reper- cussions on the asymptotic behavior of the solution of (4.45). Issues of this type will be addressed in greater detail in Chapter 6. 161 CHAPTER 2: Response of Linear Systems *2.5 LINEAR  PERIODIC  SYSTEMS We  now  consider  linear  homogeneous  systems  of  first-order  ordinary  differential equations X =  A(t)x -co  <t  <oo^ where A  G  C(R  /?"^")  and A(t)  =  A(t  +  T) -00  <  ^  <  00 (P) (5.1) for  some T  >  0. We call (P) a. periodic  system  and T a. period  for  system  (P). The  principal  result  of  this  section  involves  the  notion  of  the  logarithm  of  a matrix which we introduce in the following  result. THEOREM 5.1.  For every nonsingular matrix B there exists a matrix A called a loga rithm ofB  with the property that B. (5.2) The matrix A is not unique. Proof. Let  B  be  similar  to  B.  Then  there  exists  a  nonsingular  matrix  P  such  that P-^BP  =  B.  Now  if  e^  =  B  then  we have  B  =  PBP'^  =  Pe^P'^  =  e^^^~\  It follows that PAP'^  is also a logarithm of 5. Therefore it suffices  to prove the theorem when the matrix B is in suitable canonical form. Let Ai... Ayt denote the distinct  eigenvalues  of B with respective  multiplicities ni..  .nk.  Without loss of generality we may assume that B is in the block diagonal form B = r^i 0 0 Hj =  OJ  =  h...  k.  We  note  that  A^  T^ 0 y  = where  Bj  =  Ay[/„.  +  (1/\J)NJIN''/ l...k since  B  is  nonsingular.  Using  the  power  series  expansion  log(l  +  x)  = Z^p=i[(-iy^^/p]xP\x\  <  1  we  formally  write  A^  =  logBj  =  /„.logAy  + log[/.  +  (l/\j)Nj]  =  InjlogXj  +  X;=d(-^y^'^PWAjy> since  7V7  =  0  we actually have Aj = /„^logA + X  L ^ / ^ T j  =  i„„k (5.3) where we note that logA^ is defined  since Xj 7^  0. Now recall that e'°8(i+j:)  =  I +  x. Performing  the  same  operations  with  matrices  we  obtain  the  same  terms  and  there 162 Linear Systems is  no problem  with  convergence  since  the  series  (5.3)  for  Aj  =  ^ogBj  terminates. Accordingly  we  obtain  e^J  =  exp(Inj log\j)Qxp{Z.''J~^[(-l)P^^/p](Nj/Xj)P}  = \j[Inj  + (Nj/\j)]  = Bj j  =  1... ^. If now we let A = 0 where Aj  is defined in (5.3) we obtain 0 e^i .A ^ =  B 0 e^k 0 Bk which is the desired result. We conclude by noting that the matrix A is not unique since for example e^^^'^^^^  = ^A^iTTj  ^  ^A f^j. ^u integers k (where j  =  v - 1 ). • We are now  in  a position  to  state  and prove one of the principal  results  of  this section. THEOREM 5.2.  Assume that (5.1) is true and that A E  C{R /?">^"). If <|)(0 is a funda mental matrix for (P) then so is ^{t  +T)t  EL  R. Furthermore for every ^  there exists a nonsingular matrix P that is also periodic with period T and a constant nXn  matrix R such that ^(0  =  P{t)e'^. Proof. Let ^(0  =  ^(?  + T\t  E  R. Since i>(0  =  A(t)^(t\  t  E  R we have ^ (0  = 4)(^ + 7)  =  A(t + T)^(t  + r)  =  A{t)<i>(t + T)tE  R. Therefore ^  is also a solution of ^  =  A(0^ A(t)  = A{t + T\t  E R. Furthermore since ^{t  + T) is nonsingular for all t E R/ii  follows that ^  is a fundamental matrix for (P). Therefore by Theorem 2.5 there exists a nonsingular matrix C such that ^{t  + T)  = ^{t)C  and by Theorem 5.1 there exists a constant matrix R such that e^^  =  C. Therefore Defining P by ^(t  + T)  =  ^(t)e TR P(t)  =  ^(0^  -tR (5.4) (5.5) and using (5.4) and (5.5) we now obtain P{t + T)  = ^(r  +  T)e-^^-^^^^  = ^{t)e^^  X ^-(t+T)R  ^  (^(t)e-tR =  p{t).  Therefore  P{t)  is  nonsingular  for  dll t  E  R  and  it  is periodic. • The above result allows us to conclude that the determination  of a  fundamental matrix  O for  system  (P)  over  any  time  interval  of  length  T  leads  at  once  to  the determination  o/O  over (-oo oo). To show this assume that ^(t)  is known only over the interval  [^o. to>  -^T]  Since ^(t  + T)  =  ^(t)C  we obtain by setting t  =  toC  = 0(^)~iO(ro  +  T)  and  R  =  T~^ log C. It follows  that  P(t)  -  $(0^~^^  is now  also known  over  [to to -\-  T].  However  P(t)  is periodic over (-0000). Therefore  4>(0 is givenover  (-00 00) by $ (0  =  P(t)e^^. Next  let  0  be  any  other  fundamental  matrix  for  (P)  with  A(t)  =  A(t  +  T). ^(t)e^^ Then O  =  $5  for  some constant nonsingular matrix S. Since ^(t  +  T)  = 163 CHAPTER  2: Response of Linear Systems we have that ^(t  + T)S  =  Mt)Se'^^  or (5.6) This  shows  that  every fundamental  matrix  O  of(P)  determines  a matrix  Se^^S~^ that is similar  to the matrix  e^^. Conversely  let S be  any  constant  nonsingular  matrix.  Then  there  exists  a  fun damental matrix of (P) such that Eq. (5.6) holds. Therefore even though $  does not determine  R  uniquely  the  set  of  all  fundamental  matrices  of  (P)  and  hence  of A determines  uniquely  all parameters  associated  with  e^^  that  are  invariant  under  a similarity transformation. In particular the set of all fundamental  matrices of A deter mines a unique set of eigenvalues  of the matrix  e^^  denoted by Ai... A„ that are called the Floquet  multipliers  associated with A. Note that none of these vanish since nf=i  A/  =  dete^^  #  0. The eigenvalues of P are called the c/z^rac^^mr/c ^x/76>/i^n/5'. Next we let 2  be a constant nonsingular matrix that transforms R into its Jordan canonical form  i.e. /  ^  Q~^RQ  where /  = Jo 0 0 /i 0 0 Now let O  =  <i>Q and let P  =  PQ.  In view of Theorem 5.2 we have that ^ (0  -  P{t)e'\ Pit)  =  Pit  +  T). (5.7) Let the eigenvalues of R be denoted by p i  . . .  p^. Then 0 0 e'^' 0 JJ 0 0 e'P' 0 0 e'P^ 0 0 ptJs 0 0 e'Pi O^JO = where and ' ^ jJi _ Jpq^i 0  1 2 t (n -  1)! fn-2 in -  2)! 0  0  0 1 i  = l...s q  + ^rt i=l -=  n. Now  A;  =  e^P'. Therefore  even  though  the  p  are  not  uniquely  determined  their real  parts  are.  It  follows  from  (5.7)  that  the  columns  4>\>--->4>n  of  "J* are  linearly independent  solutions  of  {P).  Let  pi...  p„  denote  the  periodic  column  vectors 164 Linear Systems of P.  Then 01 (0  =  e^'^piit) hit)  =  e'P^p2(t) 4>q{t)  =  e'P^Pqit) 4>q+\{t)  =  e'P^-'pg+i(t) 4>q+2(t)  =  e'P^-'(tPg+i(t)  + Pq+2(t)) (5.8) ^q-n(t)  = e''^-^ (^  -  iyPq+l(t)  +  •••  +  tpg+r-l(t)  + Pq+r(t) 4 - r  + l (0  =  e'P^-^ Pn-rs +  l(t\ fVs-l 4>n(t) otPq+s Pn-rs + l(0  +  • • • +  tpn-\{t)  +  pnit) (rs -  1)! From  (5.8)  it  is  easy  to  see  that  when  Re pt  — at  <  0  or equivalently  when |A/| <  1 there exists a A: >  0 such that \h(t)\  <  kie^'^^''^^^ ->  0 as f ^  oo. This shows that if the eigenvalues  pu i  =  I...  n of R have negative real parts then any norm of any solution of (P) tends to zero as ^ -^  -\-^ at an exponential  rate. From  (5.5)  we  can  easily  verify  by  direct  computation  that  AP  -  P  =  PR. Accordingly for the  transformation X =  Pit)y (5.9) we  obtain  x  =  A(t)x  =  A(t)P(t)y  =  P(t)y  +  P(t)y  =  (d/dt)[P(t)y]  or  j  = P~\t)  X [A(t)P(t)  -  P(t)]y  =  p-\t)[P(t)R]y =  Ry.  In  other  words  the  trans formation  (5.9) reduces  the linear homogeneous  periodic  system  (P)  to the  system y  =  Ry  a linear  homogeneous  system  with  constant  coefficients. We conclude this section with a specific  example. EXAMPLES.1.  Consider the scalar system X =  -(sin/  + 2)x (5.10) Then A(t)  =  -(sin/  + 2) and A(t)  is periodic  with period  T  =  ITT. A  fundamental matrix for (5.10) is given by 0(0  =  exp(cosr  -  1 -  2/) as can be verified  by substi tuting into the relation i>(0  =  A(t)^(t).  Letting t  = 0 and T  =  ITT in (5.4) we obtain 0(27r)  -  ^"^^  =  0(0)^^''^  =  e^""^  or R  =  - 2. The equivalence matrix P(t) is now given by (5.5) as P{t) =  exp (cos t - l-  2t)e^^  = e^^^^~^ which is clearly periodic with period  T  =  ITT. The given system (5.10) is transformed  by P{t) into the system y  = (^i-^«^0[(-l)(sin/  + 2)^'^°^^-i  + smte''''''-^]y  =  -(e^-^^'')(2e'''''-^)y  =  -2y  =  Ry. We will address some of the qualitative properties of periodic systems in  further detail in Chapter 6. 2.6 STATE EQUATION  AND  INPUT-OUTPUT  DESCRIPTION OF  CONTINUOUS-TIME  SYSTEMS 165 CHAPTER 2: Response of Linear Systems This section consists of three subsections. Using the material of the preceeding  sec tions of this chapter  we first study the response  of Hnear continuous-time  systems. Next we examine transfer functions  of Unear time-invariant systems given the state equations of such systems. Finally we explore the equivalence of internal represen tations of systems. A.  Response  of Linear  Continuous-Time  Systems Returning  now to Sections  1.1  and  1.14  we consider  once more systems  described by linear time-varying  equations of the  form X =  A(t)x  -h  B(t)u y  =  C(t)x  -h  D(t)u (6.1a) (6.1b) where  A  G  C(R  R'"'"'') B  G  C(R  /?^><^) C  G  C(R  RP''''\  D  G  C(R  RP"""^)  and u  : R-^  R^  is  assumed  to be  continuous  or piecewise  continuous.  We recall  that in  (6.1a)  and  (6.1b) x  denotes  the  state  vector  u denotes  the  system  input  and  y denotes the system output. From Section  1.14  we recall that for  given initial condi tions ^0 ^  R> ^(to)  =  XQ  E:  R^  and for  a given input u the unique solution of  (6.1a) is given by (l)(t to xo)  =  0(r  to)xo + ^(t s)Bis)u(s)ds (6.2) for t  E:  R where O denotes the state transition matrix of A(0. Furthermore by sub stituting  (6.2)  into  (6.1b) we  obtain  [as in  (14.6)  of  Chapter  1] for  all  t  G  R  the total system  response  given by y(t)  =  C(t)^(t  to)xo +  C(t) Jto 0(^ s)B(s)u(s)ds  +  D(t)u(t). (6.3) Recall  that the total response  (6.3)  may  be viewed  as consisting  of the  sum of two components the zero-input  response  given by the term il/(t to Xo 0)  =  C(t)<^(t to)xo (6.4) and the zero-state  response  given by the term p(t to 0 u)  =  C{t) ^{t  s)B(s)u(s)ds -h D(t)u(t). (6.5) Jto The cause  of the former  is the initial  condition  xo  [and  can be  obtained  from  (6.3) by letting u(t)  =  0] while for the latter the cause is the input u [and can be obtained by setting  XQ  =  0 in  (6.3)]. The  zero-state  response  can  be  used  to  introduce  the  impulse  response  of  the system  (6.1a)  (6.1b). Returning  to  Subsection  1.16C  we  recall  that  by  using  the 166 Linear Systems Dirac delta distribution  S  we can rewrite (6.3) with  XQ  =  0 as y(t)  =  f  [C(tmt  T)B(T)  + D(t)8(t  -  T)]u(T)dT Jto =  [  H(tT)u(T)dT Jto (6.6) where H(t  r) denotes the impulse response matrix of system (6.1a) (6.1b) given by H{t T) [  C(t)<^(h  T)B{T)  +  D{t)8{t  -  r) [0 t^  T t<  T. (6.7) When in (6.1a) (6.1b) A{t)  =  A B(t)  =  B C(t)  =  C and D(t)  =  D we obtain the time-invariant  system X =  Ax  +  Bu y  =  Cx  -^ Du. We recall that in this case the solution of (6.8a) is given by (6.8a) (6.8b) (6.9) the total  response  of system (6.8a) (6.8b) is given by y{t)  =  Ce'^^'-'^ho  +  C  [  e^^'-'^Bu{s)ds  +  Du{t\ (6.10) and  the  zero-state  response  of  (6.8a)  (6.8b)  is  given  by  ^(0  =  /^^[C^^^^~'^^B+ \l  H{tT)u{T)dT  =  1^ H(t  -  T)u(T)dT  where  the  impulse D8(t  -  T)]u(T)dT  - response  matrix  H of system (6.8a) (6.8b) is given by Ce^^'-^^B  +  D8(t  -  r) 0 or as is more commonly  written H(t  -T)  = t^ r t  <T H(t)  = Ce^'B  +  D8{t\ 0 t>  0 ^ < 0. (6.11) (6.12) At this point it may be worthwhile to consider  some specific  cases. EXAMPLE  6.1.  In (6.1a) (6.1b) let A{t)  = -1 ^2n €' 0 -1 B{t)  =  0 C(0  =  [e\ 1] D  =  0 and  consider  the  case  when  t^  =  0 x(0)  =  (0 1)^ u is  the  unit  step  function  and r >  0. Referring to Example 3.6 we obtain (p(t to XQ) = (f)h{t t^ XQ)  + (j)p{t to xo) = 2 {e'  - with  ^  =  0 and  for  ^ >  0. The  total system response  y(t)  = €-') te-' 0 C(t)x(t)  is  given  by the  sum of the  zero-input  response  and  the zero-state response y(t to Xo u)  = il/(t to Xo 0) + p(t to 0 u)  =  [\{e'^^ -  1) + e~^] + tt>  ^. Note that the zero-input response ip is due to the homogeneous part of the solution 4>  (given by (f)h) while  the zero-state  response  p  is  due to the particular  solution  of (/> (given  by (/)p). EXAMPLE  6.2.  In  (6.8a)  (6.8b)  let  A [o  o} ^ - [y C  =  [01]  D  =  0  and consider  the  case  when  to  =  0 x(0)  =  (1 - 1 ) ^  u is the unit  step and  t  >  0. We  can easily compute the solution of (6.8a) as 167 CHAPTER  2: Response of Linear  Systems (/)(^ to  Xo)  =  (t)h(t  to  Xo)  +  (l>p(t to  Xo)  = n -^1 -1  + ^t^ with ^0 =  0 and for ? >  0. The total system response y(t)  =  C(t)x(t)  is given by the sum of  the  zero-input  response  and  the  zero-state  response  y(t  to xo u)  =  il/(t to xo 0)  + p(t to0u)=-l-\-tt>0. • We  note  that  when  x(0)  =  0  Example  6.1  (a time-varying  system)  and  Exam ple  6.2  (a time-invariant  system)  have  identical  output  responses  given  by  y(t)  =  t r  >  0  when  u(t)  is the  unit  step.  [Is this  true  for  any  input  u(t)l] EXAMPLE6.3.  Consider the time-varying  system given above in Example 6.1. In this case we have 0(? T)B(T)  =  [e~\  0]^ and the impulse response has the rather  unusual form Hit  T)  = C{t)^{t  T)B(T)  =  1 0 t^ T t<T. In other words the response of this system to an impulse input for zero initial conditions is the unit  step  and  this  is  independent  of the  time  r  at which  the  impulse  is  applied! Note that in the present case the response to a step is a ramp  t as can easily be  verified from  (6.6) (see also Example 6.1). Therefore  this system behaves to the outside  world for  zero initial conditions as a time-invariant  system. This is interesting; however it is • not a typical  situation when dealing with time-varying  systems. EXAMPLE  6.4.  Consider  the time-invariant  system  given  above in Example  6.2. It is easily verified  that in the present  case (^(t)  =  e^'  = n  ^1 0  1 Then H(t  r)  =  Ce'^^'-^'^B =  I  for t  >  r  and H{t  r)  =  OfoYt< r.  Thus the  response of this system to an impulse input for zero initial conditions is the unit step.  Comparing this with the impulse response of the system given above in Example 6.3 we note that they are identical. In other words the behavior of these two systems to the outside world one a time-varying  system and the other a time-invariant  system is characterized by the same response to an impulse input when the initial conditions are zeros. Indeed in this case  both  systems  behave  like  a time-invariant  system  with  H(t  r)  =  H(t  -  r 0)  = H(t  0)  =  1. Note however that when the initial conditions  are not zero the  responses of these two systems are quite different. • finite-dimensional The  preceding  two  examples  demonstrate  as  one  might  expect  that  external descriptions  of linear  systems  are  not  as  complete  as  internal  de scriptions  of  such  systems.  Indeed  the  utility  of  impulse  responses  is  found  in  the fact  that  they  represent  the  input-output  relations  of  a  system  quite  well  assuming that  the  system  is  at  rest.  To  describe  other  dynamic  behavior  one  needs  in  general additional  information  [e.g. the initial  state vector  (or perhaps  the past history  of  the system  input  since  the  last  time  instant  when  the  system  was  at  rest)  as  well  as  the internal  structure  of  the  system]. Internal  descriptions  such  as  state-space  representations  constitute  more  com plete descriptions than external descriptions. However  the latter are simpler to  apply 168 Linear Systems than the former.  Both types  of representations  are useful.  It is quite  straightforward to obtain external descriptions  of systems from  internal descriptions as was demon strated in this  section. The reverse process however is not quite as  straightforward. The process of determining  an internal  system description  from  an external descrip tion  is called  realization  and  will be  addressed  in  Chapter  5.  The principal  issue  in system realization is to obtain minimal order internal descriptions that model a given system avoiding the generation of unnecessary  dynamics. B. Transfer  Functions Next  if  as in  (16.51)  in  Chapter  1 we take the Laplace  transform  of both  sides of (6.12) we obtain the input-output  relation y{s)=H{s)u{s). (6.13) We recall  from  Section  1.16  that H{s)  is called  the  transfer function  matrix  of system  (6.8a)  (6.8b).  We  can  evaluate  this  matrix  in  a  straightforward  manner  by first taking the Laplace transform  of both sides of (6.8a) and (6.8b) to obtain sx{s) - x ( 0)  =Ax{s)  +Bu{s) y{s)  =  Cx{s)  +Du{s). Using (6.14) to solve foxx{s)  we obtain x{s)  =  {sI-A)-^x(Qi)  + {sI-A)-^Bu{s). Substituting  (6.16) into (6.15) yields y{s)  = C{sl-A)-^x{0) ^C{sI-A)-^Bu{s) +Du{s) and y{t)  = ^~^y(s)  = C/'jc(0)  +C  / Jo e^^'-'^Bu(s)ds^Du(t) as expected. (6.14) (6.15) (6.16) (6.17) (6.18) If  in  (6.17)  we  let x(0)  =  0  we  obtain  the  Laplace  transform  of  the  zero-state response given by y{s)  = [C{sI-A)-^B^D]u{s) =  H{s)u{s) where H{s)  denotes the transfer  function  of system (6.8a) (6.8b) given by H{s)=C{sI-A)-^B^D. (6.19) (6.20) Recalling that ^[e^^]  =  O(^)  =  {si—A)  ^ [refer to (4.32)] we could of course have obtained (6.20) directly by taking the Laplace transform  of H{t)  given in (6.12). EXAMPLE 6.5.  In Example 6.2 let ^o = 0 and x(0) = 0. Then H{s) = C{sI-A)-^B  + D =  [01] '  " rl s ^[01] l1 1 s and H{t)  = ^ ^H{s) =  1  for ^ > 0 as expected (see Example 6.2). 169 CHAPTER  2: Response of Linear Systems Next as in Example 6.2 let x(0)  =  (1-1)^ and let u be the unit step. Then y(s)  = C(sl  -  A)-ix(O) + H(s)u(s)  =  [0 1/^](1 -1)^ + (l/s)(l/s)  =  -1/s  + 1/s^ and y(t)  = • ^~^[y(s)]  =  -1  + ^ for / >  0 as expected (see Example 6.2). We note  that  the  eigenvalues  of  the  matrix A  in  Example  6.5  are  the  roots  of the  equation  det  (si  -  A)  =  s'^ =  0  and  are  given  by  ^-i  =  0 ^2  =  0  while  the transfer function  H(s)  in this example has only one pole (the zero of its denominator polynomial) located at the origin. It will be shown in Chapter 5 (on realization) that the poles  of  the  transfer function  H(s)  (of  a  SISO  system)  are  in  general  a  subset of  the  eigenvalues  of A.  In  Chapter  3  we  will  introduce  and  study  two  important system theoretic concepts called controllability  and observability.  We will show in Chapter  5 that  the eigenvalues  of A are precisely  the poles  of the transfer  function H(s)  =  C(sl  -  Ay^B  + D if and only if the system (6.8a) (6.8b) is observable and controllable. This is demonstrated  in the next example. r 5  = -2^ EXAMPLE 6.6.  In (6.8a) (6.8b) let A = 0' w  C  =  [-33]/)  =  0. The eigenvalues ofA are the roots of the equation J^r (5/-A)  =  s^ + 2s+l  =  (^+1)^  = 0 given by ^i  =  - 1  5*2  =  - 1  and the transfer function of this SISO system is given by 0 [-1 H(s)  = C(sl-A)-^B  + D  = [-33]^ s . - 11 _. -1 1  ^ + 2 =  3 [ - l  l] 1 (S  +  1)2 5 + 2  1 -1 s 3(s -  1) (S+ 1 ) 2' with poles (the zeros of the denominator polynomial) also given by si -1^2  = - 1. If  in  Example  6.6  we  replace  B  =  [0 1]^ and  D  =  0  by  B  = and D  =  [0 0] then we have a multi-input  system whose transfer  function  is given by H(s)  = ' 3 ( ^ - 1) (S+  1)2^(^+1) 3 The concepts  of poles and  zeros for  MIMO  systems  (also called  multivariable  sys tems)  will be introduced  in Chapter 4. The determination  of the poles  of  such  sys tems  is  not  as  straightforward  as  in  the  case  of  SISO  systems.  It  turns  out  that  in the  present  case  the  poles  of  H(s)  axe si  =  -1S2  =  - 1 the  same  as  the  eigen values of A. Before proceeding to our next topic the equivalence of internal representations an observation concerning the transfer  function  H(s)  of system (6.8a) (6.8b) given by (6.20) H(s)  =  C(sl  — A)~^B  +  D is in order. Since the numerator matrix poly nomial  of  (si  —  A)"^  is of degree  (n  -  1) (refer  to Subsection  2.1G) while its de nominator  polynomial  the characteristic  polynomial  a(s)  of A is of degree  n it is clear that \im H(s)  =  D a real-valued mXn  matrix and in particular when the  ''direct link matrix"  D in the output equation  (6.8b) is zero then lim^(^)  =  0 170 Linear Systems the m X n matrix with zeros as its entries. In the former case (when D 7^  OorD  =  0) ^(^)  i^ ^^^^ ^^ ^^  di proper  transfer function  while in the latter case (when D  =  0) H(s)  is said to be a strictly proper  transfer  function. When discussing the realization of transfer functions by state-space descriptions (in  Chapter  5) we  will  study  the  properties  of  transfer  functions  in  greater  detail. In this connection  we will also encounter  systems that can be described by  models corresponding to transfer functions H(s)  that are not proper  The differential  equation representation  of a differentiator  (or an inductor)  given by  y(t)  =  (d/dt)u{t)  is one such example. Indeed in this case the system cannot be represented by Eqs. (6.8a) (6.8b) and the transfer  function  given by H(s)  =  s is not proper. Such systems will be discussed in Chapter 7. C.  Equivalence  of Internal  Representations In  Subsection  2.4B  it  was  shown  that  when  a  linear  autonomous  homogeneous system  of  first-order  ordinary  differential  equations  i  =  Ax  is  subjected  to an  ap propriately  chosen  similarity  transformation  the resulting  set  of  equations  may  be considerably  easier to use  and may  exhibit  latent properties  of the  system  of  equa tions. It is therefore natural that we consider a similar course of action in the case of the linear systems (6.1a) (6.1b) and (6.8a) (6.8b). We begin by considering  (6.8a) (6.8b) first letting X =  Px (6.21) where P  is a real nonsingular matrix (i.e. P is a similarity transformation).  Consis tent with what has been  said thus far  we see that  such transformations  bring  about a change  of basis  for  the state space of system  (6.8a) (6.8b). Application  of  (6.21) to this  system will result  as will be  seen in a system description  of the  same  form as (6.8a) (6.8b) but involving different  state variables. We will say that the  system (6.8a) (6.8b) and the system obtained by subjecting (6.8a) (6.8b) to the transforma tion  (6.21) constitute  equivalent  internal  representations  of an underlying  system. We will  show that equivalent  internal representations  (of the same system)  possess identical  external  descriptions  as  one  would  expect  by  showing  that  they  have identical  impulse responses  and transfer  function  matrices. In connection  with  this discussion  two  important  notions  called  zero-input  equivalence  and  zero-state equivalence  of a system will arise in a natural  manner. If  we  differentiate  both  sides  of  (6.21)  and  if  we  apply  x  =  P~^x  to  (6.8a) (6.8b) we obtain the equivalent internal representation  of (6.8a) (6.8b) given by where A  =  PAP-\ B  =  PB C  =  CP-\ D  =  D k  =^  Ax  + Bu y  =  Cx  + Du (6.22a) (6.22b) (6.23) and where x is given by (6.21). It is now easily verified that the system (6.8a) (6.8b) and  the  system  (6.22a)  (6.22b)  have  the  same  external  representation.  Recall  that for  (6.8a) (6.8b) and for  (6.22a) (6.22b) we have for the impulse  response H(t  T)  ^  Hit  -  T 0)  =  { ^'"""'^ +  ^^(' -  ^)' ;  J  ;> (6.24) and H{t  r)  =  H{t  -  r 0) C^ia-T)^  +  D8(t  -  T) 0 (6.25) t  <  T. Recalling from  Subsection  2.4B  [see Eq. (4.13)] that p^A(t-T)p-l ^A(r-T)  ^ (6.26) 171 CHAPTER  2: Response of Linear Systems weobtainfrom  (6.23) to (6.25) that C^^~^^-^)B+DS(r-T)  = CP-^Pe^^'-'^p-^PB^-DS(t  -T)  =  Ce^^^-'^^B + D8(t  -  r) which proves in view of (6.24) and (6.25) that and this in turn shows that H(t  T)  -  H(t  T) His)  =  H(s\ (6.27) (6.28) This last relationship can also be verified  by observing that^(^)  == C(sl  -  A)  ^  X B  -{-  D  =  CP-\sI -  A)-^p-^PB  +  D  = C(sl  -  A)-^B  +  D  -  H(s). -  PAP-^)-^PB  +  D  =  CP-^P(sI Next recall that in view of (6.10) we have for  (6.8a) (6.8b) that y(t)  =  Ce^^'-'^^xo  +\  Hit- T0)u(T)dT J to =  il/(t to XQ 0) +  p(t to 0 u) (6.29) and for  (6.22a) (6.22b) that y(t)  =  Ce^^'-'^ho  + H(t-T0)u(T)dT =  {{/(t to xo 0) +  p(t to 0 u) (6.30) where ifj and ^  denote the zero-input response of (6.8a) (6.8b) and (6.22a) (6.22b) respectively  while  p  and  p  denote  the  zero-state  response  of  (6.8a)  (6.8b)  and (6.22a)  (6.22b)  respectively.  The  relations  (6.29)  and  (6.30)  give  rise  to  the  fol lowing  concepts:  Two  state-space  representations  are zero-state  equivalent  if  they give  rise  to the  same  impulse  response  (the  same  external  description).  Also  two state-space representations are zero-input  equivalent  if for any initial state vector for one representation  there  exists  an  initial  state  vector  for  the  second  representation such that the zero-input responses for the two representations  are identical. The following  result is now clear:  if two state-space  representations  are  equiv alent  then  they  are  both  zero-state  and  zero-input  equivalent.  They  are  clearly zero-state  equivalent  since  H{t r)  ^  H{t  r).  Also  in  view  of  (6.29)  and  (6.30) we  have  Ce^^'-'^^xo  =  (CP-^)[Pe^^'-'^^p-^]xo  =  Ce'^^'-'^ho  where  (6.26)  was used. Therefore  the two state representations  are also zero-input  equivalent. The  converse  to  the  above  result  is  in  general  not  true  since  there  are  repre sentations  that  are both  zero-state  and  zero-input  equivalent  yet not equivalent.  In Chapter  5 which  deals  with  state-space  realizations  of  transfer  functions  we  will consider this topic  further. EXAMPLE 6.7.  System (6.8a) (6.8b) with 0 -2 1 - 3. B = 0 .1. C  =  [-1-5] D  = 1 172 Linear  Systems has the transfer  function H{s)=C{sI-A)-^B + D Using the similarity  transformation - 5 ^ -1 5^ +  35 +  2 +  1 ( . - 1 )^ (5+l)(5 +  2)' 1 -1 -1 1" -2 2 -1 1" -1 yields the equivalent representation  of the system given by PAP-B =  PB-c = cp-^[49] and  D  =  D  =  1.  Note  that  the  columns  of  P~^  given  by  [1-1]^  and  [1-2]^  are eigenvectors  of A corresponding  to  the  eigenvalues  Ai  =  —1^2  =  —2 of A  that  is  P was chosen to diagonalize A. Notice that A is in companion form so that its characteristic polynomial  is  given  by  5^ +  3^ +  2 =  {s-\-1){s-\-2).  Notice  also  that  the  eigenvectors given  above  are  of  the  form  [1A;]^/  =  12.  The  transfer  function  of  the  equivalent representation  of the system is now given by 1 H(s)  = C{sI-A)-^B  + D =  [40]  5 +1 0 0 1 5 +  2. +  1 - 5 5 -1 (5+l)(5 +  2) +  1 =  H(s). Finally it is easily verified  that e^' Pe^'p-K From  the  above  discussion  it  should  be  clear  that  systems  [of  the  form  (6.8a) (6.8b)]  described  by  equivalent  representations  have identical  behavior  to the  outside world  since both  their  zero-input  and  zero-state responses  are the  same. Their  states however  are  in  general  not  identical  but  are  related  by  the  transformation  x{t)  = Px{t). In the time-invariant  case considered  above transformation  P preserves the  qual itative properties  of the equivalent  representations  of a system  since in particular  the eigenvalues  of A  and A  are  identical. Next  we  consider  time-varying  systems  given  by x=A{t)x  + B{t)u y =  C{t)x  + D{t)u (6.31a) (6.31b) where  all  symbols  are  defined  as  in  (6.1a)  (6.1b).  Let  P  G C^{R^R^^^)  and  assume that P~^  (t)  exists  for  dllt  eR  and  is continuous.  Let X =  P(t)x. (6.32) T\\Qnx  =  P{t)x  + P{t)x= +  B{t)udind y  =  C{t)P~^{t)x  +  D{t)u  =  C{t)x  +  D{t)u.  These  relations  motivate  the  following definition:  the  system [P{t)+P{t)A{t)]p-^{t)x + P{t)B{t)u=A{t)x x=A{t)x  + B{t)u y =  C{t)x  + D{t)u (6.33a) (6.33b) where x  =  P{t)x  P  E  C^{R R^^^)  and P  ^ is assumed to exist and be continuous for  all  t  G  R  and  where  A(t)  =  [P(t)A(t)  +  P(t)]p-\tX  Bit)  =  P(t)B(t)  C(t)  = C{t)P~^{t)  bit)  =  D(t)  is said to be equivalent  to the system  (6.31a) (6.31b). As in the time-invariant case the relations between the state transition  matrices 173 CHAPTER  2: Response of Linear Systems ^{t  to) and ^(t  to) for the systems of  equations and X =  A(t)x X =  A(t)x (6.34) (6.35) respectively  and the relations between the impulse responses H(t  r)  and H(t  r)  of (6.31a)  (6.31b)  and  (6.33a)  (6.33b)  respectively  are  easily  established.  Indeed since  the  solutions  of  (6.34)  and  (6.35)  are  given  by  (pit to xo)  =  ^it  to)xo  and 4>it to Xo)  =  ^(t  to)xo  respectively  we  have  in  view  of  (6.32)  (assuming  that P~^  exists  for  all  t  G  R)  P~\t)4>it  to xo)  =  ^it  to)[P~\to)xo]  or  4>it to xo)  = to)P~^ito)xo.  Since the  solutions  of  (6.34)  and  (6.35)  are unique we  have Pit)^it that 0(rT)  = Pit)<^itT)p-\T) for  all tT ^R Recalling  that  the  columns  of  a fundamental  matrix  "^  of  (6.34)  and  a  funda mental  matrix  #  of  (6.35)  are  linearly  independent  it  is  not  hard  to  show  using (6.32) that # (0  -  P ( 0 ^ (0  for  all t  G  R Next  recalling  that  the  impulse  responses  of  the  equivalent  systems  (6.31a) (6.31b) and (6.33a) (6.33b) are given by Hit  T)  = Cit)^it  T)BiT)  +  Dit)8it 0 - T ) and Hit  T)  ^  J  Cit)^it  T)BiT)  +  Dit)8it  -T) 0 respectively it is easily  shown that Hit  T)  =  Hit  T). Indeed we have that Hit  T)  =  Cit)^it  T)BiT)  +  Dit)8it  -  T) t^  T r <  T t^  T t<T =  Cit)p-\t)Pit)^it =  Cit)^it  T)BiT)  +  Dit)8it  -  T) =  Hit  T) T)p-\T)PiT)BiT) +  Dit)8it  -  T) for t  >  T. We conclude by noting that the notions of zero-state  equivalence  and  zero-input equivalence  introduced  for  time-invariant  systems  of  the  form  (6.8a)  (6.8b)  carry over  without  changes  for  time-varying  systems  of  the  form  (6.31a)  (6.31b).  Fur thermore  identically  to  the  time-invariant  case  it  can  be  shown  that  in  the  case of time-varying  systems if  two  state representations  [such  as  (6.31a)  (6.31b)  and (6.33a) (6.33b)]  are equivalent  then they are both zero-state and zero-input  equiv alent. The converse to this statement however is not true. 174 Lhi^Systems 2.7 STATE EQUATION  AND  INPUT-OUTPUT  DESCRIPTION OF DISCRETE-TIME  SYSTEMS In this section which consists of five subsections we address the state equation  and input-output  description  of  Hnear  discrete-time  systems.  In  the  first  subsection  we study the response of Hnear time-varying  systems and linear time-invariant  systems described  by  the  difference  equations  (1.15.3a)  (1.15.3b)  [or  (1.1.7a)  (1.1.7b)] and  (1.15.4a)  (1.15.4b)  [or  (1.1.8a)  (1.1.8b)] respectively.  In  the  second  subsec tion  we  consider  transfer  functions  for  linear  time-invariant  systems  while  in  the third  subsection  we address the equivalence  of the internal representations  of time-varying  and  time-invariant  linear  discrete-time  systems  [described  by  (1.15.3a) (1.15.3b) and (1.15.4a) (1.15.4b) respectively]. Some of the most important classes of discrete-time systems include linear sampled-data systems that we develop in the fourth  subsection. In the final part of this section we address modes and  asymptotic behavior of linear time-invariant  discrete-time  systems. A.  Response  of Linear Discrete-Time  Systems We now return  to  Section  1.15  to consider  once  again  systems  described  by  linear time-varying  equations of the  form x(k  +1)  =  A(k)x(k)  +  B(k)u(k) y(k)  =  C(k)x(k)  +  D{k)u{k) (7.1a) (7.1b) /?^><^ B  : Z-^  T^^^^ C  : Z-^  RP""^  md  D  : Z  ^  RP"""^.  When where  A:Z^ A(k)  =  AB(k)  =  BC(k)  =  C and/)(/:)  =  Z) we have systems described by linear time-invariant  equations given by x(k  +  1)  =  Ax(k)  +  Bu(k) y(k)  =  Cx(k)  +  Du(k). (7.2a) (7.2b) We recall that in (7.1a) (7.1b) and in (7.2a) (7.2b) x denotes the state vector u de notes the system input and y denotes the system output. For given initial conditions ko E  Z x(ko)  =  Xk^ G R^  and for  a given input w both equations  (7.1a) and  (7.2a) possess unique solutions x(k)  that are defined  for  all  k  >  ^o^ and thus the response y(k)  for  (7.1b) and for  (7.2b) is also defined  for all  k  >  ko. Associated  with  (7.1a)  is  the  linear  homogeneous  system  of  equations  given by xik  +  1)  =  A(k)xik). We recall from  Section  1.15  that the solution of the initial-value  problem x(k+l)  =  A(k)x(k) x(ko)  =  Xk (7.3) (7.4) is given by x(k)  =  ^{k  ko)xk  =  n  Mj)xk k  >  ko (7.5) where  ^(k  ko)  denotes  the  state  transition  matrix  of  (7.3)  with ^(h k)  =  I (7.6) [refer  to  (15.9)  to  (15.12)  in  Chapter  1]. Common  properties  of  the  state  transition  matrix  (&(/: /)  such  as  for  example the  semigroup  property  given  by 175 CHAPTER 2: Response of Linear  Systems ^{k /)  =  0 ( ^  m)<^(m  I) k>  m> I can quite easily be derived from  (7.5) (7.6). We caution the reader however that  not all  the  properties  of  the  state  transition  matrix  $(f  r)  for  continuous-time  systems X  =  A(t)x  carry  over  to  the  discrete-time  case  (7.3).  In  particular  we  recall  that  if for  the  continuous-time  case  we  have  t  >  r  then  future  values  of the  state  (p at  time t  can  be  obtained  from  past  values  of  the  state  cf) at  time  r  and  vice  versa  from  the relationships  (/)(0  =  ^(t  T)(f){T)  and  (/)(T)  =^  ^~^{t  r^t) =  0 ( T  0</>(0.  i-e.  for continuous-time  systems  a principle  of  time  reversibility  exists.  This  principle  is  in general  not  true  for  system  (7.3) unless  A~^(k)  exists  for  all  k  G  Z.  The  reason  for this  lies  in  the  fact  that  <i>(k  I)  will  not be  nonsingular  if  A(k)  is not  nonsingular  for a l U. Associated  with  (7.2a)  is the linear  autonomous  homogeneous  system  of  equa tions  given  by x(k^ 1)  =  Axik). From  (7.5)  it  follows  that  the  unique  solutions  of  the  initial-value  problem x(k  +  1)  =  Ax(k\ x(ko)  =  Xk^ are  given  by xik)  =  ^{h  ko)xk  =  A^'-^'hk^ k  ^  ko. (7.7) (7.8) (7.9) EXAMPLE  7.1.  In  (7.3)  we  let  ^o  =  0  and  A(k) A(k- 1)-"A(0)  = \(k  -  1) (k-  1)2 +  1 x(3)  =  A(2)  • A(l)  • A(0)  •  x(0) 0 a)'-' [2  5 0 Ik 0 (k^  +  1)1 a)' Then  ^(k  0) If  for  example  k  =  3  then x(0) "0 0 x(0).  Given x(0)  we  can  now  readily  determine  x(3).  In  view  of  the  form  A(0)  we  have  for  any To  xl 0 ^  >  0  ^{h  0) zero  for  any  k.  Clearly  then  for  any  initial  condition  x(0)  =   that  is  to  say  the  first  column  of  0(^ 0)  will  always  be a  E  /?  we  have \0] x(k) for  all k>0. EXAMPLE  7.2.  In  (7.5) let A  = 1  0 0  0 x(0)  = a  E: R.  The initial  state  x(0)  at ko  ^  0 for any a  G R will map into the state  x(l) . Accordingly in this case time reversibility  will not apply. EXAMPLE  7.3.  In  (7.8)  let  A  = 1  2 0  1 .  In  view  of  (7.9)  we  have  that  A^^  ^o^  = 1 0 1 k  >  ^0  i.e.  A^^  ^o)  =  A  when  (k  -  ko)  is  odd  and 176 Linear Systems j^(k  kQ)  ^  /when(^-/:o)iseven. Therefore given ^0 = Oandx(O) thQnx(k)  = Ax(0) k=  135  ...andx(y^)  -  /x(0)  ^  =  2 4 6  ....A  plot of the states x(k)  =  [xi(k) X2(k)]^ is given in Fig. 2.3. i W| I x^ik) 2.OH ^ f T T f  ^-^ 1.0-—•- - •- —•- -•— 0  1 2 3 4 5 6 7 8— FIGURE 2.3 Plots of states for Example 7.3 1.0 k 0 12 3 45 6 7 8' Continuing  we  recall  that  the  solutions  of  initial-value  problems  determined by  linear  nonhomogeneous  systems  (15.13)  of  Chapter  1 are  given  by  expression (15.14). Utilizing (15.13) the solution of (7.1a) for given x(ko)  and  u(k)  is given as x(k)  =  ^(k  ko)x(ko)  +  X  ^(^> J  +  ^)B(jMjX k  >  ko (7.10) This expression in turn can be used to determine  [as in (15.17) and (15.18) of Chap ter  1] the system response for  system (7.1a) (7.1b) as y(k)  =  C(kmk ko)x(ko) k-\ +  ^  C(k)<^(k  j  +  l)B(j)u(j)  +  D{k)u{k) k  >  ko y(ko)  =  C(ko)x(ko)  +  D{ko)u(ko). (7.11) Furthermore  for  the time-invariant  case  (7.2a)  (7.2b) we have  for  the  system re sponse the expression y(k)  =  CA^^'-^'hiko)  +  ^  CA^-^J-'^^Bu(j)  +  Du(k) k  >  k^ k-i y(ko)  =  Cx(ko)  +  Du(ko). (7.12) Since the system  (7.2a) (7.2b) is time-invariant  we can let  ^Q  =  0 without loss of generality  to obtain from  (7.12) the expression y(k)  =  CA^x(0)  +  ^  CA^-^J^^^Bu(j)  +  Du(kX k  >  0. (7.13) k-i j=o As in the continuous-time case the total system  response  (7.11) may be viewed as consisting of two components the zero-input  response  given by il/(k)  =  C(k)^(k  kQ)x(kol k  >  ko and the zero-state  response  given by k-\ p(k) = X  C(kmk  j + l)B(j)u(j)  +  D(kMkl p(ko) D(ko)u(koX 177 CHAPTER 2: Response of Linear Systems k>  k 0. k  =  ko (7.14) Finally in view  of  (16.20)  of Chapter  1 we recall that the  (discrete-time)  unit impulse response matrix of system (7.1a) (7.1b) is given by r C(k)^(k I + l)B(ll H(kJ)  =  loikl [0 k>U k=l k<l and the unit impulse response matrix of system (7.2a) (7.2b) is given by H{kJ)^lD [0 k = h k<l and in particular  when /  =  0 (i.e. when the pulse is applied  at time / = 0) \  CA k- 'B Hik0)  = { D U EXAMPLE  7.4.  In (7.2a) (7.2b) let k>0 k=^0. k<0. (7.15) (7.16) (7.17) A = 0  1" 0 - 1. B = 0 1. ' C^  = "11 oj D  = 0. We first determine A^ by using the Cay ley-Hamilton Theorem (Theorem 3.1 of Chap ter  2).  To this  end  we  compute  the  eigenvalues  of  A as  Ai  = 0 A2 ^  - 1 we  let A^ = /(A)  where  f(s)  = s^ and we let g(s)  = ais  + ao. Then  /(Ai)  == ^(Ai) or ao  = 0 and  /(A2)  = gi^i)  or (-1)^  = -ai  + ao. Therefore  A^ = aiA  + aol  = -(-1) 0 0 0  12 k=  12  ...or A^ (-1)' 8(k) 0 (-l)'-'p(k-l) (-l)'p(k)  where A^ = / and where p(k) denotes the unit step given by p(k) 1  k^O 0 i^ < 0. The above expression for A^ is now substituted into (7.12) to determine the response y(k) for /: >  0 for a given initial condition x(0) and a given input u(k) ^ >  0. To deter mine the unit impulse response we note that H(k 0) = 0 for  ^ <  0 and k  = 0. When k>0  H(h  0) = CA'^-^B = (- l)^-^p(k  -  2) for i^ >  0 or H(K 0) = 0 for i^ =  1  and H(k  0) = (-1)^-^ for y^  =  2 3  .... • B.  The Transfer  Function  and the  z-Transform We assume that the reader is familiar with the concept and properties of the  one-sided z-transform  of a real-valued  sequence {f(k)}  given by t{f{k)}  = f{z)  = J^z-JfU). (7.18) 178 Linear Systems An  important  property  of  this  transform  useful  in  solving  difference  equations  is given by the relation nf(k  + i)} = ^z-^fu  + i) =  ^z O'-i) / ( ;) .j=o (7.19) If we take the z-transform  of both  sides of Eq.  (7.2a) we obtain in view of (7.19) zx(z)  -  zx(0)  =  Ax(z)  +  Bu(z)  or -  /(O)]  =  zf(z)  -  zf(0) =  zmf(k)} x(z)  =  (zl  -  AyhxiO)  +  (zl  -  Ay^Buizy (7.20) Next by taking the z-transform  of both sides of Eq. (7.2b) and by substituting  (7.20) into the resulting expression we obtain y(z)  =  C(zl  -  Arhx(0) +  [C(zI  -  Ay^B  +  D]u{z). (7.21) The time  sequence  {y{k)}  can be recovered  from  its  one-sided  z-transform  y{z)  by applying the inverse  z-transform  denoted by 2E~^[};(z)]. In  Table  7.1  we  provide  the  one-sided  z-transforms  of  some  of  the  commonly used  sequences  and  in  Table  7.2  we  enumerate  some  of  the  more  frequently  en countered properties of the one-sided  z-transform. The transfer function  matrix H(z)  of system (7.2a) (7.2b) relates the z-transform of the output y to the z-transform  of the input u under the assumption that  ;IL:(0)  =  0. We have where H(z)  =  C(zl  -  Ay^B  +  D. y(z)  =  H(z)u(zl (7.22) (7.23) To relate H(z)  to the impulse  response  matrix H(k  /) we notice that ^{d(k -/)}  =  z~K where 8  denotes the discrete-time  impulse  (or unit pulse  or unit  sample) defined  in (16.5) of Chapter  1 i.e. 8{k  -  I) 1 0 k  =  U k¥^L (7.24) TABLE 7.1 Some commonly used z-transforms {f(k)}k^^ 8(k) Pik) k e a' (k + 1V [(imxk+iy acosak  +  bsinak '(k + DW  />  1 f(z)  = ^{f(k)} 1 1/(1-z-0 z-V(l - z-')^ [z-'il  + 1/(1-az-') 1/(1-az-'f 1/(1-az-'y^' [a + z~^(bsina  -  acosa)]/(l  --Iz  ^ cosa  + z ^) z-'Wil-z-'f TABLE7.2 Some properties of z-transforms Time shift —Advance Time shift —Delay Scahng Convolution Initial value Final value {/(*)}*  >o f(k+l) fik  + l) fik-l) f{k-l) / >1 l>\ km lT=om8(k-l) = /(/)with/(fc)=0 \imi^„f{k) m* 8(k) k<l 179 CHAPTER  2: Response of Linear Systems m zf{z)-zm z'fiz)-zlUz'-'fii--^) z-'f{z)+f(-l) z-'f{z)  + Kz/a) -z{d/dz)fiz) f(zmz) \im^„  z'fizY lim^i(l-z-i)/(z)tt l!i=iZ-'+'f{-i) ^ If the limit exists. • ' • t i f ( i - ^ - i^ - z  ^ )f{z)  has no singularities on or outside the unit circle. This implies that the z-transform  of a unit pulse applied at time zero is ^  { 5 (^)} =  1. It is not difficult  to see nov^ that {H{k 0)}  =  ^~^  [y{z)] v^here y{z)  = H{z)u{z)  with u{z) =  1. This shows that ^-\H{z)] =  ^-\C{zJ-A)-^B + D\={H{kQi)} (7.25) where the unit impulse response matrix H{k^ 0) is given by  (1.11). The  above  result  can  also  be  derived  directly  by  taking  the  z-transform  of {H{k0)}  given  in  (7.17)  (prove  this).  In  particular  notice  that  the  z-transform  of {A^-^}k=  12...  is  (zI-A)-^ since z - 1/ \I -1 z  -A + z  ^A 2A2  + •••) • z-Hl-z-'A) = {zI-A)--1 (7.26) 1 + A + A^  +  ---  was Above the matrix  determined  by the expression  (1 — A)~^ used.  It is easily  shown that the corresponding  series involving A  converges.  Notice also that  ^{A^}k  =  012...  is z{zl  — A)~^.  This fact can be used to show that the inverse z-transform  of (7.21) yields the time response (7.13) as expected. We conclude this subsection with a specific  example. EXAMPLE 7.5.  In system (7.2a) (7.2b) we let C=[10] D = 0. To verify that ^  ^[z{zl — A)  ^]  = A^ we compute 1 " z(zi-Ay -1 z +1 = z(z+l) 1 zTT  . 180 Linear Systems and or -'[z{zI-Ar']  U  ^  8(k) 0 (-lY-'p(k-l) {-l)'p(k) A'  = 1  0' 0  1 0 0 (-1)^-^ (-1)^ when k  = 0 when A:  =  1 2... as expected from Example 7.4. Notice that -'[(zI-A)-'] 1 z 0 1 z(z + 1) 1 z+l '' 8(k -  l)p(k  -  1)  8(k -  l)p(k  -  1) -  (-l)^-^p(k  -  1) 0 for "0  0 .0  0. 0 0 -(-l)^-i' (-i)^-M and ^-i[(z/-A)-i] h  ^ (-l)^-^p(k-l) and 1  0 0  1 for k  = 1 for yl =  2 3... which is equal to A^ /: >  0 delayed by one unit i.e. it is equal to A^~^  k  =  12... as expected. Next  we  consider  the  system  response  with  x(0)  =  0  and  u(k)  =  p(k).  We have y(k)  = ^-'[y(z)]  = ^-'[C(zl -  AT'B  • u{z)] ^"^ = _  r 0 1 (z +  l)(z -  1) z-  1 z+  1 \[{i)'-'-{-iY-']p{k-i) ^  =  0 r 0  k  = 0 =  lo [l k= 135... k  =  2A6.... Note that if jc(0)  =  0 and M(^)  =  6(/:) then y{k)  =  %-'[C{zI-A)-'B] iRl  ^ 1 z{z + 1) 8{k-\)p{k-\)~{-\f-'p{k-\) y^  = 01 = ^  I 0 which is the unit impulse response of the system (refer to Example 7.4). C.  Equivalence  of Internal  Representations Equivalent  representations  of linear  discrete-time  systems  are defined  in  a  manner analogous to the continuous-time  case. 181 CHAPTER  2: Response of Linear Systems For systems (7.1 a) (7. lb) we let ^o denote initial time we let P(k)  denote a real nX  n matrix that is nonsingular for  all  k  ^  ko and  we consider the  transformation jc(^)  =  P(k)x(k).  Substituting  the above into (7.1a) (7.1b) yields the system where x(k  +  1)  =  Aik)x(k)  +  B{k)u{k) y(k)  =  C(k)x(k)  +  D{k)u{k) \)A(k)p-\k) A{k)  =  P(k  + B(k)  =  P(k  +  l}B{k) C(k)  =  C(k)p-^ D{k)  =  D{k). (7.27a) (7.27b) (7.28) We  say  that  system  (7.27a)  (7.27b)  is  equivalent  to  system  (7.1a)  (7.1b)  and  we call  P{k)  an equivalence  transformation  matrix. If <i>(^ /) denotes the state transition matrix of (7.3) and ^{k  I) denotes the state transition matrix of then xik  +  1)  =  Aik)x(k) Mkl)  = P{k)Mkl)P~\l) (7.29) (7.30) as can be seen by observing that <J>(^/)  =  A{k-\)---A{l) 1)]-••[/>(/  +  1)A(1)P-\1)]  = P{k)^{kl)p-\l). = [P(k)A(k-l)P-\k-In a similar manner as above it can also be shown that H{k  I)  =  H(k  I) (7.31) where H(k  I) and H(k  I) denote the unit pulse response matrices of systems (7.1a) (7.1b)  and  (7.27a)  (7.27b)  respectively.  [The  reader  should  verify  (7.31).]  Thus equivalent  representations  of linear  discrete-time  system  (7.1a) (7.1b)  give rise to the same unit pulse response matrix. Furthermore zero-state  equivalent  representa tions and zero-input  equivalent  representations  are defined  for system (7.1a) (7.1b) in a similar manner as in the case of linear continuous-time  systems. Turning  our  attention  now  briefly  to time-invariant  systems  (7.2a)  (7.2b)  we let P denote a real nonsingular  nX  n matrix and we  define x{k)  =  Px(k). (7.32) Substituting  (7.32) into (7.2a) (7.2b) yields the equivalent  system  representation where A  =  PAP-x(k  +  1)  =  Axik)  +  Bu(k) y(k)  =  Cx(k)  +  Du(k) B  =  PB c  = cp-(7.33a) (7.33b) (7.34) D  =  D. We note that the terms in (7.34) are identical to corresponding terms obtained for the case of linear continuous-time  systems. We conclude by noting that if H(z)  and H(z)  denote the transfer functions  of the unit impulse response matrices of system (7.2a) (7.2b) and system (7.33a) (7.33b) respectively then it is easily verified  that H(z)  =" H(z). 182 Linear Systems D.  Sampled-Data  Systems Discrete-time  dynamical  systems  arise  in  a  variety  of  ways  in  the  modeling  pro cess.  There  are  systems  that  are inherently  defined  only  at  discrete  points  in  time and there are representations  of continuous-time  systems  at discrete points in time. Examples  of the former  include  digital  computers  and  devices  (e.g. digital  filters) where the behavior of interest of a system is adequately described by values of vari ables  at  discrete-time  instants  (and  what  happens  between  the  discrete  instants  of time is quite irrelevant  to the problem  on hand); inventory  systems  where  only  the inventory  status at the end of each day  (or month) is of interest; economic  systems such as banking where e.g. interests are calculated  and added to savings  accounts at  discrete  time  intervals  only  and  so  forth.  Examples  of  the  latter  include  simu lations  of continuous-time  processes by means  of digital computers making use of difference  equations that approximate the differential  equations  describing  the pro cess in question; feedback  control  systems  that  employ  digital  controllers  and  give rise to sampled-data  systems  (as discussed further  in the following);  and so forth. In  providing  a  short  discussion  of  sampled-data  systems  we  make  use  of  the specific  class  of linear  feedback  control  systems  depicted  in  Fig.  2.4. This  system may be viewed  as an interconnection  of a subsystem S\  called tht  plant  (the  object to be controlled)  and a subsystem ^2 called the digital  controller. u{t) x=A(t)x+ B{t)u y=  C(t)x+  D(t)u y{t  ) D/A i I u ik) FIGURE 2.4 Digital control system w{l<+^)  = F(k)w(k)  + G{k)  y(k) u{k)  = H{k)w{k)  +  Q(k)y(k) \ A/D yik) The plant is described by the  equations X =  A(t)x  +  B(t)u y  =  C(t)x  +  D(t)u (7.34a) (7.34b) where all symbols in (7.34a) (7.34b)  are defined  as in (6.1a) (6.1b)  and where  we assume that ^ >  ^  >  0. The  subsystem  ^2  accepts  the  continuous-time  signal  y{t)  as  its  input  and  it produces the piecewise continuous-time  signal  u(t)  as its output where t  ^  to. The continuous-time  signal y  is  converted  into  a discrete-time  signal  {y(k)}  k> k{)> 0 k ko E  Z by means of an analog-to-digital  (A/D) converter  and is processed  ac cording to a control algorithm given by the difference  equations w(k  +  1)  =  F(k)w(k)  +  G(k)y(k) u(k)  =  H(k)w(k)  +  Q(k)y(k) (7.35a) (7.35b) 183 CHAPTER  2: Response of Linear Systems where  the  w(k)  y{k)  u{k)  are  real  vectors  and  the  F{k)  G{k) H{k)  and  Q{k)  are real  matrices  with  a consistent  set  of  dimensions.  Finally  the  discrete-time  signal {u{k)} ^  >  ^0  — 0.  is  converted  into  the  continuous-time  signal  u by  means  of  a digital-to-analog  (D/A) converter. To simplify  our discussion we assume in the fol lowing that ^0 =  t]^^. An (ideal) A/D  converter  is a device that has as input a continuous-time  signal in  our  case  j  and  as  output  a  sequence  of  real  numbers  in  our  case  {y{k)} k  = k{)  ko +  \...  determined by the relation y{k)  =  y(tk). (7.36) In  other  words  the  (ideal)  A/D  converter  is  a  device  that  samples  an  input  sig nal  in  our  case  y(t)  at  times  toJi... producing  the  corresponding  sequence {y(tol  y(hl  •' •}• A D/A  converter  is a device that has as input a discrete-time  signal in our case and as output  a continuous-time  signal in the sequence {u{k)} k  =  ki^  k^  +  \.. our case w determined by the relation u{t)  =  u(k) tk^ (7.37) In other words the D/A converter is a device that keeps its output constant at the last value of the sequence entered. We also call such a device a zero-order  hold. k  =  ko ko -\-  I... t  <  tk+h The  system  of  Fig.  2.4  as  described  above  is  an  example  of  a  sampled-data system  since  it involves  truly  sampled  data  (i.e. sampled  signals) making  use of an  ideal A/D  converter.  In practice the  digital  controller  ^2 uses digital  signals  as variables. In the scalar case such  signals  are represented  by real-valued  sequences whose numbers belong to a subset ofR  consisting of a discrete set of points. (In the vector case the previous statement applies to the components of the vector.) Specif ically in the present  case after  the  signal  y{t)  has been  sampled  it must be  quan tized (or digitized)  to yield a digital  signal  since only such signals are representable in  a digital  computer.  If  a computer  uses  e.g.  8-bit words  then  we  can  represent 2^  =  256 distinct levels for a variable which determine the signal quantization. By way of a specific  example if we expect in the representation  of a function  a  signal that varies from  9 to 25 volts we may choose a 0.1-volt  quantization  step. Then  2.3 and  2.4 volts  are represented  by two different  numbers  (quantization  levels); how ever 2.315 2.308 and 2.3 are all represented by the bit combination  corresponding to 2.3. Quantization is an approximation  and for short wordlengths may lead to sig nificant  errors. Problems  associated  with  quantization  effects  will not be  addressed in this book. In  addition  to  being  a  sampled-data  system  the  system  represented  by  Eqs. (7.34)  to  (7.37)  constitutes  a  hybrid  system  as  well  since  it  involves  descriptions given by ordinary differential  equations and ordinary difference  equations. The anal ysis  and  synthesis  of  such  systems  can  be  simplified  appreciably  by  replacing  the description  of  subsystem  Si  (the  plant)  by  a  set  of  ordinary  difference  equations valid  only  at  discrete  points  in  time  t^  k  =  0 1 2 [In  terms  of  the blocks  of Fig. 2.4 this corresponds to considering the plant Si  together with the D/A and A/D devices to obtain  a system  with  input  U(k) and  output  y(k)  as shown  in Fig.  2.5.] To accomplish this we invoke the variation of constants formula in (7.34a) to obtain x(t)  =  ^(tJkXh) +  I  ^(tT)B{T)u(T)dT (7.38) 184 Linear Systems u(k)^ D/A  ^(0 x=A(t)x+ B(t)u y=  C(t)x+  D(t)u y(t)  A/D y(k) FIGURE 2.5 System described by (7.40) and (7.43) where  the  notation  ^{ttkx{tk))  = x{t)  has  been  used.  Since  the  input  u{t)  is  the output of the zero-order hold device (the D/A converter) given by  (131)  we obtain from  (7.38) the expression :^{tk+i)=^{tk+htk)x{tk)-/ Jtj 0(f^+iT)5(T)JT u{tk). (7.39) Since x{k)  =  x{tk)  and  u{k)  =  u{tk) we  obtain  a discrete-time  version  of the  state equation for the plant given by where x{k^l) =A{k)x{k)+B{k)u{k) ^{tk+iT)B{T)dT. (7.40) (7.41) Next we assume that the output of the plant is sampled at instants ?[ that do not necessarily  coincide  with  the instants  t^ at  which  the input  to the plant  is  adjusted and we assume that h  <t'i^< f^+i.  Then (7.34) and (7.38) yield y{t'k)=c{ti)^it'„tk)x{tk)-C(t')f''0{tiT)B{T)d Jth u{tk)+D{t'k)u{tk). (7.42) Defining y{k)  =  y{t'j^ we obtain from  (7.42) y{k)=C{k)x{k)  +  D{k)u{k) (7.43) where C{k)  ^ C{t')^{ify) D{k)^C{t') f'^{t[x)B{x)dx +  D{t'^. (7.44) Summarizing  (7.40)  and  (7.43)  constitute  a state-space representation  valid  at discrete  points  in  time  of  the  plant  [given  by  (7.34a)]  and  including  the  A/D  and D/A devices [given by (7.36) and (7.37); see Fig. 2.5]. Furthermore the entire hybrid system  of Fig. 2.4  valid  at discrete points in time  can now be represented  by Eqs. (7.40) (7.43) (7.35a) and (7.35b). We  now  turn  briefly  to  the  case  of  the  time-invariant  plant  where  A{t)  = A^B{t)  = 5C{t)  =  C and D{t)  =  D and we assume that f/.+i  —tk = T and tj^ — tk =  a for alU  =  012....  Then the expressions given in (7.40) (7.41) (7.43) and  (7.44) assume the  form x{k+l)=Ax{k)^Bu{k) y{k)=Cx{k)+Du{k) (7.45a) (7.45b) where A  =  €""8  = „AT  dT\B C  =  C e ^ "  D  -^ij?"-dr  \B  +  D. 185 CHAPTER 2: Response of Linear  Systems (7.46) If  t[  =  tk  or  a  = 0  then  C  =  C  stnd D  =  D. In the preceding  T  is called  the sampling  period  and  1/T  is called  the  sampling rate.  Sampled-data  systems  are  treated  in  great  detail  in  texts  dealing  with  digital control  systems  and  with  digital  signal  processing. EXAMPLE  7.6.  In the control system of Fig. 2.4 let A  = B  = C  =  [10] D  =  0 let T denote the sampling period and assume that a  =  0. The discrete-time  state-space representation  of the plant preceded by a zero-order hold (D/A converter) and  followed by  a  sampler  [an  (ideal)  A/D  converter]  both  sampling  at  a  rate  of  1/r  is  given  by x(k  +  1)  =  Ax(k)  + Bu(k\  y(k)  =  Cx{k\  where A  =  „AT -m A^  = T  = B^[\ e^^dr\B  = dr 2 T [10]. 0 c =^  c The transfer  function  (relating  y to u) is given by H(z)  =  C(zl  -  Ay^B =  [10] [10] z -1 0 1 iz-l) 0 T^  (z  +  1) 2 (z-ir 1 2 T (z -  ly 1 7^ 2 T The transfer  function  of the continuous-time  system (continuous-time  description  of the plant) is determined  to be H(s)  =  C(sl  -  A)~^B  =  IIs^  the double  integrator. The behavior  of the  system  between  the  discrete  instants  ttk  — t  <  tk+i can  be • determined by using  (7.38) letting  x(tk)  =  x{k)  and  u(tk)  =  u{k). An  interesting  observation  useful  when  calculating  A  and  B  is that both can  be /-hrA-h(r^/2!)A^H--h  • • •  =  X^-=o(T^ -T^(T)BAf expressedintermsofasingleseries.  In particular  A  =  e^^  = • • •  =  /  -h  TA'i^iTX  where  ^ ( 7)  =  /  +  (r/2!)A  +  {T^IV.)A^ (j  +  l)!)AAThenJ5  -  {\^  e^'dr)B ^ ( r)  is  determined  first  then  both  A  and  B  can  easily  be  calculated. =  (Z^J=Q{TJ^^IU +  iyW)B = 186 Linear Systems EXAMPLE  7.7.  In  Example  7.6  ^(7)  = I +  TA TA^iT)  =  1  T 0  1 and B = 7^(7)5 = \T^I2  as expected. 1  T 0  1 .  Therefore A = I + E.  Modes  and Asymptotic  Behavior  of Time-Invariant  Systems As in the case of continuous-time systems we study in this subsection the qualitative behavior  of  the  solutions  of  linear  autonomous  homogeneous  ordinary  difference equations x{k  +  1) = Ax{k) (7.47) in  terms  of  the modes  of  such  systems  where A  G R^^^  and  x{k)  G  R^  for  every k ^Z^.  From before the unique solution of (7.47) satisfying  x(0)  =  XQ is given by (f>{k 0 xo)  = A^jco. (7.48) Let Ai... Ao- denote the a distinct eigenvalues  of A where A/ with / =  1... cr is assumed to be repeated  nt times so that Xr= i ^i  = n. Then det(zI-A) =  f](z-AO"^ (7.49) i = i To introduce the modes for  (7.47) we first derive the  expressions A^ = X / =1 m-i / =  i AioXfp{k)  + 2  Auk(k  - ! ) • • • ( / : - /+  l)\t^p(k -  /) -  X[^^o^'/^(^)  +  ^nfcAf-V(^  -  1) i = l +  • • • +  Ai^n-i)k(k  -l)'"(k-ni + 2)Af-^"^-iV(^ "  m + 1)] where A.7 = 1 II (Hi- 1 l-/)!z-.A \im{[(z-Xir(zI-Ar'] U(ni-l-l) (7.50) (7.51) }. In (7.51) ['l^^^ denotes the ^th derivative with respect to z and in (7.50) p(k)  denotes the  unit  step  [i.e.  p(k)  = 0 for fc < 0 and  p(k)  = 1 for fc >  0]. Note  that if an eigenvalue A^  of A is zero then (7.50) must be modified.  In this case fii-i ^Aul\8(k-l) /=o (7.52) are the terms in (7.50) corresponding to the zero eigenvalue. To prove  (7.50) (7.51) we proceed  as in the proof  of (4.36) (4.37). We recall that {A^} = '^'^[zizl  -  A)~^] and we use the partial fraction  expansion  method to determine  the  z-transform.  In  particular  as  in  the  proof  of  (4.36)  (4.37)  we  can readily verify  that ( = 1  / = 0 (7.53) where the An  are given in (7.51). We now take the inverse z-transform  of both sides of (7.53). We first notice that e-1  [z(z-AO-^'+'^]  = r'[z-\i  - Xiz-'r^'-^'h  =  f(k - i)p(k - /) f(k  -  I) 0 for  k> I otherwise. 187 CHAPTER  2: Response of Linear  Systems Referring  to Tables 7.1 and 7.2 we note that f(k)p(k)  =  ^"^[(1  -  XiZ'^y^^-^^^]  = [l/n(k  +  1) • • • (yfc +  /)]Af  for Ai #  0 and /  ^  1. Therefore ^-^[llziz llf{k-l)p(k-l) A/z"^)~^]  =  Af.  This  shows that (7.50) is true when  A #  0. Finally if A  =  0 we note that ^-^[l\z~^]  =  l\8(k  -  /) which implies (7.52). l.For/  =  0  w e h a v e S - i [ ( l-=  k(k-l)'"(k-l + l)Xf-Kl^ -  A/)-^^+i)]  = Note that one can derive several alternative but equivalent expressions for (7.50) that  correspond  to different  ways  of determining  the inverse z-transform  of  z(zl — A)~^  or of determining A^  via some other methods. In  complete  analogy  with  the  continuous-time  case  we  call  the  terms Aiik(k  -  I)'  "(k  -  I -\-  l)Xf~^  the modes  of  the system  (7.47). There  are  ni  modes corresponding  to the eigenvalues  A/ /  =  0  . . .  n^ -  1 and the system  (7.47) has a total of/I  modes. It is particularly interesting to study the matrix A^  k  =  012... using the Jor dan  canonical  form  of A  i.e.  /  =  P~^AP  where  the  similarity  transformation  P is constructed by using the generalized  eigenvectors of A. We recall once more that [/j]  where  each  rit X Ui  block  //  corresponds  to  the /  =  diag[Ji... eigenvalue  A/  and  where  in  turn  Ji  =  diaglJn... ///.]  with  Jfj  being  smaller square blocks the dimensions  of which  depend  on the length  of the chains of  gen-erahzed  eigenvectors corresponding  to Ji  (refer  to Subsections  2.3G and 2.4B). Let Jij  denote a typical Jordan canonical form block. We shall investigate the matrix  J^j since A^  -  p-^j'^P  =  P'^ diag[jfj]P. J(j\=diag Xi 1 0 0  A/ '•. Let Jij  = =  A//  +  A^- (7.54) 0 0 1 A/ where Ni  = 0  1 0  0 0  0 and where we assume that Jij  is at  X  t matrix. Then (jijr  =  (A-/ +  NiY =  Af/  +  kXf-'^Ni  +  '^''  ^'kf'-Nf ^ ( ^ - l ) v i - 2 ^ r2 + +  kXiNf'  + Nf. (7.55) n Linear  Systems Now  since  A^^^  =  0  for  k  ^ t  a. typical  t  X  t  Jordan  block  Jtj  will  generate terms  that  involve  only  the  scalars  Af Af~^  . . . Xf~^^~^\  Since  the  largest  pos sible block  associated  with the eigenvalue  A/ is of dimension  nt  X ut the  expression of  A^  in  (7.50)  should  involve  at  most  the  terms  Af  Xf~\ ...  Af"^''''^^  which  it does. The  above  enables  us to prove  the following  useful  fact:  given  A^R^^^ there exists  an integer  k>  0  such  that A^  =  0 (7.56) if and only if all the eigenvalues  A/ of A are at the origin. Furthermore the smallest k for  which  (7.56) holds is equal to the dimension  of the largest block Jij  of the Jordan canonical  form  of A. The  second  part  of the above  assertion  follows  readily  from  (7.55). We ask the reader  to prove  the first  part  of the  assertion. We  conclude  by  observing  that  when  all  n  eigenvalues  A/  of  A  are  distinct then n A*  =  ^ A / A f  y t>  0 i = \ where A-  = lim[(z  -  A / ) ( z /-  A r M- (7.57) (7.58) If \i  =  0 we use 8(k)  the unit pulse in place of Af in (7.57). This  result is  straight forward  in view  of (7.50)  (7.51). EXAMPLE 7.8.  In (7.47) we let A  = 0  n 1 -'4 The eigenvalues of A are Ai _  1 and therefore  rii  =  2 and a  =  I. Applying  (7.50) (7.51) we obtain Aio\'lp(k)  + 1  0 0  1 P(k)  + AnkX\-'p(k-l) r_ 1 1 i  1 4 2 (k) k-i p(k  -  1). EXAMPLE  7.9.  In  (7.47)  we let A  = -1  2 0  1 . The eigenvalues  of A  are A1  = - 1 A2 =  1 (so that a  =  2). Applying  (7.57) (7.58) we obtain ik  ^  AioAj  +  A20A2 k  _  1 0 -1 0  ( - 1 )'  + 0  1 0  1 k^O. Note that this same result was obtained by an entirely different  method in Example 7.3. EXAMPLE  7.10.  In  (7.47)  we  let  A  = [0 [0 1 -1 .  The  eigenvalues  of  A  are Ai 0 A2 =  -1  and o-  =  2. Applying  (7.57) (7.58) we obtain Ai  =  l i m [ z ( z / - A ) - i]  = 1  ^ z +1  1 z 0 A2  =  lim  ( z + D-1 z(z  + 1) z+  1  1 z 0 and A^  =  Ai8(k)  + A2(-lf = 1  1 0  0 8{k)  + 1  1 0  0 {-l)\k^ 0. z = 0 -1 0 0 1 0  - 1" 1 0 189 CHAPTER  2: Response of Linear Systems As  in  the  case  of  continuous-time  systems  described  by  (L)  various  notions of  stability  of  an  equilibrium  for  discrete-time  systems  described  by  linear  au tonomous homogeneous  ordinary  difference  equations  (7.47) will be studied in de tail in Chapter 6. If 4>(k 0 Xe) denotes the solution of system (7.47) with x(0)  =  Xe then  Xe is  said  to  be  an  equilibrium  of  (7.47)  if  (pik 0 Xe)  =  Xe for  all  k>  0. Clearly Xe =  0 is an equilibrium of (7.47). In discussing the qualitative properties it is customary  to speak  somewhat  informally  of the  stability properties  of  (7.47) rather than the stability properties of the equilibrium  x^  =  0 of system (7.47). The  concepts  of stability  asymptotic  stability  and  instability  of  system  (7.47) are  now  defined  in  an  identical  manner  done  in  Subsection  2.4C  for  system  (L) except  that  in  this  case  continuous  time  t{t  E  7?+)  is  replaced  by  discrete  time k{k  G Z+). By inspecting  the modes of system  (7.47)  [given by (7.50) and  (7.51)] we can readily establish the following  stability  criteria: 1.  The system (7.47) is asymptotically  stable  if and only if all eigenvalues  of A are within the unit circle of the complex plane (i.e. |Ay| <  1 j  =  1...  a). 2.  The  system  (7.47)  is  stable  if  and  only  if  |Aj|  <  1  j  =  1... cr  and  for  all eigenvalues  with  \Xj\  =  1 having multiplicity  rij >  1 it is true that lim [[z -  XjTKzI  -  Ar^]^^J-^-^^]  =  0 z-^Aj for/  =  l..nj- 1. (7.59) 3.  The system (7.47) is unstable  if and only if (2) is not true. EXAMPLE?.11.  The system given in Example 7.8 is asymptotically stable. The system given in Example 7.9 is stable. In particular note that the solution (j){k 0 x(0))  = A^x(O) for Example 7.9 is bounded. • When  the eigenvalues  A/ of A  are distinct  then  as in the continuous-time  case [refer to (4.42) (4.43)] we can readily  show that A'=Y.^j4^J='^J^J^ ^ - 0 (7.60) where the vy and Vj are right and left  eigenvectors of A corresponding to Ay respec tively. If Ay  =  0 we use 8{k)  the unit pulse in place of A^ in (7.60). In proving (7.60) we use the same approach as in the proof of (4.42) (4.43). We have A^  =  Qdiag  [\\...  A^]g~^  where  the columns  of  Q are the n right  eigen vectors and the rows of Q~^ are the n left  eigenvectors  of A. As in the continuous-time case [system (L)] the initial condition x(0) for system (7.47)  can  be  selected  to be  colinear  with  the  eigenvector  vt to  eliminate  from  the solution of (7.47) all modes except the ones involving  Af. EXAMPLE 7.12.  As in Example 7.9 we let A = . Corresponding to the eigen 1A2 1 we have the right  and left  eigenvectors  vi  =  (1 0)^ V2 -1  21 ij 0 values  Ai  = (11)^ VI = (1 1) andv2  = (01). Then V I V I A[  +  V2V2A2 "1 0 (-1)^  + -1 0. 0 I 0  1. (1)* yfc>  0. 190 Linear Systems Choose x(0)  =  a(l 0)^  -  avi  with a  9^ 0. Then which contains only the mode associated with Ai  = - 1. We conclude the discussion  of modes  and  asymptotic  behavior  by briefly  con sidering the state equation x(k  +  1)  =  Ax(k)  + Bu(k) (7.61) where  x u A and B  are as defined  in  (7.2a). Taking  the 2X-transform  of both  sides of (7.61) and rearranging  yields x{z)  =  z(zl  -  Ar^x(0)  +  (zl  -  Ar^Bu(z). (7.62) By taking the inverse 2-transform  of  (7.62) we see that the  solution  cf) of  (7.61)  is the  sum  of  modes  that  correspond  to the  singularities  or poles  of  z(zl  -  A)~^x(0) and of (z/  -  A)~^Bu{z).  If in particular  system  (7.47) is asymptotically  stable [i.e. for  x{k  +  1)  =  Ax{k)  all eigenvalues  A^ of A are such that  \Xj\ <  \  j  =  \.. .n\ and  if  u{k)  in  (7.61)  is  bounded  [i.e.  there  is  an  M  such  that  \ui{k)\  <  M  for  all k>  0i  =  I...  m] then it is easily  seen that the solutions  of  (7.61)  are  bounded as well. 2.8 AN  IMPORTANT  COMMENT  ON  NOTATION For the most part Chapters  1 and 2 are concerned with the basic (qualitative) proper ties of systems of first-order ordinary differential  equations such as e.g. the system of equations given by X =  Ax (8.1) where x  E. R^ and A G fi^xn^ ^^ ^^^ arguments and proofs to establish various prop erties for  such systems we highlighted the solutions by using the (/)-notation. Thus the  unique  solution  of  (8.1)  for  a  given  set  of  initial  data  (^Q XQ)  was  written  as ^{t  to XQ)  with  </)(^ to xo)  =  XQ. A  similar  notation  was  used  in  the  case  of  the equation given by and the equations given by X =  fit  X) X =  A(t)x  +  B(t)u y  =  C(t)x  +  D(t)u (8.2) (8.3a) (8.3b) where in (8.2) and in (8.3a) (8.3b) all symbols are defined  as in (E)  (see Chapter  1) and as in (6.1a) (6.1b) of this chapter  respectively. In the study of control systems such as system (8.3a) (8.3b) the center of atten tion is usually  the control  input  u and the resulting  evolution  of the system  state in the state-space and the system output. In the development of control systems theory the x-notation has been adopted to express the solutions of systems. Thus the solution of (8.3a) is denoted by x(t)  [or x(t  to XQ) when to and  XQ  are to be emphasized]  and the evolution  of the  system  output y  in  (8.3b)  is denoted  by  y(t).  In  all  subsequent chapters  except  Chapter  6  we  will  also  follow  this  practice  employing  the  usual notation utilized  in the control systems  literature. In Chapter  6 which is  concerned with  the  stability  properties  of  systems  we  will  use  the  (/)-notation  when  studying the Lyapunov  stability  of  an equilibrium  [such  as  system  (8.1)]  and  the  x-notation when  investigating  the  input-output  properties  of  control  systems  [such  as  system (8.3a) (8.3b)]. 191 CHAPTER  2: Response of Linear Systems 2.9 SUMMARY In  this  chapter  the  response  of  linear  systems  to  specific  inputs  (subject  to  partic ular  initial  conditions)  was  studied  in  detail.  State-space  descriptions  as  well  as impulse  (resp. unit pulse)  response  descriptions  and transfer  functions  were  used. Continuous-time  time-varying  and  time-invariant  systems  characterized  by  state-space  descriptions  were  studied  first.  The  time-invariant  case  was  covered  in  a separate  section  (Section  2.4)  to provide  flexibility  in the coverage  of the  material. Similarly  discrete-time  systems  were  treated  in  a  separate  section  (Section  2.7). Background material on linear algebra for the present  and subsequent chapters  was presented in Section 2.2. In  greater  detail  the  solutions  of  the  homogeneous  state  equation  x  =  A(t)x were characterized first using fundamental  matrices  and the state transition  matrix 0(/ to) in  Section  2.3. The  solutions  of  the  nonhomogeneous  state  equations  x  = A(t)x  +  B(t)u  were derived in the same  section. For time-varying  systems the state transition matrix ^(t  to) can be  determined in  closed  form  only  in  special  cases. One  such  case pertains  to time-invariant  sys tems  X  =  Ax  where  0(r ^o)  =  ^^(^-^o)  Methods  of  determining  the matrix  expo nential  e^^ were addressed  in Section  2.4. In addition the asymptotic behavior  and the stability of an equilibrium of linear time-invariant  systems  x  =  Ax  (in terms of modes and eigenvalues) were also addressed in Section 2.4. Linear periodic  systems X =  A(t)x  A{t)  =  A{t  + T)t  ^  R were treated in Section 2.5. Impulse  response  representations  (resp.  transfer  function  representations)  of linear  systems in terms of state equation  and output equation parameters  were dis cussed  in  Section  2.6. In  addition  equivalence  of  state-space  representations  were treated in Section 2.6. Discrete-time  systems  represented  by  state-space  descriptions  and by  the  unit pulse  response  descriptions  we  addressed  in  Section  2.7. Results  analogous  to  the continuous-time  case  were  derived.  Discrete-time  systems  arise  frequently  in  the description  of  sampled-data  systems. Such  systems  were briefly  treated  in  Subsec tion 2.7D. 2.10 NOTES As mentioned earlier in Chapter  1 standard references  on linear algebra and matrix theory include Birkhoff  and McLane [2] Halmos  [7] and Gantmacher  [6]. For more 192 Linear Systems recent  texts  on  this  subject  refer  to  Strang  [16]  and  Michel  and  Herget  [10]. Our presentation  in Section 2.2 is in the spirit of the coverage given in [10]. Our treatment  of basic  aspects  of linear ordinary  differential  equations  in  Sec tions 2.3 2.4 and 2.5 follows  along lines similar to the development  of this  subject given in Miller and Michel [11]. State-space  and  input-output  representations  of  continuous-time  systems  and discrete-time  systems addressed  in  Sections  2.6  and  2.7 respectively  are  covered in a variety of textbooks including Kailath  [9] Chen  [4] Brockett  [3] DeCarlo [5] Rugh [14] and others. For further material on sampled-data systems refer to Astrom and Wittenmark  [1] and to the early  works on this  subject  that include Jury  [8] and Ragazzini  and Franklin [12]. Detailed  treatments  of  the  Laplace  transform  and  the  z-transform  discussed briefly  in Sections 2.4 and 2.7 respectively can be found  in numerous texts on sig nals and linear systems control systems and signal processing. The state representation of systems received wide acceptance in systems theory beginning  in  the  late  1950s. This  was  primarily  due  to  the  work  of  R.  E.  Kalman and  others  in  filtering  theory  and  quadratic  control  theory  and  to  the  work  of  ap plied mathematicians  concerned with the stability theory of dynamical systems. For comments  and  extensive  references  on  some of the early  contributions  in these  ar eas refer to Kailath  [9] and Sontag [15]. Of course differential  equations have been used  to  describe  the  dynamical  behavior  of  artificial  systems  for  many  years.  For example in  1868 J. C. Maxwell presented  a complete treatment  of the behavior of devices  that  regulate  the  steam  pressure  in  steam  engines  called  flyball  governors (Watt governors) to explain certain  phenomena. The use of state-space representations in the systems and control area opened the way for the systematic  study of systems with multi-inputs  and multi-outputs.  Since the  1960s an alternative description is also being used to characterize  time-invariant MIMO  control  systems  that  involves  usage  of  polynomial  matrices  or  differential operators. Some of the original references  on this approach include Rosenbrock  [13] and Wolovich [17]. This method which corresponds to system descriptions by means of higher order ordinary differential  equations (rather than systems of first-order ordi nary differential  equations as is the case in the state-space description) is addressed in Chapter 7. 2.11 REFERENCES 1.  K.  J.  Astrom  and  B. Wittenmark  Computer-Controlled  Systems. Theory and Design Prentice-Hall Englewood Cliffs NJ 1990. 2.  G. Birkhoff  and S. MacLane A Survey of Modern Algebra Macmillan New York 1965. 3.  R. W. Brockett Finite Dimensional Linear Systems Wiley New York 1970. 4.  C. T. Chen Linear System Theory and Design Holt Rinehart and Winston New York 1984. 5.  R. A. DeCarlo Linear Systems Prentice-Hall Englewood Cliffs NJ 1989. 6.  F. R. Gantmacher Theory of Matrices Vols. I II Chelsea New York 1959. 7.  P. R. Halmos Finite Dimensional Vector Spaces Van Nostrand Princeton NJ 1958. 8.  E. I. Jury Sampled-Data Control Systems Wiley New York 1958. 9.  T. Kailath Linear Systems Prentice-Hall Englewood Cliffs NJ 1980. 10.  A. N. Michel  and  C. J. Herget Applied  Algebra  and  Functional  Analysis  Dover  New 193 CHAPTER 2: Response of Linear  Systems York  1993. 11.  R. K. Miller and A. N. Michel  Ordinary  Differential  Equations  Academic  Press New York  1982. 12.  J. R. Ragazzini  and G. F. Franklin Sampled-Data  Control  Systems  McGraw-Hill  New York  1958. 13.  H. H. Rosenbrock  State  Space  and Multivariable  Theory  Wiley New York  1970. 14.  W. J. Rugh Linear  System  Theory Second Edition Prentice-Hall Englewood Cliffs NJ 1996. 15.  E. D. Sontag Mathematical  Control  Theory. Deterministic  Finite  Dimensional  Systems TAM 6 Springer-Verlag New York  1990. 16.  G. Strang Linear Algebra  and Its Applications  Harcourt Brace Jovanovich San Diego 1988. 17.  W. A. Wolovich Linear  Multivariable  Systems  Springer-Verlag New York  1974. 18.  L.  A.  Zadeh  and  C.  A.  Desoer  Linear  System  Theory—The  State  Space  Approach McGraw-Hill New York 1963. 2.12 EXERCISES 2.1.  (a)  Let  {VF)  =  (R^ R).  Determine  the representation  of v  =  (1 4 0)^  with  respect to the basis v^  =  (1 - 1  0)^ v^  =  (1 0 - 1 ) ^ and v^  =  (01 0)^. (b)  Let  V  =  F^  and let F be the field of rational functions.  Determine the representa tion of V =  (^s"  +  2 1/5  - 2 )^  with respect to the basis {v^ v^ v^} given in (a). 2.2.  Find  the  relationship  between  the  two  bases  {v\ v^ v^}  and  {v^ v^ v^}  (i.e.  find the  matrix  of  {v^ v^ v^}  with  respect  to  {v\ v^ v^})  where  v^  =  (2 10)^ v^  = ( 1  0  - l ) ^ v3  =  ( l  0  0 /  vi  =  ( l  0  0 f  v2  ^  (01-1)  and  v^  =  (011).  De termine  the  representation  of  the  vector  ^2  =  (0 10)^  with  respect  to  both  of  the above bases. 2.3.  Let  a  E: R  he  fixed.  Show  that  the  set  of  all  vectors  (x ax)^  x  E  R  determines  a vector  space of dimension  one over F  =  R  where  vector  addition  and  multiplication of vectors by scalars is defined  in the usual manner. Determine  a basis for this space. 2.4.  Show that the set of all real nX  n matrices with the usual operation of matrix  addition and  the  usual  operation  of  multiplication  of  matrices  by  scalars  constitutes  a  vector space over the reals  [denoted by  (/?"^" R)]. Determine  the dimension  and  a basis  for this  space.  Is  the  above  statement  still  true  if  /?"^" is  replaced  by  R^^^^  the  set  of real  mX  n matrices? Is the above statement  still true if  i?"^" is replaced by the set of nonsingular matrices? Justify  your answers. 2.5.  Let  v^  =  (s'^ s)^  and  v^  =  (11/^)^. Is  the  set  {v\ v^} hnearly  independent  over  the field of rational functions?  Is it linearly independent  over the field of real  numbers? 2.6.  Determine the rank of the following  matrices carefully  specifying  the field: (b) 1  4 7  0 (a) J -1 where  j (c) s + 4 ^ 2 -1 -2 6 0 s 2s+  3 -s  + 4 s+  1 (d) 194 Linear  Systems 2.7.  Let V and W be vector spaces over the same field F and let ^2/ : V ^  M^ be a linear trans is a linearly independent  set then so is the set formation.  Show that if {^v^... { v \ . . . v"}.  Give an example to show that the converse of this statement is not true. ^v^} 2.8.  Let  V  and  W  be  vector  spaces  over  the  same  field  F  and  lot ^ :V  ^W ho  3.  linear transformation.  Show that ^2/ is a one-to-one mapping if and only if  J^{s^)  = {0}. 2.9.  Let ^  =  [5A5...A"-i5]  and C CA CA"" where A G /?"><"5 G Z^^^^"' and C G /?^><". (a)  Prove  that  if  7]^ G ^ ( ^ ) then  AT]^  G ^ ( ^ ).  (7]^  denotes  the  coordinate representation  of a vector v^ G /?" with respect to the natural basis  {^1... ^„}.) (b)  Prove that if  7]i  G ^ ( ^ ) then A7]i  G ^ ( ^ ). The  above  shows  that  J^{^) mation  s^  that is represented by the matrix A. and ^ ( ' ^)  are invariant  vector  spaces  under  a  transfor 2.10.  Show that  a c b d d -c a  where /S. = ad —  bc^O. 2.11.  Determine the determinant  the (classical)  adjoint  and the inverse of the matrix 45+3 s 2 -2 3 2.12.  Determine  the  matrix  X  in A 5 O  D "A-i O X D-\ where  it  is  assumed  that A and Z) are nonsingular.  Also determine the matrix 1 A  O C  D 2.13.  (a)  Show that  J^r A  O C  Z) {detA){det  D)  where A and Z) are square matrices. Hint:  For Z) nonsingular use the identity (b) If A is nonsingular  show that A  O C  D A  O O  D I D-^C O I det A  B C  D (detA)det(D-CA-^B). Hint:  Note  that A  B C  D A  O O I I C A-^B D and /  O I -C I C A-^B D A-^B D-CA-^B\ \I [o In  part  (b)  derive  an  expression  for  the  case  when  it  is  known  only  that  D  is nonsingular. (c) 2.14.  Show that ^(^1+^2)^ = e^i^e^^t  if A1A2 =  A2A1. 2.15.  Determine the characteristic  and the minimal polynomials of the  matrices 195 0 1 10 1 10 0 0  0 10 0  0  0  1 0 1 10 0 0  10 0  0 10 0  0  0  1 A3  = 0 1 10 0 0  10 0  0 11 0  0  0  1 A4  =  h CHAPTER  2: Response of Linear  Systems Hint:  These matrices  are in Jordan canonical  form. 2.16.  Determine the Jordan canonical form  of the  matrices Ax  = "2  0 1  2 .2  0 0' 0 2. A2  = "2 1 .0 0  0" 2  0 1  2_ A3  = "2 0 .0 0  0" 2  0 1  2_ 2.17.  Show that there exists a similarity transformation  matrix P such that 0 0 1 0 •• 0 1 PAP-^  =  Ar  = 0 -ao 0 - ai 0 -a2 •• • -Oin-l 0 0 1 if and only if there exists a vector b  G  R^ such that the rank of  [b Ab i.e. p[/7AZ? ...A"-iZ7]  =  n. ^b] isn 2.18 Show  that  if  A/ is  an  eigenvalue  of the  companion  matrix  Ac  given  in Exercise  2.17 then a corresponding  eigenvector is v'  =  (1 A/...  Af~^)^. 2.19. Let  A/ be  an  eigenvalue  of  a matrix A  and  let  v'  be  a corresponding  eigenvector.  Let /(A)  =  S l =o  ^k^^  be a polynomial with real coefficients.  Show that /(A^) is an eigen value  of  the  matrix  function  /(A)  =  ^[==oOCkA^.  Determine  an  eigenvector  corre sponding to /(A/). 2.20.  For the matrices Ai  = "1  2 0  0 .0  0 0" 2 1. and A2  = 0 0  10 0  0 10 0  0  0  1 0  0  0  0 determine the matrices  A}"" A^"" g^l^  and e^^\  t  E  /? 2.21.  Determine  some bases for the range and null spaces of the  matrices Ai  =  [1  0  1] Ao  = ri 0 _i 1] 0 0. and "3 3 .3 2 2 2 r 1 1. 2.22. Determine all solutions of the equation Arj  =  v  where ro A =  1 [2 1 2 0 1 3 2 -11 2 4 -1 0 2.23. Let (j)x{t) =  e~^ for  ^ G  [-11]  and let <t>2{t)  = and 2J ^ G [ - 1  0 ] t  G  [0  1]. 196 Linear  Systems Show  that  (^1  and  (/)2  are  Hnearly  independent  over  the  field  of  the  real  numbers  on [-11]  but not on  [01]. Remark:  This  example  illustrates  the fact  that linear independence  of time  func tions over a time interval  [a b] does not necessarily  imply linear independence over a time subinterval  [a\b']  C  [a b]. 2.24. Show that if two time functions  (t)\(t) (f)2{t)  are linearly  independent  over a field F on a  time  interval  [a b\  then  they  are  linearly  independent  over  F  on  any  interval  that contains  [a Z?]. Give a specific  example. 2.25. Prove  that  for  A  G  C[R /?«><«]  (3.14)  is  true  if  and  only  if  (3.21)  is  true  for  all tT^R. 2.26. Determine the state transition matrix ^{t  to) for  (LH)  with A(t)  = [0  01 0 t by (a) directly  solving differential  equations (b) using the Peano-Baker  series and (c) using  (3.15). 2.27. Determine  the  state transition  matrix  ^{t  to) for  {LH)  with  A{t)  = mine in this case the solution for  (LH)  when  x(l)  =  (11)^. / 1 0 t and  deter-2.28. Verify  that (/>i(0  =  (1/^^  - 1 / 0^  and <^2(0  =  (2/t\  -llff with are two solutions  oi{LH) A(0  = 4 2-0 (a)  Determine the state transition matrix ^(t  r)  for this  system. (b)  Determine  a solution  (j) for  this  system  that  satisfies  the initial  conditions  x(l)  = 2.29 Given is the system of first-order ordinary differential  equations x  =  f-Ax  where A  E ^nxn  ^^^  t  Ei R.  Determine  the  state  transition  matrix  ^{t  to). Apply  your  answer  to the specific  case when  t^A 2A  ^ 0 -t\ [2t^ 2.30. Show that the two linear  systems and x(2)  = 0 2-t^ t 1 1 t 1 It x(^>  ^  A2{t)x^^^ are equivalent  state-space representations  of the differential  equation y -  Ity  -  (2 -  i')y  =  0. (a)  For which choice is it easier to compute the state transition matrix ^{t  toft  For this case compute ^{t  0). (b)  Determine the relation between  x^^^ and y  and between  x^^^  and  y. 2.31.  Using the Peano-Balcer series show that when A(0  =  A then <l>(^ ^)  =  e^^^  ^o). 2.32.  For {LH)  with A{t)  = [  0 - Ij determine lim^^oo (f){t to XQ) if x(0)  =  (01)^. This example  shows  that  an  attempt  of  trying  to  extend  the  concept  of  eigenvalue  from  a constant matrix A to a time-varying  matrix  A{t)  for  the purpose  of characterizing  the asymptotic behavior of time-varying  systems  (LH)  will in general not work. 197 CHAPTER 2: Response of Linear  Systems 2.33.  For the  system X =  A(t)x  +  B(t)u (11.1) where  all  symbols  are  as  defined  in  (6.1a) derive  the  variation  of  constants  formula (3.10) using the change of variables z{t)  =  ^(to t)x{t). 2.34.  For (11.1) with x(fo)  =  Xo show under what conditions it is possible to determine M(r) so that (f){t to xo)  =  xo for all t  >  to. Use your result to find such u{t) for the particular case X  =  X + e~^u. 2.35.  Show that (d/dT)^(t  r)  =  -0(^  T)A(T)  for all tT^R. 2.36.  Determine  the  state  transition  matrix  ^{t  to)  for  the  system  of  equations  x  — e~^^Be^^x  where  A  E  R^^^  and  B  G  R^^^  Investigate  the  case  when  in  particular AB  =  BA. 2.37.  The adjoint  equation  of (LH)  is given by Let  0(r to) and  ^a(t  to) denote  the  state  transition  matrices  of  (LH)  and  its  adjoint equation respectively.  Show that 0^(r  ^o)  =  [^(to  t)V-z -A(t)^z. (11.2) 2.38.  Consider the system described by A(t)x  +  B(t)u C(t)x (11.3a) (11.3b) where all symbols are as in (6.1a) (6.1b) with D(t)  =  0 and consider the adjoint  equa tion of (11.3a) (11.3b) given by z  =  -A(tfz  +  C(tYv w  =  B(tfz. (11.4a) (11.4b) (a)  Let  H(t  r)  and Ha(t r)  denote  the  impulse  response  matrices  of  (11.3a)  (11.3b) and  (11.4a)  (11.4b)  respectively.  Show  that  at  the  times  when  the  impulse  re sponses are nonzero they  satisfy  H(t  r)  =  Ha(r  tY. (b)  If  A(0  ^  A B(t)  ^  B  and  C(t)  ^  C  show  that  H(s)  =  -Ha(-sf  where  H(s) and Ha(s) are the transfer  matrices of (11.3a) (11.3b) and (11.4a) (11.4b) respec tively. 2.39.  Show that if for  (L//) A(t)  = An(t) 0 An(t) A22(0. where A\i(t)  An(t)  and A22(t) are submatrices  of appropriate dimensions then ^(t  to) [C|>ll(^^)  Oi2(r/o)l ^22(tjQ)\ 0 198 Linear  Systems satisfies  the  matrix  equation  {d/dt)^ii(tto)  =Aii(t)^ii(tto) where  ^uit) where ^n{tto)+An{t)^22{tto) the  matrix  ^uit^to) satisfies the  equation  {d/dt)^i2(tto) with 012(^0^0)  =  O. and =Aii(t) Use the above result to determine the state transition matrix 0(r 0) for 2.40.  Compute  e^^ for 2.41.  Given is the matrix A(t) -1 0 e^n -ly [1 p [0 4 2 0 1 2 0 0 -1 -1 0 10] 0 2] 0" 0 -2 (a)  Determine  e^\  using  the  different  methods  covered  in  this  text.  Discuss  the advantages  and disadvantages  of these  methods. (b)  For system  (L) let A be as given. Plot the components  of the solution  (p(ttoxo) whenXQ  = x(0) =  (111)^  and XQ  = x(0) =  ( |  10)^.  Discuss  the  differences in these plots if any. 2.42.  Show that for A: we have e^^ 2.43.  Given is the system of  equations XI = -1 0 0" 1 cos bt -sinbt sinbt cos bt +  r 1 .•^2. withx(0)  =  (l0)^and u{t)=p{t)-ri \ 0 >0 t e elsewhere. Plot  the  components  of  the  solution  of  (j). For  different  initial  conditions  x(0) {abY  investigate the changes in the asymptotic behavior of the  solutions. 2.44.  The system  (L) with A 0  1 -1  0 is  called  the harmonic  oscillator  (refer  to Chap ter  1) because  it has periodic  solutions  (\>{t)  =  (0i(^)02(O)^-  Simultaneously  for the  same values of t plot  (pi (t)  along the horizontal  axis and (p2(t)  along the vertical axis  in the X1-X2 plane  to  obtain  a  trajectory  for  this  system  for  the  specific  initial condition x(0) =xo =  (xi(0)X2(0))^  =  (11)^.  In plotting  such  trajectories  time t is viewed  as a parameter  and arrows  are used to indicate  increasing  time. When the horizontal  axis corresponds  to position  and the vertical  axis corresponds  to velocity the  X1-X2 plane  is  called  the phase  plane  and  0i02  (resp.  x\X2)  are  called  phase variables. 2.45.  There  are various  ways  of  obtaining  the coefficients  ai{t)  given  in  (2.101).  One of these was described in Subsection 2.2J. In the following  we present another  method. We consider the relation  {d/dt)e^^  = Ae^^ and we use (2.101) to obtain J  n—\ n—2 2ay(OA^"=A2ay(OA^"  +  a „ _ i ( 0 [ - K - i A " "^  +  --- + «iA +  ao/)] dt  7=0 7=0 (11.5) where  the  Cay ley-Hamilton  Theorem  was  used.  The  coefficients  ai(t) that  satisfy this  relation  generate  a matrix  O = T.cCj(t)AJ  that  satisfies  the  equation  O = AO. For O to equal e^\ we also require that 0(0)  = Eaj(0)A^  = /  (why?) (a)  Show that the  ccj{t) can be generated  as solutions of the system of  equations 199 CHAPTER 2: Response of Linear  Systems cco{t) ai{t) "0  0 1  0 -ao —a\ cco{t) ai{t) (11.6) 0  0 1 -an-i with  ao(0) =  laj(0)  =OJ> (11.6) are linearly  independent 1.  Also  show  that  the aj(t)  generated via (b)  Express the solution of the  equation X  =Ax-\-Bu (11.7) where  all symbols  are as defined  in (6.8a)  and  x(0)  =  XQ in terms  of  aj(t). Also  show  that  for  x(0)  = XQ  = 00(^00)  =  0(r)  = JJ'jZQAJBwj{t)  where Wj{t)=J^aj{t-T)u{T)dT. 2.46.  First  determine  the  solution  (j)  of XI = "0  1" 1  0 XI .•^2. withx(O)  =  (11)^.  Next determine  the  solution  (j)  of the  above  system  for  x(0)  =  a ( l  — l ) ^ a  e Ra  7^0 and discuss the properties of the two  solutions. 2.47. In Subsection  2.4C it is shown that when the n eigenvalues  Xi of a real nxn  matrix A  are distinct  then e^^ = ^4=1^1^^'^  where A^ = lims^^.[{s  — Xi){sl  — A)~^]  = ViVi [refer to (4.39) (4.40) and (4.43)] where ViVi  are the right and left  eigenvectors of A respectively corresponding to the eigenvalue A^. Show that (a) ELi^«  ^  ^' where /  denotes the nxn  identity  matrix  (b) AAi  = XiAi (c) AiA  = XiAi (d) AiAj  =  dijAi where  5ij = lif  i = j  and  5ij = 0 when  iy^j. 2.48.  Show that two state-space representations  {ABCD}  and {ABCD}  are zero-state equivalent if and only if CA^B  = CA^Bk  = 012...  and Z) = 5. 2.49.  Find  an equivalent  time-invariant  representation  for the  system  described  by  the scalar differential  equation x =  sinltx. 2.50.  Consider the  system x =  Ax-\-Bu y =  Cx (11.7a) (11.7b) where all symbols  are defined  as in (6.8a) (6.8b) with D = 0. Let 0 3 0 0 1 0 0 -2 0 0 0 0 0 2 1 0 "0 1 0 0 0" 0 0 1 C=  [1010]. (11.8) (a)  Find equivalent representations  for  system (11.7a) (11.7b) (11.8) given by x=Ax  + Bu y = Cx (11.9a) (11.9b) 200 Linear  Systems where x  = Px  when A is in (i) the Jordan canonical (or diagonal) form  and (ii) the companion  form (b)  Determine the transfer  function  matrix for this  system. 2.51. Consider the system (11.7a) (11.7b) with B = (a)  Let 0. A  = r -1 1 0" 0 - 10 0 0 2 and  C=  [111]. If possible select x(0)  in such a manner  so that y(t)  = te~\t  > 0. (b)  Determine conditions under which it is possible to assign y{t)t>0  using only the initial data x(0). 2.52.  Consider the system given by +  0 1 7 X2 3'=  [10] (a)  Determine x(0)  so that for  u(t)  = e~^\y{t)  = ke~^\  where ^ is a real  constant. Determine  k  for  the present  case. Notice  that y{t)  does not have  any  transient components. (b)  Let  u{t)  =  e^K  Determine  x(0)  that  will result  in y{t)  =  ke^K  Determine  the conditions  on  a  for this to be true. What is k in this  case? 2.53.  Consider the system (11.7a) (11.7b)  with 0 3 -1 1 0" 1 1 0   B  = "0 1 0 0 0" 0 1 0   c = (a)  For  x(0)  =  [1111]^  and  for  u{t)  =  [11]^  ^ >  0  determine  the  solution (p(t0x(0))  and  the  output  y(t)  for  this  system  and  plot  the  components 0(^Ox(O))/=l234and};(O/=l2. (b)  Determine the transfer  function  matrix H{s)  for this  system. 2.54. Consider the  system x{k+l)=Ax{k)+Bu{k) y{k)=Cx{k) (11.10a) (11.10b) where all symbols are defined  as in (7.2a) (7.2b) with Z) =  0. Let B C = [ l  l ] and let x(0)  =  0 and w(^) =  1 ^ >  0. (a)  Determine  {y{k)}k  >  0  by  working  in  the  (i)  time  domain  and  (ii)  z-(b) transform  domain using the transfer  function  //(z). If it is known that when  u{k)  = 0 then y(0)  =y{\)  =  1 can x(0)  be  uniquely determined? If your answer is affirmative  determine x(0). 2.55. Consider y{z)  = H{z)u{z)  with transfer  function  H{z)  =  l/(z  +  0.5). (a)  Determine  and plot the unit pulse response  {h(k)}. (b)  Determine  and plot the unit step response. 201 CHAPTER  2: Response of Linear  Systems (c)  If u(k)  {i: ^ = 1  2 elsewhere determine  {y{k)}  for  k  =  0123  and  4  via  (i)  convolution  and  (ii)  the z-transform.  Plot your  answer. (d)  For u{k)  given in (c) determine y{k)  as ^ ^  oo. 2.56.  Consider the system (11.1 Oa) with x(0)  =  XQ and ^ >  0. Determine conditions under which there  exists  a sequence  of inputs  so that the  state remains  at XQ i.e.  so that x{k)  = XQ for all ^ >  0. How is this input sequence determined? Apply your  method to the specific  case B-XQ 2.57.  For  system  (7.7)  with x(0)  = XQ  and ^  >  0  it is desired  to have the  state  go to  the zero state for any initial condition XQ in at most n steps i.e. we desire that x{k)  = 0 for  any XQ  = x(0)  and for  all  k>n. (a)  Derive  conditions  in  terms  of  the  eigenvalues  of A  under  which  the  above  is true. Determine the minimum number of steps under which the above behavior will be true. (b)  For part (a) consider the specific  cases 0 0 0 1 0 0 0 1 0 A2-0 0 0 1 0 0 0 0 0   A3 0 0 0 0 0 0 0 1 0 Hint:  Use the Jordan canonical form for A. Results of this type are important in dead-beat  control  where it is desired that a system variable attain some desired value and settle at that value in a finite number of time steps. 2.58.  Consider the system representations  given by X(^+1): -1 0 0 -2 x{k)  + u(k) y{k)  =  [ll]x{k)  +  u{k) and X(^+1): 0 -2 1 -3 x{k)  + u(k) y{k)  =  [l0]x{k). Are these representations  equivalent? Are they zero-input  equivalent? 2.59.  For the Jordan block given by [Xi 1 0  A 0 1 ••• 0" 0 0 0 0 0 ••• ••• 202 Linear  Systems where Jtj  G  R^^ 'Af [ show that af-' k{k -  1) /:(fc-l)---a-f  +2)  ._(_!) 0 0 0 0 Af 0 0 0 Uf-i Af 0 0 A* when  ^  >  ^  -  1. //m^; Use expression  (7.55). 2.60.  Consider  a  continuous-time  system  described  by  the  transfer  function  H{s)  = 4/(^2  +  25 +  2) i.e. })(5)  =  H(s)u(s). (a)  Assume  that  the  system  is  at  rest  and  assume  a  unit  step  input  i.e.  u(t)  =  1 t  >  0 u(t)  =  0t  <0  Determine  and plot y(t)  for  t  >  0. (b)  Obtain  a  discrete-time  approximation  for  the  above  system  by  following  these steps:  (i)  determine  a  realization  of  the  form  (11.7a)  (11.7b)  of H(s)  (see  Exer cise 2.61); (ii) assuming  a sampler  and a zero-order hold with  sampling period  7 use (7.46) to obtain a discrete-time  system  representation x(k  +1)  =  Ax(k)  +  Bu(k) y(k)  =  Cx(k)  +  Du(k) (11.11a) (11.11b) and determine A B  and  C in terms of  T. (c)  For the unit step input u(k)  =  1 for  /: >  0 and u(k)  =  0 for  /: <  0 determine and plot y(k)  A:  >  0 for different  values of T assuming the system is at rest. Compare y(k)  with y(t)  obtained in part (a). (d)  Determine  for  (11.11 a)  and  (11.lib)  the  transfer  function  H(z)  in  terms  of  T. that  H(z)  = that  H(z)  =  C(zl  -  Ay^B  +  D.  It  can  be  shown Note (1 -  z~^M^-^[H(s)/sl^kT}. Verify  this for the given  H(s). 2.61.  Given  a proper  rational  transfer  function  matrix  H(s)  the  state-space  representation {A B C D}  is  called  a  realization  of  H(s)  if  H(s)  =  C(sl  -  Ay^B  + D.  Thus  the system  (6.8a) (6.8b) is a realization  of H(s)  if its transfer  function  matrix  is equal to H(s).  Realizations  of H(s)  are  studied  at length  in Chapter  5. When  H(s)  is  scalar  it is straightforward  to derive certain realizations and in the following  we consider  one such  realization. Given a proper rational scalar transfer  function  H(s)  let D  =  lim^^oo H(s)  and let Hp(s)  ^  H{s)  -  D bn-lS'' +  b\s  +  bo s^  +  an-\s^  ^ +  '"  -\-  a\s  + ao a strictly proper rational  function (a)  Let 0 0 1 0 0 -ao 0 -ai 0 1 0 -a2 •• 0 0 0 0 0 -Cln-2 1 ~(^n-l_ " • B = 0 0 0 1 C  =  [bo  by bn-l] and  show  that  {ABCD} is indeed  a reahzation  of H{s).  Also  show  that  {A  = is  a  reahzation  of  H(^s) as  well.  These  two  state- space representations are said to be in controller  (companion) form  and in observer (companion)  form  respectively  (refer  to Subsection  3.4D). 203 CHAPTER 2: Response of Linear  Systems (b)  In particular find realizations  in controller  and observer  form  for  (i) H{s)  = \/s^ (ii) H{s)  =  (ol/{s^  + 2l^(OnS+(ol)  and (iii) H{s)  =  {s+  1)V(^ -  1)^-2.62.  Given are the systems 5*1  and 5*2  described by the  equations X\=A\X\^B\U\ y\  =C\x\+  D\ u\ I \ X2=A2X2+B2U2 y2=  C2X2 + D2U2 (^2) where all symbols are defined  as in (6.8a) (6.8b) with an appropriate set of  dimensions for  all matrices and vectors. (a)  Determine  state-space representations  for the following  composite  systems. (i)  Systems connected in tandem  or in  series: u = u^ y = U2 y2 = y S2 FIGURE  2.6 Two systems connected in  series (ii)  Systems connected in  parallel: u^ S^ yi *J > f U2 Sz y2 FIGURE  2.7 Two systems connected in parallel f r s 1 (iii)  Systems connected in 3. feedback  configuration: ^o^ y2 FIGURE  2.8 Feedback  configuration yi U2 Hint:  In each case use as the state of the composite  system. (b)  If Hi(s)  is the transfer  function  matrix  of  Sii  = 1  2  determine the transfer  func tion matrix for each of the above composite systems in terms of the Hi{s)i  = 1  2. 204 Linear  Systems 2.63.  Assume that H(s)  is a p  X m proper rational transfer  function  matrix. Expand H(s)  in a Laurent  series about the origin to obtain H(s)  =  Ho+His'^ +  •••  +HkS~^  +  •••  = ^HkS'K 00 k = 0 The elements of the sequence {HQ HI  ...  Hj...}  are called the Markov parameters  of the system. These parameters provide an alternative representation of the transfer  func tion matrix H(s)  (why?) and they are useful  in Realization Theory (refer to Chapter 5). (a)  Show that the impulse response H(t  0) can be expressed  as H(t0)  ==  Ho8(t) +  Y.Hk (k-l)\ In the following  we assume that the system in question is described by (6.8a) (6.8b). (b)  Show that H(s)  =  D  + C(sl  -  A)-^B  =  D  + ^[CA^-^B}s~^ which  shows  that  the  elements  of  the  sequence  {D CB  CAB...  CA^ ^B...} are  the  Markov  parameters  of  the  system  i.e.  HQ =  D  and  H^  =  CA^~^B )^ =  12 (c)  Show that . . .. H(s)  =  D+  -^C[Rn-is''-^ a(s) +  • • • +  i?i^ +  Ro]B where  a(s)  =  s'^ +  a„_i5"~^  +  • • • + a\s  + ao  =  det  (si  -  A) the  characteristic polynomial  of A and Rn-i  =  /  Rn-2  =  ARn-i  +  a„-i/  =  A  + a „ - i / . . .  Ro  = A«-i  +(2„_iA"-2  +  ... Hint: WviiQ (si-A)-^ =  [l/a(s)][adj  (si  -  A)]  =  [l/a(s)][Rn-is^-^+ RQ]  and equate the coefficients  of equal powers of s in the expression '"  + Ris  + -^aiL a(s)I  =  (si  -  AMn-is""'^  +  '"  -hRis-h  Rol 2.64.  Given  the  transfer  function  of  a  system  suggest  different  methods  to  determine  its Markov parameters. Apply these methods to the specific  cases given by H(s)  =  (s^ -  l)/(s^  +  2 ^ + 1) and H(s) s 1  1 s+  1 0 2.65.  The frequency  response  matrix  of  a  system  described  by  its  p  X  m  transfer  function matrix evaluated  ats  =  jco H(co)  ^  H(s)l=j^ is a very useful  means of characterizing  a system since typically it can be  determined experimentally  and  since  control  system  specifications  are  frequently  expressed  in terms  of  the  frequency  responses  of transfer  functions.  When  the poles  of H(s)  have negative real parts the  system turns out to be bounded-input/bounded-output  (BIBO) stable (refer to Chapter 6). Under these conditions the frequency  response H(co) has a clear physical meaning and this fact  can be used to determine H(a))  experimentally. 205 CHAPTER  2: Response of Linear  Systems (a)  Consider  a  stable  SISO  system  given  by  y{s)  = H{s)u{s).  Show  that  if  u{t)  = ksm{(Oot +  (j))  with  k  constant  then y{t)  at steady-state  (i.e.  after  all  transients have died out) is given by yss{t)  = k\H{(Oo)\sm{(Oot +  0 +  0{(Oo)) where  \H{Q))\  denotes  the  magnitude  of  //(co)  and  6{Q)) =  arg  H{Q))  is  the argument or phase of the complex quantity  H{(o). From  the  above  it  follows  that  H{(o)  completely  characterizes  the  system response at steady-state  (of a stable system) to a sinusoidal input. Since u{t)  can be expressed  in terms  of  a series  of  sinusoidal terms via  a Fourier  series  H{(o) characterizes  the  steady-state  response  of  a stable  system to  any bounded  input u{t).  This physical interpretation  does not apply when the system is not  stable. (b)  For  the  /? X m  transfer  function  matrix  H{s)  consider  the  frequency  response matrix  H{(o)  and  extend  the  discussion  of  part  (a)  above  to  MIMO  systems  to give a physical interpretation  of//(co). 2.66.  Let A G /?"><" and B  e /?"><'^. (a)  Is  it  true  that  rank  [BAB...  ^A'^'^B]  =  rank  [5A5... A"-i5A"5]?  Justify your  answer. (b)  Determine  conditions  under  which  rank  [BAB..  .A"^~^B] =  rank  [A5... A"~^5A"5].  Hint:  Use  the  Sylvester  Rank  Inequality  which  relates  the  rank of the product of two matrices to the ranks of the individual matrices rankX  +  rankY  — n<  rank(XY)  < imn{rankX rankY} where X  e  /?^><" and Y  e /?"><'^. 2.67.  (Double integrator)  (a) Plot the response of the double integrator of Example 7.6 to a unit step input. (b)  Consider the discrete-time  state-space representation  of the double integrator of Example 7.6 for  7  =  0.515  sec and plot the unit step responses. (c)  Compare your answers in (b) with your result in (a). 2.68.  (Economic model for national income) [D. G. Luenberger Introduction  to  Dynamic Systems  Wiley  1979.] A simple model describing the national income dynamics  can be formulated in discrete times as follows. The national income y{k)  in year k in terms of  consumer  expenditure  c{k)  private investment  i{k)  and  government  expenditure g{k)  is  assumed  to  be  given  by  y{k)  =  c{k)  +  i{k) + g{k)  where  the  interrelations between  these  quantities  are  specified  by  c ( ^+  1)  =  ccy{k) and  /(^+  1)  =  ^[c{k — 1) — c{k)\.  The constant  a  is called the marginal propensity  to consume  while /3 is a growth coefficient.  Typically 0 <  a  <  1 and /3 >  0. From these assumptions  we obtain the difference  equations  c ( ^+  1) =  ac(^)  + ai{k)  -\- ag{k)i{k-\-1)  =  (Pa  — P)c{k)  -\-Pai{k)  + /3ag{k)  with discrete-time state-space representation  given by XI  ( ^ + 1) X 2 ( ^ + l) a p{a-l) a pa xi{k) X2{k) a pa u{k) y{k)  =  [\A] XI  (k) X2{k) +  u{k) where x\ {k)  = values (i) a  = c{k)X2{k)  =  i{k)  and  u{k)  =  g{k).  Let the parameters  a/3  take  the 0.75/3  =  1 (ii) a  = 0.75/3  =  1.5  and (iii)  a  =  1.25p  =  1. 206 Linear  Systems (a)  Determine the eigenvalues of A for all cases and express x(k) when u  =  Oin terms of the initial conditions and the modes of the  system. (b)  Plot the states for /: >  0 when  u(k) is the unit  step and x(0)  =  [0 0]^.  Comment on your results. (c)  Plot the states for/:  >  Owhenw  =  Oandx(O)  =  [5 1]^. Comment on your results. 2.69.  (Spring  mass  system)  Consider  the spring  mass  system  of Example  4.1 in  Chapter 1.  For Ml  =  1 kg M2  =  1 kg K  =  0.091 N/m Ki  = 0 .1 N/m K2  =  OA  N/m B  =  0.0036  N  sec/m  Bi  =  0.05 N  sec/m  and B2 =  0.05 N  sec/m  the  state-space representation of the system in (4.2) of Chapter  1 assumes the form 0 10 0 -0.1910 -0.0536 0.0910 0.0036 0 0 0 1 0.0910 0.0036 -0.1910 -0.0536 \xi \X2 Us |_X4_ 4-"0 1 0 0 0] 0 0 -ij r/r [/2. A where xi  =  yi  X2 = yi (a)  Determine  the eigenvalues  and eigenvectors  of the matrix  A  of the system and express  x(t)  in terms  of the modes  and the initial  conditions  x(0) of the system assuming that /i  =  /2  =  0. X3  =  y2  and X4  =  y2. (b)  Forx(O)  -  [10 0.5 0]^  and  /i (c)  Let y  =  Cx with C  = 10 0 0  1  0 =  f2  = 0 plot the states for t >  0. 0] denote the output of the system.  Determine the transfer  function  between y and u  =  [fi  fiV• (d)  For zero initial conditions fi(t)  = 8(t) (the unit impulse) and f2(t)  = 0 plot the states for r >  0 and comment on your results. (e)  It is desirable to explore  what  happens  when the mass  ratio M2/M1  takes  on dif ferent  values. For this let M2  =  OLM\ with Mi  =  1 kg and a  =  0.1 0.5 2 5. All other parameter  values remain the same. Repeat (a) to (d) for the different  values of a  and discuss your results. 2.70.  (RLC circuit) For the circuit  ofExample  4.2 in Chapter  1 let/?i  =  211 i?2 =  l i ^ Ci  =  1  mF C2  =  1  mF and L  -  0.5 H. (a)  Determine the eigenvalues of A and express x{i) when v  =  0 in terms of the initial conditions and the modes of the  system. (b)  Plot  the states  for ? >  0  when  v  -  0 and x(0)  =  [5 \ 0]^.  Repeat  for x(0)  = [0 0 5]^ and comment on your results. (c)  Compute the transfer  function  between  y  =  [vi V2  VB]^ and v. (d)  Plot the states when the input v is the unit step and x(0)  =  [0 0 0]^. Comment on your results. 2.71.  (Armature  voltage-controlled  dc servomotor)  Using  a  consistent  set of  units  for the armature voltage-controlled  dc servomotor in Example 4.3 of Chapter  1 let Ra = 2 La  =  0.5 J  =  I B  =  I KT  =  2 and Ke  =  I. The state-space  description  of this system is given by (4.8) of Chapter  1 and here assumes the form Xi~ X2 •^3_ = "0 0 .0 1 -1 -2 01 2 -4J \xi' \X2 1x3. + "0 0 .2 ea where  xi  =  ^ is the shaft  position  X2 =  ^ is the angular  velocity  X3 =  ia is the ar mature current and the input  w =  ^^ is the armature voltage. (a)  Determine the eigenvalues  and eigenvectors  of A  and express  x{t) in terms of the modes and the initial conditions of the system when  Ca = 0. (b)  Plot the  states for  r >  0 when  the input  Ca  is the unit  step and  x(0) [0 0 0]^ 207 CHAPTER  2: Response of Linear  Systems Comment on your results. 2.72  (Unit mass in an inverse square law force field) Consider Example  11.3 of Chapter 1 where for a satellite ro  -  4.218709065 X 10^ m and COQ =  7.29219108 X 10~^ rad/sec. The linearized  model about the orbit d{f)  =  COQ^ +  ^o is given by 0" 0 0 1 '  0 3col 0 1 0 0 2CL)O "0 1 0 r^i [^2 0 0 0 — + -Xi Xl is Xi\ 0  1 r \Xi 2ro(0o U2 Us X4 -• 1 0 0 ^ 0 ^0 0 ^0-' where xi(t)  =  r{t)  -  ro X2{t) =  r{t) x^it)  =  0(t)  and ^4(0  -  ^(0  -  ^o-(a)  Determine the eigenvalues of A. Is the system asymptotically  stable? Explain your answer. (b)  Plot the states for  x(0)  =  [100 000]^  and zero input. Comment on your results. (c)  Plot the states for wi(0  =  0 U2(t) =  -land;\c(0)  =  0. If the input represents  force imposed on the satellite by friction  comment on your results. 2.73.  (Magnetic ball suspension system) Consider the magnetic ball suspension  system of Exercise 1.21 in Chapter  1. It can be shown that under certain simplifying  assumptions a linearized model  x  =  Ax  + Bu y  =  Cxof  this system is given by R L 0 2Kieq 0 0 Ui U3 L-^BJ + r li z 0 .o_ y  =  [010] A  typical  set  of parameters  is Sgq  =  0.01  m  ieq =  0.125  A M  =  0.01058  kg  K  = 6.5906  X 10-4 N m^/A^ R  =  31.1 n  and L  =  0.1097  H. (a)  Determine the eigenvalues  and a set of eigenvectors  of A. (b)  Compute the transfer  function. (c)  Plot the states for ^ >  0 if the ball is slightly higher than the equilibrium  position namely if x(0)  =  [0 0.0025 0]^.  Comment on your results. 2.74.  (Automobile suspension system)  [M. L. James G. M. Smith and J. C. Wolford  Ap plied  Numerical  Methods for  Digital  Computation  Harper & Row 1985 p. 667.] Con sider the spring mass system in Fig. 2.9 which describes part of the suspension  system of an automobile. The data for this system are given  as mi  =  \  X  (mass of automobile)  =  375 kg m2  =  mass of one wheel  =  30 kg ki  =  spring constant  =  1500 N/m k2  =  linear spring constant of tire  =  6500 N/m c  =  damping constant of dashpot  =  0 375 750 and  1125 N  sec/m xi  =  displacement  of automobile body from  equilibrium position  m. 208 Linear  Systems v 1  .  2nvt u(t)=  -sin  —— 6 20 FIGURE  2.9 Model of an automobile  suspension  system X2  = displacement  of wheel from  equilibrium position m V  =  velocity of car =  9  18  27 or 36m/sec. 0 A linear model x  = Ax-\-Bu  for this system is given by 0  ] c — m\ 1 0 h m\ 0 1 kl C m\ 0 c mi 0 h k\  +^2 ^  - Xl + ^2 X3 X4 r  0  1 0 0 h Lm2^ u(t) _  1712 ni2 1712 1712] where u{t)  =  ^ sin(27rv^/20)  describes the profile  of the roadway (a)  Determine the eigenvalues of A for  all the above cases. (a)  Plot  the  states  for  r  >  0  when  the  input  u{t)  =  ^ sin(27rvr/20)  and  x(0)  = [0000]^  for  all the above cases. Comment  on your results. 2.75.  (Building  subjected  to an earthquake)  [M. L. James  G. M. Smith  and J. C. Wol-ford.  Applied  Numerical  Methods  for  Digital  Computation  Harper  &  Row  1985 p.  686.]  A  three-story  building  is  modeled  by  a  lumped  mass  system  as  shown  in Fig. 2.10. For ground  acceleration  v the differential  equations of motion in terms of mass displacements  [^1^2^3] relative to the ground are given in state-variable  form x =  Ax-\-Buhy XI X2 X^ X4 is xe — 0 ki  +^2 mi 0 h. m2 0 0 1 2c mi 0 c m2 0 0 0 k2 mi 0 ^2 +  ^3 m2 0 k3 ms 0 c mi 1 2c m2 0 c ms 0 0 0 h. m2 0 k3 ms 0 0 0 c m2 1 c m3_ XI X2 X3 X\ X5 X6 + 0 -1 0 -1 0 -1 w where  x\  =  qiX2  =  qiX3  =  q2M  =  q2xs  =  q3x^  =  ^3  and  u  =  V.  Let  k  = 3.5025  X lO^N/m  m =  1.0508 x  10^kg  and c =  4.2030 x  lO^N sec/m.  Investigate the dynamic response of the structure due to the ground acceleration ufoYT  =  0.40.6 and 0.8 sec (see Fig. 2.10). In  particular: (a)  Plot the  distortions yi yi y3 = XI X3-XI X 5 - X3 = 1 -1 0 0 0 0 0 1 -1 0 0 0 0 0 1 0 0 0 [xiX2X3X4X5X6]^. If serious damage occurs when a distortion exceeds 0.08 m will the given ground acceleration due to the earthquake cause serious damage to the building? (b)  Repeat  (a)  for  different  values  of  the  damping  parameter  c.  In  particular  let Qg^  =  acoid  where  a  =  2310  and  repeat  (a)  for  each  value  of  a.  Also determine the eigenvalues of A for each  a  and comment on your results. 209 CHAPTER  2: Response of Linear  Systems /C2 =  2/f 1  gf =  10  m/sec^ FIGURE  2.10 A model for the dynamics of a three-story  structure 4.0 2.76.  (Aircraft dynamics)  [B. Friedland  Control System Design  An Introduction  to State-Space Methods  McGraw-Hill  1986.] For purposes of control system design  aircraft dynamics are frequently linearized about  some  operating  condition called  3. flight re gimewh^YQ it is assumed that the aircraft velocity(Mach number)and attitude are con stant. The control  surfaces  and engine thrust are set or trimmed  to these  conditions 210 Linear Systems and the control system is designed to maintain these conditions i.e. to force perturba tions (deviations) from these conditions to zero. It is customary to separate the longitudinal motion from the lateral motion since in many  cases the longitudinal  and lateral  dynamics  are only lightly  coupled. As a consequence of this the control system can be designed by considering each channel independently. The aerodynamic variables of interest are summarized in Table 12.1 and Fig. 2.11. The aircraft body axes are denoted by xy  and z with the origin fixed at some refer ence point (typically the center of gravity of the aircraft).  The positive directions of these axes are depicted in Fig. 2.11. Roll pitch and yaw motions constitute rotations about the x- y- and z-axes respectively using the following sign convention: looking at Fig. 2.11a we see that the pitch angle 6 increases with upward rotation in the side view shown; in Fig. 2.11c which gives the top view of the aircraft  yaw angle if/ in creases in the counterclockwise direction; and looking at Fig. 2.lid which provides the front view of the aircraft we see that roll angle (p increases in the counterclockwise direction. We let co^  =  r(x)y  =  q and (o^ = p  denote yaw rate pitch rate and roll rate respectively. The velocity vector V is projected onto the body axes with w v and w being the projections onto the x- y- and z-axes respectively. The angle-of-attack a is the angle that the velocity vector makes with respect to the x-axis in the (positive) pitch direction and the side-slip angle jS is the angle that it makes with respect to the x-axis in the (positive) yaw direction. Note that for small angles a  —  w/u and ^  —  vlu. The aircraft  pitch motion is typically  controlled by a control  surface  called the elevator roll is controlled by a pair of ailerons and yaw is controlled by a rudder. Aircraft longitudinal motion.  As a specific example consider the numerical data for an actual aircraft the AFTI-16 (a modified version of the F-16 fighter) in the land ing approach configuration  (speed V =  139 mph). The components of the state-space equation x  = Ax  + Bu that describe the longitudinal motion of the aircraft  are given by "xA i:21 is  ~ X4 ["-0.0507 -0.00117 -0.000129 -3.861 -0.5164 1.4168 0 1 -0.4932 0 0 1 32.2] 0 0 0  J r^i \x2 Us 1x4 0 + - 0 . 0 7 17 - 1 . 6 45 u 0 where the control input  w =  5^ is the elevator  angle  and the  state variables  in the vector X = [Aw a q 6]^ are the change in speed angle of attack pitch rate and pitch respectively. TABLE  12.1 Aerodynamic variables Rates Positions Lateral p: n j8: roll rate yaw rate side-slip  angle 0: roll  angle ij/:  yaw  angle x: y: forward  displacement cross-talk  displacement Longitudinal angle of  attack a: q:  pitch  rate Aw:  change in  speed 9:  pitch  angle z: altitude Controls 8A :  aileron  deflection 8R: rudder  deflection 8E :  elevator  deflection 211 CHAPTER  2: Response of Linear Systems (a) Side view (b) Angle-of-attack a and side-slip angle Deflected rudder Deflected aileron (d) Front View FIGURE 2.11 Aircraft dynamics The longitudinal modes of the aircraft  are called short period and phugoid.  The phugoid eigenvalues which are a pair of complex conjugate eigenvalues close to the imaginary axis cause the phugoid motion which is a slow oscillation in altitude (a)  For the state-space model that describes the aircraft longitudinal motion determine the eigenvalues and eigenvectors of A. Express x{t) when w  =  0 in terms of the initial conditions and the modes of the system. 212 Linear  Systems (b)  Let  the  elevator  deflection  §£  be  -1  for  t  E  [0 T]  and  zero  afterward  where  T may  be  taken  to be  the  sampling  period  in  your  simulation.  This  corresponds  to the maneuver  made when  the pilot pulls back on the stick to raise the nose of  the airplane.  (The  minus  sign  conventionally  represents  pulling  the  stick back.)  The elevator must be restored to its original position when the desired new climbing an gle is reached or the plane will keep rotating. Plot the states for x(0)  =  [0 0 0 0]^ and comment on your results. (c)  Plot  the  states  for  x(0)  =  [00 00]^  using  a  negative  unit  step  as  the  elevator input.  This  happens  when  the  elevator  is  reset  to  a  new  position  in  the  hope  of pitching the plane up and  climbing. Comment on your results. (d)  As a second example consider the numerical data for a Boeing 747 jumbo jet flying near sea level at a speed of  190 mph. The state-space description  x  =  Ax  +  Buoi the longitudinal motion is now given by -0.0188 -0.0007 0.000048 11.5959 -0.5357 -0.4944 0 0 0 1 -0.4935 1 32.21 0 0 0  J Ui \x2 Us X4 + 0 0 -0.5632 0 (C. E. Rohrs J.  L. Melsa  and  D.  G.  Schultz Linear  Control  Systems  McGraw-Hill  1993 p. 92). Repeat  (a) to (c) for  the present case and discuss your  answers in view of the corresponding results for the AFTI-16  fighter. Aircraft  lateral  motion.  As a specific  example consider the lateral motion of a fighter aircraft traveling at a certain speed and altitude with state-space description X =  Ax  + Bu  given by 0.03690 Ui \x2 \X3 \X4 0 0 0  J Xi X2 X3 X4_ -0.746 -12.9 431 0 0.006 -0.746 0.024 1 - D.999 0.387 -0.174 0 + 0.0012 6.05 -0.416 0 O.OO92I 0.952 -1.76 0 [U2_ ' where  the  control  inputs  [ui U2V  =  [^A^RV denote  the  aileron  and  rudder  de flections  respectively and the state variables in the vector x = [/3 p r c^]^ are the side-slip angle roll rate yaw rate and roll angle respectively. (e)  The  eigenvalues  for  the  aircraft  lateral  motion  consist  typically  of  two  complex conjugate  eigenvalues with relatively low damping and two real eigenvalues. The modes caused by complex eigenvalues  are called dutch-roll.  One real eigenvalue relatively  far  from  the  origin  defines  a  mode  called  roll  subsidence  and  a  real eigenvalue near the origin defines  the spiral  mode. The spiral mode is  sometimes unstable  (spiral  divergence). Find  the modes  for  the  aircraft  lateral  motion  of  the fighter. (f)  Plot the states when x(0)  =  [0 0 0 0]^ wi is the unit step and U2  =  0. Repeat  for ui  =  0 and  U2 the unit step. Comment on your results. 2.77.  (Read/write head of a hard disk)  [MATLAB Control System Toolbox User's  Guide The Math Works Inc.  1993.]  Using  Newton's  law a simple model  for  the  read/write head of a hard disk is described by the differential  equation J6  + c0 + k0  =  Kj  / where /  represents the inertia of the head assembly; c denotes the viscous damping  coefficient 213 CHAPTER  2: Response of Linear  Systems of the bearings; k is the return spring constant; Kj  is the motor torque constant; 6 9 and 6  are the angular acceleration angular velocity and position of the head  respectively; and / is the input current. A state-space model x  =  Ax  + Bu  of this system is given by 0 k J 1] c Ui U2.  + "7-1 0 Kj -T where xi  ^  eX2  =  0 and u  =  i. Let J  =  0.01 c  =  0.004 k  =  10 and KT  =  0.05. (a)  Determine the eigenvalues  of A. With  w =  0 is the trivial solution x  =  0 asymp totically  stable?  Explain. (b)  Plot the states for r >  0 when the input is the unit step and x(0)  =  [0 0]^. (c)  Let the plant be preceded by a zero-order hold  (D/A converter)  and followed  by a sampler (an ideal A/D converter) both sampling at a rate of 1/7  where T  =  5 ms. Derive the discrete-time  state-space representation of the plant. Repeat (a) and (b) for the discrete-time  system and comment on your results. C H A P T ER  3 Controllability Observability and Special Forms It  is  frequently  desirable  to  determine  an  input  that  causes  the  states  of  a  system to  assume  different  values  in  finite  time  (e.g.  to transfer  the  state  vector  from  one specified  vector value to another). Such is the case for  example in  satellite  attitude control where the  satellite must change its orientation.  This type of desirable prop erty  leads  naturally  to  the  concepts  of  state  reachability  and  controllability  which will now be studied at length. Another  desirable property  of  systems is the ability to determine the  state  from output  measurements.  Since  it  is  frequently  difficult  or  impossible  to  measure  the state  of  a  system  directly  (for  example  internal  temperatures  and  pressures  in  an internal  combustion  engine)  it  is  extremely  desirable  to  determine  such  states  by observing  the inputs  and  outputs  of the  system  over  some finite time interval.  This leads  to  the  concepts  of  state  observability  and  constructibility  which  will  also  be studied here. The principal goals of this chapter are to introduce and study in depth the system properties  of controllability  and observability  (and of reachability  and  constructibil ity) as well as special forms for the state-space system descriptions when a system is controllable  or uncontrollable  and observable or unobservable. These  special  forms are very useful  in the study of the relationships between  state-space and input-output descriptions  of  a  system.  Note  that  controllability  and  observability  play  a  central role when  a given impulse response  or a transfer  function  description  is realized by means  of  a  state-space  description  as  will  be  shown  in  Chapter  5.  These  special forms  also provide  insight  into the mechanisms  concerning  capabilities  and  limita tions  of  state controllers  and  state  observers  as will be demonstrated  in  Chapter  4. The concepts of controllability and observability are central in the study of state feed back  controllers  (resp.  output  controllers)  and  state  observers.  State  controllability refers  to  the  ability  to  manipulate  the  state by  applying  appropriate  inputs  (in  par ticular by steering the state vector from  one vector value to any other vector value in 214 finite time). It turns out that controllability is a necessary and sufficient  condition for complete eigenvalue assignment in the system matrix A by means of state  feedback. State  observability  refers  to  the  ability  to  determine  the  initial  state  vector  of  the system from  knowledge  of the input  and the corresponding  output over time.  State observability  is a necessary  and sufficient  condition for the arbitrary  eigenvalue as signment  in an asymptotic  state estimator  or state observer that estimates  the  state of the system using input  and output measurements.  State feedback  controllers  and state observers  are studied in Chapter 4. 215 CHAPTERS: Controllability Observability and Special Forms 3.1 INTRODUCTION This  chapter  consists  of  two  parts.  In  Part  1  consisting  of  Sections  3.2  and  3.3 the important concepts of state reachability  (controllability)  and observability  (con-structibility)  are introduced. This is accomplished for continuous- and  discrete-time systems that may be time-varying or time-invariant. In Part 2 consisting of Sections 3.4 and 3.5 special forms  for  state-space representations  are developed for  control lable or uncontrollable  and observable or unobservable  time-invariant  (continuous-and  discrete-time)  systems.  In  addition  the  Smith-McMillan  form  of  a  transfer function  matrix and the poles and zeros of a system are introduced  and  studied. In  Subsection  A  of  this  section  the  concepts  of  reachability  and  controllabil ity  and  observability  and  constructibility  are introduced  using  discrete-time  time-invariant  systems. In this  way significant  insight into the concepts  is gained  early together  with  a  clear  understanding  of  what  these  properties  imply  for  a  system. Discrete-time  systems are selected for this exposition because the mathematical de velopment is simple in this case allowing us to concentrate on explaining  concepts and their impUcations. The continuous-time  case is treated in detail in Sections  3.2 and 3.3. A.  A Brief Introduction  to Reachability  and  Observability Reachability  and  controllability  are  introduced  first  for  the  case  of  discrete-time time-invariant  systems followed  by  observability  and  constructibility.  Finally  du ality is briefly  discussed. 1. Reachability  and  controllability The concepts of state  reachability  (or controllability-from-the-origin)  and  con trollability  (or controllability-to-the-origin)  are introduced here and are discussed at length in Section 3.2. In the case of time-invariant systems a state xi  is called reach able  if there exists  an input that transfers  the state of the system  x{t)  from  the zero state to xi  in some finite time T. The definition  of reachability  for the  discrete-time case is completely  analogous. Figure 3.1 shows that different  control inputs ui{t)  and U2{t) may force the state of a continuous-time  system to reach the value  x\  from  the origin  at different  finite times  following  different  paths.  Note  that  reachability  refers  to  the  ability  of  the 216 Linear Systems FIGURE 3.1 A reachable state xi system to reach xi  from the origin in some finite time; it specifies  neither the time it takes to achieve this nor the trajectory to be followed. A state XQ is called  controllable if there exists an input that transfers  the state from  XQ to the zero state in some finite time  T.  See  Fig.  3.2.  The  definition  of  controllability  for  the  discrete-time  case  is completely  analogous. Similar to reachability controllability refers to the ability of a system to transfer the  state  from  XQ to  the  zero  state  in  finite  time;  it  too  specifies  neither  the  time it takes to achieve the transfer  nor the trajectory  to be followed.  We note that  when particular types of trajectories to be followed are of interest then one seeks particular control inputs that will achieve such transfers. This leads to various control problem formulations including the Linear Quadratic (Optimal) Regulator (LQR). The LQR problem is discussed briefly  in the next  chapter. Section  3.2  shows that reachability  always  implies  controllabiHty  but  control lability  implies  reachability  only  when  the  state transition  matrix  $  of the  system is  nonsingular.  This  is  always  true  for  continuous  time  systems  but  it  is  true  for discrete-time systems only when the matrix A of the system  [or A(k)  for certain val ues of k] is nonsingular. If the system is state reachable then there always exists an input that transfers  any state XQ  to any other state xi  in finite time. FIGURE 3.2 A controllable state XQ In  the  time-invariant  case  a  system  is  said  to  be  reachable  (or  controllable-from-the-origin)  if and only if its controllability  matrix  ^ %  =  [BAB...A"-l5]G/^"><'^^ (1.1) has  full  row  rank  n  that  is  rank  %  =  n.  The  matrices  A  G  R^^^  and  B  E  R^^"^ determine either the continuous-time  state  equations 217 CHAPTER 3: Controllability Observability and Special Forms or the discrete-time  state equations X =  Ax-\-  Bu x(k  +  1)  =  Ax(k)  +  Bu(kl (1.2) (1.3) fc  >  /^o  "=  0.  Alternatively  we  say  that  the  pair  (A B)  is  reachable.  The  matrix %  should  perhaps  more  appropriately  be  called  the  "reachability  matrix"  or  the "controllability-from-the-origin  matrix."  The  term  "controllability  matrix"  how ever  has  been  in  use  for  some  time  and  is  expected  to  stay  in  use. Therefore  we shall  call  % the  "controllability  matrix"  having  in  mind  the  "controllability-from-the-origin  matrix." We  shall  now  discuss  reachability  and  controllability  for  discrete-time  time-invariant  systems (1.3). If the state x{k)  in (1.3) is expressed in terms of the initial vector x(0) then (see Section  2.7) x(k)  =  A^x(0)  + ^A^-^'^^^Bu{i) (1.4) k-\ for  ^  >  0.  It  now  follows  that  it  is  possible  to  transfer  the  state  from  some  value x(0)  =  XQ  to  some  xi  in n  steps that  is x{n)  =  xi  if there exists  an n-step  input sequence {w(0) w(l)... u{n -  1)} which  satisfies  the equation where %n =  [B AB A^-i^]  =  ^  [see (1.1)] and Xi  -  A'^XQ  =  ^nUn Un  =  [u^(n  -  1) u^(n  -  2 )  . . . w^(0)]^ (1.5) (1.6) From the theory of linear algebraic equations (1.5) has a solution Un if and only if xi  -  A'^jco  E  gi(^) (1.7) where  9l(^)  =  range (%). Note  that  it  is  not  necessary  to  take  more  than  n  steps in  the  control  sequence  since  if  this  transfer  cannot  be  accomplished  in  n  steps  it cannot be accomplished  at all. This follows  from  the Cayley-Hamilton  Theorem in view of which it can be shown that 2/l(^^)  =  2/l(^^) for  k>  n. Also note that 9l(^„) includes ^C^k)  iox k  <  n  [i.e. 9l(^„)  D 9l(^y^) k  <  nl  (See Exercise  3.1.) It  is  now  easy  to  see  that  the  system  (1.3)  or  the  pair  {AB)  is  reachable (controllable-from-the-origin) implying  that  any  state  xi  can  be  reached  from the  zero  state  (XQ =  0) in  finite  time  if  and  only  if  rank  ^  =  n  since  in  this  case 9l(^)  =  R^  the entire  state  space. Note that  x\  G 9i{%) is the condition  for  a par ticular  state  x\  to  be  reachable  from  the  zero  state.  Since  2^(^)  contains  all  such states  it is called  the  reachable  subspace  of the  system.  It is  also  clear  from  (1.5) that if the system is reachable any  state  XQ can be transferred  to any other  state  xi in n steps. In addition the input that accomplishes this transfer  is any solution  Un of 218 Linear  Systems (1.5).  Finally  depending  on  xi  and  XQ  this  transfer  may  be  accomplished  in  fewer than  n  steps  (see  Section  3.2). EXAMPLE  1.1.  Consider  x(k  +  1)  =  Ax(k)  +  Bu(k)  where  A B Here the controllability (-from-the-origin)  matrix ^  is ^  =  [BAB]  = with rank ^  =  2. Therefore  the system  [or the pair (A B)] is reachable meaning that any state  xi can  be  reached  from  the  zero  state  in  a  finite  number  of  steps  by  applying  at  most  n inputs {u(0\  w(l)... u(n  -  1)} (presently n  =  2). To see this let xi -Then  (1.5) implies that 0  1  u(l) [1 iJWO). u(l) [u(0)\ b  —  a a Thus the control M(0)  =  au{V)  =  b  -  a will transfer  the  state from  the origin  at  A: =  0 to the  state a b at  ^  =  2.  To  verify  this  we  observe  that  x(l)  =  Ax(0)  +  Bu(0)  = a  = and x(2)  =  Ax(l)  +  Bu(l) + (b-a) Reachability  of  the  system  also  implies  that  a  state  xi  can  be  reached  from  any other  state  xo in  at most  n  =  2 steps. To illustrate  this let  x(0)  = 11 . Then  (1.5)  im-plies that xi  -  A^XQ  =  L  "~ L b-a- 1 a-2  which will drive the state  from a  —  2 b~3 1 at /:  =  0 to u(l) u(0) . Solving u(l) w(0) at  ^  =  2. Notice  that  in  general  the  solution  Un of  (1.5)  is not unique  i.e.  there  are  many inputs  which  can  accomplish  the  transfer  from  x(0)  =  XQ to  x(n)  =  x\  each  cor responding  to  a particular  state  trajectory.  In  control  problems  particular  inputs  are frequently  selected  that  in  addition  to transferring  the  state  satisfy  additional  crite ria  such  as  e.g.  minimization  of  an  appropriate  performance  index  (optimal  con trol).  This  corresponds  to  selecting  a  particular  trajectory  that  e.g.  may  result  in minimum  dissipation  of control  energy. It is important  to remember  that  reachability and  controllability  guarantee  only the  ability  of  a system to transfer  an initial  state  to a  final  state  by  some  control  input  action  over  a  finite  time  interval.  By  themselves reachability  and controllability  do not imply the capability  of a system to follow  some particular  trajectory. A  system  [or the pair  (A 5 )]  is controllable  or controllable-to-the-origin when any  state  XQ can  be  driven  to  the  zero  state  in  a  finite  number  of  steps.  From  (1.5) we  see  that  a  system  is  controllable  when  A^XQ  G  9l(^)  for  any  XQ.  If  rank  A  =  n a  system  is  controllable  when  rank  ^  =  n  i.e.  when  the  reachability  condition  is satisfied.  In  this  case  the  nX  mn  matrix A-""^ = [A-''B...A-^B] (1.8) isofinterestandthesystemiscontrollableifandonlyifranA:(A~^^)  =  rank%  =  n. If  however  rank  A  <  n  then  controllability  does  not  imply  reachability  (see Section  3.2). EXAMPLE 1.2.  The system in Example 1.1 is controllable (-to-the-origin). To see this 219 we let xi  =  0 in (1.5) and write -A^XQ = 1  1 1  2 a b =  [BAB] a P. -b   where Xo  = From  this  we  obtain u(0)\ u(0) la L^. -11 -i\ —a — b at ^  =  2.   the input  that  will  drive  the  state  from 0 -I "0^ Oj EXAMPLE 1.3.  The system x(k  + 1)  =  0 is controllable since any state say x(0) =  can be transferred  to the zero state in one step. In this system however the input u(l) u(0) 1  r 1  0 a 0  1 1  1 l]\a 1 1  2 at  ^  =  0 to CHAPTER 3: Controllability Observability and Special Forms u does not affect  the state at all! This example shows that reachability is a more useful concept than controllability for discrete-time systems. • It should be pointed out that nothing has been said up to now about maintaining the desired system state after reaching it [refer to (1.5)]. Zeroing the input for k^  n i.e. letting M(fe)  =  Oforfe>  n will not typically work unless Axi  =  xi. In general a state starting at xi  will remain at xi  for all fc >  n if and only if there exists an input u(k)  k^  n such that xi  =  Axi  +  Bu(k\ (1.9) that  is  if  and  only  if  (/  -  A)xi condition may not be  satisfied. ^(B).  Clearly  there  are  states  for  which  this 2. Observability  and  constructibility In Section 3.3 definitions  for  state observability  and constructibility  are given and appropriate tests for these concepts are derived. It is shown that observability al ways implies constructibility while constructibility implies observability only when the state transition matrix O of the system is nonsingular. Whereas this is always true for  continuous-time  systems it is true for discrete-time  systems only when the ma trix A  of  the  system  [or  when  A(k)  for  particular  values  of  k]  is  nonsingular.  If  a system is state observable then its present state can be determined from  knowledge of the present  and future  outputs  and inputs. Constructibility  refers  to the ability to determine the present state from present and past outputs and inputs and as such it is of greater interest in applications. In the time-invariant case a system  [or a pair (A C)] is observable if and only if its observability  matrix  0  where C CA CA n-l ^f^pn> (1.10) has full  column  rank  i.e.  rank  €  =  n. The matrices  A  G R^^^  and  C  E RP^ given by the system  description are X — Ax  +  Bu =  Cx  +  Du (1.11) 220 Linear Systems in the continuous-time  case and by the system  description ^(^ +  1)  ^  ^^(y^) _p  ^^^^^^ ^(^)  ^  Cx(yt) +  Du(k\ (1.12) with fe >  /:o  =  0. in the discrete-time case. We shall now briefly  discuss observability  and constructibility  for the discrete-time time-invariant  case. As in the case of reachability  and controllability  this dis cussion  will  provide  insight  into  the  underlying  concepts  and  clarify  what  these imply for a system. If the output in (1.12) is expressed in terms of the initial vector x(0)  then y(k)  =  CA^x(0)  + ^  CA^-^'^^^Bu(i)  +  Du{k) (1.13) k-i for  A: >  0 (see Section 2.7). This implies  that i = 0 y(k)  =  CA^xo (1.14) for  /: >  0 where y{k)  =  y(k)  -k-l ^CA^'^'^^^Bu{i) i = 0 + Du{k) for  ^ >  0 3;(0)  =  yiO) -  Du{G) and  XQ =  x(0).  In  (1.14)  XQ  is to be  determined assuming  that the system parameters  are given and the inputs and outputs are mea sured. Note that if u(k)  =  0 for /: ^  0 then the problem is simplified  since y(k)  = y(k)  and the output is generated  only by the initial  condition  XQ. It is clear that the ability  to determine  XQ  from  output  and input  measurements  depends  only  on the matrices  A  and  C  since  the left-hand  side  of  (1.14)  is  a known  quantity.  Now if ^(0)  =  XQ is known then all x(k)  ^ >  0 can be determined by means of (1.4). To determine  XQ we apply (1.14) for ^  =  0..  .n  -  1. Then Yon-l  =  ^nXo (1.15) where ©^  =  [C^ (CA)^... (CA^-^)^]^  -  0  [as in (1.10)] and hn-i  = [fm...f(n-l)f. Now  (1.15)  always  has a solution  XQ by construction.  A system  is  observable if the solution  XQ  is unique i.e. if it is the only initial condition  that together  with the given input sequence can generate the observed output sequence. From the the ory  of  linear  systems  of  equations  (1.15)  has a unique  solution  XQ  if  and only  if the null  space of 0  consists  of only the zero vector i.e. null  (0)  =  }((€)  =  {0}  or equivalently if and only if the only x  E  R^ that  satisfies €x  =  0 (1.16) is the zero vector. This is true if and only if rank €  =  n. Thus a system is observable if  and only if  rank€  =  n. Any nonzero  state vector  x  G R^ that  satisfies  (1.16) is said to be an unobservable  state and J{{€) is said to be the unobservable  subspace. Note  that  any  such  x  satisfies  CA^x  =  0  for fc =  0 1... n  -  1. If  rank€  <  n then  all vectors  XQ  that  sansfy  (1.15)  are given by  XQ =  xop +  XQ/Z where  xop is a particular solution and XQU is any vector in }((€).  Any of these state vectors together with the given inputs could have generated the measured  outputs. .n To  determine  XQ from  (1.15)  it  is  not  necessary  to  use  more  than  n  values  for -  I  ov to  observe  y(k)  for  more  than  n  steps  in  the  future.  This y(k)  k  =  0.. is  true  because  in  view  of  the  Cayley-Hamilton  Theorem  it  can  be  shown  that J{(€n)  =  >r(0^)  for  k^ J<(€k))  for  k  <  n.  Therefore  in  general  one  has  to  observe  the  output  for  n  steps (see  Exercise  3.1). n.  Note  also  that  J{(€n) is  included  in  Ji(€k) (JV'(O^)  C 221 CHAPTER3: Controllability Observability' and  Special Forms EXAMPLE  1.4.  Consider  the  system  x(k  +  1)  =  Ax(k)  y(k)  =  Cx(k)  where  A  = and  C  =  [0 1].  Presently 0  1 1  1 the  system  [or  the  pair  (A C)]  is  observable.  This  means  that  x(0)  can  uniquely  be determined  from  n  =  2 output  measurements  (in  the  present  cases  the  input  is  zero). with  rank  0  =  2.  Therefore 0  1 1  1 C CA In  fact  in  view  of  (1.15) y{\)  -  KO)" yiP) J(l) 0  1 1  1 ^i(O) ^2(0).  or ^i(O) ^2(0)J -1  1 1  0 y(0) J d ). EXAMPLE  1.5.  Consider  the  system  x{k  +  1)  =  Ax{k)  y{k)  =  Cx(k)  where  A  = 1  0 1  1 and  C  =  [10].  Presently  0  = with  rank  0 = 1.  Therefore 1  0 1  0 C CA the  system  is  not  observable.  Note  that  a  basis  for  }((€)  is which  in  view  of (1.16)  implies  that  all  state  vectors  of  the  form R  are  unobservable.  Rela tion (1.15) implies  that  y(0) LKDJ 1  0 1  0 xi(0) X2(0)J . For a solution  x(0)  to exist as it must. we  have  that  y(0)  =  y(l)  =  a.  Thus this  system  will  generate  an identical  output  for k>  0. Accordingly all x(0)  that satisfy  (1.15) and can generate this output are given by xi(0)l .^2(0)J where  c  G R. "0" c  = a 0. a c = + _ In  general  a  system  (1.12)  [or  a  pair  (A  C)]  is  constructible  if  the  only  vector X that  satisfies  x  =  A^x  with  Cx  =  0  for  every  k  >  0  is  the  zero  vector.  When  A is  nonsingular  this  condition  can  be  stated  more  simply  namely  that  the  system is  constructible  if  the  only  vector  x  that  satisfies  CA~^x  =  0  for  every  fe  >  0  is the  zero  vector.  Compare  this  with  the  condition  CA^x  =  0  k  >  0  for  x  to  be  an unobservable  state; or with the condition that a system is observable if the only  vector X that  satisfies  CA^x  =  0  for  every  /:  >  0  is  the  zero  vector.  In  view  of  (1.14)  the above  condition  for  a  system  to be  constructible  is the  condition  for  the  existence  of a  unique  solution  XQ when  past  outputs  and  inputs  are  used.  This  of  course  makes sense since constructibility  refers  to determining  the present  state from  knowledge  of past outputs  and inputs. Therefore  when A is nonsingular  the system is  constructible if  and  only  if  the  pnX  n  matrix CA-CA-(1.17) has  full  rank  since  in  this  case  the  only  x  that  satisfies  CA '^x  =  0  for  every k  ^  0  is  X  =  0.  Note  that  if  the  system  is  observable  then  it  is  also  constructible; 222 Linear Systems however if it is constructible then it is also observable only when A is nonsingular (see Section 3.3). EXAMPLE 1.6.  Consider the (unobservable)  system in Example  1.5. Since A is non-singular OA  ^ = 1  01 .1  OJ r  1  0^ [-2 Ij 1  0" .1  0_ Since rank OA  ^ =  1 <  2 the system [or the pair (A C)] is not constructible. This can also be seen from the relation CA  ^x = 0k>  0 that has nonzero  solutions x since  C  =  [10] =  CA~^ = CA~^ =  •••  = CA~^ for /: >  0 which implies that any x =  c  '  7? is a solution. 3. Dual  systems Consider the system described by X =  Ax  +  Bu y  = Cx^Du (1.18) where A  G 7?"X^ B  G R'''''^  C G  RP''^ is defined  as the system and D G 7^^^^. The dual  system  of (1.18) XD  =  ADXD  +  BDUD yo  =  CDXD  +  DDUD. (1.19) where AD  =  A^ BD  =  C^  CD  =  B^  and Do  =  D^. LEMMA 1.1.  System (1.18) denoted by {A B C D} is reachable (controllable) if and only if its dual {A^ BD  CD DO} in (1.19) is observable (constructible) and vice versa. Proof. System {A 5 C D] is reachable if and only if ^  =  [5 AB...  A'^'^B] has full rank n and its dual is observable if and only if B^ B^A^ \B^{A^Y-^\ has full rank n. Since 0^  =  % {A B C D} is reachable if and only if {AD ^D  CD /^D} is observable.  Similarly  {A 5 C D} is observable  if and only  if  {AD ^D> CD DD}  is reachable. Now {A B C D] is controllable if and only if its dual is constructible and vice versa; recall from Sections 3.2 and 3.3 a continuous-time system is controllable if and only if it is reachable; and is constructible if and only if it is observable. For  the discrete-time  time-invariant  case the dual  system  is  again  defined  as AD  =  A^ BD  =  C^  CD  =  B^  and DD =  D^.  That  such  a  system  is  reachable if  and only  if  its dual  is  observable  can be  shown  in  exactly  the  same  way as in the proof  of Lemma  1.1. That  such  a system  is controllable  if  and only  if its dual is  constructible  when  A  is  nonsingular  is  true  because  in  this  case  the  system  is reachable if and only if it is controllable;  and the same holds for observability and constructibility. The proof for the case when A is singular involves the controllable and  unconstructible  subspaces  of  a  system  and its dual. We omit  the details. The reader is encouraged to complete this proof  after  studying  Sections 3.2 and 3.3. In  the time-varying  case  the  dual  system  is  defined  in  a  similar  manner  as given taking transposes of matrices and in addition reversing time. This will not be discussed  further  here.  We  merely  wish  to  point  out  that  the  mappings  from  the original  system to the dual  system  are in this  case of the form  {A(a  +  t) B(a  +  t) C(a  +  tl  D(a  +  t)} -^  {A^(a  -  t) C^(a  -  t) B^(a  -  t) D^{a  -  0} where a  is a fixed real number. Thus under this transformation  we mirror the image of the graph of each function  about a point a  on the time axis and then take the transpose of each matrix.  Note  that  the  mapping  between  the  state transition  matrices  is  of  the form ^^{a-ta).  With this definition it is now possible to establish results ^{a  a^-t)^ similar  to Lemma  1.1  for  the time-varying  case. The proofs  involve  the  Gramians defined  in Sections 3.2 and 3.3. Figure  3.3  summarizes  the  relationships  between  reachability  (observability) and controllability  (constructibility)  for continuous- and discrete-time  systems. 223 CHAPTER 3: Controllability Observability and Special Forms Reachability Dual Observability v Controllability Dual V Constructibility FIGURE 3.3 In continuous-time systems reachability (observability) always implies and is implied by controllability (constructibility). In discrete-time systems reachability (observability) always implies but in general is not implied by controllability (constructibility). B.  Chapter  Description This  chapter  consists  of  an  introduction  and  two  parts.  In  Part  1  consisting  of Sections  3.2  and  3.3  reachability  and  controllability  and  observability  and  con structibility are introduced and studied. In Part 2 consisting of Sections 3.4 and 3.5 special forms for state-space representations of time-invariant systems are developed for controllable/uncontrollable  observable/unobservable  (continuous- and discrete-time)  systems.  In  addition  the  poles  and  zeros  of  a  system  are  introduced  and studied. In  the  introduction.  Subsection  3.1 A  the  concepts  of  reachability  (or controllability-from-the-origin)  and  observability  are  introduced  using  discrete-time  time-invariant  systems.  The  inputs  that  accomplish  the  desired  transfers  of  a state are easily derived in terms of the controllability  (-from-the-origin)  matrix of a system.  Conditions  for  reachability  and  observability  are  derived  directly  in  terms of the controllability  and observability  matrices  of the system.  Similarly  controlla bility (or controllability-to-the-origin)  and constructibility  are also introduced.  State reachability  and  observability  are  related  by  duality.  Dual  systems  and  the  dual notions of reachability (respectively observability) and controllability  (respectively constructibility)  are also  discussed. In  Section  3.2 reachabihty  and  controllability  are discussed  at length  for  both continuous- and  discrete-time  systems. In the continuous-time  case the inputs  that 224 Linear Systems accomplish the desirable state transfers  are derived using the reachability  (and the controllability) Gramian. Since with only minimal additional work one can treat the time-varying case as well this is the approach pursued herein i.e. both time-varying and time-invariant cases are studied. The time-invariant case is discussed separately and can be treated independently of the time-varying case. This adds significant  flex ibility to the coverage of the material in this chapter. Many criteria for reachability (controllability) are developed. It is shown that reachability implies controllability and vice versa in the case of continuous-time systems. In discrete-time systems al though reachability implies controllability controllability does not necessarily imply reachability. This is due to the lack of general time-reversibility in the case of differ ence equations as pointed out in Chapter 2. (Note that a detailed section summary is included at the beginning of Section 3.2.) Observability  and  constructibility  are  addressed  in  Section  3.3 in  a manner analogous to the treatment of the dual concepts of reachability  and controllability in  Section  3.2. Both continuous- and  discrete-time  cases  are considered.  Observ ability and constructibility  Gramians are used to study these properties in the case of both time-varying  and time-invariant  continuous-time  systems. Once more the time-invariant  case is treated  separately  and  can be  studied  independently  of the more  general  time-varying  case.  Observability  always  implies  constructibility  in both continuous- and discrete-time  systems; however  constructibility  always im plies observability only in the case of continuous-time  systems. This is due to the lack of general time-reversibility of difference  equations. (Note that a detailed sec tion summary is included at the beginning of Section 3.3.) In Section 3.4 similarity transformations are used to reduce the state-space rep resentations  of  time-invariant  systems  to  special  forms.  First  standard  forms  for uncontrollable  and unobservable  systems  are developed.  These lead  to  Kalman's Decomposition Theorem and to additional tests for  controllability  and observabil ity that involve eigenvalues  and eigenvectors  of the system matrix A (in Subsec tion  B)  and  to relations  between  state-space  and  transfer  matrix  descriptions  (in Subsection C). Controller and observer forms  for controllable and observable sys tems are derived next (in Subsection D). These forms  are useful  in state  feedback control and in state observer design discussed in Chapter 4. The Structure Theo rem is introduced next. This result which involves the controller (observer) forms and relates the state-space representations to the transfer function matrix of the sys tem is used in Chapter  5 where  state-space reahzations  of transfer  functions  are addressed. In Section  3.5 the poles of a system and of a transfer  function  matrix are in troduced. There the zeros of the system the invariant zeros the input and output decoupling  zeros and the transmission  zeros which  are the zeros of the  transfer function matrix are also introduced. The Smith and Smith-McMillan forms of poly nomial and rational matrices respectively are used to define poles and zeros. Utiliz ing zeros one can render certain eigenvalues (system poles) and their corresponding modes unobservable from  the output using state feedback.  This leads to the solu tion of several control problems such as disturbance decoupling model matching and diagonal decoupling. The discussion of poles and zeros of a system {A B C D} and of the corresponding transfer function matrix H(s) also helps to clarify the rela tionship between internal (state-space) descriptions and external (transfer  function matrix) descriptions. This is studied in greater detail in Chapter 5. 225 CHAPTER 3: Controllability Observability and Special Forms C.  Guidelines  for the  Reader Reachability  which  is  controllability-from-the-origin  and  controllability  (-to-the-origin) together with observability  and construetibility  are introduced in Subsection 3.1 A using discrete-time time-invariant  systems. Careful  study of this  introductory section  leads  to  early  and  significant  insight  into  these  important  system  proper ties without requiring  the mathematical  sophistication  needed  in a careful  study of these  properties  in  the  continuous-time  case.  Duality  is  also  discussed  in  Subsec tion 3.1 A. In Part  1 reachability  and controllability  and observability  and  constmctibility are in  Sections  3.2  and  3.3  respectively  for  continuous-time introduced time-varying  and  time-invariant  systems  as  well  as  for  discrete-time  systems.  For convenience  detailed  summaries  of  the  results  with  reference  to  particular  def initions  and  theorems  are  included  at  the  beginning  of  these  sections.  At  a  first reading one may concentrate on the time-invariant  continuous-time  case  discussed in  Subsections  3.2B  and  3.3B.  (Recall  that  an  introduction  to  the  time-invariant discrete-time  case  was  presented  in  Subsection  3.1 A.)  The  time-invariant  case  is developed in a self-contained  manner in these sections providing  flexibility  in cov erage  of  the material.  Note  that  in  Corollary  2.12 in  Subsection  3.2B  it is  shown that the system is reachable if and only if the controllability  matrix  C has full  rank. Theorem  2.13  provides  an  input  u(t)  that  can  accomplish  the  transfer  of  the  state from  a  vector  value  XQ  to  another  vector  value  xi  provided  that  such  transfer  is possible while Theorem  2.17  gives  additional  tests for reachability.  A  relationship between reachability  and controllability  is established  in Theorem 2.16. In an anal ogous  manner  in  Corollary  3.8  in  Subsection  3.3B  it  is  shown  that  a  system  is observable if and only if the observability  matrix € has full rank. A relationship be tween observability  and constmctibility  is given in Theorem  3.9 while in  Theorem 3.10  additional tests for  observability  are presented. A useful  table of all  Gramians used in this chapter is provided in the summary  section  (Section  3.6). In Part  2  special  forms  for  state-space  representations  of continuous-time  and discrete-time time-invariant  systems are introduced. The standard forms  for  uncon trollable and unobservable representations and the Kalman Decomposition  Theorem are presented  in  Subsection  3.4A  and useful  eigenvalue/eigenvector  tests  for  con trollability  and  observability  are developed  in  Subsection  3.4B. The controller  and observer  forms  and  the  Structure  Theorem  are  discussed  in  Subsection  3.4D.  At a  first  reading  one  could  study  Subsections  3.4A  and  3.4B  and  cover  Subsection 3.4D  selectively concentrating  on deriving and using controller and observer  forms rather  than  proofs  and  properties.  Note  that  the  controller  and  observer  forms  are used primarily  in realization  algorithms in Chapter 5 in a method to assign  closed-loop eigenvalues  via  state feedback  in Chapter  4 and in Chapter  7 to gain  insight into the relations between  state-space and polynomial matrix representations  of lin ear  time-invariant  systems.  Furthermore  the  Structure  Theorem  discussed  in  this section  introduces  polynomial  matrix  fractional  descriptions  of  the  transfer  func tion  matrix  H(s).  These  descriptions  are  very  useful  in  control  problems  and  are discussed further  in Chapter 7. In Section 3.5 the poles and zeros of a system are in troduced using the Smith form of a polynomial matrix and the Smith-McMillan  form of a transfer function matrix H(s).  The pole and zero polynomials of H(s)  are defined next. This  gives rise to the McMillan  degree of H(s)  and to the order of a minimal 226 Linear Systems realization discussed in Subsection 5.2C of Chapter 5. The study of poles and zeros offers  significant  insight  into  feedback  control  systems.  At  a first reading  Section 3.5 may be omitted without loss of continuity. P A R TI CONTROLLABILITY  AND  OBSERVABILITY 3.2 REACHABILITY  AND  CONTROLLABILITY The  objective  here  is  to  study  the  important  properties  of  state  controllability  and reachability  when  a  system  is  described  by  a  state-space  representation.  In  Sub section  3.1 A a brief  introduction  to these concepts for  discrete-time  time-invariant systems  was  given  where  it  was  shown  that  a  system  is  completely  reachable  if and  only  if  the  controllability  (-from-the-origin)  matrix  ^  in  (1.1)  has  full  rank  n {rank%  =  n).  Furthermore  it was  shown  that  the input  sequence  necessary  to ac complish  the  transfer  can  be  determined  directly  from  % by  solving  a  system  of linear  algebraic  equations.  In  a  similar  manner  we  would  like  to  derive  tests  for reachability  and  controllability  and  determine  the  necessary  system  inputs  to  ac complish  the state transfer  for  the continuous-time  case. This is the main  objective of this  section. We note however that whereas the test for reachability  in the time-invariant case {rank ^  =  n) can be derived by a number of methods the appropriate sequence of system inputs to use cannot easily be determined directly from ^  as was the case for discrete-time  systems. For this reason we use an approach that utilizes ranges of maps in particular the range of an important nXn  matrix—the reachabil ity Gramian. The inputs that accomplish the desired state transfer can be determined directly  from  this  matrix.  However  once  this  is  accomplished  we  can  develop  all the results for the time-varying  case as well with hardly  any  additional work. This is the approach we will employ. The reader can skip the more general material how ever starting with Definition  2.1 and concentrate on the time-invariant case starting with Definition  2.9 if  so desired. The contents  of this  section  are now presented  in greater detail. Section  description In this section the concepts of reachability and controllability are introduced and discussed  in detail for  linear  system  state-space  descriptions. This is  accomplished for  continuous- and discrete-time  systems for both time-varying  and  time-invariant cases. Reachability  for continuous-time  systems is discussed first and the reachability Gramian  Wr(to ^i) is defined  (in Definition  2.7). It is then  shown  (in Corollary  2.3) that the system is reachable at t\ if and only if Wr(to t\) has full rank for some /Q — ^i • Reachability  implies that it is possible to transfer  the state from  a value  XQ to some value  xi  and  system inputs that accomplish  this transfer  are given in Theorem  2.4 and Corollary  2.5. Controllability  is discussed next and the controllability  Gramian is  defined  (in  Definition  2.8).  It is  shown  (in  Theorem  2.6)  that  a  continuous-time 227 CHAPTERS: Controllability Observability and Special Forms system  is  reachable  if  and  only  if  it  is  controllable.  Two  additional  results  (Theo rems  2.8  and  2.9)  provide  further  criteria  for  reachability  and  controllability.  All these results are then applied to the continuous-time time-invariant case. The above material is presented in a manner that makes possible the study of time-invariant sys tems independent of the time-varying case. In Lemma 2.10 a relationship  between the reachability  Gramian  Wr(0 T)  and the controllability  (-from-the-origin)  matrix %  =  [B AB...  A'^'^B]  is  estabHshed.  It  is  then  shown  in  Corollary  2.12  that  a system  is reachable  if  and  only  if ^  has  full  rank  n.  A  system  input  sequence  that transfers  the state from  XQ to xi  is derived in Theorem 2.13 and in Corollary 2.14. In Theorem 2.16 a relationship between reachability  and controllability  is  established and Theorem 2.17 provides additional tests for  reachability. For  discrete-time  systems  in  particular  discrete-time  time-invariant  systems reachability  and  controllability  are  discussed  next.  Here the  controllability  matrix % plays  a predominant  role.  It  is  shown  that  when  ^  has  full  rank  the  system  is reachable  (Corollary  2.19)  and  input  sequences  that  transfer  the  state  to  desired values are derived  (Theorem 2.20 and Corollary 2.21). In Theorem 2.22 it is shown that reachability  in the case of discrete-time  systems always implies  controllability. In contrast  to the  continuous-time  case the  converse  to this  statement  is  generally not true. This is due to a lack of time reversibility  in difference  equations. When  A is nonsingular  then  controllability  also implies  reachability.  Finally  in  Definitions 2.13 and 2.14 the reachability and controllability Gramians for the discrete-time case are defined  for  sake of completeness. A.  Continuous-Time  Time-Varying  Systems We consider the state equation X =  A{t)x  +  B{t)u (2.1) where A{t)  G  R'''^''  B{t)  G R^^^^  and  u(t)  G  R"^ are defined  and  (piecewise)  con tinuous on some real open interval  {a b). The state at time t is given by x{t  to XQ)  ^  x{t)  =  ^{t  to)x(to)  + 0(^  T)B(T)U(T)  dr (2.2) J to where ^(t  r) is the state transition matrix of the system to t  G {a b) and x(fo)  = XQ denotes the initial state at initial time. In the time-invariant  case. where A  G /^^><" B  G /^^><'^ (2.2) is still valid  with X =  Ax  +  Bu ^{t  T)  =  ^{t  -  T 0)  -  exp [{t -  T)A\  =  e^^'-^\ (2.3) (2.4) We are interested  in using  the input to transfer  the  state from  XQ  to some  other value  x\  at  some  finite  time  ti  >  to [i.e. x(ti)  =  xi].  Equation  (2.2)  assumes  the form Xi  =  $(^1 ^o)^0  + 0(^i  T)5(T)W(T)JT (2.5) 228 Linear Systems and clearly there exists u(t) t  E  [/Q h ] that satisfies  (2.5) if and only if such  transfer ^f ^^e state is possible. Rewriting  (2.5) as ch jci  -  (5(^1 ro)xo  = ^{ti  T)B{T)U{T)  dr (2.6) and letting  xi  ^^ x\  -  ^{h  to)XQ  we note that the  u{t) that transfers  the state  from XQ  at ^0 to  x\  at time  t\  will  also cause the  state to reach  x\  at ti  starting  from  the origin at to (i.e. x{to)  =  0). For system (2.1) we introduce the following  concept. DEFINITION  2.1.  A State  xi  is reachable  at time t\  if  for  some finite to < t\  there exists an input u{t\  t E  [to t\\  that transfers the state x{t) from the origin at ^o to x\  at time t\  [i.e. that transfers x(t) from x(tQ)  =  0 to x(t\)  =  xi]. Thus when xi is reachable at ti [with x(to) = 0] then in view of (2.5) there exists an input u such that xi  =  i'  c^(tiT)B(T)u(T)dr. (2.7) • We note that the times  ti  and  to are important  individually  in the  time-varying case only; in the time-invariant case as is well known by now ti -  to is the important quantity and typically ^o is taken to be ro  =  0 with ti  =  T3. finite positive number. The set of all reachable states xi  contains the origin and constitutes a linear sub-space of the state space (X R)  =  (R^ R) (verify this). This gives rise to the following. DEFINITION 2.2.  The reachable at ti subspace R[^ of (2.1) is Rr^ = {set of all states x\  reachable at ^i}. • When  the context is clear  and there is no ambiguity  we will write  Rr in  place ofR'. DEFINITION 2.3.  The system (2.1) is (completely state) reachable at ti if every state xi in the state-space is reachable at ti (i.e. Rr =  R'^). In this case we equivalently make • reference to reachable pair (A(t) B(t)) atti. A reachable  state is  sometimes  also called  controllable-from-the-origin.  Addi tionally there are also states defined  to be controllable-to-the-origin  or simply  con trollable.  In particular we have the following  notion. DEFINITION 2.4.  A state XQ is controllable at time to if for some finite t\  > to there exists an input u(t\  t E  [to ti] that transfers the state x(t) from  xo at ^o to the origin at time ti  [i.e. from x(to) = xo to x(ti)  =  0]. • In view of (2.5) there exists an input u such that -^(tu to)Xo  = f  '  ^(h JtQ T)B{T)u{T)dT or by premultiplying by ^~^{t\  to)  =  ^(to  ti)  (see Section 2.3) - xo  =  f  ' 0(^0 T)B{T)u{r)dT (2.8) (2.9) where the semigroup property ^{to  t\)^{t\y  r)  =  0(^0 r)  was used. Similar to the case of reachable  states the set of all controllable  states  includes the origin and is a linear subspace  Re of the state-space X  (i.e.. Re  C  X). DEFINITION 2.5.  The controllable at to subspace /?J? of (2.1) is R^c  = {set of all states XQ controllable at ^o}- It is denoted by Re for convenience when there is no  ambiguity. 229 CHAPTERS: Controllability Observability and Special Forms • DEFINITION2.6.  The system (2.1) is (completely state) controllable at to if every state xo in its state-space is controllable (i.e. if Re = R^). In this case we equivalently make reference to controllable pair (A(t) B(t)) at ^. • Discussion Relation  (2.7) shows that for  x  =  A(t)x  +  B(t)u  given in (2.1) and for given  ti the range of the integral  map L  =  L(u  to ti)  = ^{t\ T)B{T)U{T) dr (2.10) with  u{t) t  E  [^0 ^i]  and  with  t^  varying  over  all  finite  values  ^Q <  ^i is  exactly the reachability  subspace  Rr  since  a state  x\  is reachable  if  there exists  a t^ and u such  that  xi  E  2/l(L). Notice  that  in  view  of  (2.6) the  input  u which  transfers  the state  from  the  origin  at  t^io  x\  at  t\  also  transfers  the  state  from  XQ at  t^io  xi  at t\  where xi  =  x\  — ^{t\  ^)xo. For fixed  XQ since {x\}  spans the reachability  sub-space /^^\ this relation yields all states xi  that can be reached from  XQ in finite time t\  -  to-In  Lemma  2.1 the  range  of  L  is  shown  to  be  equal  to  the  range  of  a  matrix the reachability  Gramian  Wr(to t\)  which  is rather  easy  to determine. In the time-invariant case it is also shown to be equal to the range of the controllability  matrix %. Before  proving  these  results  the  relation  between  reachability  (controllability-from-the-origin)  and  controllability  (controllability-to-the-origin)  is  discussed;  the exact relation is proved in Theorem 2.6. In view of (2.7) and (2.8) a vector x is reachable  (controllable-from-the-origin) at  t\  if  there  exists  a  finite  ^o and  u(t) t  E  [^o ^i]  so  that  x  E  9l(L)  where  L  is defined  in  (2.10) and it is controllable  (-to-the-origin)  at ^o if 0(^i to)x  E  Sl(L). It is shown later (Theorem 2.6) that the system (2.1) or the pair (A B) is  (completely state) reachable if and only if it is controllable. This is the reason why only one term is typically  used  in  the  literature  when  describing  these properties  for  continuous-time  systems. For discrete-time  systems however  the  situation  is different.  In this case  as  will  be  shown  later  in  this  section  if  the  pair  (A B)  is  reachable  then  it is also controllable but not necessarily  vice versa; that is in the discrete-time  case controllability  does  not  necessarily  imply  reachability.  Indeed  controllability  im plies reachability  only when the state transition matrix ^(k  ko) has full rank which is not  always  true in  discrete-time  systems. As  discussed  in  Chapter  2 this  is  due to  the  lack  of  the  "time  reversibility"  property.  On  the  other  hand  in  the  case  of continuous-time  systems ^(t  r)  is always nonsingular. In such systems reachabil ity  implies  that  any  state  xi  can  be  reached  from  any  other  state  XQ  in  finite  time ti  -  to. This property is sometimes used in the literature to define  "controllability." An  input  that  achieves  this  transfer  is given  later  in  Corollary  2.5. In the  discrete-time systems literature the term that is typically used is "reachability"; however for simplicity the term "controllability" is sometimes  also used with  some sacrifice  of 230 Linear Systems accuracy.  We will  use both  terms  reachability  and controllability  with  a warning  to the reader  when  use of the term  controllability  (-from-the-origin)  is made  instead  of reachability. Now  suppose  there  exists  an  input  u  which  transfers  the  state  of  the  system from  x{to)  =  0  to  x{ti)  =  x i  that  is  (2.7)  is  true.  The  integral  in  (2.7)  is  a  map L  =  L{utoti) defined  in  (2.10)  that  maps  an  input  u{t)  ^  R^  defined  over  [^o^^i] to  states xi  ^  R^.  We are interested  in the range  of L  ^ ( L)  since  it contains  all the states that can be reached  from  the origin x{to)  =  0 at time ti  by varying the input  u. Note  that L has infinite-dimensional  domain  and therefore  it is not easy to  determine its  range  directly.  In  the  following  we  show  that  ^ ( L)  is  equal  to  the  range  of  an important  matrix  the reachability  Gramian. DEFINITION  2.7.  The reachability  Gramian  of the system x = A(t)x-\-B(t)u nxn  matrix is the Wr{toA)  =  r O ( n  T ) 5 ( T ) 5 ^ ( T ) 0 ^ ( n  T ) J T Jto where 0(r T)  denotes the state transition matrix. (2.11) • Note  that  Wr is  symmetric  and  positive  semidefinite  for  every  ti  >  to;  that  is Wr  =  W^  and  Wr >  0  (show  this).  Now  let  to <  ti  be  given.  Then  the  following lemma  can be  shown. L E M MA  2.1.  ^{L{utoti)) =^{Wr{toti)). Proof  We  first  show  that  ^(W^)  C  ^ ( L ).  Let  xi  G ^(W^);  that  is  there  exists 7]i  eR^  such  that  WrT]i  =xi.  Choose  UI{T) = B^(T)^^{tiT)rii. Then  L{uitoti)  = Jl^ 0{tiT)B{T)B^ {T)0^  {tiT)dT\rii  =Wrrii  =xi.  Therefore  xi  G ^ ( L ) and since xi  is arbitrary it follows  that ^{Wr)  C ^ ( L ). We  shall  now  show  that  ^ ( L)  C ^(Wr)  which  together  with  ^(Wr)  C ^ ( L) proves  that  ^ ( L)  =  ^(W^).  Let xi  G ^ ( L )  i.e.  there  exists  an  input  ui  such  that L{uitoti)  = xi.  We assume  that  xi  ^  ^(W^) and we  shall  show  that  this  leads  to a contradiction.  This implies that the null  space of W^(^0^1) is nonempty.  Wr is  symmet ric  and so the range of Wr is the orthogonal  complement  of its null  space  (prove this). Thus  for any u G ^{Wr)  and v G yK(Wr)  u^v  = 0. Also  we may write xi  = x^^ +x'/ with  x[  G ^(Wr)  and  x'l  G yK{Wr)  (x'l ^  0  since  xx  ^  ^(Wr)).  Then  there  exists X2 G yK(Wr)  such  that  X2x'( ^  0  which  implies  X2X1  7^ 0. Now ^2 Wr(^0^1 )-^2  =  0  = Ho [ 4 ^ ( ^ i ' ^ ) ^ ( ^ )]  [xl^{tuT)B{T)fdT =  g'  II x^O(riT)5(T)  11^ J T  which  shows that^2 0(^1 T)B{T)  = 0 for every  T G [^0^1]• This in turn implies that^2xi  = X2L{u\)  = = 0  which  is  a  contradiction  since  ^2X1 7^ 0.  Therefore Jl^[xl^{tuT)B{T)]ui{T)dT XI  G ^(Wr)  which implies that ^ ( L)  C ^(Wr). • Lemma  2.1  shows  that  the  set  of  all  states  that  can be  reached  at  time  ti  from the  range  of  the the  origin  at  some  finite  time  to <  ti  is  given  by  ^(Wr{toti)) reachability  Gramian. THEOREM  2.2.  Consider the systemi  = A(^)x + 5(^)w givenin  (2.1). There exists an input  u that  transfers  the state to xi  at ti  from  the origin  at some  finite  time  ^0 <  ^1 ^  if and only if there exists finite time ^0 < h  so that XI e^{Wr{toti)). Furthermore an appropriate u that will accomplish this transfer  is given by 231 CHAPTER 3: Controllability Observability and  Special Forms u(t)  =  B^{t)^^{ti t)j]i (2.12) with 171  a solution of Writ^ ti)ri\  =  xi  and t G [to ti]. Proof  In view of Lemma  2.1 and the definition  of L(u) in (2.10) the proof  of the first part of the theorem is straightforward.  To prove the second part of the theorem note that • (2.12) was used in the proof of Lemma 2.2 to accomplish the transfer  to xi. COROLLARY  2.3.  The system  x  =  A(t)x  +  B(t)u  is (completely  state)  reachable at ti  or the pair (A(t)  B(t)) is reachable at ti if and only if there exists finite to < t\  such that rank  Wr{to t\)  =  n. (2.13) Proof.  In  view  of  Theorem  2.2 all  states  xi  can be reached  at  ti  if  and only  if for some  to  <  h^{Wr(toyt\))  =  ^"  the  entire  state  space.  This  is  true  if  and  only  if • rank  Wr{to ti)  =  n for some finite ^  <  ^1. The  following  result  is  useful  in  accomplishing  the transfer  from  a  state  XQ to another  state  x\  in some  given  finite  time  t\ -to. THEOREM  2.4.  There  exists  an  input  u  that  transfers  the  state  of  the  system  x  = A(t)x  + B(t)u from  xo at time to to xi  at time ti  >  to if and only if Xi  -  ^(ti to)Xo  E  ^(Writo h)). Furthermore  such input is given by u(t)  =  B^(t)^'^(ti t)r]i with 171  a solution of W'^(^o ^i)'»7i  =  -^1 -^{tito)xo' (2.14) (2.15) (2.16) Proof  The proof is straightforward  in view of Theorem 2.2 and the fact that there exists an input  which  transfers  the state from  xo at to to xi  at t\  if and only if it transfers the • state from the origin at to to xi  =  x\  -  0(fi to)xo at t\  [see (2.6)]. EXAMPLE  2.1.  Consider i:  =  A(Ox + 5(0w where A(0  = B{t)  =  0 The  state  transition  matrix  was calculated  in Example  3.4 Section  2.3 (of Chapter 2) to be -1 0 e^' -1 ^{U  T) O-it-T) 7+3x1 i^z.^ W^' — e -a-r) Here ^{t T)B(T) 0 and the reachability  Gramian of the system is WritoJl) to  L  0 dr  =  '(ti  -  to)e-^' 1  0' 0_ 0 It  is  clear  that  rankWr{toy ti)  <  2  =  /2  for  any  to <  t\  and  therefore  the  system is  not reachable  at  t\.  Note  that  since  t\  is  arbitrary  the  system  is  not reachable  at any  finite  time.  However  the  state  can  be  transfered  from  the  origin  to  a  state xi  E  ^{Wr{to  h)).  In  particular  in  view  of  Theorem  2.2  let  xi  = \(X I  a  E  R 232 Linear  Systems and solve Wr{to ti)r]i  =  xi  to obtain r/i  = -to where p  G R arbitrary. Then _e^h in  view  of  (2.12)  u(t)  =  [<^(tit)B{t)fr]i  =  [^"^i0] - ^0 -e^^ will to drive  the  state  from  the  origin  at  ^o to  xi  at  ti.  To  verify  this  we  note  that  x(ti)  = \;'^{hT)B{T)u{T)dT = 0 -e'^ih  -to)  =  _.  Notice  that  for  the  transfer t\  —  to to  be  accomplished  in  a  short  period  of  time  t\  -  to  =  e  with  e  small  the  required control magnitude can be quite large since  uit)  =  (a/e)e^^. • The last observation  in Example  2.1 points to two important  aspects that we  now elaborate  on. First  we  note  that  the  faster  the  state  of  the  system  is  required  to  move  (the smaller  the  ti  -  to  =  e)  and  the  further  away  the  desired  state  x\  is  (the  larger  the a )  the  larger  the  required  control  magnitude  will  be.  This  makes  intuitive  sense since  it  simply  states  that  the  more  sudden  and  drastic  the  change  in  the  state  the larger  the  required  control  force  will  be  (think  e.g.  of  a  simple  mechanical  spring system). Second  it  is  clear  that  the  property  of  reachability  (controllability)  implies  the ability  to  change  the  state  of  the  system  very  fast  indeed  paying  for  this  of  course in  terms  of  increased  control  magnitude  (see  Example  2.1  and  Exercise  3.12).  In tuitively  this  is  not  always  possible  in  the  case  of  physical  processes  where  only limited  control  action  is typically  available.  This  points  to  some  of the  limitations  of linear  system  models  that  do  not  include  information  about  input  saturation  limits nonlinear  behavior  limitations  of  output  sensors  and  the  like. EXAMPLE  2.2.  Consider  the  system  described  by  i  =  A(t)x  +  B(t)u  where  A(t)  = -1 0 e^' -1 B(t)  = .  The  state  transition  matrix  ^(? T)  is  given  in  Example 2.1. Here 0(? r)B(T)  = the system is reachable at ti  (show this). and the reachability Gramian  Wr(to t\)is  such that • e~ -t+2T-\ The following  result demonstrates  the importance  of reachability  in  determining an  input  u to  transfer  the  state  from  any  XQ to  any  x\  in  finite  time. COROLLARY  2.5.  Let the system  i:  =  A(t)x  +  B(t)u  be (completely  state)  reachable at time  t{  or let the pair  (A(t)  B(t))  be reachable  at  ti.  Then  there  exists  an  input  that will transfer  any state xo at some finite time to <  ti  to any state xi  at time ti.  Such input is given by u(t)  =  B^(t)^^(tit)W;\toti)[xi -<^(tito)xo] (2.17) fort  E  [^0^1]. Proof  In view of Corollary  2.3 reachability  implies that given ti  rank Wr(to ti)  =  n for  some  ^o <  t\  or that ^{Wr{to  t\))  =  /?" the whole  space for  some to. This  implies that any vector xi  - ^ ( ^ i  to)xo G ^(W;.(^ ^0) which in view of Theorem 2.2 and (2.12) implies that the input in (2.17) is an input which will accomplish  this transfer. • There  are  many  different  control  inputs  u  that  can  accomplish  the  state  trans fer  from  Xo  at  ^  =  to  to  x\  at  t  =  ti.  It  can  be  shown  that  the  input  u  given  by (2.17)  accomplishes  this  transfer  while  expending  a  minimum  amount  of  energy. In  particular  among  all  the  control  inputs  u(t)  that  will  transfer  the  state  from  XQ at  to  to  xi  at  t\  u(t)  in  (2.17)  minimizes  the  cost  functional  J/^  ||W(T)|P J r  where \\u(t)\\  =  [w^(Ow(0]^^^theEuchdean  norm  of  u(t). We  shall  now  establish  a connection  between  controllability  and  reachabihty  of the  continuous-time  system  x  =  A(t)x  +  B(t)u. 233 CHAPTER  3: Controllability Observability and  Special Forms THEOREM  2.6.  If the  system  x  =  A(t)x  +  B(t)u  or the pair (A(t)  B(t))  is  reachable at  ti  then  it  is  controllable  at  some  ^  <  ^i. Also  if  it  is  controllable  at  ^o then  it  is reachable at some ti  >  to. Proof.  It was  shown  in  Corollary  2.3 that  for  reachability  of  (2.1)  at ti  we must  have rank Wr(to ti)  =  n. A  similar test for controllability can be derived in an identical man ner. In particular in view of (2.9) it is clear that the range of L  =  L{utQti)  = (^(to T)B(T)U(T) dT (2.18) is  of  (present)  interest  [compare  with  L  in  (2.10)  used  to  prove  reachability].  A  re sult similar to Lemma  2.1 can now be established  using  an identical  approach  namely that ^(L(u to ti))  =  '3i(Wc(to ^i)) where  Wdto  ti)  is the controllability  Gramian  de • fined next. DEFINITION2.8.  The controllability  Gramian  of the system  x nX  n matrix A(t)x  + B(t)u  is the Wc(toti) ^(to  T)B{T)B^{T)^^{to T)dT (2.19) where ^{t  r)  denotes the state transition  matrix. Continuing  the proof of Theorem 2.6 we note that it can be shown that the input uiit)  = -B^(t)^^(toJ)vi (2.20) with 171  such that  Wdto  ^1)171  =  ^0 satisfies  L(wi to ti)  =  -XQ  or relation  (2.9). Thus 171(0 drives the state from  xo at time to to the origin at time ti  >  to (compare with Theo rem 2.2). As in Corollary 2.3 for the case of reachability it can be shown in an analogous manner that the system is (completely  state) controllable  at to if and only if there  exists ti  >  to so that Next we note that in view of the definitions  of  Wr and  Wc rank  Wdto  ti)  =  n. Wr(toti)  = ^(tiJoWc(toti)^^(tuto)-(2.21) (2.22) Since  0(ri ^o) is  nonsingular  for  every  to and  t\  rank  Wr(to ti)  =  rank Wdto  ti)  for every to and ti.  Therefore  the system  is reachable  if and  only  if it is controllable. • EXAMPLE2.3.  Consider the system  x lability Gramian is given by A{t)x  +  B{t)u  of Example  2.1. The control-Wc{toti) [e-'^0]dT  = 0 to (ti-to)e-^'o 0 0 0 Compare this with the reachability Gramian of Example 2.1 and note that 4>(ri to)) Wc(toti)<^^(tito) 0 Wr(to ti)  as expected  [see (2.22)]. (ti  -  to)e-^'^+'o)  0 0 ^ ^ ( ^ 1  ^ 0) = to) 'dti 0 234 Linear Systems Before proceeding we note that a relation similar to (2.17) can be derived using ^^^ Gramian  Wdto  ti) and (2.20) in place of Wr(to ti). In particular an appropriate input that transfers  the state from  XQ at to to xi  at ti is given by u(t)  =  -B^(t)^^(to t)W;\to ti)[xo  -  $ (% h)xil (2.23) We ask the reader to show  that this relation  can also be derived  from  (2.17) using (2.22). Additional  criteria for reachability and controllability First recall from  Chapter 2 the definition  of a set of linearly  independent  func tions  of  time  and  consider  in  particular  n  complex-valued  functions  fi(t\  i  = 1...  /i  where  f^{t)  G  C^.  Recall  that  the  set of  functions  fii  =  1... n is linearly  dependent  on a time interval  [ti ^2] over the field of complex numbers  C if there exist complex numbers at i  =  \..  .n  not all zero such that ^ i / i (0  +  • • *  +  ^nfn(t)  = 0 for all t in [ti ^2]; otherwise the set of functions  is said to be linearly  independent  on [ti ^2] over the field of complex  numbers. It  is possible  to test  linear  independence  using  the Gram  matrix  of the  func tions  fi. LEMMA 2.7.  Let F(t) G C^"^ be a matrix with fi(t) G C^^"" in its /th row. Define the Gram matrix of fi(t) i  = 1... ^ by W(tiJ2)  = F{t)F\t)dt (2.24) where ( • )* denotes the complex conjugate transpose. The set fi{t\  i  = I..  .nis  lin early independent on [ti ^2]  over the field of complex numbers if and only if the Gram matrix  W(ti ^2) is nonsingular  or equivalently  if and only if the Gram  determinant det W{ti t2)  7^ 0. Proof {Necessity) Assume the set fii  = I...  n is linearly independent but W{t\ ^2) is singular. Then there exists some nonzero a  ^  C^^"^ so that a W^(ri ti)  = 0 from which aW(tit2)a*  = l^[\aF(t))(aF(t)ydt  = 0. Since (aF(t))(aF(t)y  >  0 for all r this im plies that aF(t)  = 0 for all t in [^1 ^2] which is a contradiction. Therefore  W(ti ^2) is nonsingular. (Sufficiency)  Assume that  W(ti ^2) is nonsingular but the set fii  =  1... n is linearly dependent. Then there exists some nonzero a ^  C^^^ so that aF(t) = 0. Then aW(ti  ^2) = J/^ aF(t)F*(t)dt  = 0 which is a contradiction. Therefore the set fi i = 1... /t is linearly independent. • We will use the above result to derive additional tests for reachability  and con trollability  in this  section and for observability  and constructibility  in the next  sec tion. In the following  two theorems we repeat some earlier results for convenience. THEOREM 2.8.  The system x  = A(t)x + B(t)u is (completely state) reachable at ti (i) if and only if there exists finite ^0 <  ^1 such that rankWritoJi)  = n (2.25) j /^  <I>(ri T)5(T)B^(T)^^(ri T)(iT  the  reachability  Gramian  or \l'^(tuT)l )^l)  ^ where  Wr(toti)  - equivalently (ii) if and only if there exists finite ^o  < ^i such that the n rows of 235 ^{h  t)B{t) (2.26) are Hnearly independent on [t^ t\\ over the field of complex numbers. Proof. Part (i) was established in Corollary 2.3 while part (ii) is a direct consequence of the previous lemma and the definition of the reachability Gramian. • Similar results can be derived for controllability.  Specifically  we have the fol CHAPTER 3: Controllability Observability and Special Forms lowing  result. THEOREM 2.9.  The system x  = A(t)x + B(t)u is (completely state) controllable at to (i) if and only if there exist finite ti > to such that rank Wdto ti)  = n (2.27) where Wc(^ ti)  =  j^^^ 0(^0 T)5(T)5^(T)0^(^  T) (ir the controllability Gramian or equivalently (ii) if and only if there exists finite ti > to such that the n rows of are linearly independent on [^o ^i] over the field of complex numbers. Proof The proof is analogous to the proof for Theorem 2.8. 0(^ t)B(t) (2.28) • Notice that premultiplication  of $ ( ^  t)B(t)  by the nonsingular matrix 0(?i ^o) yields $(fi t)B(t)  [refer to (2.26) in the reachability theorem; compare with (2.22)]. This can be used to prove in an alternative way the result of Theorem 2.6 that reach ability  (controllability-from-the-origin)  implies  and  is  implied  by  controllability (-to-the-origin) in the case of continuous-time  systems (show this). B.  Continuous-Time  Time-Invariant  Systems We  shall  now apply  the results  developed  above  to  time-invariant  systems  x  = Ax  +  Bu  given  in  (2.3). In this  case  the state  transition  matrix  <tf(t r)  is explic itly  known  and is given  by ^(t  r)  =  ^^(^~'^) in (2.4). Because  of time  invariance the difference  t\  -  to =  T  rather than the individual  times  ^Q and ti  plays  an im portant role. Accordingly for the time-invariant case we can always take ^o "^ 0 and ti  =  T. This practice will be adopted in the following. The  definitions  of reachability  Definitions  2.1 to 2.3 and controllability  Def initions  2.4 to  2.6 are certainly  also  valid  in the time-invariant  case.  We repeat them here for convenience specializing them to the the system x  = Ax-\- Bu given in (2.3). DEFINITION  2.9.  (i)A State xi is reachable if there exists an input u{t) t E  [0 T] that transfers the state x{t) from the origin at r =  0 to ;ci in some finite time T. (ii)  The set of all reachable states Rr is the reachable subspace of the system i: = Ax  + Bu or of the pair (A B). (iii)  The system x  = Ax + Bu or the pair {A B) is {completely state) reachable if every state is reachable i.e. if Rr = R^. • DEFINITION 2.10.  (i)A State xo is controllable if there exists an input u(t) t G [0 T] that transfers the state x(t) from xo ^tt  = 0 to the origin in some finite time T. 236 Linear Systems (ii)  The  set  of  all  controllable  states  Re  is  the  controllable  sub space  of  the system i  =  Ax + Bu  or of the pair  (A 5). (iii)  The system i  = Ax+Bu or the pair (A 5) is {completely  state)  controllable • if every  state is controllable i.e. if Re = R^. DEFINITION  2.11.  The  n  X n  reachability  Gramian  of  the  time-invariant  system x=  Ax-\-Bu  is Wr{0 T)  ^ r  e^^-'^^BB^e^^-'^'^'dT. Jo (2.29) g Note  that  Wr is  symmetric  and  positive  semidefinite  for  every  T  >  0  i.e.  W^  = matrix (-from-the-origin) Wj  and  Wr  >  0  (shovv^ this).  Let  the  n  x  mn  controllability (or more  precisely  the  reachability  matrix)  be and  recall  that  ^  vv^as also  defined  in  Section  3.L ^ = [ 5  A 5  . . .  A ^ - ^ 5 ] (2.30) It  is  now  shovv^n  that  in  the  time-invariant  case  the  range  of  Wr(0 T)  denoted by  ^ ( W r ( 0 r ) )  is  independent  of  T  i.e.  it  is  the  same  for  any  finite  T{>  0) and  in  particular  it  is  equal  to  the  range  of  the  controllability  matrix  ^.  Thus  the reachable  subspace  Rr  of  a  system  is  given  by  the  range  of  ^^(^) or the  range  of Wr{0  T)^{Wr{0 r ) )  for  some  finite  (and  therefore  for  any) T>0. LEMMA  2.10.  ^ ( W  ( 0  r ))  = ^ ( ^)  for every  T  >  0. Proof  We  first  show  that  ^(Wr)  C  ^ ( ^)  for  some  T  >  0.  Let  xi  e  ^{Wr) for some  r  >  0.  In  view  of  Lemma  2.1  xi  G ^ ( L )  i.e.  there  exists  ui  such  that L(wi0r)  =  /Q {Qxp[(T — T)A]}Bui{T)dT  = xi.  Using  the  series  definition  exp[A^]  = Er=o(^  /^-M  '  -^1 ^^^  t)e  written  as xi  =  Yk=o'^^ f^{iT-Tf/kl)ui{T)dT\ or  in view of the Cay ley-Hamilton Theorem xi  =  2^~Q A^5ay^(r) where  ak{T)  is appropri ately defined.  This imphes thatxi  G ^ ( ^ ).  Since xi  is arbitrary ^{Wr)  C  ^ ( ^ ). We shall now show that ^ ( ^)  C^{Wr).LQtxi  G ^ ( ^ ) i.e. there exists 7] i  eR"""^ such  that  ^ r |i  =  xi.  Assume  that  xi  ^  ^(W^)  for  some  7  >  0.  We  shall  show  that this  leads  to  a  contradiction.  This  implies  that  the  null  space  of  Wr  is  nonempty. Wr  is  symmetric  and  so  the  range  of  Wr  is  the  orthogonal  complement  of  its  null space  (prove  this).  Thus  for  any  u  e  ^(Wr)  and  v G yK{Wr)u^v  =  0.  Also  we  may write  XI  =  x[  +x'/  with  x[  G ^{Wr)  and  x'l  G ^ ( W )  (x'l  ^  0  since  xi  ^ ^(Wr)). Then  there  exists  X2 G yl^{Wr)  such  that  X2x'( ^  0  which  implies  X2X1 7^ 0.  Next consider  xlWr{0T)x2  =  0  =  /o^[x^{exp[(r  -  T)A]}5][x^{exp[(r  -  T)A]}B]^dT  = JQ  \\xl{Qxp[{T  -  T)A]}B\\ldT  which  shows  that  x^exp[(r  -  T)A]B  =  0  for  every T G [0 r ].  Taking  derivatives  of  both  sides  with  respect  to  T and  evaluating  at  T =  T we  obtain x^B  =  -x^AB  =  ---  =  {-ifx^A^B =  0 for  every  ^  >  0.  Thus x^A^B  =  0 for  every  ^  >  0  and  therefore  ^2X1  =  X2^r\\  =  0  which  is  a  contradiction  since x^xi  y^ 0.  Therefore  xi  G  ^(W^)  which  implies  that ^ ( ^)  C ^(W^).  This  together with ^(Wr)  C ^ ( ^ )  shows that ^{Wr)  = ^ ( ^ ). • Lemma  2.10  shovv^s that  given  the  time-invariant  system  x  =  Ax-\-Bu if x(0)  = 0  then  the  set  of  all  states  that  can  be  reached  in  finite  time  i.e.  the  reachability the range of the controllability  matrix  or  equivalently subspace Rr is given by ^(^) by  ^ ( W r ( 0  r ) )  the  range  of  the  reachability  Gramian  vv^here  T  >  0  is  any finite time. EXAMPLE  2.4.  For the system i:  =  Ax  + Bu  v^iih A  = 0  1 0  0 andB  we have 1 t 0  1 and  e^^B  = . The  reachability  Gramian  is  ^^^(0 T)  = T -T 1 [T  -  Tl]dT  = (T  -rf T  -T T  -  7 1 dr  = .  Since  det  W(07)  = 237 CHAPTER 3: Controllability Observability and  Special Forms ^ r^  7^ 0  for  any  T  >  0  ra^/:  W(0 T)  =  ^  and  (A 5)  is  reachable.  Note  that % 12 and that ^(WM T))  =  S?l(^)  =  R^  as expected  (Lemma  2.10). [5 AB]  = 0  1 1  0 If  B  --1 0 instead  of then  ^  =  [B AB] 1  0 0  0 and  (A B)  is  not  reach-able. In this case e^^B = and the reachability matrix is Wr(0 T)  = \^ T  1  0 0  0 dr  = T 0 0  0 Notice again that =  ^(Wr(0  T))  for every  T  >  0. THEOREM  2.11.  Consider  the  system  x  =  Ax  -\-  Bu  and  let  x(0)  =  0.  There  exists input u that transfers the state to xi in finite time if and only if xi  G S/l(^) or equivalently if and only if xi  G ^(Wr(0  T)) for  some  finite  (and  therefore  for  any)  T.  Thus the reachable  subspace  Rr  =  S/l(^)  = ^(Wr{0  T)).  Furthermore an appropriate  u that will accomplish  this transfer  in time T is given by u(t)  =  ^ V ^ ^ - ^i (2.31) with r/i  such that Wr(0 T)r]i  = xi  and t G [0 T]. Proof. Apply Theorem 2.2 to the time-invariant case and then use Lemma 2.10. • Note that in  (2.31)  no restrictions  are imposed  on time  T other than  that  T  be finite.  T can be as small as we wish i.e. the transfer  can be accomplished  in a very short time indeed. COROLLARY 2.12.  The system x  = Ax + Bu or the pair (A 5) is (completely state) reachable if and only if or equivalently if and only if rank"^ = n rankWr(0T)  = n (2.32) (2.33) for some finite (and therefore for any) T. Proof Apply Corollary 2.3 to the time-invariant case and use Lemma 2.10. • THEOREM2.13.  There exists input u that transfers the state of the system x  =  Ax+Bu from XQio x\  in some finite time T if and only if or equivalently if and only if XX -  e^^XQ  G  m.{%) xi  -  e^^XQ G ^{WXO  T)). (2.34) (2.35) 238 Linear  Systems Such input is given by u(t)  =  B^e^^^^-'^i]i with t E  [0 r ] where r/i is a solution of Wrifi T)7]i  =  xi- e^^XQ. (2.36) (2.37) Proof  Apply Theorem 2.4 to the time-invariant  case and use Lemma 2.10. • The  above  leads  to the next  result  which  establishes  the importance  of  reacha bility  in determining  an input  u to transfer  the state  from  any  XQ to any xi  in  finite time. COROLLARY  2.14.  Let the system  i:  ^  Ax +  5M  be (completely  state) reachable or the pair (A B) be reachable. Then there exists an input that will transfer  any state  XQ to any other state x\  in some finite time T. Such input is given by u{t)  =  B^e^^^^-'^W;\0T)[xi -  e^^x^-] (2.38) for re  [0 r ]. Proof  This  result is the time-invariant  version  of Corollary  2.5. In view  of  Corollary 2.12 reachability implies that  ra^y^W;-(0r)  =  ^ for some T or that 2?l(W^(0 T))  = /?" the  whole  state  space. This  implies  that  any vector  xi  -  e^-^xo E  '3i(Wr(0 T))  that in view of Theorem 2.13 implies that the input in (2.38) is an input which will  accomplish this transfer. • There  are many  different  control  inputs  u that  can accomplish  the transfer  from xo  to  xi  in  time  T.  It  can be  shown  that  the input  u  given  by  (2.38)  accomplishes this transfer  while expending  a minimum  amount of energy; in fact  u minimizes the cost  functional  j^  ||W(T)|P J r  where  \\u(t)\\  =  [u^(t)u(t)y^^ denotes  the  Euclidean norm  of  u(t). EXAMPLE  2.5.  The system x  = Ax  + Bu with A 0  1 0  0 and  5 is reachable (see Example  2.4). A control input  u(t) that will transfer  any state  XQ to any other  state Xi in some finite time T is given by (see Corollary 2.14 and Example 2.4) u(t)  =  B^e'^'^^^-'^W;\0  T)[xi XQ] =  [T-t  1] 12 6 j2 J2 4 J Xi Xo EXAMPLE 2.6.  For the (scalar) system x  =  -ax  + bu determine u(t) that will transfer the state from  x(0) =  XQ to the origin in T sec; i.e. x{T)  = 0. We shall apply Corollary  2.14. The reachability  Gramian is Wr(0 T)  = -[1 [e' 2a ~^''^].  Note  [see (2.41) below]  that the controllability  Gramian is Wc(0 T) 1]. Now in view of (2.38) we have ^(0  =  be-^^-'^'' [-e-^'^x^] -2aT e'^XQ 2a FT e-^'^ -laT 2a b  1 2a b  ^2«r _ I 1 ^"'Xo. 239 CHAPTER  3: Controllability Observability and  Special Forms To verify  that  this  u{t) accompHshes  the desired  transfer  we compute  x{t)  =  ^^^xo + ''e^^'-^^Bu(T)dT  =  e-XQ  +  Jo e-'''e''^bu{r)dT  =  e'^\x^  +  JQ e'^'b X 2a 1 dr  = e 1  -^2at  1 XQ. Note that x{T)  =  0 as desired and also  that  x(0)  =  XQ. The above  expression  shovv^s  also  that  for  t  >  T  the  state  does not remain at the origin. An important point to notice here is that as T ^  0 the control magnitude \u\^  oo. Thus although it is (theoretically) possible to accomplish the desired transfer instantaneously this will require infinite control magnitude. In general the faster • the transfer  the larger the control magnitude required. We  shall novs^ establish  the relationship  between  reachability  and  controllability for  the continuous-time  time-invariant  systems  (2.3). Applying  (2.8) to the time-invariant  case  XQ is  controllable  v\^hen  there  exists u(t)  t  G  [0 Tl  so  that -e^^XQ  = or when  e^^xo  £  ^(Wr(0  T))  or equivalently  in view  of Lemma  2.1  when for  some  finite  T.  Recall  that  xi  is reachable  when e^^XQ  G  2/l(^) XX G  3l(^). We  require  the following  technical  result. (2.39) (2.40) LEMMA  2.15.  If X E ^{%)  then Ax  G S/l(^); i.e. the reachable subspace Rr  =  ^{%) is an A-invariant  subspace. Proof  If  X G S^(^)  this  means  that  there  exists  a  vector  a  such  that  [B  AB... A«-i5]a  -  X. Then Ax  =  [AB A^B...  A"5]a. In view of the Cay ley-Hamilton The orem A" can be expressed as a linear combination of A"~^ . . . A / which implies that Ax  = %fi for some appropriate vector  jS. Therefore Ax  G S^(^). • THEOREM  2.16.  Consider the system x  = Ax  + Bu. (i)  A state x is reachable if and only if it is controllable (ii)  Re  =  Rr. (iii)  The system (2.3) or the pair (A B) is (completely  state) reachable if and only if it is (completely  state)  controllable. Proof  (i)Letxbe reachable; that is xG2?l(^).Premultiplyxby^^^  =  X1=o(T^/kl)A^ and notice that in view  of Lemma  2.15 Ax A^x...  A^x  G 2?l(^). Therefore  e^^ x G 2/l(^) for any T which in view of (2.39) implies  that x is also controllable. If now x is controllable i.e. e^^x  G ^ ( ^ ) then premultiplying by e~^^ the vector e~^^ [e^^x)  = X will  also be in ^(%).  Therefore  x is also reachable.  Note that the second part of (i) that controllabihty implies reachability is true because the inverse {e^'^Y^  =  e~^^ does 240 Linear Systems exist. This is in contrast to the discrete-time case where the state transition matrix 4>(/c 0) ^^ nonsingular if and only if A is nonsingular  [nonreversibihty of time in discrete-time systems (see Section 2.7)]. Parts (ii) and (iii) of the theorem follow directly from (i). • The reachability  Gramian  for the time-invariant  case  Wr(0 T) was defined  in (2.29). Similarly in view of Definition  2.8 we make the following  definition. DEFINITION2.12.  The controllability Gramian in the time-invariant case is the  nXn matrix Wc(0T)^\ rT Jo e-^'BB^e-^^'dr. (2.41) • We note that which can be verified  directly  [see also (2.22)]. As  was  done  in  the  time-varying  case  above  we  now  introduce  a  number  of additional tests for reachability  and controllability  of time-invariant  systems.  Some earlier results are also repeated here for  convenience. THEOREM 2.17.  The system x  = Ax + Bu'\^ reachable  (controllable-from-the-origin) (i)  if and only if rank Wr(fi T)  = n for some finite T  > 0 rT where WM  T)  = e^^-''^^BB^ e^'^'^^^^ dr (2.42) the reachability Gramian or (ii)  if and only if the n rows of (2.43) are linearly independent on [0 ^)  over the field of complex numbers or alter natively if and only if the n rows of {sI-Ay^B are linearly independent over the field of complex numbers or (iii)  if and only if rank% = n where % =  [BAB...  A"~^B] the controllability matrix or (iv)  if and only if rank [sil -  AB]  = n (2.44) (2.45) (2.46) for all complex numbers st or alternatively for si i  =  \..  .n  the eigenval ues of A. Proof Parts (i) and (iii) were proved in Corollary 2.9. In part (ii) rank W^(0 T)  = n implies and is implied by the linear independence of the n rows of e^^'^^^B on [0 T] over the field of complex numbers in view of Lemma 2.7 or by the linear independence of the n rows of e^^B where t = T -t  on [0 T]. Therefore the system is reachable if and only if the n rows of e'^^B are linearly independent on [0 oo) over the field of complex numbers. Note that the time interval can be taken to be [0 ^) since in [0 7] T can be taken to be any finite positive real number. To prove the second part  of  (ii)  recall  that  ^{e^^B)  =  (si  — A)~^B  and  that  the  Laplace  transform  is  a one-to-one linear  operator. Part (iv) will be proved later in this chapter in Corollary 4.6. • Results  for  controllability  that  are  in  the  spirit  of  those  given  in  Theorem  2.17 can  also be  established.  The  reader  is  asked  to  do  so. This  is  of  course  not  surprising since  it  vv^as  shovv^n  (in  Theorem  2.16)  that  reachability  implies  and  is  implied  by controllability.  Therefore  the  criteria  developed  in  the  theorem  for  reachability  are typically  used  to test the  controllability  of  a  system. 241 CHAPTER  3: Controllability Observability and  Special Forms EXAMPLE  2.7.  For the  system x  = Ax-\- Bu  where  A 0 0 1 0 and  5: (as in Example 2.4) we shall verify  Theorem  2.17. The system is reachable  since (i)  the reachability Gramian Wr(07): for  any 7  >  0 or  since \T^ \T^-2^ has rankWr{0T) •• (ii)  e^^B  • has  rows  that  are  linearly  independent  on  [0oo)  over  the  field of  complex  numbers  (since  ai  -1 -\- a2 -  I  =  0  where  ai  and  a2 are  complex numbers  implies  that  ai  =  a2 =  0).  Similarly  the  rows  of  {si — A)~^B  = lA^l ^ are  linearly  independent  over  the  field  of  complex  numbers.  Also smce (iii)  rank ^  — rank  \B^AB\ — rank (iv)  rank  [5;7 —A5]  =  rank eigenvalues of A. If  5 in place of then 0 1 1 0 -1 Si 0 1 : n or 2  =  n  for  Si =  0i  =  12 the (i)  WriOJ) T 0 0 0 (see Example  2.4) with  rank Wr{0T)  =  I  <2  = n and (ii)  e^^B  • and  {sI-A)-^B • \/s 0   neither  of  which  has  rows  that  are linearly independent  over the complex numbers. Also (iii)  rank' 1 0 0 0 I  <2  = n and (iv)  rank  [sil — AB\  =  rank 1 -1 Si  OJ I  <2  = n for Si  = 0. Based on any of the above tests it is concluded that the system is not reachable. C.  Discrete-Time  Systems The  response  of  discrete-time  systems  was  studied  in  Section  2.7  of  Chapter  2.  We consider  systems  described  by  equations  of the  form 242 Linear Systems x(k  +  1)  -  A(k)x(k)  +  B(kMkl (2.47) ^j^^j^  ^^^^  ^  j^nxn^ 5(^)  ^  ^nxm^ ^j^^j ^^^ ^^^^^ ^^^^  ^  ^m ^^^ defined  for  it ^  ko. The state x(fe) for  k>  kois  given by k  ^  ko y t -l 4 ^)  =  ^(k  ko)x(ko)  +  ^ / = ^ ^(k i +  l)B{i)u{i) (2.48) where  the  state  transition  matrix  ^{k  ko)  is  given  by  ^(k  ko)  =  A(k  -  1) X A(k  -  2)''  'A(ko)  for  k  >  ko and O(^o. ko)  =  L In the time-invariant  case we have x{k+\)  =  Ax{k)  +  Bu{k\ k  ^  ko (2.49) where A  G /^'^X" and B  E  /?"><^. The state x(yfc) of (2.49) is given by (2.48)  with ^{k  ko)  =  A^-^' k  ^  ko. (2.50) Let the state at time  ko be  XQ. For the state at some time  ki  >  ko to assume the value xi  an input u must exist that  satisfies xi  =  3>(A:i ^o)^o  +  ^ ^(kh i +  l)B(i)u(i) (2.51) Reachability  and controllability  are defined  for discrete-time systems in a com pletely analogous fashion as in the continuous-time case. The mathematical develop ment however involves  summations  instead of integrals  and is easier to deal with. The  time-varying  case  can be  developed  in  a manner  similar  to the  time-invariant case.  For  this  reason  the  discrete-time  time-varying  case  will  not  be  developed presently. Instead we shall concentrate on the time-invariant case. Note that some of the results given below for the discrete-time time-invariant  case have already  been presented in Section 3.1 Introduction. Discrete-time time-invariant  systems For the  time-invariant  system  x(k  +  1)  == Ax(k)  +  Bu(k)  given  in  (2.49)  the elapsed time  fei-/co  is of central interest and we therefore take ^0  =  Oandfei  =  K. Recalling that ^{k  0)  =  A^ we rewrite (2.51) as when ^  >  0 or xi  =  A^xo  + ^A^-^'^^^Bu{i) /=o XX =  A^XO  +  '^KUK where and %K =  [B AB...  A^-^B] UK  =  [u^{K  -  \)  u^(K  -  2 )  . . .  u^(0)f. (2.52) (2.53) (2.54) (2.55) The definitions of reachable state xi  reachable sub space Rr and a system  being {completely  state)  reachable  or the pair  (A B)  being  reachable  are the same as in the continuous-time case (see Definition  2.9 and use integer K in place of real time T).  Similarly the definitions  of controllable  state  XQ controllable  subspace  Re and a system  being (completely  state)  controllable  or the pair  {A B) being  controllable are  similar  to  the  corresponding  concepts  given  in  Definition  2.10  for  the  case  of continuous-time  systems. To determine the finite input sequence for discrete-time systems that will accom plish  a desired  state transfer  if  such a sequence  exists one does not have to  define matrices comparable to the reachability  Gramian  Wr as in the case for  continuous-time systems. In particular we have the following  result. 243 CHAPTER 3: Controllability Observability and Special Forms THEOREM 2.18.  Consider the system x{k + V) = Ax{k) + Bu{k) given in (2.39) and let  x(0)  =  0. There  exists  input  u that  transfers  the  state  to  xi  in finite time  if  and only if xx E  ^{%). In this case xi  is reachable and Rr = S^(^). An appropriate input sequence {u{k)} k  = 0... w -  1 that accompHshes this transfer in n steps is determined by Un =  [u^(n  -l)u^(n  -  2)...  w^(0)]^ a solufion to the equation "^Un  = xi. (2.56) Henceforth with an abuse of language we will refer to Un as a control sequence when in fact we actually have in mind {u(k)}. Proof In view of (2.52) xi  can be reached from  the origin in K  steps if and only if xi  =  ^KUK  has a solution  UK  or if and only if  x\  E  ^(^K)-  Furthermore all input sequences that accomplish this are solutions to the equation x\  = ^RUK- For x\  to be reachable we must have xi  E ^(^K)  for  some finite K. This range however cannot increase beyond the range of ^„  = %  i.e.  ^("^K)  = S^(^n) for K  >  n. This  follows from the Cayley-Hamilton Theorem which implies that any vector x in ^(^A:) K  >  n can be expressed as a linear combination of B AB...  A^~^B. Therefore  x  E S?l(^„). It  is  of  course  possible  to  have  x\  E  ^{%K)  with  ^  <  n  for  a particular  x\\  how ever in this case xi  E S^(^„) since %K is a subset of ^„. Thus xi  is reachable if and only  if  it  is in  the range  of ^„  =  %. Clearly  any  t/„  that  accomplishes  the  transfer satisfies (2.56). • As pointed  out in the  above proof  for  given  x\  we may  have  x\  E  ^(^K) for some K  <  n.\n  this case the transfer can be accomplished in fewer than n steps and appropriate inputs are obtained by solving the equation ^KUK  =  ^i-COROLLARY 2.19.  The system x(k  +  1)  =  Ax(k)  + Bu(k) given in (2.49) is (com pletely state) reachable or the pair (A B) is reachable if and only if (2.51) Proof Apply Theorem 2.18 noting that S/l(^)  = Rr =  /?" if and only if rank ^  =  n. rank'^  = n. THEOREM  2.20.  There  exists  an  input  u  that  transfers  the  state  of  the  system x(k  +  1)  =  Ax(k)  +  Bu(k)  given  in  (2.49)  from  XQ to  x\  in  some finite number of steps K if and only if xi  -  A^xo  E m.C^Kl (2.58) u^(0)]^ is determined by solving Such input sequence UK  =  [u^(K -l)u^(K-2)... the equation Proof  The proof follows directly from (2.53). "^KUK  = xi-A^xo. (2.59) • 244 Linear  Systems The  above  theorem  leads  to the  following  result  that  establishes  the  importance of reachability  in determining  u to transfer  the  state from  any  XQ  to  any  xi  in  a  finite number  of  steps. COROLLARY2.21.  Let the sy Stem x(^+1)  =  Ax(^) + 5w(^) given in (2.49) be (com pletely  state) reachable  or the pair  (A B)  be reachable.  Then  there  exists  an input  se quence to transfer  the state from  any  XQ to any x\  in a finite number of steps. Such input is determined by solving Eq. (2.60). Proof  Consider (2.54). Since (A B) is reachable r<2n^^„  =  rank%  =  nandS/l(^)  = R^.  Then %Un  =  XX  - A " xo (2.60) always  has  a  solution  Un  =  [u^(n  -  1)...  w^(0)]^ for  any  XQ  and  xi.  This  input  se quence transfers  the state from  XQ  to xi  in n steps. • Note  that  in  view  of  Theorem  2.20 for  a particular  XQ and  xi  the  state  transfer may  be  accomplished  in  K  <  n  steps using  (2.59). EXAMPLE  2.8.  Consider  the  system  in  Example  1.1  namely  x(k  +  1)  =  Ax(k)  + 5M(^)whereA  = 0  1 1  1 and 5  = .SincQ rank^  =  rank[BAB]  =  rank 0  1 1  1 2  =  n  the  system  is  reachable  and  any  state  XQ  can  be  transferred  to  any  other  state xi  in  2  steps.  Let  xi  = 1  1 1  2 or u(l) u(0) Xo  = -1  1 1  0 .  Then  (2.60)  implies  that 0  1 1  1 u(l) u(0) 0  1 1  1 ao bo} b-l-bo a-  ao -  bo This  agrees with the results obtained in Example  1.1. In view of (2.59) if Xi and XQ are chosen so that xi  -  Axo 0  1 1  1 a-  bo b-  ao-  bo. is in the 2/1(^0  -  ^(B)  =  span then  the  state  transfer  can  be  achieved  in  one  step.  For  example  if  xi  = and thenBu(0)  = Xo xi  can be accomplished  in this case in  1 <  2  =  fz steps with  u(0)  =  2. u(0)  =  xi  -  Axo  = implies that the transfer  from  xo to EXAMPLE 2.9.  Consider the system x(k  +  1)  =  Ax(k)  + Bu(k)  with A 0  1 0  0 and B  = . Since ^  =  [B AB] 0  1 1  0 has full rank there exists an input sequence that will transfer the state from  any x{0)  =  xo to any x(n)  =  xi  (in n steps) given by (2.60) U2  = U{1) Lw(0)J \xi -  A^xo)  = 0  1 1  0 (xi  -  Xo). Compare  this with Example 2.5 where the continuous-time  system had the same system parameters A and B. • We  shall now  establish  the relationship  between  reachability  and  controllability for  the  discrete-time  time-invariant  systems  x(k  +  1)  =  Ax(k)  +  Bu(k)  given  in (2.49). Consider  (2.51).  The  state  XQ is  controllable  if  it  can  be  steered  to  the  origin xi  =  0 in  a  finite  number  of  steps  K.  That  is  XQ is  controllable  if  and  only  if A^xo  =  %KUK (2.61) for some K  or when for  some K. Recall that x\  is reachable  when (2.62) (2.63) 245 CHAPTER  3: Controllability Observability and Special Forms THEOREM 2.22.  Consider the system x{k +  1)  =  Ax{k) + Bu{k) given in (2.49). (i)  If state X is reachable then it is controllable (ii)  RrdRc-(iii)  If the system is (completely  state) reachable or the pair (A B) is reachable then the system  is also  (completely  state)  controllable  or the pair  (A B) is controllable. Furthermore if A is nonsingular then relations (i) and (iii) become if and only if statements since controllability  also implies reachability and relation (ii) becomes an equality i.e. Re = Rr. Proof (i) If X is reachable then  x  G S/l(^).  In view  of Lemma  2.14 S/l(^) is  an A-invariant subspace and so A"x G S/l(^) which in view of (2.61) implies that x is also controllable. Since x is an arbitrary  vector in Rr this implies  (ii). If S^(^)  =  /?" the whole state space then A^x for any x is in S/l(^) and so any vector x is also controllable. Thus reachability implies controllability. Now if A is nonsingular then A~" exists. If x is controllable i.e. A^x  G 2?l(^) then x  G ^(^) i.e. x is also reachable. This can be seen by noting that A~" can be written as a power series in terms of A which in view of Lemma 2.15 implies that A~"(A"jc)  =  ;c is also in ^(%). • Matrix A being nonsingular is the necessary and sufficient  condition for the state transition  matrix  0(/: ^o) to be  nonsingular  (see  Section  2.8)  which  in  turn  is  the condition for  "time  reversibility'' in discrete-time  systems. Recall that reversibility in time may not be present in such systems since 0(^  ^o) may be singular. In contrast to this in continuous-time systems ^(t  to) is always nonsingular. This causes  differ ences  in behavior  between  continuous-  and  discrete-time  systems  and  implies  that in  discrete-time  systems  controllability  may  not  imply  reachability  (see  Theorem 2.22). Note that in view  of Theorem  2.16 in the case of continuous-time  systems it is not only reachability  which  always  implies  controllability  but  also vice  versa controllability  always implies  reachability. In  the  following  we  introduce  the  discrete-time  reachability  and  controllabil ity  Gramians  for  system  (2.49).  These  are  defined  in  a  manner  analogous  to  the continuous-time  case. DEFINITION2.13.  The reachability Gramian is defined by K-l Wr(0 K)  = ^  A^-^^-'^^BB^(A^f-^'^^\ /=o (2.64) It is not difficult  to verify that W(0 K)  = Xf~o  A'BB^{A^j  = "^K^- • LEMMA 2.23.  ^{%)  = ^(Wr(0  K)) for every K  ^  n. Proof This result can be established in a way similar to the proof of the corresponding result in the continuous-time case (Lemma 2.7). The details are left to the reader. • When  a  system  is reachable  the  input  sequence  that  transfers  XQ  aX  k  =  0 to x\a.tk  =  K can be determined in terms of the reachability Gramian. In particular let 246 Linear  Systems rank  %  = n = rank  Wr{^  K) for K > n and  notice  that  the  relation UK  =  '^KW;\0  K)(xi  -  A^xo) (2.65) satisfies  (2.59)  since  W(0 K) =  ^ / ^ ^ J. DEFINITION2.14.  The controllability  Gramian  is defined  as Wc{^ K)  = ^  A'^'^^^BB^(A^y^'^^\ (2.66) We  note  that  WdO  K) is  well  defined  only  when A is  nonsingular.  The  reacha bility  and  controllability  Gramians  are related  by Wr{0K)  =  A^WMK)(A^f (2.67) as  can  easily  be  verified. When A  is  nonsingular  the input  that  will  transfer  the  state from  XQ at  /:  =  0 to xi  = Oinn  steps  can  be  determined  using  (2.60).  In  particular  one  needs  to  solve [A-^'^Wn = [A-''B...A-^B]Un  = xo (2.68) for  Un  =  [u^(n-1)... g/l(^)  or if and  only if XQ G  gi(A~"^)  for A  nonsingular. w^(0)]^. Note that  XQ is controllable if and only if  -A^XQ  G Clearly  in  the  case  of  controllability  (and  under  the  assumption  that A is  non-singular) the matrix  A~^%  is of interest  instead  of ^  [see  also  (1.18)]. In  particular a  system  is  controllable if and  only if ranfc (A~'^^)  =  rank%  = n. EXAMPLE  2.10.  Consider the system x(k  + 1)  = Ax(k)  + Bu(k)  where A = 1  1 0  1 and 5 . Since ra^^^ =  rank[B  AB]  = rank 1  1 0  0 =  \ <2  = n this  system is not (completely)  reachable  (controllable-from-the-origin).  All reachable  states  are of the form a  where a E R  since is a basis for the 2?l(^)  =  Rf  the  reachability subspace. The reachability Gramian for i^  = n = 2isWX0 2)  = BB^  +  (AB)(AB) T  _ 1  0" 0  0 ^(Wr(0  2)) which verifies  Lemma 2.23. Note  that a basis  for  ^(Wr(0  2)) is '2  0' 0  0 1  0 0  0 and  S?l(^) = + In view of (2.62) and the Cay ley-Hamilton  Theorem  all controllable  states  XQ  sat isfy  A^xo E 9l(^);  i.e.  all controllable  states  are of the form  <^ L   where a  G R.  This verifies  Theorem 2.22 for the case when A is nonsingular. Note that presently  Rr  =  Re EXAMPLE  2.11.  Consider the system  x{k  + 1)  = Ax(k)  + Bu{k)  where A 0  1 0  0 and 5 SincQ rank ^  = rank[BAB]  =  rank 1  0 0  0 =  1 < 2 == ^  the  system is not  (completely)  reachable.  All reachable  states  are  of the form a where a E: R smce is a basis for ^(^)  = Ry the reachability  subspace. To determine the controllable subspace R^  consider (2.62) for K  = n/m  view of the Cay ley-Hamilton Theorem. Note that A~^%  cannot be used in the present case since A is singular. Since A^XQ  = Xo G S/l(^) any state XQ will be a controllable state i.e. the system is (completely) controllable and Re = R^. This verifies Theorem 2.22 and illustrates that controllability does not in general imply reachability. Note that (2.60) can be used to determine the control sequence that will drive any state xo to the origin (xi  =  0). In particular 247 CHAPTERS: Controllability Observability and Special Forms ^f/„  = u(l) u(0) = -A^XQ. Therefore  w(0)  -  a  and w(l)  =  0 where a  E  /? will drive any state to the origin. To verify this we consider x(l) Ax(0) + Bu(0) x(2)  = Ax(l) + Bu(l)  --X02  + Oi 0 Xoi •^02. 0 0  < 0  = X02  +  ex 0 and 3.3 OBSERVABILITY  AND  CONSTRUCTIBILITY In applications the  state of a system is frequently  required  but not accessible.  Un der such conditions the question  arises whether it is possible to determine the state by  observing  the  response  of  a  system  to  some  input  over  some  period  of  time.  It turns  out that the answer to this question  is affirmative  if the  system is  observable. Observability  refers to the ability of determining the present state x(to) from knowl edge  of  future  system  outputs  y(t)  and  system  inputs  u(t) t  ^  to.  Constructibil-ity  refers  to  the  ability  of  determining  the  present  state  x(fo)  from  knowledge  of past  system outputs y{t)  and  system inputs  u{t) t  ^  to. Observability  was  briefly addressed  in  Section  3.1. In  this  section  this  concept  is  formally  defined  and  the (present)  state is explicitly  determined  from  input  and output measurements. As in Section 3.2 (dealing with reachability and controllability) the reader can concentrate on the time-invariant  case  starting  with  Definition  3.9 if  so desired  and  omit  the more  general  material  (dealing  with  time-varying  systems)  that  starts  with  Defini tion 3.1. Section  description In this  section  observability  and  constructibility  are introduced  and  discussed in detail  for  given  linear  system  state-space  descriptions. This  is  accomplished  for continuous  and discrete-time  systems  and for both time-varying  and  time-invariant cases. Observability  in  continuous-time  systems  is  addressed  first  with  introduction of the observability  Gramian  Wo(to ti)  (Definition  3.4). It is shown  (Corollary  3.2) that  a  system  is  observable  at  to if  Wo(to ti)  has  full  rank  for  some  ti  >  to and furthermore  if the system is observable how an initial state can be determined. Ob servability refers to the ability to determine the current state of a system from  future system outputs (and inputs). Constructibility which refers to the ability of determin ing the current state of a system from past system outputs (and inputs) is  addressed next with the introduction of the constructibility Gramian (Definition 3.8). It is shown 248 Linear Systems (Theorem  3.3)  that  a continuous-time  system  is observable  if  and  only  if  it is  con-structible.  Next  additional  tests  for  observability  and  constmctibility  are  obtained (Theorem 3.4). All these results are then applied in the study of the  continuous-time time-invariant  case. The material is arranged in such a manner that the continuous-time time-invariant  systems can be studied independently  of the time-varying  case. The relation between the observability  Gramian  Wo(0 T)  and the observability  ma trix  0  =  [C^ {CAf... (CA""^)^]^  is  estabhshed  next  (Lemma  3.6).  It  is  then shown  (Corollary  3.8)  that  a system  is  observable  if  and  only  if  0  has  full  rank  n. Next the relation between observability and constmctibility is established  (Theorem 3.9). Finally additional tests for observability  are derived  (Theorem  3.10). A discussion of observability and constmctibility for discrete-time systems with particular  emphasis  on time-invariant  systems  is presented  next.  It is  shown  that  a system is observable if the observability matrix 0 has full rank (Corollary 3.12) and for this case an expression for the initial state XQ is given as a function  of future  out puts  (and  inputs).  A  similar  result  involving  the  observability  Gramian  Wo{0 K) in  place  of  0  is  also  established  (Corollary  3.14).  Next  it  is  shown  that  observ ability  in  discrete-time  systems  always  implies  constmctibility.  In  contrast  to  the continuous-time case the converse of the above statement is generally not true. This is due to the lack of time reversibility in difference  equations. When A is nonsingular then constmctibility  also implies observability. Finally the constmctibility  Gramian Wcnd^y K)  is also introduced  (Definition  3.16). A.  Continuous-Time  Time-Varying  Systems We consider  systems described by equations of the  form X =  A(t)x  +  B(t)u (3.1) where  A{t)  G  iR^^^ B{t)  G  R''''^  C{t)  E  7?^><^ D{t)  G  /?^><^ and  u{t)  G  R^  are defined  and  (piecewise)  continuous  on some real open interval  {a b). It was  shown in Chapter 2 that the output y{t)  is given by y  =  C{t)x  -H  D{t)u y{t)  =  C(t)^(t  to)x(to)  + C(t)<i^(t  T)B{T)U(T)  dr  +  D(t)u(t) (3.2) t Jto for  to t  G {a h) where ^{t  r)  denotes the state transition matrix. This can be writ ten as y{t)  -  C{tmu to)xo (3.3) where j (0  =  y(t) ^  C(t)<i>(t r)B(r)u(T)dT  +  D{t)u(t) andxo  =  x(ro)-Wewill find it convenient to first give the definition  of an unobservable  state. DEFINITIONS.1.  A State X is unobservable at time to if the zero-input response of the system is zero for every t >  to i.e. if C(t)(^(t to)x = 0 for every t >  to. DEFINITION 3.2.  The unobservable at to subspace R^^  of (3.1) is R^^ = { set of all unobservable at ^o states x}. (3.4) • • When  the context  is clear  and there is no ambiguity  we will write  Ro in place ofR'l 249 CHAPTER 3: Controllability Observability and  Special Forms DEFINITION  3.3.  The  system  (3.1)  is  {completely  state)  observable  at ^  or the  pair (A(0  C{t))  is observable  at ^  if the only  state  x  G  /?" that  is unobservable  at fo is the • zero state jc  =  0 i.e. if  R^^  =  {0}. We  will  show  later  that  observability  depends  only  on  the  pair  (A(t)  C(t)).  Ac cording  to  Definition  3.1  a  nonzero  unobservable  state  x  cannot  be  distinguished from  the  zero  state  if  only  the  (future)  outputs  are  known;  that  is  an  unobservable state  cannot  be  determined  uniquely  from  knowledge  of  the  inputs  and  outputs  of the  system.  This  can  be  seen  from  (3.3)  where  for  the  unobservable  states  x  at ^0. y(t)  =  0  for  r  ^ to.  This  implies  that  the  states  x  which  together  with  the  in put  u(t)  produce  the  output  y(t)  cannot  be  distinguished  from  the  zero  state  since they  both  produce  the  same  output. In  a  manner  analogous  to  the  development  of  reachability  in  Section  3.2  we make  the  following  definition. DEFINITION  3.4.  The observability  Gramian  of the system (3.1) is the n X ri matrix Wo{totx)= <D^(T^)C^(T)C(T)CD(T^O)^T. (3.5) Note that  Wo is symmetric and positive semidefinite  for every ti  >  to i.e. Wo  = Wj  and  Wo^ 0  (show  this). THEOREM  3.1.  A State X is unobservable  at to if and only if xGX(Wo(to.ti)) (3.6) for every  t\  >  ^o where J{(-) denotes the null space of a map. Proof.  If  X is  unobservable  then  (3.4)  is  satisfied.  Postmultiply  (3.5)  by  x  to  obtain Wo(to ti)x  =  0 for  every  ti  >  to i.e.  x  G }((Wo(to  ti))  for  every  ^i  >  to. Conversely let X be in the null space of  Wo. Then  x'^Wo(to ti)x  =  |^^^ ||C(T)^(T  ro)x|p dr  =  0  for every ti  >  to. This implies that (3.4) is true or that the state x is unobservable. • EXAMPLE  3.1.  Consider the system i:  =  A(t)x  y  =  C(0^ where A(0  = -1  e^^ 0 -1 and  C(t)  =  [0 e~^].  The  state transition  matrix  in this  case is  (see Example  2.1 in  this and  C(0 chapter) 0(r r)  = ^-(t-r) 0 i^^t+T  _  ^-r+Sr-) e Then  C(T)^(T to)  =  [0 e  ^^+^0] and the observability  Gramian is given by WoitoJi)  = ^2^0 0  - 2T [0e~^']dT 0 0 0  ^-4^ dr to 0 0 0 ^-4/1  _ ^-4to It is clear that this system is not observable since rank Woito ti)  =  I  <2  =  n.ln  view of Theorem 3.1 all unobservable  states are given by  where a  E  R. 0 Notice  that  };(0  =  C(t)^(tto)xo  =  [0 e-^^^'o]xo  =  0 for  JCQ  =  that  is none of the (unobservable)  states can be distinguished  from  the zero state. 250 Linear Systems Clearly  x  is  observable  at  t^  if  and  only  if  there  exists  a  fi  >  ^o  such  that ^ovo  ti)X  ¥"  0. COROLLARY 3.2.  The system (3.1) is {completely state) observable at to or the pair (A(t) C(t)) is observable at to if and only if there exists a finite ti  > to such that If the system is observable the state xo at to is given by rankWoitoJi)  =  n. Xo =  W^\toti) fh ^0 C D ^ ( T  ^ O ) C ^ ( T ) K T ) ^T (3.7) (3.8) Proof The system is observable if and only if the only vector x that satisfies (3.6) is the zero vector. This is true if and only if there exists (at least one) finite time ti for which the null space of Wo(to ti) contains only the zero vector or if and only if (3.7) is satisfied. To determine the state XQ  at to given the output and input values over [^o. ^i] premultiply (3.3) by (^^(t to)C'^(t) and integrate over [to ti]. Then in view of (3.5) Woito h)xo  =  r  CD^(T fo)C^(T)KT) Jr. When the system is observable (3.9) has the unique solution (3.8). (3.9) • It  is  clear  that  if  the  state  at  some  time  to  is  found  then  the  state  x(t)  for r ^  ^0 is  easily  determined  given  u(t) t  >  to via  the  variation  of constants  form ula (2.2). We mention  here  that  alternative  methods  to  (3.8)  to  determine  the  state  of  a system  when the system is observable  are given in the next chapter  (in Section  4.3 on state estimation). EXAMPLE  3.2  For  the  scalar  system  x  =  a{t)xy  =  c{t)x  where  a{t)  =  -1  and c{t) =  e\  we have <b{t r)  =  ^"(^~^> and C{T)^(J  to) =  e^e'^'"'^^^  =  e^^.  The observ ability  Gramian  in this case is  Woito ^i)  =  //^ e'^^^ dr  =  e'^^^it\  -  to) which implies that  the  system  is  observable  at  ^o since  rankWoito t\)  =  1  =  w for  any  t\  y^  to. Suppose  now  that  the  observed  output  is  y(t)  =  y(t)  =  ae^^  fort  >  to. Then  xo can  be  determined  using  (3.8)  Xo =  [e-^'^/(ti  -  to)][\l'e\ae')dT]  =  a.  Indeed y(t)  = c(t)^(t  to)a = ae^o as observed. • Observability  utilizes  future  output  measurements  to  determine  the  present state. In contructibility past output measurements  are used to accomplish this. Con-structibility  is defined  below  and its relation to observability  is  established. DEFINITION 3.5.  A State X is unconstructible at time t\ if for every finite time t ^  t\ the zero-input response of the system is zero for all t i.e. C{t)^{t ti)x  =  0 for every t <  ^i. (3.10) DEFINITION3.6.  The unconstructible at t\ sub space S^^ of (3.1) is (3^^ = {set of all states x unconstructible at ^i}. • It is denoted  in the following  by  R^  for  convenience  when  there is no ambi guity. 251 CHAPTER 3: Controllability Observability and  Special Forms DEFINITION3.7.  The system (3.1) is {completely  state)  constructible  at t\  or the pair (A(0 C{t))  is constructible  at ti  if the only  state  x  G  R"  that is unconstructible  at to is • X =  0i.e.if/?^  =  {0}. THEOREM  3.3.  If the system  (3.1) or the pair  (A(0 C(0) is observable  at ^  then  it is constructible  at some ti  >  ^;  if it is constructible  at ti  then it is observable  at  some to^ ti. It  was  shown  in  Corollary  3.2  that  the  system  is  observable  at  ^o if  and  only  if rank  Wo(to t\)  =  n for some ti  >  to. Similar results can be established for constructibil-ity. We will require the following  concept. DEFINITION  3.8.  The constructibUity  Gramian  of (3.1) is the n X n matrix Wcn{totl) ^^(r  ti)C^{T)C{T)^{T tOdr. (3.11) Proof  of  Theorem  3.3.  A similar result as Theorem 3.1 but for unconstructible  state  x can be derived. Next using a proof similar to the proof of Corollary 3.2 it can be shown that the system is constructible  at ti  if and only if there exists finite to <  ti  such that Note that rank  Wcn(to> ti)  =  n. WoitoJl) = ^\tiJoWcn(to>tO<^(thtol (3.12) (3.13) which  implies  that  rank  Wo(to ti)  =  rank  Wcn(to h)  for  every  to  and  ^i.  Note  that • ^(ti  to) is nonsingular for every to and ti. EXAMPLE  3.3.  (i)  Consider  the  system  x  =  A{t)x  y constructibility  Gramian is C{t)x  of  Example  3.1. The Wcnito h)  = 0 0 0 [0e  -2T + t] ]dT 4 Compare this with the observabiUty  Gramian of Example  3.1 and note that ^HhJoWcn(toh)^ituto) = -\e^'' 0 Wo(totil as expected  [see (3.13)]. Presently C{t)^{tti)x  =  [0e -2t+t^i 0 for every t  ^  ti  implies in view of Definition  3.5 that all unreconstructible  states (at ^i) are of the form Example  3.1).  a  G /?. Note that they  are identical  to the unobservable  states  (see (ii)  For  the  scalar  system  x  =  -xy  =  e^ of  Example  3.2  the  constructibility Gramian is Wcn(to h)  =  e^^^iti -  to) and ^^(^i to)Wcn(to ^1)^(^1 to)  =  ^-(^1-^0)^2^1  >< (ti  -  ro)e~(^i~^o)  =  e^^^iti -  ^o)  =  Wo(to ti)  as expected in view  of (3.13). • We shall now use Lemma 2.7 in Section 3.2 to develop additional tests for ob servability and constructibility. These are analogous to corresponding results that we established for reachability and controllability (Theorems 2.8 and 2.9). 252 Lh^^Systems THEOREM 3.4.  The system i  =  A(t)x+B(t)u y  = C(0-^+/)(Ow is (completely state) observable at ^o (i)  if and only if there exists a finite ti > to such that rankWo(toti)  = n (3.14) where  Wo{tQt\) =  \^^^^ <^^(Tto)C'^(T)C(T)^(TJo)dT  is  the  observability Gramian or equivalently (ii)  if and only if there exists finite ti > to such that the n columns of C(t)^(tJo) (3.15) are linearly independent on [to ti] over the field of complex numbers. Proof. Part (i) was shown in Corollary 3.2 and part (ii) is a consequence of Lemma 2.7 (compare with the corresponding Theorem 2.8 for reachability). • Similar  results  can be derived  for  constructibility.  In particular  we have the following  result. THEOREM  3.5.  The system  x  = A(t)x + B(t)u y  = C(t)x + D(t)u is (completely state) constructible at ti (i)  if and only if there exists finite to  < t\ such that rankWcn{tot\) = n (3.16) where  WcnitoJi)  =  ^^^^ ^^(rJi)C'^(r)C(r)^(Tti)dT Gramian or equivalently the  constructibility (ii)  if and only if there exists finite to  < ti such that the n columns of are linearly independent on [to ti] over the field of complex numbers. Proof The proof is analogous to the proof of the corresponding results on observability. C(tmtti) (3.17) • Note that postmultiplication  of C(0^(^ ^i) by the nonsingular  matrix  ^(ti  to) yields  C(t)^(t  to) in (ii) of Theorem  3.4 [compare  with  (3.13)]. This  shows  again the  result  given  in Theorem  3.3 that  observability  implies  and is implied  by con structibility in the case of continuous-time  systems. B.  Continuous-Time  Time-Invariant  Systems We shall now study observability and constructibility for time-invariant systems de scribed by equations of the  form x  =  Ax-^Bu y  =  Cx  + Du (3.18) where A  G /^^^^^ B  E  T^^^^ C G /?^><^ D  G 7^^><^ and u(t) G R"^ is  (piecewise) continuous. As was shown in Chapter 2 the output of this system is given by y(t)  =  Ce'^'xiO) +  Ce^^'-'^Bu{7)dT  + Du{t). rt 0 (3.19) We  recall  once  more  that  in  the  present  case  <I>(r r)  =  ^(t  -  r 0)  = exp [A{t -  T)]  and that initial  time can always be taken to be to  =  0. We will find 253 CHAPTER  3: Controllability Observability and  Special Forms it convenient  to rewrite  (3.19)  as m : C / % (3.20) whcvcy{t)  =  y{t)- \J^Ce^^'-^^Bu{T)dT^Du{t) andxo  =x(0). DEFINITION  3.9.  A state  x is unobservable  if the zero-input response  of the  system (3.18) is zero for every ^ >  0 i.e. if -- 0 for every ^ >  0. (3.21) The  set  of  all  unobservable  states  xR^  is  called  the  unobservable  subspace  of (3.18).  System  (3.18) is  (completely  state)  observable  or the pair  (AC)  is  observable if the only  state x  e  R^  that is unobservable is x =  0 i.e. if R^  = {0}. Definition  3.9  states  that  a  state  is unobservable  precisely  when  it  cannot  be  dis tinguished  as  an initial  condition  at time  0 from  the  initial  condition x(0)  =  0.  This  is because in this case the output is the same as if the initial condition were the zero vector. DEFINITION  3.10.  The observability  Gramian  of  system (3.18) is \hQnxn  matrix Wo(0 T)^ Vo{()J)= [  e""^ ^C  Ce^^dT. I e^^'C^Ce^'dT. Jo (3.22) We  note  that  Wo  is  symmetric  and  positive  semidefinite  for  every  T  >  0  i.e. ^o  =  ^J  and  Wo>0 (show  this). Recall  that  the pnxn observability  matrix ^: C CA CA n-\ (3.23) was  defined  in  Section  3.1. We  now  show  that  the  null  space  of  ^ ^ ( 0  7 )  denoted  by  ^ ( ^ ^ ( 0  7 ) ) is independent  of  T  i.e.  it  is  the  same  for  any  T  >  0  and  in  particular  it  is  equal to  the  null  space  of  the  observability  matrix  0'.  Thus  the  unobservable  subspace  RQ of  the  system  is  given  by  the  null  space  of  ^  ^ ( ^ )  or  the  null  space  of  Wo(0 T) ^ ( W o ( 0  r ))  for  some  finite  (and  therefore  for  all)  T  >  0. LEMMA3.6.  J/{0)  = ^(Wo(0T)) for every  7  >  0. Proof  If  X  G ^ ( ^ )  then  ^x  =  0.  Thus  CA^x  =  0  for  all  0  <  ^  <  n -  1  which  is also true for  every ^ >  n —  1 in view  of the Cay ley-Hamilton  Theorem.  Then Ce^^x  = C[E^=o(^^/^-Mi-^ =  0 for every finite t. Therefore  in view of  (3.22) ^^^(0 T)x  = 0  for every 7  >  0 i.e. x G yK{Wo{0 T))  for every 7  >  0. Now letx  G yK{Wo{0 T))  for  some r  >  0  so that x^ W{0  T)x  = f^ ^0  II Ce^^x  f  dT  =  0  or Ce^'x  =  0 for  every  t  G [0 T]. Taking  derivatives  of  the  last  equation  with  respect  to  t  and  evaluating  at  ^ =  0  we obtain  Cx  =  CAx  =  • • • =  CA^x  =  0  for  every  ^  >  0.  Therefore  CA^x  =  0  for  every ^ > 0  i.e.  ^x  =  O o r x G ^ ( ^ ). • THEOREM  3.7.  A State X is unobservable if and only if or equivalently if and only if X G ^ ( ^ ) xe^(Wo{0T)) (3.24) (3.25) 254 L h ^ S y s t e ms for  some  finite  (and therefore  for  all) T  > 0.  Thus  the unobservable  subspace  Ro = ^ ( ^)  = ^ ( ^ o ( 0 T))  for some T  >0. Proof  If X is unobservable  (3.21) is satisfied.  Taking  derivatives  with respect to t and evaluating  at ^ =  0 we obtain  Cx = CAx =  ••• = CA^x =  0 for ^ >  0 or CA^x = 0 for every  ^ >  0. Therefore  ^x  = 0 and (3.24) is satisfied.  Assume  now that  ^x  = 0 i.e. CA^x =  O f o r O < ^ < n — 1  which is also true for every ^ > n — 1 in view of the Cay ley-Hamilton  Theorem.  Then Ce^^x = C[Y^^Q{t^/k\)A']x  = 0 for every  finite t i.e. (3.21) is  satisfied  and x is unobservable.  Therefore  x is unobservable  if and only  if  (3.24) is satisfied.  In view of Lemma 3.6 (3.25) follows. • Clearly x is observable  if and only  if  ^x  7^ 0 or ^ ^ ( 0 T)x  ^  0 for  some  T  >  0. COROLLARY  3.8.  The  system  (3.19)  is  (completely  state)  observable  or the  pair (A C) is observable if and only if or equivalently if and only if rank  ^  = n rankWo{0T)=n (3.26) (3.27) for  some  finite (and therefore  for all) 7  >  0. If the system is observable the state XQ  at ^ =  0 is given by xo =  W-\OJ) Jo (3.28) Proof  The system  is observable  if  and only  if the only  vector  that  satisfies  (3.20) or (3.21) is the zero  vector.  This is true if and only  if the null  space is empty  i.e. if and only if (3.26) or (3.27) are true. To determine the state XQ at ^ =  0 given the output and input  values  over  some  interval  [07]  we premultiply  (3.20) by e^  ^C^  and integrate over  [0 T] to obtain Wo{0T)xo= /''C^y{T)dT [ Jo (3.29) in view of (3.22). When the system is observable (3.29) has the unique solution (3.28). • Note  that  T  >  0 the time  span  over  which  the input  and output  are observed  is arbitrary.  Intuitively  one would  expect  in  practice  to  have  difficulties  in  evaluating XQ accurately  when  T  is small  using  any numerical  method.  Note  that for very  small r  |Wo(0r)|  can be  very  small  which  can  lead  to  numerical  difficulties  in  solving (3.29).  Compare  this  with  the  analogous  case  for  reachability  where  small  T  leads in  general  to large  values  in control  action. It is clear  that if the state  at some  time t^ is determined  then  the state x{t)  at any subsequent  time is easily  determined  given  w(f) f >  fo via the variation  of  constants formula  (3.2) where  0(f  T)  =  exp[A(f  -  T)]. Alternative  methods  to  (3.29)  to  determine  the  state  of  the  system  when  the system  is observable  are provided  in the next  chapter  in Section 4.3. EXAMPLE  3.4.  (i)  Consider  the  system  x  = Axy  =  Cx  where  A 0 0 1 0 and C = [ 1  0 ].  Here  e^' 1 0 t 1 and  Ce^^ =  [lt].  The  observability  Gramian  is  then 255 CHAPTER 3: Controllability Observability and  Special Forms Wo(0T)  =  \n \[lT]dT  =  \^ 1  T T T dr \rp2 3^ NoticQih2itdetWo(0T) j^r^  7^ 0  for  any  T  >  0  i.e.  ra^^  W^(0 T)  =  2  =  fz  for  any  T  >  0  and  there fore  (Corollary  3.8)  the  system  is  observable.  Alternatively  note  that  the  observabil ity  matrix  0  = M(Wo(0  T))  = C CA 0 0 1  0 0  1 and  rank€  =  2  =  n.  Clearly  in  this  case  J{(€)  which verifies  Lemma  3.6. (ii)If  A  = 0  1 0  0  as before but  C  =  [0 1] in place of  [1  0] then  Ce^^  =  [0 1] and  the  observability  Gramian  is  WoiO T)  = [01] JT -0  0 0  T We  have rank  Wo(0T)  =  I  <  2  =  n  and  the  system  is  not  completely  observable.  In  view of Theorem  3.7 all unobservable  states  x  E  XiWoiO  T))  and are therefore  of the  form a  E^  R.  Alternatively  the  observability  matrix  0  = c CA 0  1 0  0 Note  that jvr(O) = >r(Wo(o  T)) span Observability  utilizes future  output measurements  to determine the present  state. In (re)constructibility  past output measurements  are used. Constructibility  is  defined in  the  following  and  its  relation  to  observability  is  determined. DEFlNITlON3.il.  A State x is unconstructible  if the zero-input response of the system (3.18) is zero for  all t  <  0 i.e. Ce^'x  =  0 for every  ^ <  0. (3.30) The  set  of  all  unconstructible  states  x R^ is  called  the  unconstructible  subspace  of (3.18).  The  system  (3.18)  is  (completely  state)  {re)constructible  or  the  pair  (A C) is  (re)constructible if  the  only  state  x  E  /?"  that  is  unconstructible  is  ;c  =  0  i.e. Ren  = {0}. We shall now establish a relationship between observability and constructibility  for the continuous-time  time-invariant  systems  (3.18). Recall that x  is unobservable  if  and only if Ce'^'x  =  0 for every t  >  0. (3.31) THEOREM  3.9.  Consider the system x  =  Ax  + Bu y  =  Cx  + Du  given in (3.18). (i)  A state x is unobservable if and only if it is unconstructible. (ii)  Ro  =  R^. (iii)  The  system  or the pair  (A C) is  (completely  state)  observable  if  and only if it is (completely  state)  (re)constructible. Proof  (i)  If  X  is  unobservable  then  Ce^^x  =  0  for  every  t  >  0.  Taking  deriva tives  with  respect  to  t  and  evaluating  ai  t  =  0  we  obtain  Cx  =  CAx  =  "•  = CA^x  =  0  for  A:  >  0  or  CA^'x  =  0  for  every  /:  >  0.  This  in  view  of  Ce^^x  = implies  thai Ce^^x  =  0 for every r<  0 i.e. x is unconstructible. The ^"l=Q(t^/k\)CA^x converse is proved in a similar manner. Parts (ii) and (iii) of the theorem follow  directly from  (i). • The  observability  Gramian  for  the time-invariant  case  Wo(0  T)  was  defined  in (3.22).  In  view  of  (3.11) we  make  the  following  definition. 256 Linear  Systems DEFINITION  3.12.  The constructibUity  Gramian  of system (3.18) is ih^nXn  matrix W(0  T) AHr~T)^T^^A(r-T)^^^ (3.32) Note  that Wo(0T)  = e^"^Wcn{0T)e AT (3.33) as  can  be  verified  directly  [see  also  (3.13)]. As  in  the  time-varying  case  above  we  now  introduce  a  number  of  additional criteria  for  observability. THEOREM  3.10.  The system x  =  Ax  + Bu y  =  Cx  + Dui^  observable (i)  if and only if rank  W^(0 T)  =  n (3.34) for  some  finite  T  >  0 where  Wo(0 T)  =  \Q  e^^'^C^Ce^'  dr  the  observabil ity Gramian or (ii)  if and only if the n columns of are linearly independent on  [0 oo) over the field of complex numbers or alter natively if and only if the n columns of Ce"^' (3.35) are linearly independent  over the field of complex numbers or (iii)  if and only if C(sl  -  A)-^ where 0  -C CA CA n -l (iv)  if and only if rank  €  =  n the observability  matrix or rank Sil  -  A C (3.36) (3.37) (3.38) for all complex numbers 5/ or alternatively for all eigenvalues  of A. Proof.  The proof of this theorem is completely  analogous to the (dual) results on reach ability  (Theorem 2.17)  and is omitted. • Similar  results  as  those  given  in  Theorem  3.10  can  be  derived  for  constructibil-ity  and  the  reader  is  encouraged  to  state  and  prove  these  for  the  cases  (i)  and  (ii). This  is  of  course  not  surprising  since  it  was  shown  (in  Theorem  3.9)  that  observ ability  implies  and  is  implied  by  constructibility.  Accordingly  the  tests  developed in  the  theorem  for  observability  are  typically  also  used  to  test  for  constructibility. EXAMPLE  3.5.  Consider  the  system  x  =  Ax  y  =  Cx  where  A  = andC 0  0 [1 0] as in Example  3.4(i). We shall verify  (i) to (iv) of Theorem  3.10 for this case. (i)  For the observability Gramian Wo (0 r)  = 2^  we have rank Wo(0 T) i T2 2^ =  2  = nfor  any T >  0. (ii)  The columns of Ce^^ =  [lt]  are linearly independent on [0 oo) over the  field of complex numbers since ai  • I + a2-1  = 0 implies that the complex num bers ai  and a2  must both be zero. Similarly the columns of C{sl —  A)~^  = [1/5 \ls^] are linearly independent over the field of complex numbers. 257 CHAPTERS: Controllability Observability and Special Forms = rank =  2  = n. "1  0' .0  1. - 1" (iii)  rank€ }  =  rank r c' [cA_ (iv) rank Sil  -  A] C  J = rank values of A. Consider again A = 0  1 0  0 'Si 0 .1 =  2  =  n for Si =  0  /  =  1 2 the eigen-= Si 0. butC  =  [0 1] [in place of [10] as in Example 3.4(ii)]. The system is not observable for the reasons given below. (i)  Wo(0 T)  = with rank Wo{0 T)  =  l<  2  = n. (ii)  Ce^^ =  [01]  and  its  columns  are  not  linearly  independent.  Similarly  the columns of C(sl  — A)~^  =  [0 l/s] are not linearly independent. (iii)  rank€  = rank C CA = rank (iv) rank Sil  -  A C = rank of A. C.  Discrete-Time  Systems 0  1 0  0 -1 1<2  = n. =  I  < 2  =  n  for Si =  0  an  eigenvalue We consider  systems described by equations of the  form x(k  +  1)  =  A(k)x(k)  +  B(k)u(k\ y(k)  =  C(k)x(k)  +  D(k)u(k\ (3.39) where  A(k)  G  /('"^^ B(k)  G  /^"^"^ C(k)  G  i?^><^ D(k)  G  RP"""^  and  the  input u(k)  G  R"^ are  defined  for  k  ^  ko  (see  Section  2.7).  The  output  y(k)  for  ^  > ^Q is given by y(k)  =  C(k)^(k ko)x(ko)  +  ^  C(k)^(k i +  l)B(i)u(i)  +  D(k)u(kl (3.40) k-\ i = ko where the state transition matrix ^(k  ko) is given by ^(k  ko)  =  A(k  —  l)A(k  —  2) ... A(ko)  for  k  >  ko and <P(ko  ko)  =  I. In the time-invariant  case (3.39) assumes the  form x{k  +1)  =  Ax(k)  +  Bu(kl y(k)  =  Cx(k)  +  Du(k) k  ^  ko (3.41) where A G iR"><^ C  G jR^^^ C  G /^^x^ D  G  RP"""^ and (3.40) is still valid with the state transition matrix 0(fc ko) given in this case by ^(k  ko)  =  A^-^o k  ^  ko. (3.42) 258 Linear Systems Observability  and  (re)constmctibility  for  discrete-time  systems  are  defined  as ^^ the  continuous-time  case.  Observability  refers  to  the  ability  to  uniquely  deter mine the state from  knowledge of current and future  outputs and inputs while con-structibility  refers  to  the  ability  to  determine  the  state  from  knowledge  of  current and past outputs and inputs. In discrete-time  systems the time-varying  case can be developed in a manner analogous to the time-invariant case and will therefore not be developed  here. Instead  we  shall concentrate  on the time-invariant  case. Note that some of the following  results have already been presented in Section 3.1. Discrete-time time-invariant  systems Consider the time-invariant system (3.41) and the expression for its output  y(k) given in (3.40). Without loss of generality we take  ^o  =  0. Then y(k)  =  CA^x{0)  + ^  CA^-^'^^'^Buii)  +  Du{k) (3.43) k-i for  yfe  >  0 and j(0)  =  CJC(O) +  Dw(0). Rewrite (3.43) as /=o y{k)  =  CA^xo (3.44) for  ^  >  0  where  y{k)  =  y(k) XfZo  CA^-^'+^^Bu(i)  -h  Du(k)  for fc >  0  and y(0)  =  y(0)  -  Du(Ol  and  XQ =  x(0). DEFINITION  3.13.  A State  x  is  unobservable  if  the  zero-input  response  of  system (3.41) is zero for all ^ >  0 i.e. if CA^x  = 0 for every y^ >  0. (3.45) The set of all unobservable states x Ro is called the unobservable subspace of (3.41). The system (3.41) is {completely state) obse^able  or the pair (A C) is observable if the only state x E  /?" that is unobservable is x  =  0 i.e. if Ro = {0}. • The pnX  n observability  matrix  0  was defined  in  (3.23). Let J{(G) denote  the null space of  0. THEOREM3.il.  A State X is unobservable if and only if X E K(€X (3.46) i.e. the unobservable subspace Ro = M(€). Proof. If X  E Jvr(O) then Ox  =  0 or CA^x  =  O f o r O < ^ < n - l.  This statement is also true fork>n- 1 in view of the Cay ley-Hamilton Theorem. Therefore (3.45) is satisfied and x is unobservable. Conversely if x is unobservable then (3.45) is satisfied and €x  = 0. • Clearly x is observable if and only if €x  ¥"  0. COROLLARY  3.12.  The  system  (3.41)  is  (completely  state)  observable  or the pair (A C) is observable if and only if rank€  = n. (3.47) 259 CHAPTER  3: Controllability Observability and  Special Forms If the system is observable the state XQ at ^ = 0 can be determined as the unique  solution of (3.48) where l'0.«-l  =  b ^ ( 0 )  / ( l )  . . .  / ( « - l ) ] ^ 6 / ? ' ' « t/o.«-i  =  [M^(0)M^(l)...«^(n-l)]^  €/?"•" and Mfi is the pn x mn matrix given by D CB 0 D 0 0 0" 0 Mn--CA^'-^B CA^'-^B D ••  CB D Proof  The system is observable if and only if the only vector that satisfies  (3.45) is the zero  vector.  This is true if and only if ^ ( ^)  =  {0} or if (3.47) is true. To determine the  state XQ apply  (3.43) for ^ =  0  1  . . . n — 1 and rearrange in a form  of a system of linear equations to obtain  (3.48). • The  matrix M^  defined  above has the special  structure  of a ToepUtz  matrix.  Note that  a matrix  T  is Toeplitz  if its  (/ 7)th entry  depends  on the value  i —  j'  that  is T is "constant  along  the diagonals." Similarly  to the continuous-time  case we now define  the observability  Gramian. DEFINITION  3.14.  The observability  Gramian  of the system (3.41) is iho nxn  matrix ^ -1 i=o If  ^k  =  [C^ ( C A ) ^  . . . (CA^-i)^]^  (with  ^n  =  ^ ) then Wo{0K)  =  ^^^K- The  following  result is  apparent. LEMMA 3.13.  ^ ( ^)  =  ^(Wo(0K)) for every  K>n. (3.49) (3.50) Proof  The proof is left  as an exercise for the reader. • COROLLARY  3.14.  The system  (3.41)  is  (completely  state)  observable  or the pair (A C) is observable if and only if rankWo{0K)=n (3.51) for  some  (and consequently  for all) K  > n. If the system  is observable  the state XQ  at ^ =  0 is given by xo = W-\0K)^^[YoK-i -MkUoK-i]. (3.52) where K  >n. Proof  Statement  (3.51) is a direct consequence of Corollary  3.12 and Lemma 3.13. To obtain (3.52) rewrite  (3.48) in terms of K premultiply by ^ ^ and use relation  (3.50). 260 Linear  Systems EXAMPLE  3.6.  Consider  the  system  in  Example  1.4  x{k  +  1)  =  Ax{k)y{k)  = Cx(k)  where  A  = ' 1  1 and  C  =  [01].  The  observability  Gramian  Wo{0K) foYK  =  n  =  2is  given  by  (3.49)  Wo(02)  = Xi^o(A^yC^CA^ [0 1]  + ro^ [i. 10  IJ ro  r ro  11 iJ .1 of full  rank. Therefore  the system is observable. Note that 0  = .1.  [01] + ni .1  1.  — ro^ 0  0 0  1  + i_ [11] 1  1 ll  1 1  1 1  2  which is is of full  rank as ro  1 Ll  1 well  (see Example  1.4).  Notice also that rank  Wo(0 K)  =  2 for  every  K  >  2 and  that [refer  to (3.50)]  W^(0 2) "1  1 1  2  = "0  1 1  1 T  r 0  1 1  1 0^0.  The unique vector  x(0) can be determined  from  y(0) and yil)  using (3.52) to obtain xi(0) ^2(0)J =  w; Ho 2)0^ 3^(0) 3^(1) 3^(1) -  3^(0) This is the same as the result obtained in Example  1.4  using an alternative  approach. Constructibility  refers  to  the  ability  to  determine  uniquely  the  state  x{0)  from knowledge  of current  and past outputs  and inputs. This is in contrast to  observability which  utilizes  future  outputs  and inputs. The  easiest  way  to define  constructibility  is by the use of (3.44) where  x(0)  =  XQ is to be determined  from  past data y{k)  A:  <  0. Note however  that for fc <  0 A^ may  not exist; in fact  it exists only when A is  non-singular.  To  avoid  making  restrictive  assumptions  we  shall  define  unconstructible states in a slightly  different  way than anticipated.  Unfortunately  this definition  is not very  transparent.  It  turns  out  that  by  using  this  definition  an  unconstructible  state can  be  related  to  an  unobservable  state  in  a  manner  analogous  to  the  way  a  control lable  state  was  related  to  a reachable  state  in  Section  3.2  (see  also  the  discussion  of duality  in  Section  3.1). DEFINITION  3.15.  A State X is unconstructible  if for every  ^  >  0 there exists x i such that /?« =  Ak A'x Cx  =  0. (3.53) The  set  of  all  unconstructible  states  R^  is  called  the  unconstructible  subspace.  The system  (3.41)  is  (completely  state)  constructible  or the  pair  (A C)  is  constructible  if • the only state x  E  /?" that is unconstructible  is x  =  0 i.e. if R^  =  {0}. Note that if A is nonsingular  then  (3.53)  simply  states that x is unconstructible  if CA~^x  =  0  for  every  k>  0  (compare  this  with  Definition  3.13  of  an  unobservable state). The  results  that  can  be  derived  for  constructibility  are  simply  dual  to the  results on  controllability.  They  are presented  briefly  below but  first  a technical  result  must be  established. LEMMA  3.15.  If  X E  M'(€) then  Ax J{(€)  is an A-invariant  subspace. >r(0)  i.e.  the  unobservable  subspace  Ro  = Proof  Let  x  E  >r(0) so that Ox  =  0. Then  CA^x  =  O f o r O < / : < n - l.  This  state ment  is  also  true for  k>  n  — 1 in  view  of  the  Cayley-Hamilton  Theorem.  Therefore OAx  =  0 i.e.. Ax  E  M{p). • THEOREM 3.16.  Consider the  systemx(^+1): given in (3.41). --Ax{k)+Bu{k)y{k)=Cx{k)+Du{k) 261 CHAPTER  3: Controllability Observability and  Special Forms (i)  If a state x is unconstructible then it is unobservable. (ii)  RcnCRo. (iii)  If the system is (completely  state) observable or the pair  (A C) is observable then  the  system  is  also  (completely  state)  constructible  or  the  pair  {AC)  is constructible. If A is nonsingular  then relations  (i) and  (iii)  are if  and only  if  statements. In  this case constructibility  also implies observability. Furthermore in this case (ii) becomes an equality i.e. Rcw = Ro-Proof  This  theorem  is dual  to Theorem  2.22  which  relates  reachability  and  control lability  in  the  discrete-time  case.  To  verify  (i)  assume  that  x  satisfies  (3.53)  and premultiply  by  C  to  obtain  Cx  =  CA^x  for  every  ^  >  0.  Note  that  Cx  =  0  since  for k = 0^  x = x  and  Cx  =  0.  Therefore  CA^x  =  0  for  every  ^  >  0  i.e.  x  G ^ ( ^ ).  In view of Lemma 3.15 x = A^x  G ./K(^) and thus x is unobservable. Since x is arbitrary we  have  also  verified  (ii).  When  the  system  is  observable  Ro =  {0}  which  in  view of  (ii)  implies  that  Ren =  {0}  or  that  the  system  is  constructible.  This  proves  (iii). Alternatively  one  could  also prove this  directly:  assume  that the  system  is  observable but  not  constructible.  Then  there  exist  xx  7^ 0  which  satisfy  (3.53).  As  above  this implies that x G ./K(^) which is a contradiction  since the system is observable. Consider  now  the  case  when  A  is  nonsingular  and  let  x  be  unobservable.  Then in  view  of  Lemma  3.15 x  = A~^x  is  also  in  ^ ( ^ )  i.e.  Cx  =  0.  Therefore  x  =  A^x is  unconstructible  in  view  of  Definition  3.15.  This  implies  also  that  R^  C  Ren  and therefore  Ro =  RCE which proves that in the present  case constructibility  also  implies observability. • EXAMPLE  3.7.  Consider the  system in Example  1.5 x{k+l)= Ax{k)y{k)  =  Cx{k) where A and C =  [10]. As shown in Example  1.5  rank^  =  rank [Ol a 1  <2  =  n  i.e.  the  system  is  not  observable.  All  unobservable  states  are  of  the  form where  a  ^  R  since m is  a  basis  for  yK{^)  =  Ro  the  unobservable  sub-is  {[:]}  and .  Note  that  a  basis  for  yK(Wo(02)) space.  The  observability  Gramian  foYK  =  n  =  2is  Wo(02)  =  C^C  +  (CA)^(CA) 2  0 0  0 0 0  + 1 0 1 0 0 0 ^ ( ^)  =  J^{Wo{02)).  This verifies  Lemma 3.13. In Example  1.6  it was  shown that  all the  states x  that  satisfy  CA~^x  = 0 for  every ^  >  0  i.e.  all  the  unconstructible  states  are  given  by  a a  e  R.  This  verifies Theorem  3.16  (i) and (ii) for the case when A is  nonsingular. EXAMPLE  3.8.  Consider  the  system  x(k+  1)  =  Ax(k)y(k)  =  Cx{k)  where  A 0 1 0 0 and C =  [10]. The observability  matrix 1 0 0 0 is of rank  1 and there-fore  the system is not observable. In fact  all states of the form  a I are  unobservable states  since < >  is a basis for J^{^). To  check  constructibility  the  defining  relations  (3.53)  must  be  used  since  A  is 5ck  consti singular.  Cx  =  [l0]x  =  0  implies  x Substituting  into  x  =  A^x  we  obtain 262 Linear  Systems for  k  =  0x  = X  and x =  0  for fc >  1.  Therefore  the  only  unconstructible  state  is X =  0  which  imphes  that  the  system  is  constructible  (although  it  is  unobservable). This  means  that  the initial  state x(0) can be uniquely  determined  from  past  measure-rxi(o)i  _ X2(0)J ments. In fact  from  x{k +  1) = Ax{k)  and y{k)  = Cx{k)  we obtain x(0) x i ( - l) X2(-l). Therefore x(0) 0 xi(-iy 0 y ( - l) and  y ( - l)  =  C x ( - l)  =  [10] XI ( - 1) X2(-l). = x i ( - l ). DEFINITION  3.16.  The constructibility  Gramian  is defined as (3.54) We  note  that  Wc^(0^)  is well  defined  only  when A  is nonsingular.  The  observ ability  and constructibility  Gramians  are related  by Wo(0K)  = {A^)^Wcn{0K)A^ (3.55) as can easily  be  verified. When A  is nonsingular  the state XQ at ^ =  0 can be determined  from  past  outputs and  inputs  in the following  manner.  We consider  (3.44)  and note  that in this  case is  valid  for ^ <  0 as well.  This  implies  that y{k)  =  CA^jco 'CA-""' -l-n "Xo Xo (3.56) CA-' with  Y-i^-n  = [j^(—n).. .j^(—1)]^.  Equation  (3.56)  must  be  solved  for XQ. Clearly  in  the  case  of  constructibility  (and  under  the  assumption  that  A  is  non-singular)  the  matrix  ^A~^ [compare  this  with  the dual  results  in  (2.66)].  In  particular  the  system  is  constructible  if  and  only  if rank  {^A~'^)  =  rank  ^  =  n. is  of  interest  instead  of  ^ EXAMPLE  3.9.  Consider  the  system  in  Examples  1.4  and 3.6  namely  x ( ^+ 1)  = Ax{k)y{k)  =  Cx{k)  where  A [0 1 ll 1 check constructibility  we consider  ^A and  C =  [01].  Since  A  is  nonsingular  to CA-^ CA-'  which has full  rank. Therefore the system is constructible (as expected) since it is observable. To determine x(0) in view of (3.56) we note  that which XI (0) X2(0) namely  (A^)^Wc„(02)A }'(-2) 1 1 XI (0) X2(0)   from ffA-^x{Q) }'(-2) } ' ( - 2) K-l)+K-2)  . It is also easy to verify  (3.55) • = W(02). 263 CHAPTERS: Controllability Observability and Special Forms PART  2 SPECIAL  FORMS  FOR  TIME-INVARIANT  SYSTEMS 3.4 SPECIAL  FORMS In  this  section  important  special  forms  for  the  state-space  description  of  time-invariant  systems  are  presented.  These  forms  are  obtained  by  means  of  similarity transformations  and  are  designed  to  reveal  those  features  of  a  system  that  are  re lated to the properties  of controllability  and observability.  In Subsection  A  special state-space  forms  that  display  the  controllable  (observable)  part  of  a  system  and that  separate  this  part  from  the  uncontrollable  (unobservable)  part  are  presented. These forms referred  to as the standard  forms  for uncontrollable  and  unobservable systems  are  very  useful  in  establishing  a  number  of  results.  In  particular  these forms  are used in Subsection B to derive alternative tests for controllability  and ob servability  and in Subsection  C to relate  state-space  and input-output  descriptions. (Additionally  these  forms  are further  used  in  Chapters  4  and  5.)  In  Subsection  D the controller and observer state-space forms  are introduced. These are useful  in the study  of  state feedback  and  state estimators  (to be  addressed  in  Chapter  4) and  in state-space realizations  (to be addressed in Chapter 5). A.  Standard  Forms  for  Uncontrollable  and Unobservable  Systems We consider time-invariant  systems described by equations of the  form X =  Ax  +  Bu y  =  Cx  +  Du (4.1) where A  £  R""^" B  G T?"^™  C  £  RP^"  and D  £  RP^"".  It was shown earlier in this chapter that this system is state reachable or controllable-from-the-origin  if and only if the n X mn  controllability  matrix %  = [BAB...A''-^B] (4.2) has  full  row  rank  n  i.e.  rank  %  =  n.  Recall  that  9l(^)  =  Rr  is  the  reachable subspace  which  contains  all  the  state  vectors  that  can  be  reached  from  the  zero vector  in  finite  time  by  applying  an  appropriate  input.  If  the  system  is  reachable (or controllable-from-the-origin)  then it is also controllable  (or  controllable-to-the-origin) and vice versa (see Section  3.2). It was also shown earlier in this chapter that  system  (4.1) is state observable if and only if the pnX  n observability  matrix C CA CA n-\ (4.3) has full  column rank i.e.  rank  €  =  n. Recall that J{(€)  =  RQ  is the  unobservable subspace that contains all the states that cannot be determined uniquely  from  input 264 Linear Systems and  output  measurements  in  finite  time. If  the  system  is observable  then  it is  also constructible and vice versa (see Section  3.3). Similar  results  were  also  derived  for  discrete-time  time-invariant  systems  de scribed by equations of the  form x(k  +1)  =  Ax(k)  +  Bu(kl y(k)  =  Cx(k)  +  Du(k) (4.4) in Sections 3.1 3.2 and 3.3. Again rank  %  =  n and rank  0  =  n are the necessary and sufficient  conditions for state reachability and observability respectively. Reach ability always implies controllability and observability always implies constructibil-ity as in the continuous-time case. However in the discrete-time case controllability does not necessarily  imply  reachability  and constructibility  does not imply  observ ability unless A  is  nonsingular. Next  we  will  address  standard  forms  for  unreachable  and  unobservable  sys tems both for the continuous-time  and the discrete-time time-invariant cases. These forms  will be referred  to as  standard  forms  for  uncontrollable  systems  rather  than unreachable  systems  and  standard  forms  for  unobservable  systems  respectively. This is to conform  with the established terminology in the literature where the term "controllable"  is used  instead  of  "reachable"  perhaps  because  of  emphasis  on  the continuous-time  case. It  should be noted however  that by  the term  controllable  in this section we mean controllable-from-the-origin  i.e. reachable. 1. Standard form for uncontrollable  systems If  the  system  (4.1)  [or  (4.4)]  is not  completely  controllable  (-from-the-origin) then  it is possible  to  "separate" the controllable  part  of the  system  by means  of  an appropriate similarity transformation. This amounts to changing the basis of the state space  (see Section  2.2)  so that  all the vectors  in the controllable  (-from-the-origin) or reachable subspace Rr have certain structure. In particular let rank  ^  =  nr <  n i.e.  the pair  (A B)  is  not  controllable.  This  implies  that  the  subspace  Rr  =  2/l(^) has  dimension  n^. Let  {vi V2... v„J  be  a basis  for  Rr.  These  n^ vectors  can  be for example any nr linearly independent  columns of ^.  Define  the  nX  n  similarity transformation  matrix Q=  [VlV2...VnQn-n^ (4.5) where  the  nX  (n  -  nr) matrix  Qn-nr contains  n  -  nr linearly  independent  vectors chosen  so  that  Q  is  nonsingular.  There  are  many  such  choices.  We  are  now  in  a position to prove the following  result. LEMMA 4.1.  For (A B) uncontrollable there is a nonsingular matrix Q such that A = Q-'AQ  =  Ai  An 0  A2 and B  =  Q'^B  = (4.6) where Ai  E R'^rXnr^ ^i  G R'^rX'^^ and the pair (Ai Bi) is controllable. The pair (A B) is in the standard form for uncontrollable systems. Proof  We need to show that AQ  =  A[Vi  . . .  Vn  Qn-nr]  = l^h  • • •  V„  Qn-nr] =  QA. 0  A2. Since the subspace Rr is A-invariant (see Lemma 2.15 in this chapter) Av/ G Rr which can be written as a linear combination of only the nr vectors in a basis of Rr. Thus Ai in A is an rir X rir matrix and the (n — rir) X rir matrix below it in A is a zero matrix. Similarly we also need to show that 265 CHAPTERS: Controllability Observability and Special Forms B  =  [vi...v„ Q„-„J  0 -QB. But this is true for similar reasons: the columns of 5  are in the range of'^  or in Rr. The n X nm  controllability  matrix '^ of (A B) is ^  =  [BAB...A"-'B] =  0 0 Ar'Bi 0 (4.7) which clearly has ranfcC  = that rank[BiAiBi... A"{''^Bi...A"^-^Bi] =  n.Note (4.8) The range of ^  is the controllable  (-from-the-origin)  subspace of (A B).  It contains vectorsonlyoftheform[a^  0]^ wherea  G JR"^ Since dim  S/l(^)  =  rank%  =  rir every  vector  of  the  form  [a^  0]^  is  a  controllable  (state)  vector.  In  other  words the  similarity  transformation  has changed  the basis  of  /?" in  such a manner  that  all controllable  (-from-the-origin)  vectors  expressed  in  terms  of  this  new  basis  have this very particular  structure of zeros in the last n —  rir entries. Given  system (4.1)  [or (4.4)] if a new  state x(t)  is taken to be x(t)  =  Q~^x(t) then k  =  Ax  + Bu y  =  Cx  + bu (4.9) where  A  =  Q~^AQB  =  Q~^BC  =  CQ  and  D  =  D  constitutes  an  equivalent representation  (see Chapter 2). For Q as in Lemma 4.1 we obtain Xi X2 An Ai. 0 uy  =  [Ci C2] +  Du (4.10) where  x  =  {x[  X2]  with  xi  G R^'  and where  (Ai ^i)  is controllable. The matrix C  =  [Ci C2] does not have any particular  structure. This representation  is called a standard form  for  the  uncontrollable  system.  The  state  equation  can  now  be  writ ten as X\  ^  A i ^i  +  BiW  +  A12X2 X2  =  A2X2 (4.11) which  shows  that  the  input  u does  not  affect  the trajectory  component  X2(t) at  all and therefore  X2{t) is determined  only by the value of its initial vector. The input u certainly affects  xi (t). Note also that the trajectory component xi (t) is also influenced by X2(t). In  fact. xi(t)  =  ^^i^Jci(O)  + e'^'^'~^^Biu(T)dT  + oMit-T. -^Ane^^'dr  X2(0). (4.12) The  nr eigenvalues  of Ai  and the corresponding  modes  are called  controllable eigenvalues  and controllable  modes  of the pair (A B) or of system (4.1) [or of (4.4)]. The n-  nr eigenvalues of A2 and the corresponding modes are called the  uncontrol lable  eigenvalues  and uncontrollable  modes  respectively. It  is  interesting  to  observe  that  in  the  zero-state  response  of  the  system  (zero initial  conditions)  the  uncontrollable  modes  are  completely  absent.  In  particular. 266 Linear Systems in  the  solution  x(t)  =  e^^x(0)  +  \Q e^^^ ^'^Bu{T)dr  of  x  =  Ax  + Bu  given  x(0) notice that eMt-r)^  ^  [Qe^(^-^^Q-'][QB]  =  Q 0 (show this) where Ai  [from (4.6)] contains only the controllable eigenvalues. There fore  the input  u(t)  cannot  directly  influence  the uncontrollable  modes. Note how ever that the uncontrollable modes do appear in the zero-input response e^^x(0).  The same observations  can be made for  discrete-time  systems  (4.4) where the  quantity A^B  is of interest (show this). EXAMPLE 4.1.  Given A  =1 ro -1 -2 LO 1 11 1 - ij and 5  = "1  0" 1  1 .1  2. tern (4.1) to the standard form (4.6). Here we wish to reduce sys-% =  [B AB A^B] 1 1 1 0 1 2 : : : 1 0 -1 :  0 :  0 :  0 -1 0 1 and rank % = nr'=2<3  = n. Thus the subspace Ry = S?l(^) has dimension rir  = 2 and a basis {vi V2} can be found by taking two linearly independent columns of ^ say the first two to obtain [VlV2 Gl]  = 1  0 1  1 1  2 0 0 1 The third  column of  Q was  selected  so that  Q is nonsingular.  Note that the first two columns of Q could have been the first and third columns of ^  instead or any other two linearly independent vectors obtained as a linear combination of the columns in %. For the above choice for Q we have. A =  Q-'AQ  = 1 -1 1 0 1 -2 0 1 -2 -1 -1 4 1] 1 ri  0  0" 1 10 [1  2  1. 1  -ij 01 0 ij 11 0 2] -1 To 1 -2 [0 ri  0  01 1 10 [1  2 i_ 0 0 0 1 -1 : : 1 0 Ai :  An : -2_ .0 :  M_ B=  Q-^B  = 1 -1 1 0 1 -2 n 1 1 01 0 ij 0] 1 2^ = "  1 0 0  ' 1 0 0 J [B11 = 0 where (Ai Bi) is controllable [verify this and show that ^  = Q  ^^  i.e. verify  (4.8)]. 267 CHAPTER 3: Controllability Observability and Special Forms The  matrix  A  has  three  eigenvalues  at  0  - 1  - 2.  It  is  clear  from  (A B)  that the  eigenvalues  0-1  are controllable  (in  Ai) while  -2  is  an uncontrollable  eigen • value (in A2). 2.  Standard form for unobservable  systems The standard form  for  an unobservable  system can be derived  in a similar  way as the  standard  form  of uncontrollable  systems. If the  system  (4.1)  [or (4.4)] is not completely  state observable then  it is possible to "separate" the unobservable  part of the system by means of a similarity transformation.  This amounts to changing the basis of the state space so that all the vectors in the unobservable  subspace Ro have a certain  structure. As  in the preceding  discussion  concerning  systems  or pairs  (A B)  that  are  not completely  controllable  we  shall  presently  select  a  similarity  transformation  Q to reduce a pair  (A C) which is not completely  observable to a particular  form.  This can be accomplished  in two ways. The  simplest  way  is to invoke duality  and  work with the pair (AD  =  A^Bo  =  C^) which is not controllable (refer to the discussion of dual systems in Section 3.1). If Lemma 4.1 is applied then AD  =  Q'DADQD = AD\ 0 ADU AD2 BD  -  QD  BD - BDI 0 where (Aoi  Boi)  is controllable. Taking the dual again we obtain the pair  (A C) which has the desired  proper ties. In particular ' A^ A = A'^ = QhAUQor'  = QoMGhr' = A^ ^D12 c  = Bi = BUQlr'  =  c(Qir 1  _  [B: IvOl 0 A^ ^D2 (4.13) where (A^p Bj^^) is completely  observable by duality  (see Theorem  1.1). EXAMPLE  4.2.  Given  A  = 0 -1 1 1 -2 1 0] 1  andC = - ij we wish to reduce system (4.1) to the standard form (4.13). To accomplish this let AD  = A^ and BD  =  C^ • Notice that the pair (Ao Bo) is precisely the pair (A B) of Example 4.1. • A  pair  (A C)  can  of  course  also  be  reduced  directly  to  the  standard  form  for unobservable  systems. This is accomplished  in the  following. Consider the system (4.1)  [or (4.4)] and the observability  matrix 0  in (4.3). Let rank  €  =  rio <  n i.e. the pair (A  C) is not completely observable. This implies that the unobservable subspace Ro  =  J^(€)  has dimension n -  HQ. Let {vi... v„-„J  be a basis for 7?^  and define  an n X n similarity transformation  matrix  Q as Q  = [QnoVi...Vn-nol (4.14) where the n^irio  matrix  Qn^  contains UQ linearly independent vectors chosen so that Q is nonsingular. Clearly there are many  such choices. LEMMA 4.2.  For (A  C) unobservable there is a nonsingular matrix Q such that A = Q-'AQ  = \Ai 0 U21  A2 and C  =  CQ  =  [Ci 0] (4.15) 268 Linear Systems where Ai  G /?«^^«« Ci  E T^^^^^ and the pair (Ai d)  is observable. The pair (A C) is in the standard form for  unobservahle systems. Proof  We need to show that Since the unobservahle subspace Ro is A-invariant (see Lemma 3.15) Av/ E T^^ which can be written as a linear combination of only the n-  rio vectors in a basis of Ro. Thus A2 in A is an (n ~ rio) X (n -  Ho) matrix and the rio X (n -  rio) matrix above it in A is a zero matrix. Similarly we also need to show that 0 A2.  = eA. A21 CQ  =  C[Q„vi...v_„J  =  [Ci0]  =  C. This is true since Cvi  = 0. The pnX  n observability  matrix 0  of (A C) is which clearly  has C CA Ci CiAi CA n-\ CA\-' Ci CiAi rank€  =  rank CiA «o-i =  ^« Note that ciAr =  €Q • (4.16) (4.17) The null space of © is the unobservable  subspace of (A C). It contains vectors only of the form  [0 a'^f  where a  G  /^"""o.  Since dim >f(d)  =  n- rank  €  =  n  -  no every  vector  of the form  [0 a ^ ]^  is an unobservable  (state)  vector. In other words the  similarity  transformation  has changed  the basis  of  R'^ in  such  a manner  that  all unobservable  vectors expressed in terms of this new basis have this very  particular structure—zeros  in the first no entries. For Q chosen as in Lemma 4.2 and (4.9) it assumes the  form Ai A21 0 A2 Bi Bi uy  =  [Ci 0] +  Du (4.18) where  x  =  [x[  x^Y  with  xi  G  7?"^ and  where  {A\  C\)  is observable. The  matrix B  =  [BJ B^]^  does  not  have  any  particular  form.  This  representation  is  called  a standard form  for  the unobservable  system. The  no eigenvalues  of Ai  and  the  corresponding  modes  are  called  observable eigenvalues  and  observable  modes  of  the  pair  (A C)  or  of  the  system  (4.1)  [or of (4.4)]. The n -  no eigenvalues of A2 and the corresponding modes are called  unob servable  eigenvalues  and unobservable  modes  respectively. Notice  that  the  trajectory  component  x(t)  which  is  observed  via  the  output 3; is not influenced  at all by X2 the trajectory  of which is determined primarily by the eigenvalues  of A2. The unobservable  modes  of the  system  are completely  absent  from  the  output. In particular given x  =  Ax  -\-  Bu y  =  Cx  with initial state x(0) we have 269 CHAPTERS: Controllability Observability and Special Forms y{t) Jo and  Ce"^'  =  [CQ-^][Qe^'Q-^]  =  [Cie'^^'0]Q-^ (show  this)  where  Ai  [from (4.15)] contains only the observable eigenvalues. Therefore the unobservable modes cannot  be  seen  by  observing  the  output.  The  same  observations  can  be  made  for discrete-time  systems where the quantity  CA^  is of interest (show this). EXAMPLE  4.3.  Given  A  =  \  ^ 2 ^ | and C  =  [1 1] we  wish  to  reduce  system -3J (4.1)  to  the  standard  form  (4.15).  To  accomplish  this  we  compute  0  =  which has rank €  =  rio  =  1 < 2  =  n. Therefore  the  unobservj space Ro = M(€) has dimension n -  Uo  =  1. In view of (4.14) Q  =  [Qhvi]  = : 0 1 1  :  -ij where vi  =  [1 -1]^ is a basis for Ro and Qi was chosen so that Q is nonsingular. Then A  =  Q-'AQ 1  1 1  0 "0 1 1 -1 Ai A21 0 A2 C=  CQ=  [11] [10]  =  [Ci0] where (Ai Ci) is observable [show this and verify that 6  =  €Q i.e. verify (4.17)]. The matrix A has two eigenvalues at  - 1  - 2. It is clear from (A C) that the eigen • value -2  is observable (in Ai) while -1  is an unobservable eigenvalue (in A2). 3. Kalman's Decomposition  Theorem Lemmas 4.1 and 4.2 can be combined to obtain an equivalent representation of (4.1)  where the reachable  and  observable parts  of this  system  can readily  be  iden tified.  To this end we consider  system  (4.9) again  and proceed in the following  to construct the  nX  n required  similarity transformation  matrix  Q. As before we let rir denote the dimension of the controllable  (-from-the-origin) subspace 7?r i-e. ^r  =  dim Rr  =  dim9l(^)  =  ran^^.  The dimension of the unob servable subspace/?^  =  >r(0)isgivenby  n^  =  n-rankG  =  n-n^.Let/i^^bethe dimension of the subspace Rro  =  Rr(^  Ro that contains all the state vectors x  E  R^ that are controllable but unobservable. We choose Q  =  [Vl  . . .  Vn-nro  + h  . • .  Vnr^  QN  Vi  . . .  V ^  - «  J (4.19) 270 Linear  Systems where  the  rir  vectors  in  { v i  . . .  v ^ ^}  form  a  basis  for  Rr.  The  last  riro  vectors {^nr-nro+ii"'i^nr} ii^  the  basis  for  Rr  dire chosen  so  that  they  form  a  basis  for Rro =  Rr^Ro'  The Ho — Uro  =  {n — HQ — Uro)  vcctors  { v i . . .  Vn^-Tiro)  ^^^  Selected  so that  when  taken  together  with  the  rird vectors  {v^^_^^-+i... V^^} they  form  a  basis for  Ro  the  unobservable  subspace.  The  remaining  N  =  n—  {fir + rio — Uro) columns in  Q^  are  simply  selected  so that  Q is  nonsingular. The  following  theorem  is  called  the  Canonical  Structure  Theorem  or  Kalman's Decomposition Theorem. THEOREM  4.3.  For  (A5)  uncontrollable  and  {AC)  unobservable nonsingular matrix  Q such that 0 there  is  a A = Or^AQ = B =  Q-'B  = 1 (4.20) Al3 0  " All A21  A22  A23  A24 0 A33 A43  A44_ 0 0 0 0 'B{ B2 0 0 C =  Ce=[Ci0C30] where (i) (A„5)with Ac^ "All 0  " A21  A22_ and Be  = B{ is controllable  (-from-the-origin)  where A^ G R^'^^'Bc  G  R^^^^ {AoCo)  with (ii) \An  Ai31 A33 0 and Co = [CiC3] is observable  where Ag  G R^o><no  ^^^  Q  ^  j^pxno  ^j^^^ where the  dimensions of the matrices Aij.Bi  and Cj  are A ll  :  {nr  -  Uro)  X  {rir  -  Uro) A33 : {n-{nr  + no-nro))  x  {n -  {ur + no -  firo))  A44 : {fio-firo)  x B\ B2  '.  Uro  X m Ci  :  px(nr-nro) C3  : p  x  (n-(rir  +  no-nro)) : {fir — firo) X m A22  :  Uro  X  Uro {fio-firo) (iii) the triple  (Ai 1 5 i Ci)  is such that  (Ai 15i)  is controllable  (from-the-origin) and  (AiiCi)  is observable. Proof  For  details  of  the proof  refer  to  [8]  and  to  R.  E.  Kalman  "On  the  Computa tion of the Reachable/Observable  Canonical Form" SI AM J. Control  and  Optimization Vol. 20 No. 2 pp. 258-260  1982 where  further  classifications  to  [8] and  an  updated method of  selecting  Q are given. • The  similarity  transformation  (4.19)  has  altered  the  basis  of  the  state  space  in such  a  manner  that  the  vectors  in  the  controllable  (-from-the-origin)  subspace  Rr the  vectors  in  the  unobservable  subspace  RQ  and  the  vectors  in  the  subspace Rro^Ro all  have  specific  forms.  To  see  this  we  construct  the  controllability  matrix C  =  [ 5  . ..  A^~^5]  whose  range  is  the  controllable  (-from-the-origin  or  reachable) subspace  and  the  observability  matrix  S  =  [ C ^  . . . (CA^~^)^]^ whose  null  space  is the unobservable  subspace. Then  all controllable  states are of the form  [^^^2  00]^ all  the  unobservable  ones  have  the  structure  [0^2 0^4]^  while  states  of  the  form [0^2 00]^  characterize  Rro  i.e. they  are  controllable  but  unobservable. Similarly  to  the  previous  two  lemmas  the  eigenvalues  of  A  or  of A  are  the eigenvalues of An A22 A33 and A44 i.e. |A/  -  A\  =  |A/ -  A\  =  \XI -  AiillA/  -  A22IIA/ -  A33IIA/ -  A44I. (4.21) If in particular we consider the representation {A B C D} given in (4.9) where Q was selected  as in the canonical structure theorem given above then 271 CHAPTERS: Controllability Observability and Special Forms 0 Ai3 An A21  A22  A23 0 0  A33 0  A43 [  0 0  1 A24 0 A44J pi U2 p3 [x4 + r^ii B2 0 0 -1 y  =  [Ci.0C30] Xl X2 +  Dw. (4.22) This shows that the trajectory components corresponding to X3 and X4 are not  affected by the input u. The modes associated with the eigenvalues of A33 and A44 determine the trajectory components for X3 and f 4 (compare this with the results in Lemma 4.1). Similarly to Lemma 4.2 the trajectory  components for X2 and X4 cannot be observed from y and are determined by the eigenvalues of A22 and A44. The following  is now apparent  (see also Fig. 3.4): The eigenvalues of All  are controllable and observable A22 are controllable and unobservable A33 are uncontrollable  and observable A44 are uncontrollable  and unobservable. CO I FIGURE 3.4 Canonical decomposition (c and c denote controllable and uncontrollable respectively). The connections of the c/c and 0/0 parts of the system to the input and output are emphasized. Note that the impulse response (transfer function) of the system which is an input-output description only represents the part of the system that is both controllable and observable (see Section 3.4C). 272 Linear Systems EXAMPLE  4.4.  Given 1 1 -1 ro A  -  1 [o -1 -2 1 5  = "1  0" 1  1 .1  2_   and  C  =  [01 0]  we wish to reduce  system (4.1) to the canonical structure (or Kalman decomposition) form (4.20). The appropriate transformation matrix Q is given by (4.19). The matrix ^  was found in Example 4.1 and C CA 0 1 2 1 -2 4 0 1 -2 A basis for R^  = >r(0) is {(1 0 -1)^}. Note that rir  = Zrio  =  1 and riro  =  1- There fore Q  =  [Vl V2  QN] 1 1 1 1 0 -1 0 0 1 is an appropriate similarity matrix (check that det Q ¥- 0). We compute 0 0 1 A =  Q-'AQ 1 -1 -2 11 1 -ij 01 0 ij 1 0 -1 1 1 1 0 1 1 1 2 1 ro 1 [o -1 0 Ai3 All A21  A22  A23 0 A33 0 ^  n-^n  = B  =  Q-'B 0 1 1 1 -1 -2 01 0 ij ri  0' 1 1 Ll 2. = "  1 1  ' Bi' B2 . 0. 0 0 -1 = 0 and C  = CQ  =  [0 1 0] 1 1 1 1  0 0  0 -1  1 [100]  = [Ci0C3]. The eigenvalue 0 (in An) is controllable and observable the eigenvalue -1  (in A22) is controllable and unobservable and the eigenvalue  -2  (in A33) is uncontrollable and observable. There are no eigenvalues that are both uncontrollable and unobservable.  • B.  Eigenvalue/Eigenvector  Tests for  Controllability  and  Observability There  are  tests  for  controllability  (-from-the-origin)  and  observability  for  both continuous-  and  discrete-time  time  invariant  systems  that  involve  the  eigenvalues and eigenvectors  of A. Some of these criteria  are called PBH tests after  the initials of the codiscoverers  (Popov-Belevitch-Hautus)  of these tests. These tests are  useful in  theoretical  analysis  and  in  addition  they  are  also  attractive  as  computational tools. THEOREM  4.4.  (i) The pair (A B) is uncontrollable if and only if there exists a 1 X ^ (in general) complex vector vt #  0 such that v / [ A / / - A  5]  =  0 (4.23) where A/ is some complex  scalar. (ii) The pair (A C) is unobservable if and only if there exists annX I (in general) complex vector v/  T^ 0 such that 273 CHAPTER 3: Controllability Observability and  Special Forms [A// -  A] C 0 (4.24) where A/ is some complex  scalar. Proof  Only part (i) will be considered since (ii) can be proved using a similar argument or directly by duality  arguments. (Sufficiency)  Assume  that  (4.23)  is satisfied.  In view  of v/A  =  A/V/ and ViB  = 0 ViAB  =  XiViB  =  0 and vtA^B  =  0  ^  =  0 1 2  . . ..  Therefore  Vi% =  Vi[B  AB... A^~^B]  =  0 which shows that (A B) is not completely  controllable. (Necessity)  Let (A B) be uncontrollable  and assume without loss of generality the standard form for A and B given in Lemma 4.1. We will show that there exist A/ and v/ so that (4.23) holds. Let Aj be an uncontrollable eigenvalue and let v/  =  [0 a] a^  G  C"~"% where  a(XiI  -  A2)  =  0 i.e. a  is a left  eigenvector  of A2 corresponding  to A/. Then • v/[A/  -AB]  =  [0 a(\il  -  A2) 0]  =  0 i.e. (4.23) is satisfied. COROLLARY  4.5.  (i) The pair  (A B) is controllable if and only if no left  eigenvector of A is orthogonal to all the columns of B. (ii) The pair (A C) is observable if and only if no right eigenvector of A is orthogonal to all the rows of C. Proof  The proof  follows  directly from  Theorem 4.4. • COROLLARY  4.6.  (i) A/ is an uncontrollable  eigenvalue of (A B) if and only if there exists a 1 X /I (in general) complex vector v/ 7^  0 that satisfies  (4.23). (ii) A/ is an unobservable  eigenvalue  of (A C) if and only if there exists  an /i X 1 (in general) complex vector v  7^ 0 that satisfies  (4.24). Proof  Only  part  (i) will  be considered  since  part  (ii) can be proved  using  a  similar argument or directly by duality  arguments. (Sufficiency)  Assume  that  (4.23)  is satisfied.  Now v/[A//  -  A B]  =  0 in view of the  sufficiency  proof  of Theorem  4.4 implies  that  v/^  =  v/[5 AB...  A^'^B]  =  0. Therefore  (A B) is not controllable.  Without  loss  of generality  assume  that  (A B) is in the standard  form  of Lemma  4.1. In this case the controllability  matrix has the form (4.7)  with  its top nr rows  linearly  independent  and its lower  n -  nr rows  being  zero. Therefore  in view  of Vj^  =  0 v/ has the form  v/  =  [0 a\  for some  a  E  C"~"^  Now v/(A//  -  A)  =  0 implies that a (A// -  A2) =  0 which shows that A/ is an eigenvalue of A2 i.e. it is an uncontrollable  eigenvalue. (Necessity)  Let A/ be an uncontrollable eigenvalue of (A B). Assume without loss of generality that the pair (A B) is in the standard form of Lemma 4.1. Then v/  =  [0 a ] where a i s s u c h t h a t a ( A / /-  A2) =  0 (see the proof of Theorem 4.4) satisfies v/(A//-  A)  =  0. Also ViB  =  [0 a] 0  =  0. So (4.23) is  satisfied. EXAMPLE  4.5.  Given  are A  = "1  0' 1  1 .1  2. Example 4.4. The matrix A has three eigenvalues Ai  =  0 A2 -1 -2 1 1 1 -1 0 1 0 B  =  and C  =  [01 0]  as in ^ - 1  and A3 =  - 2 with 274 Linear  Systems corresponding right eigenvectors vi  =  [111]^ V2 =  [10 - 1 ] ^ V3 =  [11 - 1 ]^ and with  left  eigenvectors  vi  =  [^0  ^]V2  =  [1 ~ 1  0]  and  V3  =  [-^  1 - 5 ]  respec tively. In view of Corollary 4.6 viB  =  [11]  7^ 0 implies that Ai  =  0 is controllable. This is  because  vi  is  the  only  nonzero  vector  (within  a multiplication  by  a nonzero  scalar) that satisfies  vi(Ai/  -  A)  =  0 and so Vi5  7^ 0 impUes that the only  1 x3  vector a  that satisfies a[\il -  A B]  =  0 is the zero vector which in turn imphes that Ai is controllable in  view  of  (i) of Lemma  4.6. For  similar reasons  Cvi  =  1 7^ 0 imphes  that  Ai  =  0  is observable; see (ii) of Lemma 4.6. Similarly V2B =  [0 - 1]  7^ 0 implies that A2  =  -1 is controllable and  Cv2  =  0 implies that A2 =  -1  is unobservable.  Also v^^B  =  [0 0] implies  that  A3  =  -2  is  uncontrollable  and  CV3  =  1 7^ 0  implies  that  A3  =  -2  is observable. These results agree with the results derived in Example 4.4. • COROLLARY  4.7. (RANK  TESTS)  (ia) The pair (A B) is controllable if and only if rank  [A/  -  A 5]  =  n for all complex numbers  A or for all n eigenvalues  A/ of A. (ib) Xi is an uncontrollable eigenvalue of A if and only if (iia) The pair (A C) is observable if and only if rank  [A//  -  A 5]  <  n. rank XI  -  A C for all complex numbers  A or for all n eigenvalues A/. (iib) Xi is an unobservable eigenvalue of A if and only if rank \XiI  -  A] C <  n. (4.25) (4.26) (4.27) (4.28) Proof.  The  proofs  follow  in  a  straightforward  manner  from  Theorem  4.4.  Notice  that the only values of A that can possibly reduce the rank of [XI -  A B] are the  eigenvalues ofA. • EXAMPLE  4.6.  If  in  Example  4.5  the  eigenvalues  Ai A2 A3 of A are known  but  the corresponding  eigenvectors  are not consider the system  matrix P(s)  = si  -  A  B 0 -C s -1 0 1 s  + 2 -1 -1 -1 -1 s+\ 1 1 1 0 1 2 0 0 0 and determine  rank  [XJ  -  A B] and  rank Xil  -  A C . Notice that rank si  -  A C 5 =  ^2 =  rank and rank  [si  —  A B]s=x^ =  rank -1 -1 0 0 -2 -1 0 1 1 -1 1 2 0 -1 -1 -1 -1 =  2<3  =  n =  2<3  =  n. In view of Corollary 4.7 A2 =  -1  is unobservable and A3 =  -2  is uncontrollable. Verify  that these are the only uncontrollable  and unobservable eigenvalues by apply ing the rank tests of Corollary 4.7. Compare these results with the results in Example 4.5. E X A M P LE  4.7.  Let  A and B with  A 1  the  eigenvalues of A. We would like to determine  which  of the eigenvalues  are uncontrollable. Note that (A B) is in the standard form for uncontrollable  systems of Lemma 4.1 namely. Ai 0 An Ai . We know by inspection that the eigenvalue A2 =  1  of A2 is uncon 275 CHAPTERS: Controllability Observability and Special Forms U z -l [0 -1 11 A -  1  oj and for V2 =  [0 1] V2[A2/ trollable. Presently [A// -  A B] A B] =  [0 0] which in view of Theorem 4.4 and Corollary 4.6 imphes that A2 =  1  is an uncontrollable eigenvalue and that V2 =  [0 1] is the corresponding left  eigenvector. Note that V2  is of the form  [0 a] as discussed in the proof of Theorem 4.4. The other eigenvalue Ai  =  1 is controllable. It is the eigenvalue of Ai  =  1 where (Ai Bi)  is controllable. Note that the corresponding  eigenvector to the controllable eigenvalue is vi  =  [01] the same as V2- Therefore when using the eigenvalue/eigenvector tests one can detect in the present example only that at least one of the multiple eigenvalues is uncontrollable; this test is unable to detect that the other eigenvalue is controllable. This situation arises when there are multiple eigenvalues in which case care should be taken when using the eigenvalue/eigenvector  tests. When the eigenvalues  are distinct then each of them can specifically be identified as being controllable or uncontrollable by the eigenvalue/eigenvector test. For another example try A =  1  0" 0  1. and 5  = 1' 0. C.  Relating  State-Space  and Input-Output  Descriptions The system x  =  Ax-^  Bu y  =  Cx-^  Du  given in (4.1) has pX  m transfer  function matrix H(s)  =  C(sl  -  A)-^B  + D  =  C(sl  -  Ay^B  +  A (4.29) where {A B C D} is the equivalent representation  given in (4.9) (see also  Sections 2.5 and 2.6). Consider now the Kalman  Decomposition  Theorem  and the  represen tation (4.22). We wish to investigate which of the submatrices A/y Bi Cj  determine H(s)  and which do not. The inverse of si  -  A  can be determined by repeated  appli cation of the  formulas and -1 0  8 -1 la  0" [y  8 a 0 § -1 a ya  -1 0 (4.30) where  a  (5 7 8  are  matrices  with  a  and  8  square  and  nonsingular.  It  turns  out (verify)  that H{s)  =  Ci(sl  -  Aii)"iJ5i  +  A (4.31) that  is  the  only  part  of  the  system  that  determines  the  external  description  is {All Bi Ci D}  the  subsystem  that  is  both  controllable  and  observable  [see  The orem  4.3(iii)]. Analogous  results  exist  in  the  time  domain.  Specifically  taking  the 276 Linear  Systems inverse Laplace transform  of both  sides in  (4.29) the impulse response  of the  system for  r  >  0  is  derived  as  (see  Chapter  2) H{t  0)  =  de^'^'Bi +  D8{t) (4.32) which  depends  only  on  the  controllable  and  observable  parts  of  the  system  as  ex pected. Similar  results  exist  for  discrete-time  systems  described  by  (4.4). For  such  sys tems  the  transfer  function  matrix  H{z)  and  the  pulse  response  H{k  0)  (see  Chap ter  2)  are  given  by H{z) Cx{zI-Axi)-^Bi+D and H{k  0) D k>0 k  =  0 (4.33) (4.34) These  depend  only on the part  of the  system that is both controllable  and  observable as  in  the  continuous-time  case. EXAMPLE  4.8.  For the system x  =  Ax  + Bu y  =  Cx where A B C are as in Exam ples 4.4 and 4.5 we have i/(5)  =  C{sI-A)-^B =  (1)(1/^)[11]  = [l/s  l/s].  Notice  that  only  the  controllable  and  observable  eigenvalue  of  A Ai  =  0 (in  All)  appears  in  the  transfer  function  as  a  pole.  All  other  eigenvalues  (A2  =  - 1 A3 =  - 2)  cancel out. • =  Ci(sI-An)~^Bi EXAMPLE  4.9.  The  circuit  depicted  in Fig.  3.5  is  described  by  the  state-space  equa tions 1 (RiC) 0 0 L Xi(t) X2(t) 1 (RiC) 1 v(0 i(t)  = 1 Ri 1 Xi(t) MO where the voltage v(t)  and current  i(t)  are the input  and output variables  of the  system xi(t)  is the voltage across the capacitor and X2(t) is the current through the inductor. We have  i(s)  =  H(s)v(s)  with the transfer  function  given by His)  =  C(sl  -  AT^B  +  D  =  (^'.C-L)s^(R-R2) ^ ^ ^ ^ (Ls  + R2){R\Cs  + Ri) ^  1 Ri The  eigenvalues  of  A  are  Ai  =  -l/(RiC) and  A2 -R2IL.  Note  that  in  general rank  [A//  —AB]  =  rank [A//  -  A C =  2  =  n  i.e.  the  system  is  controllable  and /•(O ^2(0 v{t) ^i(0 FIGURE  3.5 observable unless the relation  R1R2C  =  Lis  satisfied.  In this case Ai and the system matrix  P(s)  assumes the  form P(s)  = si  -  A  B D -C = '^T 0 ^ .+  ^ In the following  assume that R1R2C  =  Lis  satisfied. -k -(i) Let Ri  7^  R2 and take Ri-L 1 1 1 A2 -Ri/L 277 CHAPTERS: Controllability Observability and  Special Forms L^h V2I  — R2  Ri .1 1  . ' -  rvi I M 1 ~^  — i<2 -~Ri 1 1 -R R2. to  be  the  linearly  independent  right  and  left  eigenvectors  corresponding  to  the  eigen values Ai  =  A2  =  -R2IL.  The eigenvectors could have been any two linearly  indepen dent  vectors  since  XJ  -  A  =  0.  They  were  chosen  as  above  because  they  also  have the  property  that  V2^  =  0  and  Cv2  =  0  which  in  view  of  Theorem  4.4  implies  that A2  =  -R2IL  is both  uncontrollable  and unobservable.  The  eigenvalue  Ai  =  -R2IL  is D  1 r D both controllable and observable since it can be seen using 2  =  K ^\x.o  reduce the representation to the canonical structure form (Kalman Decomposition Theorem)  (verify this). The transfer  function  is in this case given by {s +  RxlL){s  +  R2IL) Ri(s  +  R2/L)(s  +  R2/L) Ri(s  +  R2/Ly s  +  RilL His) that  is  only  the  controllable  and  observable  eigenvalue  appears  as  a  pole  in  H(s)  as expected. (ii) Let Ri  =  R2  =  R and take Lvi V2J [VlV2]  ^  = In  this  case  viB  =  0  and  Cv2  =  0.  Thus  one  of  the  eigenvalues  Ai  =  -RIL is  un controllable (but can be shown to be observable) and the other eigenvalue A2  = -RIL is unobservable  (but can be  shown  to be  controllable). In the present  case none  of  the eigenvalues  appear in the transfer  function.  In  fact. His) =  1. as can readily be verified. Thus in this case the network behaves as a constant resistance network. At this point it should be made  clear that the modes  that are uncontrollable  and/or unobservable  from  certain  inputs  and  outputs  do  not  actually  disappear;  they  are  sim ply  invisible  from  certain  vantage  points  under  certain  conditions.  (The  voltages  and currents  of this  network  in the  case  of constant  resistance  [H{s)  =  l/R]  are  studied  in Exercise 3.26.) • EXAMPLE 4.10.  Consider the system i  =  Ax+Buy  =  Cx where A 1 0 -2 0 0 0 0 0 -1 B  = and  C  =  [1 10].  Using  the  eigenvalue/eigenvector  test  it  can  be  shown 278 Linear Systems (verify this) that the three eigenvalues of A (resp. the three modes of A) are Ai  =  1 (resp. e^) which is controllable and observable A2 =  -2  (resp. e'^^) which is uncontrollable and observable and A3 =  -1  (resp. e^^) which is controllable and unobservable. The response due to the initial condition x(0) and the input u(t) is x(t)  = e'''xiO)+  \ e''^'-^^Bu(T)dT 0 " 0 -t ft x(0)  + -  e«-T)  -0 U{T)  dr Jo _e-('--\ and KO  =  C^^'x(O)  +  Ce^^'~^^Bu(T)dr =  [e\e~^\Q]x{0)+\ e^'''h(T)dT. Notice that only controllable modes appear in e^^B [resp. only controllable eigenvalues appear in (si  -  A)~^5] only observable modes appear in Ce^^ [resp. only observable eigenvalues appear in C(5/ - A) ~ ^ ] and only modes that are both controllable and observ able appear in Ce^^B [resp. only eigenvalues which are both controllable and observable appear in C{sl -  A)~^B  = H{s)]. For the discrete-time case refer to Exercise 3.17d. • D.  Controller  and  Observer  Forms It has been seen several times in this book that equivalent representations of systems given by the  equations =  Ax  +  Bu y  =  Cx  +  Du X =  Ax  +  Bu y  =  Cx  +  Du (4.35) (4.36) where  x  =  PxA  =  PAP-\  B  =  PBC  =  CP-\  and D  =  D may offer  advan tages  over the original representation  when P  (or Q  =  P~^)  is chosen in an appro priate manner. This is the case when P (or Q) is such that the new basis of the  state space  (see  Section  2.2)  provides  a natural  setting  for  the properties  of interest.  As a  specific  case  refer  for  example  to  Subsection  3.4A  where  Q  (and  the  new  ba sis) was chosen  so that the controllable and uncontrollable parts of the system  were separated. The same results of course apply to discrete-time systems (4.4). This sub section  shows how to select  Q when  (A B)  is controllable  [or (A C) is  observable] to  obtain  the  controller  and  observer  forms.  These  special  forms  are  very  useful especially  when  studying  state-feedback  control  (and  state observers)  discussed  in Chapter  4  and  in realizations  discussed  in  Chapter  5. These  special  forms  are  also very useful  in establishing  a quick way to shift  between  state-space  representations and another very useful  class of equivalent internal representations the polynomial matrix representations  studied in Chapter 7. Controller forms  are considered first. Observer forms  can of course be obtained directly  in  a  similar  manner  as  the  controller  forms  or  they  may  be  obtained  by duality. This is addressed in the latter part of this  section. 279 CHAPTER 3: Controllability Observability and Special Forms 1. Controller  forms The  controller  form  is  a particular  system  representation  where  both  matrices (A B) have certain  special  structure.  Since in this case A  is in the companion  form (see  Section  2.2  in  Chapter  2)  the  controller  form  is  sometimes  also  referred  to  as the controllable  companion form.  Consider the  system X =  Ax-\-  Bu y  =  Cx-\-  Du (4.37) where  A  G  /?^x^ B  G  R'''''^  C  G  7?^>'^ and  D  G  RP"""^  and  let  (A B)  be  control lable (-from-the-origin).  Then ran ^^  =  n where Assume that %  = [BAB...A''~^Bl rank  B  =  m  ^  n. (4.38) (4.39) Under these assumptions r a n ^^  =  n and rank  B  =  m. We will show how to obtain an equivalent  pair  (A B)  in controller  form  first  for  the  single-input  case  (m  =  1) and then for  the multi-input  case  (m  >  1). Before  this is accomplished  we  discuss two  special  cases  that  do not  satisfy  the  above  assumptions  that  rank  B  =  m  and that (A B) is controllable. 1.  If the m columns of 5  are not linearly independent  (rankB  =  r  <  m) then there exists an m X m nonsingular matrix K  (or equivently there exist elementary col umn operations) so that BK  =  [Br 0] where the r columns of Br are linearly in dependent (ran ^ 5^  =  r). Note that i  =  Ax  + Bu  =  Ax  + {BK){K~^u)  =  Ax^-[Br 0]  Ur same input action to the system can be accomplished by only r inputs instead of m inputs and there is a redundancy  of inputs which in control problems  clearly implies  that  a reconsideration  of  the  input  choices  is  in  order.  The pair  (A Br) which is controllable when (A B) is controllable (show this) can now be reduced to controller form  using the method developed  below. =  Ax  + BrUr which clearly shows that when rankB  =  r  <  m the Uyyi—y 2.  When  (A B)  is  not  completely  controllable  then  a  two-step  approach  can  be taken. First the controllable part is isolated (see Subsection 3.4A) and then is re duced to the controller form using the methods of this section. In particular con sider the system x  =  Ax  + Bu  with A G  R"^^"" B  G i^^X'" and rankB  =  m. Let rank  [B AB...  A^~^B]  =  nr <  n. Then there exists  a transformation  Pi  such that  PiAP-Ai 0 Al2 A2 and  PiB  = where  Ai  G  P^^^^^^ Bi  G P'^^^'^ and  (AiBi) is  controllable  (Subsection  3.4A).  Since  (Ai  Bi)  is  controllable there  exists  a  transformation  P2  such  that  P2A1P2 ^  =  Mc  and  P2B1  =  Bic where Aic Bic  is in controller form  defined  below. Combining we obtain and PiAn A2  _ -1  ^ PAp-'  = "B  = Ale 0 Bic 0 (4.40) 280 Linear Systems [where Axc ^  R'^'^'^'.Bic  G R'^rxm^  ^^^  (AicBic)  is  controllable]  which  is  in controller form. Note that P2  0 0 /  Pi-(4.41) Single-input  case  (m  =  1).  The  representation  {Ac^Bc^Cc^Dc}  in  controller form is given by Ac A = PAP-^  and Bc=  B = PB with 0 -oco 0 -ai 1 -OCn-l Br (4.42) where the coefficients  ai dire the coefficients  of the characteristic polynomial  a{s)  of A that is a{s)=  det  {si -  A)  = s"" ^  an-is""'^  ^ h^i^ +  ao- (4.43) Note  that  Q  =  C =  CP~^  and  Dc =  D  do  not  have  any  particular  structure.  The structure  of  (AcBe)  is  very  useful  (in  control  problems)  and  the  representation {Ac^Bc^Cc^Dc} shall  be  referred  to  as  the  controller form  of  the  system.  The  sim ilarity  transformation  matrix  P  is  obtained  as  follows.  The  controllability  matrix ^  =  [B^AB...  A^  ^5] is in this case minxn  nonsingular matrix and  ^ where q is the nth row of ^ and  x  indicates the remaining entries of "^ . Then qA qA n-\ (4.44) To show  that PAP~^  = Ac  and PB  = Be given  in  (4.42) note first that  qA'~^B  =  0 / =  1...  n-  1 and qA^'-^B =  1. This can be verified  from the definition  of q which implies that q^  =  [00... 1] (verify  this). Now P^  =  P[5A5...A^"^5] "0 0 1 0 0 1 X •• •• r X 1 X X •  ^ c-(4.45) which  imphes  that  | P ^|  =  | P | | ^|  7^ 0  or  that  |P|  ^  0.  Therefore  P  quaUfies  as a  similarity  transformation  matrix.  In  view  of  (4.45)  PB  =  [00...!]^  =  Be. Furthermore AcP qA n-l qA qA^ PA. (4.46) where in the last row  of  A^P the relation  -  X/^=o ^/^^  ^  ^"  was used  [which  is the Cay ley-Hamilton  Theorem namely a(A)  =  0]. EXAMPLE4.il.  Let A  = 1 0 0 0 1 0 0" 0 - 2. and  B  = "  1" -1 1. Since n  = 3 and l^-/ -  A|  =  (^ +  1)(5' -  l)(s + 2)  =  s^ +2s^ -  s -2  {Ac Be} in controller form is given 281 CHAPTER 3: Controllability Observability and Special Forms by A  = 0 0 2 1 0 1 0 1 -2 and Br  = The transformadon  matrix P that reduces  (A B) to (Ac  =  PAP  ^Bc  =  PB) is now derived. We have =  [B AB A^B] =  1 -1 n -1 1 -1 -2 and -1  _ 1  -I 2 2 The third (the nth) row of^  ^  is q  =  [-  ^ -  ^ ^] and therefore P^ qA qA^ 1 2 i 2 L i 3 2 3 4 3  J It can now easily be verified that Ac  = PAP  ^ or AcP =  PA and that  Be  =  P5.  [Verify  that  P^  = %c  is given by  (4.45)  and also compare with • Exercise 3.23 which explicitly derives ^cJ An alternative form  to (4.42) is -Oin-\ 1 Aci  = -ax 0 - ao 0 0 ••• 1 0 ^cl -(4.47) which is obtained if the similarity transformation  matrix is taken to be r^A^-i" A P^  = qA (4.48) i.e. by reversing  the order of the rows of P in (4.44). The reader  should verify  this (see Exercise  3.25). 282 Linear Systems In the above Ac is a companion matrix of the  form  0 X x" 0  and Br has  the  form  [0  0 . ..  1]^ or  [1  0 . ..  0]^  respectively.  These  representations  are very  useful  when  studying  pole  assignment  via  state feedback  control  law  and  are used in the next  chapter. / X  or 'x / A companion  matrix  could  also be of the  form  "0 / x' X or  'x X 0" / (see  Section 2.2) with the coefficients  -[ao... c^n-iV  in the last or the first column. It is shown here for  completeness how to determine  controller  forms  where Ac  are such  com panion matrices. In particular if G2  =  Pi^  = [BAB... A"-^5]  = "0  ••• 1 ••• 0 0 - ao 0 ••• 1 then Ac2 =  Qi'^Qi = Also if Bc2 =  Q2'B  = Q3  =  P3' = [ A " ~ ' B • .Bl - a „ -i 1 ...  0" then Ac3 -  Q^'AQ^ = 0 0 ...  1 ...  0 - ao Bc3 =  Q^'B  = (4.49) (4.50) (4.51) (4.52) '1 0 0 "0 0 1 (Ac Be)  in  (4.50)  and  (4.52)  are  also  in  controller  canonical  or  controllable  com panion  form.  (The reader  is encouraged  to verify  these expressions.  See also  Exer cise  3.25.)  We also  note that  if  the  structures  of Ac  and  Be  are  specified  then P  is uniquely determined  (see Exercise 3.24). That is given (A^ Be) in any of the above four  controllable  companion  forms  P  is readily  uniquely  determined  in  each  case by  P  =  ^ ^ "^  assuming  that P  also  satisfies  PA^B  ^  A^Bc  (in view  of  Exercise 3.24) which it does in the above four cases. If different  Be are desirable then an ap propriate P can be found by the same formula. Note that ^  denotes the controllability matrix of (Ac Be). EXAMPLE  4.12.  Let  A  = r -1 0 0 0 1 0 0 0 -2 and 5  =  as in Example 4.11. AI-temative controller forms can be derived for different P. In particular if \qA^ (i)P  = P  =  \ qA L q  J in Example 4.11) then 1 2 1 2 1 2 1 6 1 6 1 6 4-1 3 2 3 1 3-1 as in (4.48) {%^ ' and q were found -2  1  2"  1  0  0 0  1  0_ Be - ] 1 2 1 -2 1 6 1 6 1 6 8 3 4 3 2 3 PiA 283 CHAPTERS: Controllability Observability and Special Forms as  in  (4.47).  Note  that  in  the  present case  AciPi  =  as in (4.49). Then Bel  =  PiB. (ii) 22  =  ^  = " 1 -1 -1 -1 . 1 -2 "0 1 .0 Ac2  = r -1 4. 0 0 1  -2" 1 -2. > as in (4.50). (iii)e3  =  [A^BABB]  = "  1 -1 4 Ac3  = "-2 1 2 -1 -1 -2 1 0 0 as in (4.52). Note that gsA^s  = -1 -1 . -8 1 -1 4 Bc2  =  Q2'B  = ri] 0 LOJ 1] -1  as in (4.51). Then 1. 0" 1 0. - 1" -1 - 2_ Bc3 -roi  0 [ij =  AGs  Q3Bc3  = r  1" -1 L  1. = Multi-input  case  (m  >  I).  In  this  case  the  n  X mn  matrix  % given  in  (4.38) is not  square and there  are typically  many  sets of n columns  of % that are  linearly independent  (rank%  =  n).  Depending  on which  columns  are  chosen  and  in  what order  different  controller  forms  (controllable  companion  forms)  are  derived.  Note that  in  the  case  when  m  =  1  four  different  controller  forms  were  derived  even though there was only one set of n linearly independent columns. In the present case there  are  many  more  such  choices.  The  form  that  will  be  used  most  often  in  the following is a generalization of (A^ Be) given in (4.42) and this is the form that will be derived first. Other forms  will be discussed  as well. Let A  =  PAP~^  and B  =  PB  where P is constructed  as follows:  consider =  [bu  .bm  Ah . . . Abn  . . . A^-'b . . .  A^-'bml (4.53) where the bi.. .bm  are the m columns of 5. Select starting from the left and moving to the right  the first n independent  columns  {rank  %  =  n). Reorder these  columns by  taking first Z?i Ab\  A^bi  etc. until  all columns  involving  bi  have been  taken; then take Z?2 Ab2 etc. and lastly take bm Abm etc. to obtain %  =  [biAbi...Af''-^bi...bm-.^Af^^-^bml (4.54) an n  X ^ matrix. The integer  ixi denotes the number of columns involving  bi in the set of the first n linearly independent columns found  in % when moving from  left  to right. DEFINITION  4.1.  The m integers  /Xj /  =  1... m are the controllability indices of the system  and ^x = max fxt is called the controllability index of the system. Note that 284 Linear Systems E M/ and mil  >  n. (4.55) To illustrate the significance  of ^i note that an alternative but equivalent  defini tion for  IX  is that  /x is the minimum integer k such that rank[BAB...A''''B] = n. (4.56) Taking  this  view  one  keeps  adding  blocks  B  AB  A^B  etc.  until  n  independent columns appear (from left to right) for the first time. It is then not difficult  to see that /x  =  max  fjLi. Alternatively  jm  can be defined  as the least integer  such that rank  [B AB...  A^'^B]  =  rank  [B AB... A^B]. (4.57) Notice  that  in  (4.54)  all  columns  of B  are  always  present  since  rank  B  =  m. This  implies  also  that  /uLi  >  1 for  all  /. Notice  further  that  if  A^bi  is present  then A^~^bi  must  also be present  in  (4.54). For  if  it were not i.e.  if  it were  dependent on the previous  columns in % and had been  eliminated  then A^bt  would  also  have been dependent (write A^'^bi  as a linear combination of previous columns in ^  and premultiply by A to show this). Column A^^'bi is of course dependent on the previous ones. This relation can be expressed  explicitly  as m  mmiixuixj) A^'bt  =X y -i X k=i i-\ ^iJkA'-'bj  +  X 7 =1 ^ij^^'bj (4.58) Note  that  the  first  sum in  (4.58)  indicates  the dependence  of A^'bi  on the  linearly independent  columns  in  B AB.. .A^''^B  while  the  second  sum  shows  the  de pendence on the independent  columns in Af^'B  to the left  of A^'bi  (aijk  and pij  are appropriate reals). Now  define k a^  = XfJiu k  =  l...m / =  i (4.59) i.e. cTi  == fjLi cr2  =  ^ll  +  fJi2y...  o"^  =  /xi  +  • • • +  ^tm  — ^'  Also consider %~^ and let q^ where q^  G R^ k  =  1...  m denote its akth  row i.e. ^ -1  =  [X xqY:-- :X  X qlV. (4.60) Next  define qiA p^ (4.61) qmA [qmAf^' i -i 285 CHAPTER 3: Controllability Observability and Special Forms It can now be shown that PAP  ^  =  Ac and  PB  =  B^ with Ac  =  [Aijl ij  =  1...  m 0 0 X 0 0 X Bi B2 AH  = Aij and Be  = I f J L i -l Rf^iXf^i^  i  =  j^ X X R^^^'^J i  #  j X 0  0 Bi  = 0 0  1  X X J^^.xm^  (4.62) where  the  1 in  the  last  row  of  Bi  occurs  at  the  /th  column  location  /  =  1...  m and  X denotes nonfixed  entries. Note that  Q  =  CP~^  does not have any particular structure. The expression (4.62) is a very useful  form (in control problems) and shall be referred  to  as the  controller form  of  the  system.  The  derivation  of  this  result  is discussed below. First examples  are given to illustrate the procedure involved  and then  some alternative expressions  and properties  are also  discussed. EXAMPLE 4.13.  Given are A G R'''^'' and B E  R""^"^ with (A B) controllable and with rank B  = m. Let n  = 4 and m  = 2. Then there must be two controllability indices  ii\ and 1X2 such that n  = 4  =  Sf= i i^/  =  Mi + M2- Under these conditions there are three possibilities: (i)  fii  =  2  /X2  =  2 Ar  = All An A21  A22. (ii)  /xi  =  1  /X2  =  3 0 X 0 X 1 X 0 X : 0 :  X : 0 :  X 0 X 1 X Br  = Bi] Bi. 0 1 0 0 0 X 0 1 X XXX Ac  = 0 0 1 0 0 1 XXX "  1 X  ' Br  = 0 0 0 0 0 1 286 Linear  Systems (iii)  ixx  =  3 /X.2 =  1 1 0 X 0 1 X Ac X X Br  = "0 0 1 0" 0 X 0 1 It  is possible  to  write  Ac  Be  in  a  systematic  and  perhaps  more  transparent  way. In  particular  notice  that  Ac  Be  in  (4.62)  can  be  expressed  as Ac  — Ac  +  t>cAyi B  — Jjctjfyi (4.63) where  Ac  ^  blockdiag [An  A22....  Amm\  with [o An  = //..-I 0 0  0-•0 Be  =  block  diag R^^^'K i  = \..m and Am  G  R^^^  and  Bm  G  R^^^  are  some  appropriate  matrices  with  ^ ^^  ^ /x/  =  n. Note  that  the matrices  Ac  Be  are  completely  determined  by  the m  controllability  in dices  iJLi  i  =  1  . . .  m  (they  are  in  fact  in the  so-called  Brunovski  canonical  form— see  the  discussion  following  Lemma  4.8).  The  matrices  Am  and  Bm  consist  of  the (Tith (T2th...  cr^th rows  of A^  (entries  denoted  by  X)  and  the  same rows  ofBc  re spectively.  Note that Am  and  Bm  is that part  of the controllable  system  x  =  Ax  +  Bu that  can  be  altered  by  linear  state  feedback  u  =  Fx  +  Gv  a  fact  that  will  be  used extensively  in  the  next  chapter. EXAMPLE  4.14.  Let  A  = troller form  (4.62) consider = [BABA^B] 1 0 2 0 1 -1 and  B  = "0  r 1  1 .0  0. .  To  determine  the  con-=  [bxh2 Abu  Ab2 A^bx A^b2\  = 1 0 2 where rank^  =  3  =  n i.e. (A B) is controllable. Searching from  left to right the first three columns of ^  are selected  since they are linearly independent.  Then ^  =  [bi Abi  Z72] - ro  1  1" 1  0  1 lo  2  OJ and the controllability  indices  are ^ci jjii  + iJi2  =  3  =  n and 2 and  ^2  =  1- Also ai  =  /JLI  =  2 and (T2 " -1 0 1 1 0 0 1 2 1 2 1 2 Notice that q\  =  [0 0 \]  and q2  =  [1 0 - 1 ] the second and third rows of ^ \  respec-287 tively. In view  of  (4.61)  P  = PAP~ q\A L  qi 0 2 1 ro 0 1  -2 1 2 1 2-J P~'  = 1  0  1" 1  1  0 .2  0  0. and Ar  = CHAPTER 3: Controllability Observability and  Special Forms *c =  PB  = B{ B2\ — 0 1 0 0  ' 1 1 One can also verify  (4.63) quite easily. We have Ar  +  BrAvy : : 0 0 + o] 0 1J 0 1 0 -1 0 and 0 1 0 o" 1 1 0 BcBtr 0 : "o 1 0 ol 0 0 1J ri  1" [o  1. It is interesting to note that in this example the given pair (A B) could have already been  in  controller  form  if B  were  different  but A  the  same. For  example  consider  the following  three cases: '  1 X 0 0 0 1 0 0 1 o' X 1 ^x  = \  1X2  =  2 /xi  =  2 ^1  =  1 1.  A  = B  = 0 0 1 0 0 1 2.  A 0 2 -1 0 0 0 1 0 0 1 2 -1 3.  A  = B  = /^i Note that case 3 is the single-input  case (4.42). • Several results  involving  the controllability  indices  of  (A B)  are now  presented. First  it  is  shown  that  the  controllability  indices  /x/  of  a  system  defined  in  Defini tion  4.1 do  not  change  under  similarity  transformations  [for  if  they  were  changing then  (Ac  Be)  might  have  different  controllability  indices  from  the  original  (A  B)]. 288 Linear Systems In particular let x  =  Ax  -\-  Buhe  given where  (A B)  is controllable and  consider x  =  Ax  -i-  Bu  where A  =  PAP~^  B  =  PB  x  =  Px  and P is  nonsingular. LEMMA 4.8.  The controllability  indices  of  (A B) are identical  to the  controllability indices of A =  PAP-\  B  =  PB. Proof. We have ^  =  [B AB...  A^-^B]  =  p-^[B  AB...  A^'^B]  =  p-^^. To de termine the controllability indices  /JLI of (A B) the first n linearly independent columns of ^  are taken working from left to right. It is clear that the corresponding n columns of ^  will also be linearly independent since they are the n columns of ^  each premulti-plied by P. Furthermore as one checks linear dependence of columns in ^ from left to right if a column is dependent on the previous ones then the corresponding column in ^  will also be dependent on the previous ones in ^  (show this). Therefore the linearly independent columns generated by this left to right search are exactly the same in ^  and ^  and therefore the controllability indices are identical. • Note  that  the  controllability  indices  of  (A BG)  where  G  is  nonsingular  are equal to the controllability indices /x/ of (A B) within reordering (see Exercise 3.20). Note also that when  linear  state feedback  u  =  Fx  -\-  Gv  with  |G|  T^ 0 is applied  to X =  Ax  -^ Bu  (see Exercise 3.21) then the controllability  indices of (A +  BF  BG) are equal  to the controllability  indices  of  (A B)  i.e. the controllability  indices  are invariant under linear state feedback.  These results can be summarized  as  follows. Given  (A B)  controllable  then  (P(A  +  BGF)p-\  PBG)  will  have  the  same controllability  indices within  reordering  for  any P  F  and  G (\P\  T^ 0 \G\ T^  0) of appropriate dimensions. In other words the controllability  indices  are invariant  un der similarity  and input transformations  P and G and state feedback  F [or similarity transformation  P  and  state feedback  (F G)]. Furthermore  it can  be  shown  that  if two pairs (A/ Bt)  i  =  \ 2 have the same indices then there must exist P G and F such that Ai  =  P(A2+ ^2GF)P"^  and 5i  =  PB2G. This in fdidhXh^  completeness property  which together  with the invariance property  implies  that the  controllabil ity  indices  {/x/}  constitute  a set  of  complete  invariants  for  (A B)  under  operations P  G and F Using  (4.63) it is not difficult  to see that given any  (A2 B2) with con trollability  indices {/x/} there exist_P G F  such that Ai  =  P(A2  +  B2GF)P~^  and Bi  =  PB2G  are  exactly  equal  to  Ac  =  Ai Be  =  B\.  The  pair  {Ac Be)  is  unique and  since  any  controllable  pair  (A B)  with  controllability  indices  {^t/}  can  be  re duced  to  (Ac Be)  by  these  operations  {Ac Be)  is  a  canonical  form  of  such  (A B) under these operations. The pair {Ac Be) is called the Brunovsky  canonical form.  It should  also be  mentioned  here  that  the {jUi}  are the right  Kronecker  indices  of  the pencil  [si  -  Ac Be].  (Recall  that  [si  -  Ac Be]  =  s[I 0]  -  [Ac -Be]  =  sE  -  A a  matrix  form  called  a linear  matrix  pencil.)  The  derivation  of  A^ Be  in  (4.62)  is discussed  next. The exact  structure  of Ac  and  Be depends  on the  selection  of the n linearly  in dependent columns in % and on the choice of the equivalence transformation  matrix P.  The n linearly  independent  columns  in ^  were  selected by  a search from  left  to right; if a column  was found  dependent  on previous  ones then it was dropped.  The dependence  relation  is  given  in  (4.58). This  relation  together  with  the  expression for P  given in (4.61) are central in the derivation of A^ Be. In view of ^ - ^^  =  / qt^  -  qi[biAb...Af'^-'bu-.-.A^^~'bi...bm.....A^--'b^] ^^^^^ -  [0...  0 1 0  . . .  0] i  = l...m 289 CHAPTER 3: Controllability Observability and  Special Forms where  the 1 occurs  at the cr^th column.  These  relations  can be written  as qtA^'^bi  =  0 ^  =  1  . . .  /x/ -  1 and qiA^^-'bi - I (4.65) where /  =  1  . . .  m and j  =  1  . . .  m. In view of (4.64) (4.65) and the dependence relations  (4.58) it can be shown that P^  is nonsingular  which in view of the fact  that 1^1 7^ 0  implies  that  \P\ T^ 0  as well  i.e. P  qualifies  as a  similarity  transformation. We  note  that  the proof  of  this  result  is  rather  involved  (see Section  3.7 Notes  for appropriate references). In view of the relationship  PA  =  AcP  it is now not  difficult to  see  that  n  -  m  rows  of  Ac  will  contain  fixed  zeros  and  ones  as  in  (4.62).  The (m)  (Tith  cr2th...  cr^th  rows  of Ac  that  are denoted  by Am in (4.63)  are given by qiA^' ^rn  — P-\ qmAf^^ (4.66) where  Ac  =  Ac  + Be Am.  Similarly  in view  of the equality  Be  =  PB  and (4.63) it is  not difficult  to see that  n  -  m  rows  of Be  must  be zero. The (m) remaining  crith 0-2th...  amth  rows  of Be  that  are denoted  by Bm in (4.63)  are given  by qiA^'-^ Bm — B qmA^^ (4.67) where  Be  =  BcB  The  matrix  Bm in  (4.67)  is in  fact  an  upper  triangular  matrix with  ones  on the diagonal.  This  result  follows  from  the special  form  of P^  that was used  above  to show  that P is  nonsingular. At  this  point  it may perhaps  be beneficial  to examine  a  special  case  that  is not only interesting but will  also provide  some indication  of the type of proof  required to establish  these  results.  In particular  it will  be  shown  that  when  jui  <  ^l2 —  * *'  — fjLm the upper  triangular  matrix  Bm in (4.67)  is in fact  diagonal. LEMMA  4.9.  If/Xi  <  ^l2 diagonal i.e. Bm  =  Im-fjLm then Bm in (4.67) is diagonal with ones on the Proof  The matrix  Bm in  (4.67)  is  upper  triangular.  When  in  addition  /xi  <  )Li2  ^ •••  <  fjLm. then  qiAf^^-^B  -  ^iA^i-i[Z?i Z?2 • •- W  =  [1 0  . . .  0] in viewof  (4.65); in particular  qiA^^~^bi  =  1 and q\Af^^~^b2  =  0 since q\A^~^b2  =  0 k  =  1... JLXI and  /xi  <  ^l2. Similar  statements  hold  for  the columns  bs b^ ...ybm-  The proof  for ^2^^2-15^... qmA^^'^'^B is completely  analogous. • Note  that  if  different  relations  among  the controllability  indices  /x^ exist  then different  entries  of Bm (above  the diagonal)  will be zero. EXAMPLE 4.15.  We wish to reduce A  = 0 0 0 1 0 2 0" 1 - 1_ andB  = 1  r 0  1 _0  0. to controller form.  Note  that A  and B are almost  the same  as in Example  4.14; however  presently 1  <  2  =  /X2 as  will  be  seen.  We  have  %  =  [B AB A^B]  =  [bu bi Abu Ml 290 Linear  Systems Abo 1 1 10 0 0  10 0  0  0  2 . Searching from  left  to right the first three linearly in dependent columns are bi  b2 Ab2 and ^  =  [bi b2 Ab2]  =  from which we 1  1  1" 0  1  0 0  0  2. [1  -1 1 0 .0 0 -^1 0 |. conclude that jLii  =  1 fjL2  =  2ai  =  1 and 0-2  =  3. We compute pute^^i  -Note that  qi  =  [1 - 1 -  ^]  and  ^2  =  [0 0  ^]  the first and third  rows of ^ ~\  respec tively. Then p  = qi  ' qi qiA. -"1 0 .0 -1 0 1 "1 0 .0 2  r 1  1 2  0. Ac  =  PAP-^ A21 ^22 Bi] = 52J "  1 0 " 0 0 0 1 c  =  PB = and •1 0 0 2 qi qiA Presently  (4.67)  yields  5^ B  and equals I2 as expected  in view  of the  above lemma  (JJLI <  1^2)- In general Bm is an upper triangular  matrix. For the present example we ask the reader to also verify  relations (4.65) determine • Am by (4.66) and compare these with the above results. In  the  case  of  single-input  systems  m  =  1  ^tl  =  n  and  P  in  (4.61)  is  exactly the transformation  matrix given in (4.44)  (verify  this). In the case m  =  1 if the  order of rows in P is reversed  and  Pi  in (4.48) is used instead then an alternative  controller form  is  obtained  shown  in  (4.47).  Similar  results  apply  when  m>  1: if  the  order  of the  rows  of P  in  (4.61)  is reversed  within  the  fit  X  n  blocks  then qi Pi (4.68) in  which  case  Ad  =  P\AP^  ^Bc^  =  P\  and  B  are  given  by Aci  =  [Aijl ij -  1  . . .  m X 0 /?MXM^ j  =  j^ X 0 R^^iX^^j^  i  ^ j^ 291 CHAPTERS: Controllability Observability and Special Forms Aii  = X X V-i Aij  = X 0 0 "fii and Bel  = Bi  = 0 0 0 •••  0  1  X •••  0  0  0 •••  X ••• 0 0  0  0 0 J^t^i (4.69) where  the  1 on  the  first  row  of  Hi occurs  at the  iih  (i  =  1...  m) location  and  X denotes nonfixed  entries. These formulas  are similar to (4.62) and can be derived in a completely  analogous  fashion. As  in  the  case  m  =  I  [see  (4.49)  to  (4.52)] the  columns  of  P~^  can  also  be selected to be n linearly independent columns of ^  in which case A  =  PAP~^  and B  =  PB  will be in  alternative  controller  forms.  There  are many  ways  of  selecting these  n  linearly  independent  columns  of  the  n  X  nm  matrix  ^  as  was  discussed before. For example one may search ^  from left to right as above or one may check for  linear  independence.  The  controller  forms  resulting  in bi Abi.. this way are generahzations of (4.50) and (4.52). These forms  are not as useful  to us and are omitted here. .b2  Ab2... Structure  theorem—controllable  version.  The  transfer  function  matrix  H{s) of the system x  =  Ax  + Bu  y  =  Cx  + Duis  given by H(s)  =  C(sl  -  Ay^B  +  D. If  (A B)  is  in  controller form  (4.62) then  H{s)  can  alternatively  be  characterized by the  Structure  Theorem  stated  in Theorem  4.10. This result  is very  useful  in  the realization of systems addressed in Chapter  5 and in the study of state feedback  in the next  chapter. Let  A  =  Ac  =  Ac  + Be Am and  B  =  Be  =  BcBm as in  (4.63) with  | 5 ^| T^ 0 and let C  =  Q  and D  =  Dc.  Define vf^l e ^2 A W^ S(s)  =  block  diag 1 s /  — 1... m Note  that  S{s)  is  an  n  X  m  polynomial  matrix  (n  =  Xf^i  i^O. i-^.  a matrix  with polynomials as entries. Now define the m X m polynomial matrix D(s)  and the /? X m polynomial matrix N(s)  by D(s)  ^  B-'[A(s) -  AmS(s)l N(s)  ^  CcS(s)  +  DeD(s). (4.71) The following  is the controllable version of the Structure  Theorem. (4.70) 292 Linear  Systems THEOREM  4.10.  H(s)  =  N(s)D~\s)  where N(s)  and D(s)  are defined  in (4.71). Proof  First note that (si  -  Ac)S(s)  =  BcD{s). (4.72) To see this we  write  BcD{s)  =  BcBmB;;^^[A(s)  -  AmS(s)]  =  BcA(s)  -  BcAmS(s)  and (si  -  Ac)S(s)  =  sS(s)  -  (Ac  + BcAm)S(s)  =  (si  -  Ac)S(s)  -  BcAmS(s)  =  BcA(s) BcAmS(sXwhichpro\ts Dc  =  [CcS(s)  +  DcD(s)]D-\s)  =  N(s)D-\s). -(4.72). NowH(s)  =  Cc(sl-  AcT^Bc  + Dc  =  CcS(s)D-\s)  + • EXAMPLE  4.16.  Let  Ar  = 0 2 -1 1 1 0 0 0 0. and  Be  =   as  in  Example  4.14. 0" 0. y  Bm  —  \ r^^  0 .ThenA(5)  =  r^ 0 s ro  01 1  1 [o  1. 1  11 0 i j' Here  /JLI  =  2 iui2  =  I  and A^  = ri  01 S(s)  =  \s 0   and ij LO D(s)  =  B-'[A(s)-AnS(s)] 2 1 -1 0 = 1 .0 "1.0 IJL -1 Now  Cc  =  [011]  and Dc  =  [0 0] N(s)  =  CcS(s)  +  DcD(s)  =  [s  1] -s + 2  0 0 1 I J [ LO S^ 's^  + s -  I -1 —s s and  H(s)  =  [sl] 5 ^ + 5 -1 -1 -1 --s S =  [s  1] s [I =  - j^ - [ 5^  +  12^^  +  5 - 1] s(s^  +  5 - 2) s 1 s^ + s  -  l\  s(s^  +  5 - 2) 2 EXAMPLE  4.17.  Let  Ac  = =  Cc(sl  -  AcY^Bc  +  Dc. "0 0 2 1 0 1 0 1 -2 Be  = Cc  = [010] and Dc  =  0 (see Example 4.11). In the present case we have A^  =  [21 - 2 ] B^  =  1 A(5)  =  s\S(s)  =  [l552]^and D(s)  =  1 . [5^ -  [21 -2][1  5 s^f]  =  5^ +  2 5 ^ - 5 - 2 N(s)  = 5. Then H(s)  = N(s)D-\s) +  2 5 ^ - 5 -2 =  Cc(sl  -  AcT^Bc  +  Dc. 2.  Observer  forms Consider  the  system  x  =  Ax  +  Bu  y  =  Cx  -\-  Du  given  in  (4.1)  and  assume that  (A  C)  is  observable  i.e.  rankG  =  n  where C CA CA' n-\ (4.73) 293 CHAPTERS: Controllability Observability and Special Forms Also assume that the p  X n matrix  C has a full  row rank p  i.e. rankC  =  p  ^  n. (4.74) Presently  it  is  of  interest  to  determine  a  transformation  matrix  P  so  that  the equivalent  system representation  {A^ Bo Co Do} with Ao PAP~\ Bo  =  PB Co  =  CP~\ Do  =  D (4.75) will  have  {Ao Co)  in  an  observer  form  (defined  below).  As  will  become  clear in  the  following  these  forms  are  dual  to  the  controller  forms  previously  dis cussed  and  can  be  derived  by  taking  advantage  of  this  fact.  In  particular  let A  ^  A^  B  =  C^  [(A B)  is controllable]  and  determine  a nonsingular  transforma tion  P  so that Ac  =  PAP~^  Be  =  PB  are in controller form  given in (4.62). Then Ao  =  A^  and  Co  =  B^  is in observer form.  In fact the equivalent representation  in this case is given by (4.75) where  P  =  (P^)~^  (show this). It will be demonstrated  in the following  how to obtain observer  forms  directly in a way that parallels the approach described  for  controller forms.  This is done  for the  sake  of  completeness  and  to define  the  observability  indices.  Our  presentation will  be  rather  brief.  The  approach  of  using  duality just  given  can  be  used  in  each case to verify  the results. We first note that if  rank C  =  r  <  /? an approach  analogous  to the case  when rank B  <  m can be followed  as above. The fact  that the rows  of  C are not  linearly independent means that the same information  can be extracted from  only  r outputs and  therefore  the  choice  for  the  outputs  should  perhaps  be  reconsidered.  Now  if (A C) is unobservable one may use two steps to first isolate the observable part and then reduce it to the observer form  in an analogous  way to the uncontrollable  case previously  given. Single-output  case  (p  =  I).  Let p -i  =  (2  A [q^Aq...A''-^q] (4.76) where q is the ^th column in 0~^  Then An  = 0 0 1 -ao -ai -OCn-\ Co  =  [0...01L (4.77) where  the  at  denote  the  coefficients  of  the  characteristic  polynomial  a(s)  = =  ^^ + a:„-i^'^"i + ---+ai^ + ao.HereA^  =  PAP'^  =  Q~^AQXo  = det(sI-A) CP'  ^  =  CQ  and the desired result can be established by using a proof that is com pletely  analogous  to  the  proof  in  determining  the  (dual)  controller  form  presented earlier  in  this  section.  Note  that  Bo  =  PB  does  not  have  any  particular  structure. The representation  {Ao Bo Co Do} will  be referred  to  as the  observer form  of  the system. Reversing the order of columns in P~ ^ given in (4.76) or selecting P to be exactly 0 or to be equal to the matrix obtained after  the order of the columns in 0 has been reversed leads to alternative observer forms  in a manner analogous to the controller form case. We leave it to the reader to investigate these possibilities  further. 294 Linear Systems EXAMPLE  4.18.  Let  A -1 0 0 0 0 1 0 0 -2 and  C  =  [1 -11].  To derive  the ob server form (4.77) we could use duality by defining  A  = A^  B  =  C^ and deriving the controller form of A B i.e. by following the procedure outlined above. This is left to the reader as an exercise. We note that the A B are exactly the matrices given in Exam ples 4.11 and 4.12. In the following the observer form is derived directly. In particular we have c CA CA\ = " 1 -1 -1 -1 . 1 -1 1" -2 4.  0-1  = 1 1 3 .-1 - 1 2 1 2 0 1 2 1 6 1 3 and in view of (4.76) .76) Q  =  p -i  =  [q^Aq^A^q]  = r L 1 2 1 6 1 3 1 2 1 6 2 3 1  1 2 1 6 4 3  J Note that q = I  I 6'  3  the last column of 0"^. Then Ao  =  Q-'AQ  = 0 0 1 2 1 -2 and Co =  CQ  =  [0 01] where l^-/ -  A|  = s^ + 2s -  s -  2  = s^ -{- a2S^  + ais  + ao. Hence GA.  = ii 2 i 6  =  AQ. Multi-output  case  (p  >  1).  Consider C CA CA « -i ci ciA CpA ciA « -i LCpA'^-(4.78) where  c\.. .Cp  denote thep  rows of  C and  select the first n linearly  independent rows in 0  moving  from  the top to bottom  (rank €  =  n). Next reorder the  selected rows by first taking all rows involving  ci then  C2 etc. to obtain c\A ciA  v^-l 295 CHAPTERS: Controllability Observability and Special Forms (4.79) Sin  n  X  n matrix. The integer  Vi denotes  the number  of rows involving  c/ in the  set of the first n Unearly independent rows found in 0 when moving from top to bottom. DEFINITION 4.2.  The/? integers vt i  =  1... p are the observability indices of the system and v  = max Vi is called the observability index of the system. Note that Y^Vi  = n and pv  ^  n. (4.80) When  rankC  =  p  then  ^/  >  1. Now  define k d-k =^Pi / -I k  =  I A (4.81) i.e. (Ti  =  1^1 (T2  =  v\  -}- V2y' "y^p  =  v\  +  '"  -\- Vp  =  n.  Consider  0  ^ and  let qk  E  R^  k  =  1... /? represent its or^th column i.e. g-i  =  [X---  X^il  X •••  Xg2|**-| X •••  X qpl (4.82) Define P-'  =  Q=  [qi...A-^-'q...qp.A^p-'qpl (4.83) ThenA^  =  PAP'^  =  Q-^AQmdCo =  CP~^  =  CQ are given by Ao  =  [Aijl  ij=\... p 0 •••  0  X An  = /^^^•^^/  =  7  Aij  = X 0 X 0 X /^^'•><^^/#y and Co  -  [Cl C2...  Cp] Ci — ro  •• 0 0 0 0 0 1 0  X 0  X J^p-XVi (4.84) where  the  1 on the  last  column  of  C/  occurs  at the  /th-row  location  (/  =  \... p) and  X denotes  nonfixed  entries. Note  that  the  matrix  BQ  =  PB  =  Q~^B  does  not 296 Linear Systems have any particular  structure. Equation  (4.84) is a very useful  form  (in the observer problem)  and shall be referred  to as the observer form  of the  system. Analogous to (4.63) we express  AQ  and  Co as /\.o ^o '  ^p^O) ^o ^p^oy (4.85) where  Ao  =  blockdiag  [A\ A2...  Ap\  with  A/  = RVi^^Vi^  C^  = 0 •• 0 e  7?^s/ =  1... ;?) and Ap  G T?"^^ and Cp G  RP'^P  are block  diag {[0...0lY appropriate matrices (Xf= 1  ^/  =  ^). Note that AQ  CO are completely determined by the/? observability indices  ^'/ /  =  I...  p  and A^ and Cp contain this  information in the o-ith...  or^th columns  of Ao  and  in the  same  columns  of  Co respectively. Note also that Ap is that part of the observable system i  =  Ax + Bu  y  =  Cx^-  Du that  can  be  altered  by  the  gain  of  a  state  observer.  This  is  discussed  further  in  the next  chapter. Results  that  are  in  the  spirit  of  Lemmas  4.8  and  4.9  also  exist  and  can  be  es tablished directly in a completely  analogous  way or by duality. The same is true  for proving that Q in (4.83) is nonsingular  and that Ao  and  Co in (4.84) have their par ticular structure. Furthermore by reversing the order of the columns of P~^ in (4.83) or by selecting the columns of P directly from 0 one can derive alternative observer forms.  These are dual to the controller forms  discussed  before. EXAMPLE  4.19.  Given  A  =1 0 ro 0 01 LO 1 andC  = 2 - ij "0  1  0" 1  1  0_ we wish to reduce these to observer form. This can be accomplished using duality i.e. by first reducing A =  A^ 5  =  C^ to controller form. Note that A B are the matrices used in Example 4.14 and therefore the desired answer is easily obtained. Presently we shall follow the direct algorithm described above. We have C CA CA^ 0 1 1 1 0 0 1 1 0 0 2 2 0 0 2 2 -2 -2 Searching from top to bottom the first three linearly independent rows are ci C2 c\A and '  ci  ' c\A .  C2 . = "0  1  0" 1  0  2 .1  1  0. Note that the observability indices are z^i =  2 z^2 =  lando-i  ==  20-2  =  3. We compute Then  Q  =  [qi  Aqi  ^2] a n d g -i  = r 0 1 2  J "X  0 X  0 = 1" 0 L^ 2 2^ T 1  2" 0  1  0 .1  0  0. . Therefore 2 -1 0 1 0 1 :0 1 :1 297 CHAPTERS: Controllability Observability and Special Forms + 0 0 0 "  2  11 -1  0 0  Oj ro  1  0' [o  0  1. Ao  = Q-'AQ  = All All 0 0 0 1 0 0 Co = CQ  =  [Ci :  Ci\  = We can also verify  (4.85) namely 0 1 2 -1 0 0 0  1 0  1 : : Ao and Co  = —  Ao + ApC^o 0 0 ^  p^o 1  0 1  1 0  1  0 0  0  1 Structure  Theorem—observable  version.  The transfer function  matrix H{s)  of systemi  =  Ax + Buy  =  Cx + Duisgiytnhy  H{s)  = C(sI-A)~^B-^Dlf(AC) is in the observer form  given in (4.84) then H(s)  can alternatively be characterized by the Structure Theorem  stated in Theorem 4.11. This result will be very useful  in the realization of systems addressed in Chapter 5 and also in the study of observers in the next  chapter. Let A  =  Ao  =  Ao + ApCo  and C  =  Co  =  CpCo as in (4.85) v^ith \Cp\ #  0 let B  =  Bo and D  =  Do and  define A(^)  =  diag  [^^S s''^  . . .  s^'p]  S(s)  =  block  diag  ([1 ^...  s""^'^] /  =  1... p). (4.86) Note that S{s)  is  a p  X  n polynomial  matrix  where  n  =  ^f^^vi.  Now  define  the pX  p  polynomial matrix D(s)  and the p  X  m polynomial matrix N(s)  as D(s)  ^  [Ms)  -  S(s)Ap]Cp N(s)  =  S(s)Bo  +  D(s)Do. (4.87) The  following  result  is  the  observable  version  of  the  Structure  Theorem.  It  is  the dual  of  Theorem  4.10  and  can  therefore  be  proved  using  duality  arguments.  The proof given is direct. THEOREM4.il.  H{s)  = D~H^)^('y) where 7V(5) 6(5) are defined in (4.87). Proof First we note that b{s)Co  = S(s){sl -  Aol (4.88) To see this write D(5)Co  =  [A(s)-S(s)Ap]C-^CpCo  = A(5)C^-5^(>y)A^Co and also S(s)(sl  -  Ao)  = S(s)s -  S(s)(Ao + ApCo) = S(s)(sl  -  Ao) -  S(s)ApCo =  A(^)C.  -S(s)ApCo which  proves  (4.88). We now  obtain  H{s)  =  Co{sI -  Ao)~'^Bo  + Do = D-\s)S(s)Bo  + Do = D~\s)[S(s)Bo  + D(s)Do] = D-\s)N(s) • 298 Linear  Systems EXAMPLE  4.20.  Consider A^  = 0 1 0 2 -1 0 and  Co  = 0  1  0 0  1  1 of Example 4.19. Here  z^i  =  2  1^2  =  I  Ms)  = s^  0 0 s  and S{s)  = [A(s)  -  S(s)Ap]C-' 0 s 1  5  0 0  0  1 -s  + 2 0 i 0 1  0 -1  1 s"-  +  s-- 2 -1 0 2  1 -1  0 0  0. 1  0 1 -1 1 s  0 0  0  1 1-1 1  0 1  1 . Then D{s)  = 's^  01 0 s 's'^  +  s -\ -1 Now if  Bo  =  [0 1 1]^ Do  =  0 and A^(^)  =  S(s)Bo  +  D(s)Do  =  [s if D-\s)N(s)  =  {l/[s(s^  + s-  2)]}[s^ +  h2s^  + s- If  =  Co(sI  -  AoT^Bo  +  Do. then H(s)  = • 3.5 P O L ES  A ND  Z E R OS In  this  section  the  poles  and  zeros  of  a  time-invariant  system  are  defined  and  dis cussed.  The  primary  reason  for  considering  poles  and  zeros  of  a  system  at  this  time is  that  poles  and  zeros  are  related  to  the  (controllable  and  observable  resp.  uncon trollable  and  unobservable)  eigenvalues  of A.  These  relationships  shed  light  on  the eigenvalue  cancellation  mechanisms  encountered  when  input-output  relations  such as  transfer  functions  are  formed.  These  relationships  also  provide  greater  insight into the realization theory  addressed  later in this book. Furthermore the  relationships between  uncontrollable  or unobservable  eigenvalues  decoupling  zeros  and  cancel lations in the transfer  function  matrix to be discussed  below provide  also insight  into how  state feedback  can  alter  system behavior.  State feedback  will be  studied  later  in Chapter  4. In the following  development  the fmio^ poles  of  a  transfer  function  matrix  H(s) [or  H(z)]  are  defined  first  (for  the  definition  of  poles  at  infinity  refer  to  Exercise 3.36). It should be noted here that the eigenvalues  of A are  sometimes  called  poles  of the  system  {A  B  C D}.  To  avoid  confusion  we  shall  use  the  complete  itrxn. poles  of H(s)  when  necessary.  The  zeros  of  a  system  are  defined  using  internal  descriptions (state-space  representations). Smith  and  Smith-McMillan  forms To define  the  poles  of H(s)  we  shall  first  introduce  the  Smith  form  of  a  polyno mial  matrix  P(s)  and  the  Smith-McMillan  form  of  a rational  matrix  H(s). The  Smith  form  Sp(s)  of  a p  X  m  polynomial  matrix  P(s)  (in  which  the  entries are  polynomials  in  s)  is  defined  as  (see  also  Subsection  7.2C  of  Chapter  7) Sp(s)  = 'A(s) 0 (5.1) er(s)]  where  r  =  rank  P(s).  The  unique  monic  poly with  A(^)  =  diag  [ei(s)... nomials  €i(s)  (polynomials  with  leading  coefficient  equal  to  one)  are  the invariant factors  of  P(s).  It can be  shown  that ei(s)  divides  e/+i(5')  /  =  1  . . .  r  -  1. Note  that 299 CHAPTERS: Controllability Observability and Special Forms €i(s) can be determined  by ei(s)  = Di(s) i  =  1  ...r where Di(s) is the monic greatest common divisor of all the nonzero /th-order minors of P(s)  with Do(s)  =  1. The Di(s)  are the determinantal  divisors  of ^(5"). A matrix P{s)  can be reduced  to Smith  form  by  elementary  row  and  column  operations  (see Subsections  7.2B  and  C  in  Chapter  7). This  ensures  that  the properties  of  interest are preserved  when P{s) is reduced to its (unique) Smith form. In particular we are interested here in the invariant factors  ei{s)  of P{s)  that can be determined  directly from the determinantal divisors Di{s) without having to reduce P{s) to its Smith form via elementary  operations. Consider now a /? X m rational matrix H{s).  Let d{s) be the monic least common denominator of all nonzero entries and write 1 Ks) N{s) H{s) (5.2) where N{s)  is a polynomial matrix. Let SN{S)  =  diag  [ni(s\  . . .  nr(s) Op-rm-r] be the  Smith  form  of  N(s)  where  r  =  rank  N(s)  =  rank  H(s).  Divide  each  nds)  of SN(S)  by d(s)  cancelling  all common factors  to obtain the Smith-McMillan  form of H(s) SMH(S) = 'A(s)  0 0 0 (5.3) with  A(s)  =  diag  [e\(s)/ilji(s).. ei(s)  divides  ei+i(s)  i  =  \2.. r-  1. .er(s)/il/r(s)]  where  r  =  rank  H(s).  Note  that .r  ~  I  and  if/i+iis)  divides  iptis)  i  = \2... Poles Given a. pX  m rational matrix H(s)  its characteristic  polynomial  or pole  poly nomial  PH(S)  is defined  as PH(S)  = il/i(sy"il/r(s\ (5.4) where  the  i///  /  =  1...  r  are  the  denominators  of  the  Smith-McMillan  form  of H(s).  It  can  be  shown  that  PH(S) is  the  monic  least  common  denominator  of  all nonzero minors of  H(s). DEFINITION 5.1.  The poles ofH(s)  are the roots of the pole polynomial PH(S). • Note that the monic least common denominator of all nonzero first-order minors (entries) of H(s)  is called the minimal polynomial  of H(s)  and is denoted by  mnis). The  mH(s)  divides  PH(S) and  when  the roots  of  PH(S)  [poles  of H(s)]  are  distinct J^H(S)  =  PH(S)  since the additional roots in  PH(S)  are repeated roots of  mnis). It is important to note that when the minors of H(s)  [of order 12  . . . min (p m)] are formed by taking the determinants of all square submatrices of dimension  1 x 1 2 x 2  etc. all cancellations of common factors between numerator and denominator polynomials  should be carried  out. In the  scalar case p  =  m  =  1 Definition  5.1  reduces  to the well-known  def inition  of  poles  of  a  transfer  function  H(s)  since  in  this  case  there  is  only  one 300 Linear Systems minor (of order 1) H(s)  and the poles are the roots of the denominator polynomial of H{s).  Notice that in the present case it is assumed that all the possible  cancellations have taken place in the transfer  function  of a system. Here PH(S)  =  mnis)  that is the pole or characteristic polynomial equals the minimal polynomial  of H(s).  Thus PH(S)  =  ^H(S)  are equal to the (monic) denominator of 77(5'). EXAMPLE  5.1.  Let  H(s) [l/[^(^ +  1)]  1/^ ^ ' 0 ^ 0 1  1  ^ ^ / 9 • The nonzero minors of order l/s^j ^ .  1  are the nonzero entries. The least  common  denominator  is 5'^(^ +  1)  =  mnis)  the minimal polynomial of H(s). The nonzero minors of order 2 are l/[s^(s +1)]  and 1/^-^ (taking columns 1  and 3 and 2 and 3 respectively). The least common denominator of all minors (of order 1 and 2) is s^(s + 1)  =  PH(S)  the characteristic polynomial of H(s). The poles are {0 0 0 -1}. Note that mnis) is a factor of PH(S) and the additional root at 5 =  0 in PH(S) is a repeated pole. To obtain the Smith-McMillan form of H(s) write H(s) 1 sHs + 1) s(s + 1) 0 s\s  + 1) (s + 1) 1 d(i) N(sl where d(s)  = s^(s +  1)  =  fnnis) [see (5.2)]. The Smith form of N(s) is SN(S)  = 0 1 0 0  s{s +  1)  0 since Do  =  I Di  ==  I D2 = s{s -\-  I)  [the determinantal divisors of A^(5)] and ni  = DJDQ  =  ln2  = D2/D1  = s(s + 1) the invariant factors of N(s). Dividing by d(s) we obtain the Smith-McMillan form of H(s) SMH(S)  = el h 0 0 51 i//2 1 sHs + 1) 0 0 1 0 0 Note that 1/^2 divides ij/i  and ei  divides €2. Now the characteristic or pole polynomial of H{s) is PH(S)  = il^iil^i  = s^(s -\-  1) and the poles are {0 0 0-1} as expected. • 1 s + 2 EXAMPLE  5.2.  Let  H(s)  = If a  7^  1 then the second-order minor is 1^(^)1 =  (1 -  a)/(s + 2)^. The least common denominator of this nonzero second-order minor \H(s)\  and of all the entries of H(s) (the first-order  minors) is (s + 2)^  =  PH(S) i.e. the poles are at {-2  -2}. Also mnis)  = s + 2. Now  if  a  =  1 then  there  are  only first-order nonzero  minors  (\H(s)\ =  0). In this case PH(S)  = mnis)  = S + 2 which is quite different  from the case when a  7^ 1. Presently there is only one pole at - 2. The reader should verify these results using the Smith-McMillan form of H(s). m In  view  of  Subsection  3.4.C  it  is  clear  that  all  the  poles  of  H(s)  are  roots  of l^/ -  All | that is they are some or all of the controllable and observable  eigenvalues of the  system. In fact  as will be  shown in Chapter  5 the poles of H(s)  are  exactly the controllable and observable eigenvalues of the system (in An)  and no factors of l^-/ -  All I  in H(s)  cancel. In general for the set of poles of H(s)  and the eigenvalues of A we have {poles of H(s)}  C  {eigenvalues of A} (5.5) with equality holding when all the eigenvalues of A are controllable and  observable eigenvalues of the system. Similar results hold for discrete-time  systems and  H(z). EXAMPLE 5.3.  Consider A ro 1 [o -1 -2 11 1 1  -ij B  = 1  0" 1  1 .1  2. and C  =  [0 1 0]  (refer to Example 4.6 in Section 3.5). Then the transfer  function  H(s)  =  [l/s l/s]. H(s) has only one pole ^i  =  0(PH(S)  = s) and Ai  =  0 is the only controllable and observable eigenvalue. The other two eigenvalues of A A2 =  - 1  A3 =  - 2 that are not both con • trollable and observable do not appear as poles of H(s). 301 CHAPTER 3: Controllability Observability and Special Forms EXAMPLE 5.4.  Recall the circuit in Example 4.9 in Section 3.4. If R1R2C ¥^  L then {poles of//(5')}  =  {eigenvalues of A at Ai  =  -\I{R\C)  and A2 =  -R^II^.  In this case both eigenvalues  are controllable  and observable. Now if  R1R2C = L with Ri 7^  R2 then H(s) has only one pole ^i  =  -R2IL  since in this case only one eigenvalue Ai  = -R2/L  is  controllable  and  observable.  The  other  eigenvalue  A2 at the  same location -R2IL  is uncontrollable  and unobservable. Now if  R\R2C  =  L with  Ri  =  R2 =  R then one of the eigenvalues becomes uncontrollable  and the other (also at  -RIL)  be comes unobservable. In this case H{s) has no finite poles {H{s) =  \IR). • Zeros In  a  scalar  transfer  function  H{s)  the roots  of  the  denominator  polynomial  are the poles  and  the roots  of  its numerator  polynomial  are the  zeros  of H{s).  As  was discussed iht  poles  ofH{s)  are some or all of the eigenvalues of A (the eigenvalues of A are sometimes also cdlXtd poles  of the system  {A B C D}). In particular it v^as shown in Subsection  3.4.C  that the uncontrollable  and/or unobservable  eigenvalues of A can never be poles of H(s).  In Chapter 5 it is shown that only those eigenvalues of A that are both controllable and observable appear as poles of the transfer  function H(s).  Along similar lines the zeros ofH(s)  (to be defined later) are some or all of the characteristic values of another matrix the system matrix P(s).  These  characteristic values are called the zeros  of the system  {A B C D]. The  zeros  of  a  system  for  both  the  continuous-  and  discrete-time  case  are  de fined and discussed next. We consider now only finite zeros. For the case of zeros at infinity  refer  to the exercises. Let  the system  matrix  (also  called Rosenbrock's  system  matrix)  of {A B C D] be P{s)^ si  -  A -C B D (5.6) Note that in view of the system equations  x Ax  +  Bu  y Cx  +  Du  we have P(s)  -x(s) U(s) 0 where x(s)  denotes the Laplace transform  of  x(t). Let r  =  rank P(s)  [note that n  ^  r  ^  min (p-\-nm-\-  n)] and consider all those rth-order  nonzero  minors  of  P(s)  that  are  formed  by  taking  the  first  n rows  and  n columns  of  P(s)  i.e. all rows and columns  of 5'/ -  A and then  adding  appropriate r  -  n rows  (of  [-C  D])  and  columns  (of  [B^ D^Y).  The  zero polynomial  of  the system  {A B C Z)}  zp{s)  is  defined  as  the  monic  greatest  common  divisor  of  all these minors. DEFINITION5.2.  The zeros of the system {A B C D} or the system zeros are the roots of the zero polynomial of the system zp(s). • 302 Linear Systems In  addition  we  define  tlie  invariant  zeros  of  the  system  as  tlie  roots  of  tlie invariant polynomials of  P{s). In particular consider the  (p + n)  x  (m + n)  system matrix P{s)  and let Sp{s) 'A{s)  0" 0 0 A(^) =  diag[ei (^)... e^(^)] (5.7) be its Smith form. The invariant zero polynomial  of the system  {A  5 C D}  is defined as (5.8) and  its  roots  are  the  invariant  zeros  of  the  system.  It  can  be  shown  that  the  monic greatest common divisor of all the highest order nonzero minors of P(^) equals Zp{s). Z^p{s) =  ei{s)e2{s)---er{s) In general {zeros of the system} D {invariant zeros of the system}. When  p  =  m  with  det  P{s)  ^  0  then  the  zeros  of  the  system  coincide  with  the invariant zeros. Now  consider  the  nx  {m-\-n)  matrix  [si — AB]  and  determine  its  n  invariant factors  e/(^)  and its Smith form.  The product of its invariant factors  is a polynomial the  roots  of  which  are  the  input-decoupling  zeros  of  the  system  {A5CD}.  Note that  this  polynomial  equals  the  monic  greatest  common  divisor  of  all  the  highest order nonzero minors  (of  order n) of  [si — AB].  Similarly  consider  the  {p-\-n)  xn \sl  —  A\ matrix decoupling  zeros of the system  {A5CD}. ^ and its invariant polynomials the roots  of which define  the  output-Using  the  above definitions  it is not difficult  to  show that the  input-decoupling zeros  of  the  system  are  eigenvalues  of  A  and  also  zeros  of  the  system  {A5CD} (show  this).  In  addition  note  that  if  A/  is  such  an  input-decoupling  zero  then rank  [A// —A5]  <  n  and  therefore  there  exists  a  1 x  n  vector  v/  ^  0  such  that v/[A// —A5]  =  0. This however implies that A/  is an uncontrollable eigenvalue of A (and Vi is the corresponding left eigenvector) in view of Subsection 3.4B. Conversely it can be shown that an uncontrollable eigenvalue is an input-decoupling  zero. There fore  the  input-decoupling  zeros  of  the  system  {A5CD}  are  the  uncontrollable eigenvalues  of A.  Similarly  it can be  shown that the output-decoupling  zeros  of the system  {A5CD} are the unobservable  eigenvalues of A.  They are also zeros of the system as can easily be seen from  the  definitions. There are eigenvalues of A that are both uncontrollable and unobservable. These can be  determined  using  the left  and right  corresponding  eigenvector  test  or by  the Canonical  Structure  Theorem  (Kalman  Decomposition  Theorem)  (see  Subsections 3.4A and B).  These uncontrollable and unobservable eigenvalues of A are zeros of the system that are both input-  and output-decoupling  zeros  and are called  input-output decoupling  zeros.  These input-output  decoupling  zeros  can  also be defined  directly from P{s)  given in (5.6); however care should be taken in the case of repeated zeros. If  the  zeros  of  a  system  are  determined  and  the  zeros  that  are  input-  and/or output-decoupling  zeros  are  removed  then  the  zeros  that  remain  are  the  zeros  of H{s)  and can be found  directly  from  the transfer  function  II{s).  In particular  if  the Smith-McMillan  form  of H{s)  is given by (5.3) then ZH{S)  =ei{s)e2{s)"'er{s) (5.9) is  the  zero polynomial  of  H(s)  and  its  roots  are  the  zeros  of  H(s).  These  are  also called the transmission  zeros  of the  system. DEFINITION  5.3.  The zeros of H(s) or the transmission zeros of the system are the roots of the zero polynomial of H(s) ZH(S)' • The relationship between the zeros of the system and the zeros of H(s)  can easily be determined using the identity 303 CHAPTER 3: Controllability Observability and Special Forms P(s)  = si -A B -C  D si -A -C (si  -  A)-H(s) 'B for  the  case  when  P(s)  is  square  and  nonsingular.  Note  that  in  the  present  case |P(^)|  =  1^/ -  A||//(5')|. It is also possible to obtain this result using the special struc ture of the matrices in the special form of Subsection 3.4A. In this case the invariant zeros of the system  [the roots of IPC^")]] which are equal here to the zeros of the sys tem  are the  zeros  of H(s)  [the roots  of  |//(^)|]  and  those  eigenvalues  of A that  are not both controllable and observable  [the ones that do not cancel in \sl  -  A\\H(s)\] Note that the zero polynomial of H(s)  ZH(S)  equals the monic greatest common divisor  of  the  numerators  of  all  the  highest  order  nonzero  minors  in  H(s)  after  all their  denominators  have  been  set  equal  to  PH(S) the  characteristic  polynomial  of H{s).  In the scalar case (p  =  m  =  I)  our definition  of the zeros of H{s)  reduces to the  well-known  definition  of  zeros namely  the roots  of the numerator  polynomial of  H(s). EXAMPLE  5.5.  Consider  H(s)  of  Example  5.1. From  the  Smith-McMillan  form of H(s) we obtain the zero polynomial ZH(S)  =  1 and H(s) has no (finite) zeros. Alterna tively the highest order nonzero minors are l/[s^(s +  1)] and l/s^  = (s + l)/[s^(s +1)] and the greatest common divisor of the numerators is ZH(S)  = 1. EXAMPLE  5.6.  We  wish  to  determine  the  zeros  of  H(s)  = s r s+  1 1 0 s+  1 s+  1 1 s +  1 s + I'  s + I' Then  PH(S)  =  s^(s +  1) s^ and The first-order minors  are  the  entries  of  H(s)  namely 1 s' there  is  only  one  second-order  minor s+  1 s + I s^ the least common denominator is the characteristic polynomial. Next write the highest (second-)  order  minor  as  -and  note  that  s(s +  1)  is  the s s(s +  1)  _  s(s + 1) sKs +  1)  " PH(S) zero polynomial of H(s) ZH(S) and the zeros of H(s) are {0 -1}. It is worth noting that the poles and zeros of H(s) are at the same locations. This may happen only when H(s) is a matrix. If  the  Smith-McMillan  form  of  H(s)  is  to  be  used  write  H(s) 0 's' U' (s + If Do = lDi --1 and ^2  = D2/D\ —--N(s).  The  Smith  form  of  N(s)  is  now d(s) lD2  = ^-^(^+1)2 with invariant factors of A/^(5') given by ^1  =  DI/DQ  = s^(s+lf.  Therefore the Smith-McMillan form (5.3) of H{s) is sHs + 1) 0 S\S  +  1)2  since SMH(S)  = 1 s\s  + 1) 0 0 s(s + 1) i  J r ^l ^l 0 0 1 ^2 IA2-I 304 Linear Systems The zero polynomial is then ZH(S)  = e\e2  = s(s + 1) and the zeros of H(s) are {0  -1} as expected. Also the pole polynomial is PH(S)  = i//ii/^2  =  s^{s + 1) and the poles are • {00-1}. s+  1 1 s+  1 0 0 5+  1 1 s The EXAMPLE  5.7.  We  wish  to  determine  the  zeros  of  H(s) 1 1 s'  s + I'  s(s + I) 1 second-order minors are s^(s +  1). Rewriting the highest (second-) order minors as s(s +  \)IPH{S)  s^lpnis) and SIPH{S)  the greatest common divisor of the numerators is s i.e. the zero polynomial of H{s) is ZH{.S)  = s. Thus there is only one zero of H(s) located at 0. Alternatively note that the Smith-McMillan form is and the characteristic polynomial is PH(S) SMH(S) 1 sHs + 1) 0 0 Relations between poles zeros and eigenvalues of A Consider  the  system  x  =  Ax  -\-  Bu  y  =  Cx  -\-  Du  and  its  transfer  function matrix H(s)  =  C(sl  -A)~^B-\-D.  Summarizing the above discussion the following relations can be shown to be true. 1.  We have the set relationship {zeros of the system}  =  {zeros of  H{s)} U {input-decoupling  zeros} U {output-decoupling  zeros} (5.10) -  {input-output decoupling  zeros}. Note that the invariant zeros of the system contain all the zeros of H(s)  (trans mission zeros) but not all the decoupling  zeros (see Example 5.8). When  P(s)  is square  and  nonsingular  the  zeros  of  the  system  are  exactly  the  invariant  zeros of the system. Also in the case when {A B C D} is controllable and observable the zeros of the system the invariant  zeros and the transmission  zeros  [zeros of H(s)]  all coincide. 2.  We have the set relationship {eigenvalues of A (or poles of the system)}  =  {poles of  H(s)} U {uncontrollable eigenvalues  of A} U {unobservable eigenvalues of A} -  {both uncontrollable  and unobservable eigenvalues  of A}. (5.11) 3.  We have the set relationships {input-decoupling  zeros}  =  {uncontrollable eigenvalues  of A} {output-decoupling  zeros}  =  {unobservable eigenvalue of A} {input-output decoupling  zeros}  =  {eigenvalues of A that are both and uncontrollable and unobservable}. (5.12) 4.  When  the  system  {A B  C D}  is  controllable  and  observable  then {zeros  of  the  system}  =  {zeros  of  H(s)} {eigenvalues  of A  (or poles  of  the  system)}  =  {poles  of  H(s)}. (5.13) and Note  that  the  eigenvalues  of  A  (the  poles  of  the  system)  can  be  defined  as  the  roots of  the  invariant  factors  of  ^/  -  A  in  P(s)  given  in  (5.6). EXAMPLE  5.8.  Consider the system {A B C} of Example 5.3. Let 305 CHAPTER 3: Controllability Observability and  Special Forms P(s)  = si  -  A  B D -C s -1 1 ^ +  2 -1 -1 0 -1 s+  1 1 1 1 0 1 2 0 -1 0 0 0 There  are two fourth-order  minors  that include  all columns  of ^/  -  A obtained  by taking  columns  1 2  3 4  and  columns  1 2  3 5  of  P{s)\  they  are  {s +  \){s  +  2)  and {s +  \){s  + 2) (verify  this). The zero polynomial of the system is zp  =  {s+  l){s  + 2) and the zeros of the system are {-1  - 2 }. To determine the input-decoupling  zeros consider all the third-order minors of [si -  A 5]. The greatest common divisor is 5* + 2 (verify this) which implies that the input-decoupling zeros are {-2}. Similarly consider \sl  —  Al and show that 5 -t- 1 is the greatest common divisor of all the third-order minors and that the output-decoupling  zeros  are {-1}.  The transfer  function  for  this  example  was  found  in Example5.3 tobe//(>y)  =  [I/5 1/^]. The zero polynomial of//(i-)  is z//(^)  =  land  there are no  zeros  of H{s).  Notice  that  there  are  no input-output  decoupling  zeros. It  is  now clear that relation  (5.10) holds. The  controllable  (resp.  uncontrollable)  and  the  observable  (resp.  unobservable) eigenvalues  of A  (poles  of  the  system)  have  been  found  in  Examples  4.4  and  4.5  in Section  3.4. Compare these results to show that  (5.12) holds. The poles of H(s)  are {0}. Verify  that (5.11) holds. One  could  work  with  the  Smith  form  of  the  matrices  of  interest  and  the  Smith-McMillan  form  of  H(s).  In  particular  it  can  be  shown  (do  so)  that  the  Smith  form  of P(s)  is "1  0  « 0  1 0  0 0  0 '^ 0 0 s + 2 0^ ^1  0 I of  [si  -  A B]  is  ' 0 0 s  + 2 0  01 of si -A -C 0 0 "1  0 0  1 0  0 0  0 the Smith-McMillan  form  of H(s)  is  and of  [si  -no -A]  is  0  1 s + 1 0 0 0 0  0  ^r^ -1-1){ . Also it can be  shown  that SMH(S)  = -o It is straightforward  to verify the above results. Note that in the present case the invariant zero polynomial is Zp(s)  =  s  + 2 and there is only one invariant zero at  - 2. • EXAMPLES.9.  Consider the circuit of Example 5.4 and of Example 4.9 in this chapter and the system matrix  P(s)  for the case when  R1R2C  =  L given by 306 Linear Systems P(s) = si  -A  B -C  D s + R2 L 0 0 s + R2 R2 L 1 Ri ^ '  Ri (i)  First let Ri ¥^  R2. To determine the zeros of the system  consider  \P{s)\ = {\IR\){s + R\IL){s + R2IL) which  impHes that the zeros of the  system  are  {—R\IL -R2IL}.  Consider now all second-order (nonzero) minors of [si -  AB] namely {s + R2/Lf. (l/LXs  + R2/L) and -(R2/L)(s  + R2/LX from which we see that {~R2/L} is the input-decoupling zero. Similarly we also see that {-7^2/^} is the output-decoupling zero. Therefore {-R2/L} is the input-output decoupling zero. Compare this with the results in Example 5.4 to verify (5.13). (ii) When Ri  = R2 = R then  \P(s)\ = (l/R)(s  + RILf  which impHes that the zeros of the system are at {-R/L  -R/L}.  Proceeding as in (i) it can readily be shown that {—R/L} is the input-decoupling zero and {—R/L} is the output-decoupling zero. To determine which are the input-output decoupling zeros one needs additional information to the zero location. This information can be provided by the left and right eigenvectors of the two zeros at -R/L  to determine that there is no input-output decoupling zero in this case (see Example 4.9). In both cases (i) and (ii) H(s) has been derived in Example 4.9 of Section 3.4. • Verify relation (5.10). Zero directions and pole-zero  cancellations There are characteristic vectors or zero directions associated with each invariant and decoupling zero of the system {A B C Z)} just as there are characteristic vectors or eigenvectors associated with each eigenvalue of A (pole of the system). Consider  the system  matrix  P(A) given  in (5.6) at 5* =  A. If A is an invariant zero of the system then there exist nonzero vectors v^ and v^ associated with A such that P(A)v  =  0 vP(A)  =  0. (5.14) This  is  so  because  if  A is  an  invariant  zero  of  the  system  then  ranii  P(X) < rank  P(s) <  min(^ -\-  pn  -\-  m) and therefore  the columns  (resp.  the rows) are linearly  dependent.  Here  rank  P(s)  denotes  the number  of  linearly  independent columns or rows over the field of rational functions  in s [it is called normal  rank of P(s)];  rank  P(A) is the rank of P(A) over the field of complex numbers. The  vector  v^ is called  an invariant  zero  direction  or a zero  direction  corre  that is. sponding to A. A physical interpretation of this is as follows. Let v^ = -u let XI  -  A  B D -C (5.15) and assume that the system is at rest at ^ =  0. If an input of the form w(0  =  ue 0 is applied and if A is not a pole of the system then it will produce a state of the form  x{t)  =  xe^^ and an output r..^t yit)  -  0 r >  0. This is sometimes referred to in the literature as the output-zeroing  or blocking prop- erty of zeros. This property is a generahzation of the blocking property  of zeros orig- inally expressed in terms of the transfer  function  and its zeros for  a SISO system. For decoupling  zeros it is quite  easy  to  see what  the corresponding  directions will be. In particular  if  A  is  an input-decoupling  zero then  there  exists  a nonzero vector V such that v[A/  -  A B]  =  0 which implies that 307 CHAPTER 3: Controllability Observability and Special Forms vPW  =  [v0] \XI - A B] -c  D\ [00]. (5.16) It is clear that the vector v is the left eigenvector of A corresponding to A which is also an eigenvalue of A. This in fact  determines  the exact relationship  (location and  corresponding  directions)  between  input-decoupling  zeros  and  uncontrollable eigenvalues. Similar results can be derived for the output-decoupling  zeros. In fact if A is an [A/ — AI v - 0 -C output-decoupling  zero then there exists a nonzero vector v so that which implies that ^(A)v  = XI -A -C (5.17) It is clear that the vector v is the right eigenvector of A corresponding to A which is also an eigenvalue of A. This provides the exact relation between  output-decoupling zeros and unobservable  eigenvalues. Consider now an input-decoupling zero A and the corresponding direction [v 0]. As  was  shown  above  A v are an  (uncontrollable)  eigenvalue  and its  corresponding left  eigenvector  respectively.  Let  v be the  corresponding  right  eigenvector  to  A. If \I  — A] _ ^  V =  0 then A is also an unobservable eigenvalue and an output-decoupling zero. In particular  A is an input-output  decoupling  zero and also an eigenvalue  that is both uncontrollable  and unobservable.  The vectors  [v 0]  and  [v^ 0]^ are the di rections associated with such zero. In general the directions v^ and v^ in (5.14) associated with the invariant zeros do not have any particular form [v^  =  [v 0] in (5.16) for the case of an input-decoupling zero]. When the rank of P(s)  is full  that is rank  P(s)  =  n -{-  min (p  m) (5.18) the direction that corresponds to the invariant zero at A can be taken to be v^ [where P{X)Vz =  0] when min (;? m)  =  m andv^ [where V2P(A)  =  0] when min (/? m)  = p.  Note  that  when  min  (p m)  =  p  <  m  there  are  nonzero  vectors  satisfying P{K)Vz =  0 with A not necessarily  being an invariant  zero of the system.  This situa tion becomes accentuated i.e. (5.14) is satisfied for values of A that are not necessar ily zeros of the system when rank P(s)  <  n-\-mm  {p m). This phenomenon is unique to MIMO systems. In the MIMO case it is possible for a /? X m transfer  function  H{s)  to have poles and  zeros  at the same location  (see Example  5.10). This is impossible  in the  scalar case where H{s) is a 1X1 matrix since in this case common factors in the numerator and denominator will cancel in the process of forming H{s).  In state-space terms this result can be expressed  as  follows. 308 Linear Svstems LEMMA 5.1.  In the SISO case if A is both a zero of the system and an eigenvalue of "^ ^^^ P^^^ ^^ ^^^ system) then A must be an input- and/or output-decoupling zero (un controllable and/or unobservable eigenvalue). This is not necessarily true in the MIMO case. Proof Assume that A is not an input- and/or output-decoupling  zero or equivalently that all eigenvalues of A are controllable and observable. Then all eigenvalues of A will be poles of H{s) and all zeros of the system will be zeros of H{s). This is not possible however since by definition the numerator and denominator of H{s) cannot have a com mon factor [presently {s -  A)]. Therefore A must be an input- and/or output-decoupling zero. • EXAMPLE  5.10.  For  H{s) s I s s+  1 1 0 ^ +1 s^ -s + I the poles  and zeros were  determined in Example 5.6 to be {poles of H{s)} = {0 0 -1} and {zeros of H(s)} = {0 -1}.  Note that in this case the poles and zeros are at the same locations 0 and  - 1. • We conclude by noting that as will be shown in Section 4.2 of Chapter 4 one can arbitrarily  assign values to the controllable eigenvalues and to a certain extent  one can alter their corresponding  eigenvectors using linear state feedback.  In the scalar case  assigning  a  closed-loop  eigenvalue  at  an  open-loop  zero  location  guarantees that  the eigenvalue  will become  unobservable  (linear  state feedback  does not  alter controllability) and will not appear in the transfer function  (refer to Exercises 4.6 and 4.7 in Chapter 4). In the MIMO case the eigenvectors of the closed-loop eigenvalues must also be assigned appropriately for the eigenvalues to become unobservable and cancel  out  when  forming  the  transfer  function  matrix  (see Exercise  4.19  in  Chap ter 4). 3.6 SUMMARY In  this  chapter  the  system  properties  of  reachability  (or  controllability-from-the-origin)  and  controllability  (-to-the-origin)  together  with  the  dual  properties  of ob servability  and constructibility  respectively  were developed.  These concepts  were introduced  using  discrete-time  time-invariant  systems  (Subsection  3.1 A)  and  were further  developed in Part  1 of the chapter  (Sections  3.2 and  3.3). In Section  3.2 the reachability  Gramian  of a continuous-time  system was used to derive inputs that transfer  the state of the system from  one desirable vector value to another. This was accomplished for both time-varying and time-invariant systems. The time-invariant case was developed in Subsection 3.2B so that it may be studied independently.  It was  shown that for  continuous-time  systems reachability  implies controllability and vice-versa; however for discrete-time systems although reacha bility always implies controllability  controllability  may not imply reachability  (un less A  has  full  rank).  Analogous  results  were  developed  in  Section  3.3  regarding observability  and  constructibility. In  Part  2  Section  3.4  useful  special  forms  for  (continuous-time  and  discrete-time)  state-space  descriptions  of  time-invariant  systems  were  developed.  The standard  forms  for  uncontrollable  (resp.  unobservable)  systems  lead  to better  un derstanding  of the relationships  between  state-space  and  transfer  function  descrip tions  of  systems.  The  controller  and  observer  forms  provide  important  structural information  about a system that is useful  in state-space realizations of transfer  func- tion matrices  and in feedback  control. Polynomial  matrix  fractional  descriptions  of transfer  matrices  were also introduced by the structure theorem. Finally poles and zeros  of  systems  were  addressed  in  Section  3.5.  They  were  introduced  using  the Smith form  of polynomial matrices  and the Smith-McMillan  form  of transfer  func- tion matrices. The reachability  (controllability)  and observability  (constructibility)  Gramians played an important role in this chapter. These are now summarized for convenience. 309 CHAPTER 3: Controllability Observability and Special Forms Summary  of Gramians  Introduced  in This  Chapter Reachability  Gramians A.  Wr(totl)= rh J to ^{tiT)B{T)B^{T)^{tiT)dT. (2.11) (2.29) B.  WM  T)  =  f  e^^-'^^BB^e^^~'^^' dr. Jo K-l C.  Wr{0K)  =  ^A^-^'^^^BB^(A^f-^'+^^ K-\ = ^A'BB^(A^y. (2.64) 1 = 0 ControUabihty  Gramians A.  Wc(toJl)= ^{tQT)B{T)B^{T)^^{tQT)dT. B.  WMT)=^ Jto rT Jo K-l e-'^^BB^e-'^^^dr. C.  WM  K)  =  ^  A-~^'^^^BB^{A^)-^'^^\ /=o Observabihty  Gramians \A\ T^ 0. A.  W(ro h)=  V  ^\T  ro)C^(T)C(T)c&(T ro) d7. Jto B.  Wo(0T)=l e^^'C^Ce^'dr. Jo K-\ C.  Wo{0K)  = ^(A^yC^CA\ i = 0 Constructibility  Gramians A.  Wcnito tl)= \'  0^(T  ^ I ) C ^ ( T ) C ( T ) 0 ( T  ti)dT. Jto B.  Wcn(0T)= l' Jo e^'^'-^^C^Ce^^'-^Ur. C.  Wcn(0 K)  =  ^(A^)"^^'+^^C^CA-^^+^\ \A\ 7^ 0. i=o (2.19) (2.41) (2.66) (3.5) (3.22) (3.49) (3.11) (3.32) (3.54) 310 Linear Systems where the Gramians in A B and C in each case are the Gramians of the  systems i:  =  A(t)x  +  B(t)u  y  =  Cx(t)  +  D(t)u. A. B.  X  =  Ax  -\-  Bu y  =  Cx  -\-  Du. C.  x(k  +  I)  =  Ax(k)  +  Bu(k\  y(k)  =  Cx(k)  +  Du(k)  respectively. In B and C the controllability  and observability  matrices  are respectively ^  -  [BAB..A''-^Bl €  =  [C^(CA/  ...(CA^~i)^]^. Note that reachability  is the dual concept to observability  and controllability  is dual to  (re)constructibility. 3.7 NOTES The concept of controllability  was first encountered  as a technical  condition in cer tain  optimal  control  problems  and  also  in  the  so-called  finite-settling-time  design problem for  discrete-time  systems  (see Kalman  [6]). In the latter  an input must be found  that  returns  the  state  XQ  to  the  origin  as  quickly  as  possible.  Manipulating the  input  to  assign  particular  values  to the initial  state in  (analog-computer)  simu lations  was  not  an  issue  since  the  individual  capacitors  could  initially  be  charged independently. Also observability  was not an issue in simulations due to the partic ular  system  structures  that  were used  (corresponding  e.g.  to observer forms).  The current definitions for controllability and observability and the recognition of the du ality between  them were worked out by Kalman in  1959-1960  (see Kalman  [9] for historical comments) and were presented by Kalman in [7]. The significance  of real izations that were both controllable  and observable  (see Chapter 5) was  established later in Gilbert  [3] Kalman  [8] and Popov  [12]. For further  information  regarding these historical issues consult Kailath  [5] and the original sources. Note that [5] has extensive  references  up  to  the  late  seventies  with  emphasis  on  the  time-invariant case and a rather complete set of original references  together with historical remarks for the period where the foundations  of the state-space system theory were set in the late fifties and sixties. Special  state-space  forms  for  controllable  and  observable  systems  obtained  by similarity  transformations  are  discussed  at  length  in  Kailath  [5]  (refer  also  to  the discussion  on various canonical forms). Wolovich  [19] discusses the algorithms  for controller and observer forms and introduces the Structure Theorems. The controller form is based on results by Luenberger  [11] (see also Popov [13]). A detailed deriva tion of the controller form  can also be found  in Rugh [16]. Original  sources  for  the Canonical  Structure Theorem  include  Kalman  [8] and Gilbert [3]. The  eigenvector  and  rank  tests  for  controllability  and  observability  are  called PBH  tests  in  Kailath  [5]. Original  sources  for  these  include  Popov  [14]  Belevich [1] and Hautus  [4]. Consult  also Rosenbrock  [15] and for  the case when A  can be diagonalized  via a similarity  transformation  see Gilbert  [3]. Note that in the eigen value/eigenvector  tests  presented  herein  the  uncontrollable  (unobservable)  eigen values  are  also  explicitly  identified  which  represents  a modification  of  the  above original results. The  Brunovsky  canonical  form  is  developed  in  Brunovsky  [2]. The  fact  that  the  controllability  indices  appear  in  the  work  of  Kronecker  was recognized  by  Rosenbrock  [15]  and  Kalman  [10]. For  an  extensive  introductory  discussion  and  a  formal  definition  of  canonical forms  see Kailath  [5]. Note that certain  special forms  exist for time-varying  systems as  well  but  they  are  not  considered  here. Multivariable  zeros  have  an  interesting  history.  For  a review  see  Schrader  and Sain  [17]  and  the  references  therein.  Refer  also  to  Vardulakis  [18]. 311 CHAPTER 3: Controllability Observability and  Special Forms 3.8 R E F E R E N C ES 1.  V. Belevich  Classical  Network  Theory Holden-Day  San Francisco  1968. 2.  P. Brunovsky  "A  Classification  of  Linear  Controllable  Systems" Kybernetika  Vol. 3 pp.  173-187  1970. 3.  E. Gilbert  "Controllability  and Observability  in Multivariable  Control Systems" SIAM J.  Control  Vol.  1 pp.  128-151  1963. 4.  M.  L.  J.  Hautus  "Controllability  and  Observability  Conditions  of  Linear  Automonous Systems" Proc.  Koninklijke  Akademie  van  Wetenschappen  Serie  A  Vol. 72 pp. 443-448  1969. 5.  T. Kailath Linear  Systems  Prentice-Hall Englewood  Cliffs  NJ  1980. 6.  R. E. Kalman  "Optimal  Nonlinear  Control  of Saturating  Systems  by  Intermittent  Con trol" IRE WESCON  Rec  Sec. IV pp.  130-135  1957. 7.  R. E. Kalman "On the General Theory of Control Systems" in Proc.  of the First  Intern. Congress  on Automatic  Control  pp. 481-493 Butterworth London  1960. 8.  R. E. Kalman "Mathematical Descriptions of Linear Systems" SIAM J. Control Vol. 1 pp.  152-192  1963. 9.  R. E. Kalman Lectures  on Controllability  and  Observability  C.I.M.E. Bologna  1968. 10.  R. E. Kalman "Kronecker Invariants and Feedback" in Ordinary Differential  Equations L. Weiss ed. pp. 459-471  Academic  Press New York  1972. 11.  D. G. Luenberger "Canonical Forms for Linear Multivariable Systems" IEEE  Transac tions  on Automatic  Control  Vol.  12 pp. 290-293 1967. 12.  V. M.  Popov  "On  a  New  Problem  of  StabiUty  for  Control  Systems" Autom.  Remote Control  pp.  1-23  Vol. 24 No.  1 1963. 13.  V. M.  Popov  "Invariant  Description  of  Linear  Time-Invariant  Controllable  Systems" SIAM  Journal  of Control  and  Optimization  Vol.  10 No. 2 pp. 252-264  1972. 14.  V  M. Popov Hyperstability  of Control  Systems  Springer-Verlag Berlin  1973. 15.  H. H. Rosenbrock State-Space  and Multivariable  Theory Wiley New York  1970. 16.  W. J. Rugh Linear  System  Theory Prentice-Hall Englewood  Chffs  NJ 1993. 17.  C. B. Schrader  and M.  K.  Sain  "Research  on System Zeros: a Survey" Int.  Journal  of Control  Vol. 50 No. 4 pp.  1407-1433 1989. 18.  A.  I.  G.  Vardulakis  Linear  Multivariable  Control.  Algebraic  Analysis  and  Synthesis Methods  Wiley  New York 1991. 19.  W. A. Wolovich Linear  Multivariable  Systems  Springer-Verlag New York  1974. 3.9 EXERCISES 3.1.  (a)  Let %k =  [B AB...  A^'^B]  where A^R "><«igG/?"^'".Showthat ^ ( ^ ^)  =  ^{%n)  for  k>n and k) C  ^{%n)  for k<n. 312 Linear  Systems (b)  Let 0^  =  [C^ (CAf... A  srT (CA'^-^ff  where A  E  R^''^  C E  /?^^^ Show that M{^k)  =  >r(0„) for k>  n and J{(€k)  D M'(€n) for  k<n. 3.2.  Consider the state equation  x  = Ax  + Bu where A 0 3w2 0 0 10 0 0 —2w 0 0 0 0 2w 1 0 B  = 0 1 0 0 o' 0 0 1 which was obtained by linearizing the nonlinear equations of motion of an orbiting satel lite about a steady-state  solution. In the state x  =  [xi X2  ^3 ^4]^ xi  is the  differential radius  while  X3 is the differential  angle.  In the input  vector  u  =  [i/i U2V  u\  is the radial thrust and 1/2 is the tangential  thrust. (a)  Is this  system  controllable  from  ullf  y  =  yi is  the system  observable from  yl (b)  Can the  system  be  controlled  if  the radial  thruster  fails?  What  if  the  tangential thruster  fails? (c)  Is the system observable from  yi  only? From y2 only? 3.3.  Consider the state equation i 0 0 -1 (a)  lfx(0)  = derive an input that will drive the state to in T sec. (b)  For x{0) 5 -5  plot  u(t) xi(t)  X2(t) for T  =  1 2 and 5 sec. Comment  on the magnitude of the input in your results. 3.4.  Consider the state equation x(/:+l)  = "1  1  0" 0  1  0 .0  0  1_ x{k) + 0' 1 . 1. u(k)y(k)  = 1  1  0 0  1  0 x{k). (a)  Is  x^ reachable?  If yes  what  is the minimum  number  of steps  required to transfer  the state from  the zero state to x^ ? What inputs do you need? (b)  Determine all states that are reachable. (c)  Determine all states that are unobservable. (d)  If i:  =  Ax  + Bu is given with A B as in (a) what is the minimum time required to transfer  the state from  the zero state to x^l  What is an appropriate u(t)? 3.5.  Output reachability  (controllability)  can be defined in a manner analogous to state reach ability  (controllability). In particular  a system  will be called  output  reachable if there exists an input that transfers  the output from  some yo to any ^^i in finite time. Consider  now a discrete-time  time-invariant  system  x{k  +  1)  =  Ax(k)  +  Bu(k) y(k)  =  Cx(k)  + Du(k)  with A  E  /?">'^ B E  /?"^^  C E  7?^^" and D E  RP'''^.  Recall that y(k)  =  CA^x(O) + ^  CA^-^'-'^^Buii)  +  Du(k). k-i (a)  Show that the system {A B C D} is output reachable if and only if rank  [D CB CAB...  CA'^'^B]  =  p. Note that this rank condition is also the condition for output reachabihty  for continuous-time  time-invariant  systems x=Ax  + Buy  = Cx +  Du. It should be noted that in general state reachabihty is neither necessary  nor sufficient  for  output reachabihty.  Notice for  example that if  rank D  = p  then the system is output reachable (b)  Let  D  =  0.  Show  that  if  (A5)  is  (state)  reachable  then  {ABCD} is  output 313 CHAPTER  3: Controllability Observability and  Special Forms reachable if and only if rank C =  p. 11 n (c)  L e t A=  0 [o Is the system output reachable? Is it state reachable? 0] 0  5 =0 C=  [110] and D =  0. 0  -ij 0 -2 [i^ (i) (ii)  Let  x(0)  =  0.  Determine  an  appropriate  input  sequence  to  transfer  the output to yi  =  3 in minimum time. Repeat for x(0)  =  [1-12] . 3.6.  (a)  Given  x  = Ax-^Buy  = Cx^Du show  that  this  system  is  output  reachable  if and  only  if  the  rows  of  the  /? x m  transfer  matrix  H{s)  are  linearly  indepen dent  over  the  field  of  complex  numbers.  In  view  of  this  result  is  the  system His) 1  1 5 +  2 s output  reachable? (b)  Similarly  for  discrete-time  systems  the  system  is  output  reachable  if  and  only if  the  rows  of  the  transfer  function  matrix  H{z)  are  hnearly  independent  over the  field  of  complex  numbers.  Consider  now  the  system  of  Exercise  3.5  and determine if it is output reachable. 3.7.  Show  that  the  circuit  depicted  in  Fig.  3.6  with  input  u  and  output  y  is  neither  state reachable nor observable but is output reachable. "O T y 1 FIGURE 3.6 Circuit for Exercise  3.7 3.8. A  system  x  = Ax-^Buy  =  Cx + Du  is  called  output function  controllable  if  there exists  an input  u{t)t  G [0oo) that  will  cause the  output y{t)  to  follow  a  prescribed trajectory  for  0 <  ^ <  ^o assuming  that the  system is  at rest  at ^ =  0.  It is easiest  to derive a test for output function  controUabihty in terms of the /? x m transfer  function matrix H{s)  and this is the  approach  taken in the following.  We say that the m x /? rational matrix HR(S) is a right  inverse ofH{s)  if H{s)HR{s)=Ip. the sufficiency  proof  select///?  =  H^{HH^)-\ (a)  Show  that  the right  inverse HR{S) exists  if  and  only  if  rank  H{s)  =  p.  Hint:  In the (right) pseudoinverse  of//. (b)  Show  that  the  system  is  output  function  controllable  if  and  only  if  H{s)  has  a right inverse  HR{S).  Hint:  Consider y  = Hu.  In the necessity proof  show that if rank H  <p  then the system may not be output function  controllable. Input  function  observability  is  the  dual  to  output  function  controUabihty. Here the left inverse  ofH(s)HL{s) is of interest and is defined  by HL{s)H(s)=Im.. 314 Linear  Systems (c)  Show that the left inverse Hi{s)  of H{s)  exists if and only if rankH(s)  =  m.  Hint: This is the dual result to part (a). (d)  Let H(s)  = \s  +  I ll s s _ (at rest at r  =  0) to exactly follow  a step _y(^) 1/^. and characterize  all inputs  u(t)  that will cause the  system Part  (d) points to a variety  of questions  that may  arise when  inverses  are  consid ered including: Is HR(S) proper? Is it unique? Is it stable? What is the minimum degree possible? 3.9.  Consider the system x  =  Ax-\-  Bu  y  =  Cx.  Show that output function  controllability implies output controllability  (-from-the-origin  or reachability). 1  1 0  1 3.10.  Given 4 ^ + 1)  = x(k)  + u(k)  y(k)  ~ x(k)  and assume zero initial conditions. (a)  Is there a sequence of inputs {w(0) u(l)...} that transfers  the output from  ^(0) 0' to  0' w in finite time? If the answer is yes determine  such a sequence. W (b)  Characterize  all outputs  that can be reached  from  the zero output  iy{0)  = m one step. 3.11.  Consider the state equation  x(t)  = 0  0 0  1 x(t)  + 1 u(t). (a)  Show that it is controllable at any ^o E  (-0000). (b)  Suppose we are interested only in X2(t) \x(t)  = xiit) Consider  therefore. xiit)  =  X2{t) + e~^u{t). Is it possible to determine u{t) so that the state X2it) is transferred from X20 at ^ =  ^ [•^2(^0)  =  -^20] to the zero state at some t  =  t\  [^2(^1)  =  0] and then stay there? If the answer is yes find such a  u{t). (c)  In (b) let ^  =  0 and study the effects  of the sizes of t\  and  XQ on the magnitude of u{t). (d)  For the  system in  (b) determine if possible  a u{t)  so that the  state is  transferred from  xo at ^ =  ^0 to x^ at ^ =  t\  and then  stay there. 3.12.  Let F{t)  E  C"^'^ be a matrix with fi{t)  in its /th row. Let f^  G  C{R ^ ^ ).  It was shown that the set fi{t)  i  =  1...  n is linearly  independent  on  [ti ^2] over the field of com plex numbers if and only if the Gram matrix  W(ti  ^2) is nonsingular (see Lemma 2.7). If  the  fi(t)  i  =  1...  n have  continuous  derivatives  up to order  (n  -  1) then  it  can be shown that they are linearly independent if for  some ^0 ^  [h h]^ rank[F(to)F^'\to)...F^''-'\to)] =  n. If  fi(t)  i  =  \  ...n only if for any fixed to E  [^1 ^2] are analytic  on  [t\ ^2] then they  are linearly  independent  if  and rank  [Fit^)  F^'\tol... F^^'-'Xtol...]  =  n. (a)  The above results can be used to derive alternative tests for controllability. In par ticular consider the state equation X =  A(t)x  +  B(t)u where  A(t)  E  /?"^"  and  B(t)  E  /?"^'^  have  continuous  derivatives  up  to  order (n  -  1). Then  it  can  be  shown  that  (A(t)  B{t))  is  controllable  at  time  ^  if  there exists finite ^i  >  ^  such that rank[Mo{t\)M\{ti) ...Mn-\{t\)] =  n where the Mk{t)  G /?"^" are defined  by M+i(0  = -A{t)Mk{t)+j^Mk{t) 0 1 . . . /2  -  1 315 CHAPTERS: Controllability Observability and  Special Forms with Mo(0  =  B{t). (i)  Prove this result. (ii)  Show that the system i:  =  A{t)x  + B{i)uW\i\\A{t)  = is controllable  at any tQ. 1  0" 't 0 t 0 0  0  ^2 .B{t)  = "0" 1 .1. (b)  The results for linear independence of vectors fi(t)i  =  1...  n can also be used to  derive  conditions  for  certain  specialized  types  of  controllability.  In  particular given  X  =  A(t)x  +  B(t)u  as  in  (a)  a  system  is  called  differentially  controllable at  to when  the  transfer  from  any  x(to)  =  xo  to  xi  can  be  accomplished  in  an arbitrarily  small  interval  of  time.  Note  that  this  may  lead  to  large  input  magni tudes.  It  can  be  shown  that  when  A(0 B{t)  are  analytic  on  (-oo ^)  the  system is  differentially  controllable  at  every  t  E  (-0000)  if  and  only  if  for  any  fixed ^0  G  ( - 0 0  00) rank  [Mo(to) Mi(to)...  Mn-i(to)  ...]  =  «. The system is instantaneously  controllable  if and only if for  allt  G (-00 00) rank  [Mo(t)...  M„_i(0]  =  n. In this  case the transfer  of the  states can be  achieved  instantaneously  at any  time by using inputs that include 6-functions  and their derivatives up to order of n -  1. Note that in general instantaneous  controllability  implies  differential  controllabil ity which  in turn implies  controllability.  In the case when A(t)  Bit)  are  analytic on (-00 00) as above then if  {A{t) B(t))  is controllable  at some point it is  differ entially controllable at every t  G (-00 00). Show  that in the  time-invariant  case  x  =  Ax  + Bu  controllability  of  (A B) always  implies  both  differential  and  instantaneous  controllability;  i.e.  if  a  state transfer  is  possible  at  all  it  can  be  achieved  in  an  arbitrarily  small  time  inter val  or  even  instantaneously  if  the  5-function  and  its  derivatives  are  used.  For the  latter  case  see  T.  Kailath  Linear  Systems  Prentice-Hall  1980  for  further details. Remark:  In controllability the transfer of the state occurs in finite time but the time  interval  may  be  very  large. In  differential  controllability  the transfer  of  the state is possible in arbitrarily small intervals of time; however this may lead to very large  input  magnitudes  (see Example  2.1). When  the  system  x  =  A{t)x  +  B(t)u is uniformly  controllable  the transfer  of the states can be achieved  in some  finite time interval using an input with magnitude not arbitrarily large. Note that  uniform controllability  implies  controllability.  Uniform  controllability  is useful  in  optimal control theory. For  additional  discussion  on differential  and uniform  controllabil ity see C. T. Chen Linear  System  Theory and Design  Holt Rinehart and Winston 1984 and the references  cited therein. Note that dual results  also exist for  differ ential instantaneous and uniform  observability. 316 Linear Systems 3.13.  Suppose  that for  the system  x{k  + 1) 1  1  0 0  1  0 0  0  1 x{k\  y(k) x(k)  it is known  that  y{^)  =  y{l)  =  y{2) = . Based  on  this  information  what  can  be  said about the initial condition  x(0)? 3.14.  (a)  Consider the system x  =  Ax  + Bu  y  =  Cx  + Du  where (A C) is assumed to be observable. Express x(t)  as a function  of y(tX  u(t) and their derivatives. Hint:  Write y(tX  y^^\tl... u^^-^\t)  (x(t)  G /?"). (b)  Given the system x  =  Ax  + Bu  y  =  Cx  + Du  with (A C) observable. Determine x(0)  in  terms  of  y(t)  u{t)  and  their  derivatives  up  to  order  n  -  \.  Note  that  in general this is not a practical way of determining  x(0) since this method  requires differentiation  of signals which is very  susceptible to measurement  noise. /^-i>(0  in terms of x(t)  and u(t) u^^\t)... (c)  Consider  the  system  x{k  +  1)  =  Ax{k)  +  Bu(k)  y(k)  =  Cx(k)  +  Du(k)  where y(k  + n -  1) y(k  + n-  1) in terms of [x(k)  G  i?"]. Note the relation to expression (A  C) is observable. Express x(k)  as a function  of y(k)  y(k  + I)... and u{k) u(k+  1)...  y(k  + n-  1). Hint: Express y(k)... x(k)  and u(k)  u(k-\-1)...  u(k+n-1) (3.48) in Section 3.3. 3.15.  Write software  programs to implement the algorithms of Section  3.4. In  particular: (a)  Given the pair (A B) where A G /?"><" B  G 7?"^'" with rank  [B AB...  A""^5]  =  nr <  n reduce this pair to the standard uncontrollable  form A  =  PAP-^  = An Ai 0 B  =  PB where (Ai Bi)  is controllable and Ai  G /?«'^x«^ Bi  G  R^rXm^ (b)  Given the controllable pair (A B)  where A G /?"><" 5  G i?^^'^ with ran^ 5  =  m reduce this pair to the controller form  Ac  =  PAP'^  Be  =  PB. 3.16.  Determine the uncontrollable modes of each pair (A B)  given below  by (a)  reducing  (A 5) using a similarity  transformation (b)  using eigenvalue/eigenvector  criteria. A  = 1 0 0 0  0" -1  0 0  2. B  = "1  0" 0  1 .0  0_ and A  = 0  0  1 0  0  1 0  0  0 0  0  0 o" 0 0 -1 B  = 0  r 0  0 1  0 0  0 3.17.  Consider the system x  =  Ax  + Bu  y  =  Cx  +  Du. (a)  Show that only controllable  modes  appear in  e^^B  and therefore  in the  zero-state response of the state. (b)  Show that only observable modes  appear in  Ce^\  and therefore  in the  zero-input response of the  system. (c)  Show that only modes that are both controllable  and observable  appear in  Ce^^B and therefore  in the impulse response and the transfer  function  matrix of the sys tem. Consider next the system  x{k  +  1)  =  Ax{k)  +  Bu(k)  y(k)  -  Cx(k)  +  Du(k). (d)  Show that only controllable modes appear in A^B  only observable modes in  CA^ and only modes thrt  are both controllable and observable appear in CA^B  [that is inHiz)]. 317 CHAPTER 3: Controllability Observability and  Special Forms (e)  Let  A  =   B  = C  -  [1 1 0] and  D  -  0.  Verify  the  results 0 2 0 0" 0 - 1. 1" 0 .1. obtained in (d). Compare these with Example 4.10. 3.18.  Reduce the pair A  = 0 3 1 1 0 0 1 0 1 -3 4 -1 0 1 -1 0 B  = 0 1 0 0 0 0 1 0 into  controller  form  Ac  =  PAP matrix in this case? What are the controllability  indices? \  Be  =  PB.  What  is  the  similarity  transformation 3.19.  Let A  =  Ac + BcAmmdB  =  5^5^ where the A ^ ^c  are as in (4.63) with A^  G T?'"^^" Bm G  /?^x^  and  \Bm\ ^  0.  Show  that  (A B)  is reachable  with  controllabihty  indices juii. Hint:  Use the eigenvalue test to show that (A B) is reachable. Use state feedback to simplify  (A B) (see Exercise 3.21) and show that the /Xi are the controllability indices. 3.20. Show that the controllability indices of the state equation x  =  Ax-\-BGv  where |G|  T^ 0 and  (A B)  is reachable  with  A  G  /?«x" B  G  R^^^^^ are  the  same  as  the  controllabil ity  indices  of  x  =  Ax  + Bu  within  reordering.  Hint:  Write  %k  =  [BG  ABC... A^-^BG]  =  [5 A 5  . . .  A^~^B]  • [block diag  G]=%k' [block diag  G] and show that the number  of linearly  dependent  columns  in A^5G  that  occur  while  searching  from left  to right in %n  is the same as the corresponding  number in %n-3.21. Consider  the  state  equation  x  =  Ax  + Bu  where  A  G  /?">^" B  G  R"^"^ with  (A B) reachable. Let the linear state-feedback  control law be w =  F^  +  Gv F  G /^'^x^ G G f^mxm ^'^^^ 1^1 ^  Q 3how  that (a)  (A +  BF  BG)  is reachable. (b)  The controllability  indices of (A +  BF  B)  are identical to those of (A  B). (c)  The controllability indices of (A +  BF  BG)  are equal to the controllability  indices of (A B) within reordering. Hint:  Use the eigenvalue test to show (a). To show (b) use the controller forms  in Section  3.4. 3.22.  Show that if (A B) is controllable (-from-the-origin)  where A G  /?"^^ and B  G  i?"^^ and rank  B  =  m then rank  A >  n — m. 3.23.  Consider Show that 0 0 1 -OLn~l} 0  0 0  0 0 0 Br  = 0 1 1 c\ [ 5 „ A  5 „ . . .  A r ' 5 c] 0 0 1 0 1 Ci 1 Ci C2 ••• ••• ••• c„-3 Cn-2 Cn-l 318 Linear  Systems where Q  =  -Zf=oQ;„---1 ^ Oil OL3 O^n-l 1 1 0  7 1 - 1  with Co =  1. Also show that ••  an-i 1 1 01 3.24.  Given  A  G  Z^^^^ and  B  G  /e^^^'"  let ra/zy^ %  =  n  where  ^  =  [5 AB...  A^^i^]. Consider i  G /?^>'^ B  G i?'^^^^ with rank %  =  n where ^  =  [BAB...  A'^'^B]  and assume that P  G Z?"^'' with det  P 7^  0 exists  such that Show that ^  =  P5  and A  =  PAP'K  Hint:  Show that (PA  -  AP)^  =  0. 3.25.  Show that the matrices Ac  == PAP~\  Be  =  P5  are (a)  given by (4.47) if P is given by (4.48) (b)  given by (4.50) if 2 (=  P'^)  is given by (4.49) (c)  given by (4.52) if e (=  p-^)  is given by (4.51). 3.26.  In the  circuit  of Example  4.9 let  R1R2C  =  L  and  Ri  =  R2  =  R.  Determine  x(t)  = [xi{t)  X2(t)Y  and  i{t)  for  unit  step  input  voltage  v(0  and  initial  conditions  x(0)  = [a b]^.  Comment on your results. 3.27.  Consider  the  pair  (A b)  where  A  G  T?"^'^ b  G  R'\  Show  that  if  more  than  one  lin early  independent  eigenvector  can be associated  with  a single eigenvalue then  (A b) is uncontrollable. Hint:  Use the eigenvector test. Let vi V2 be linearly independent  left eigenvectors  associated  with  eigenvalue  Ai  =  A2  =  A. Notice  that  if  vib  =  ai  and vib  =  0L2 then {a7^v\  -  a^^V2)b  =  0. ro -1 1 3.28.  (a)  Consider the state equation  x  =  Ax  + Bu  x(0)  =  XQ where A  = andB  = ri 1 .1 01 1 2. . Determine  x{t)  as a function  of  u(t)  and  XQ and verify  that  the uncontrollable modes do not appear in the zero-state response but do appear in the zero-input response  (see Example 4.1). (b)  Consider  the  state  equation  x(k  +  1)  =  Ax(k)  +  Bu(k)  and  ^(0)  =  XQ  where A  and  B  are  as  in  (a).  Demonstrate  for  this  case  results  corresponding  to  (a). In (a) and (b) determine x(t)  and x(k)  for unit step inputs and x(0)  =  [11  1]^. 3.29.  (a)  Consider the system i  =  Ax+Buy  =  Cxwithx(O)  =  xo where A •2 -3J B  = and  C  =  [1 1]. Determine  y(t)  as a function  of  u(t)  and  XQ and  verify that the unobservable modes do not appear in the output (see Example 4.3). (b)  Consider  the  system  x(k  +  1)  =  Ax(k)  + Bu(k)  y(k)  =  Cx(k)  with  x(0)  = XQ where A B and C are as in (a). Demonstrate for this case results which  correspond to (a). In (a) and (b) determine and plot y(t)  and y(k)  for unit step inputs and x(0)  =  0. 3.30.  Consider the system  x(k  +  1)  == Ax(k)  + Bu(k)  y(k)  =  Cx(k)  where A  = 1 0 0 0 1 2 0 0 0 _  1 1 B C  =  [110]. Determine  the  eigenvalues  that  are  uncontrollable  and/or  unobservable.  Determine x(k)  y(k)  for  /: >  0  given  x(0)  and  u(k)  k  >  0  and  show  that  only  controllable eigenvalues  (resp.  modes)  appear  in  A^B  only  observable  ones  appear  in  CA^  and only  eigenvalues  (resp.  modes)  that  are  both  controllable  and  observable  appear  in CA^B[mH{z)l 3.31.  For the system  x  =  Ax  + Bu  y  =  Cx  consider the corresponding  sampled-data  sys tem x(k  +  1)  =  Ax(k)  +  Bu(k)  y(k)  =  Cx(k)  where 319 CHAPTERS: Controllability Observability and  Special Forms A  =  e^ B  = 'dr  B and c = a (a)  Let the continuous-time  system {A B C] be controllable  (observable)  and  assume it is a SISO  system. Show that {A B C} is controllable  (observable)  if and only if the sampling period  T is such that Im  (Xi -  Xj)  7^ ~^^'  where  /:  =  ±1 ± 2  . ..  whenever 7?^ (A/ -  Xj) 2'Trk 0 where {AJ are the eigenvalues of A. Hint:  Use the PBH test. Also consult  Appen dix D of C. T. Chen Linear  System  Theory  and Design.  Holt Rinehart  and Win ston  1984. (b)  Apply  the  results  of  (a)  to  the  double  integrator—Example  7.6  in  Chapter  2— 0 U\ C  =  [1 0]. Determine the values of T that preserve controllability  (observability). C  =  [10]  and  also  to  A I [-1  oj 0  1 0  0 where  A   B  =   B  = 0 3.32.  Given is the system  x 1 0 0 0 -1 0 x  + "1  0~ 0  1 LO  OJ ^  y  = ri  1  0] 1  0  0 (a)  Determine the uncontrollable  and the unobservable eigenvalues  (if any). (b)  What is the impulse response of this system? What is its transfer  function  matrix? (c)  Is the system asymptotically  stable? 3.33.  Consider  the  system  x(k  +  1)  = x(k)  + u(k)  y(k)  =  [I 3]x(k).  Suppose that it is known  that for  zero input  y(0) If yes find x(0)  and verify  your  answer. 1 and  _y(l)  =  1. Can  x(0)  be  determined? 3.34.  Given is the transfer  function  matrix  H(s) 0 s + 1 s + 2 0 (a)  Determine the Smith-McMillan  form  of H(s)  and its characteristic  (pole) polyno mial and minimal polynomial. What are the poles of  H(s)7 (b)  Determine the zero polynomial  of H(s).  What are the zeros of  H(s)l 3.35.  Let H(s)  = c2 1 s^ s+  1 (a)  Determine the Smith-McMillan  form  of H(s)  and its characteristic  (pole) polyno mial and minimal polynomial. What are the poles of  H(s)7 (b)  Determine the zero polynomial of H(s).  What are the zeros of  H(s)l 3.36.  A rational  function  matrix  R(s)  may  have in  addition  to finite poles  and  zeros  poles and  zeros  at  infinity  (s  =  oo). To  study  the  poles  and  zeros  at  infinity  the  bilinear 320 Linear  Systems transformation s  = biw  + aiw  +  ao with ai  7^ 0 biao  -  h^ax  y^ 0 may be used where b\lai  is not a finite pole or zero of R{s).  This transformation  maps the point s  =  b\la\  iow  =  ^  and the point of interest 5- =  00 to w  =  -aja\.  The rational matrix  R{\v) is now obtained  as R{w)  =  R 1\W  • aiw  -\-  ao and  the  finite  poles  and  zeros  of  R(w)  are  determined.  The  poles  and  zeros  at  w  = -ao/a\  are the poles and zeros of R(s)  at 5* =  oo. Note that frequently  a good choice for the bilinear transformation  is ^  =  1/w  that is bi  =  0 bo  =  1 and ai  =  I  ao  =  0. (a)  Determine the poles and zeros at infinity  of Ri(s)  = 1 s+  r Riis) Rsis)  = 1 s+  1 Note that a rational matrix may have both poles and zeros at  infinity (b)  Show that if R(s)  has a pole at 5" =  oo then it is not proper (lim^^oo R(s) • -). 3.37.  Determine the poles and zeros at infinity  of the transfer functions  in Examples  5.15.2 5.6 and  5.7. 3.38.  (Spring mass system)  Consider the spring mass given in Exercise 2.69 in Chapter 2. (a)  Is the system controllable from  [/i  /i]^?  If yes reduce (A B) to controller  form. (b)  Is the system controllable from  input  fi  only? Is it controllable from  /2 only? Dis cuss your answers. (c)  Let  y  =  Cx  with  C  = ri  0  0  01 0  10 reduce (A C) to observer  form. 0 Is  the  system  observable  from  j?  If  yes. 3.39.  (Aircraft  dynamics)  Consider  the  state-space  description  of  the  lateral  motion  of  an aircraft  in Exercise 2.76 in Chapter 2. (a)  Is the system controllable from  [8A  8R]^7 If yes reduce (A B) to controller  form. (b)  Is the system controllable using only the ailerons? Is the system controllable using only the rudder? Discuss your answers. ro  1  0  01 [o  0  1  oj (c)  Let  y  =  Cx  with  C  = reduce (A C) to observer  form. Is  the  system  observable  from  yl  If  yes. CHAPTER  4 State Feedback and State Observers =1 ^lir?:S-«^|t; • > #l  ^ i  -^ *< - >V«ct<-<<;^'^-'^"^-;;¥ Feedback is a fundamental mechanism arising in nature and is present in many nat ural processes. Feedback is also common in manufactured  systems and is essential in automatic control of dynamic processes with uncertainties in their model descrip tions and their interactions with the environment. When feedback is used the actual values  of  system  variables  are  sensed  fed  back  and used  to control  the  system. Hence a control law  decision process  is based  not only  on predictions  about the system behavior derived from  a process model (as in open-loop control) but also on information about the actual behavior (closed-loop feedback control). A common example of an automatic feedback control system is the cruise control system in an automobile which maintains the speed of the automobile at a certain desired value within acceptable tolerances. In this chapter feedback  is introduced and the problem of pole or eigenvalue assignment by means of state feedback  is discussed  at length in Section 4.2. It is possible to arbitrarily assign all closed-loop eigenvalues by linear static state feed back if and only if the system is completely controllable. This relation to controlla bility is in fact the motivation for introducing state feedback at this point. Feedback control is considered again in Chapter 7 where polynomial matrix descriptions are introduced. In the study of state feedback  it is assumed that it is possible to measure the values of the states using appropriate sensors. Frequently however it may be either impossible or impractical to obtain measurements for all states. It is therefore desir able to be able to estimate the states from measurements of input and output variables that are typically available. In addition to feedback control problems there are many other problems where knowledge of the state vector is desirable since such knowl edge contains useful  information  about the system. This is the case for example in navigation systems. State observers that asymptotically estimate the states from input and output measurements over time are also studied in this chapter. State estimation is related to observability in an analogous way that state feed back  control  is related  to  controllability.  The  duality  between  controllability  and 321 322 Linear Systems observability  makes  it  possible  to  easily  solve  the  estimation  problem  once  the control  problem  has  been  solved  and  vice  versa.  In  this  chapter  full-order  and reduced-order asymptotic estimators also called observers are discussed at length in Section 4.3. Finally state feedback  static controllers and state dynamic observers are combined to form dynamic output feedback  controllers. Such controllers are studied in Section 4.4 using both state-space and transfer  function  matrix  descriptions. 4.1 INTRODUCTION A.  A Brief  Introduction  to State Feedback  Controllers and State  Observers In  the  following  discussion  state feedback  and  state  estimation  are  introduced  for continuous- and discrete-time time-varying  and time-invariant  systems. We consider  systems described by equations of the  form i:  =  A{t)x  +  B{t)u y  =  C(t)x  +  D(t)u (1.1) where  t  G  (a b)  some real  open  interval  and  A(t)  G  R''^''  B{t)  G W'^'^^  C(t)  G j^pxn^  £)(r)  G  RP^^^  and  u(t)  G  R"^ are  (piecewise)  continuous  in  t  on  (a b)  (see Chapter 2). Let  the input  u be determined  by  a time-varying  linear state feedback  control law of the  form u  =  F{t)x  +  r (1.2) where  F{t)  G  R^^^  is  (piecewise)  continuous  in  f  G  {a b)  the  elements  of  F(t) represent  time-dependent  gains  and  r(t)  G  R^  is  an  external  input  (see  Fig. 4.1). Substituting  into  (1.1)  the  state-space  description  of  the  compensated  or  closed-loop system  is given by is given by X =  [A(0  +  B(t)F(t)]x  +  B(t)r y  -  [C(0  +  D(t)F(t)]x  +  D(t)r (1.3) We seek to select F(t)  so that the closed-loop  system has certain  desirable  qualita tive properties. For example we may wish the closed-loop system to be stable. (For definitions  of stability in the time-varying  case refer  to Chapter 6.) Stability can be achieved  under  appropriate  assumptions  involving  certain  types  of  controllability. One way of determining  such stabilizing F(t)  is to use results from the optimal Lin ear Quadratic Regulator  (LQR) theory which in fact yields the "best" F(t)  in  some System +T FIGURE 4.1 sense. Stabilization  or the achievement  of other control  objectives  for  linear  time- varying  systems  via LQR  or other methods  will not be  studied  in this chapter.  The stabilization  of linear time-invariant  systems however  is discussed  at great length in Section 4.2. In  the  time-invariant  case  we  consider  systems  described  by  equations  of  the 323 CHAPTER 4: state Feedback and State Observers form X =  Ax  +  Bu y  =  Cx  +  Du where A  E  /?^x^ B  G /^^X"^ C  G J?^><^ and D  G  RP"""^.  Let u  =  Fx  + r (1.4) (1.5) represent the linear time-invariant  or static state feedback  control law. The  compen sated  or closed-loop  system  is then given by the  equations i  -  (A +  BF)x  + Br y  =  (C  -\-  DF)x  +  Dr (1.6) In control design the gain matrix F is selected  so that the closed-loop  system  (1.6) has desired behavior. In this chapter we are particularly  interested  in selecting F to stabilize  the  system  and  we  will  concentrate  on  the  eigenvalue  assignment  prob lem.  In particular  it is  shown  in  the  next  section  that  the  controllable  eigenvalues of  the  system  [i.e.  of  the  pair  (A  +  BF  B)]  can  be  assigned  (or  shifted)  to  arbi trary  locations  and  therefore  can  be  assigned  to  locations  that  guarantee  stable behavior. In view of the results developed in the previous chapter the central role played by controllability  in feedback  stabilization  is most easily  seen in the  time-invariant case.  Recall  that  controllability  reflects  the  ability  of  an  input  to transfer  the  state of  a  system  to  any  desirable  (state)  value.  It  was  shown  in  Section  3.4  that  given a  time-invariant  system  {A B C D} there  exists  a transformation  so that  the  con trollable part  of the  state  space  can  easily  be recognized.  In  other  words there  ex ists  a  basis  for  the  state  space  so  that  all  controllable  states  have  representations of  the  form  [x[ 0^]^  and  the  uncontrollable  state  representations  are  of  the  form [x[ X2V  where X2 =  A2X2 i.e. X2 is independent of u (see Subsection 3.4A). The eigenvalues  of A2 are in this case the uncontrollable  eigenvalues that correspond to the uncontrollable modes of the system. Since u has no effect  on X2 it is reasonable to conjecture that the state feedback  control law (which in fact is a particular choice for u) will not at all affect  the eigenvalues of A2 for if it did it would also affect X2. This conjecture  turns out to be true and will formally  be  shown in the next  section. The controllable states can now arbitrarily be shifted using u. This in turn means that the controllable eigenvalues can be arbitrarily  shifted  using w or more correctly by appropriately  selecting  u in xi  =  AiXi  -\-  Biu  -^ A12X2  xi  may be made to behave as if the eigenvalues of Ai  were arbitrarily shifted.  This is true because if there were restrictions  on  the  location  of  the  eigenvalues  then  the  state  would  not  be  able  to be shifted  arbitrarily which is a contradiction. It turns out that linear state  feedback control provides  such  u and therefore  it can  arbitrarily  shift  all the eigenvalues  of a  system.  This  will formally  be  shown  in  Section  4.2. A  stabilizing  state  feedback matrix F may  also be determined  as the solution to an optimal control problem  the Linear  Quadratic  Regulator  (LQR) problem. This is briefly  addressed  at the end of Section 4.2. 324 Linear Systems We will  also  consider  time-varying  discrete-time  systems  described  by  equa tions of the  form x{k  +  1)  =  A{k)x{k)  + B{k)u{k) y(k)  =  C(k)x(k)  +  D{k)u{k) (1.7) with linear discrete-time  time-varying  state-feedback  control  law given by u{k)  =  F(k)x(k)  +  r(k) (1.8) In this case the closed-loop system is given by x(k  +  1)  =  [A(k)  -h B(k)F(k)]x(k)  +  B(k)r(k) y(k)  =  [C(k)  +  D(k)F(k)]x(k)  +  Dik)r(k). (1.9) Also we will consider time-invariant  discrete-time  systems described  by x(k  +  1)  =  Ax(k)  +  Bu(k) y(k)  =  Cx(k)  +  Du(k) (1.10) with linear discrete-time  time-invariant  state-feedback  control law (time-invariant) given by u{k)  =  Fx(k)  +  r(k\ (1.11) In this case the closed-loop system assumes the  form x(k  +  1)  -  [A +  BF]x(k)  +  Br(k) y(k)  =  [C +  DF]x(k)  +  Dr(k). (1.12) As in the continuous-time  case stabilization is emphasized  in this chapter and cor responding results are developed. In (continuous-time)  state-space system representations if the value of the state at time  to x(to)  is known then the input  u(t) t  >  ^  uniquely  determines  x(t)  and y(t)  for t  ^  to. Since knowledge of the initial value of the state is of such importance methods of determining (estimating) x(to) from input and output measurements have been devised  (see Fig. 4.2). Methods  of  estimating  the  initial  value  of  the  state  x(0)  in  time-invariant systems  are  addressed  in  Section  4.3. In  particular  full-  and  reduced-order  state observers  are  designed  that  asymptotically  estimate  the  state.  Since  controllability (-from-the-origin  or reachability)  was  the  key  property  in  state  feedback  control the dual property of observabihty  studied in Section 3.3 is the key attribute in state estimation.  Recall  that  observability  refers  to  the  ability  of  determining  the  state from  measurements  of  the  output  and  input  over  a  finite  time  interval.  (See  for instance. Corollary  3.8 in Chapter  3 where the initial  state is determined  using  the observability Gramian of the system.) Furthermore since the solution to the optimal u Sysiern y State  observer \ * FIGURE 4.2 LQR  control problem  leads to optimal  state-feedback  control  law  matrices  [F(t)  or F] a dual optimal estimation problem can be defined the solution of which provides optimal state-observer gain matrices [K(t) or K]. Corresponding discrete-time results on  state  estimation  are  also  discussed.  In  addition  in  Section  4.4  state  feedback control laws and  state estimation  are combined  to derive a dynamic-observer  based controller that receives feedback information not from the state but from the outputs of the system to be controlled. These are typically more accessible. 325 CHAPTER 4: state Feedback and State Observers B.  Chapter  Description In this chapter state feedback  controllers and asymptotic state estimators also called state  observers  are  introduced  and  studied.  The  contents  of  the  chapter  were  out lined and briefly  discussed in the previous subsection  (Subsection 4.1 A). A detailed summary  follows. A  brief  introduction  to  state  feedback  controllers  and  state  observers  for continuous-time  and discrete-time  systems is given in Subsection 4.1 A. In  Section  4.2  state  feedback  control  is  studied.  Our  development  focuses  on time-invariant  systems.  First  the  need  for  feedback  is  demonstrated  by  a  discus sion of open- and closed-loop control laws and their differences  when  uncertainties are  present  in  the  system  model  and  its  environment.  The  stabilization  of  sys tems  is  emphasized.  This  leads  in  the  time-invariant  case  to  the  eigenvalue  (or pole)  assignment  problem  which  is  studied  next  at  length.  The  flexibility  offered in  the  multi-input  case  in  the  choice  of  the  state  feedback  gains  to  accomplish control  goals  in  addition  to  pole  assignment  is  stressed.  A  number  of  pole  as signment methods are introduced including the eigenvalue/eigenvector  assignment method;  an  additional  historically  important  method  is presented  in  Exercise  4.2. It  is  pointed  out  that  stabilization  can  also  be  achieved  by  an  optimal  control  for mulation  such  as  the  Linear  Quadratic  Regulator  (LQR)  which  leads  to  a  stabi lizing  state  feedback  control  law  while  attaining  additional  control  goals  as  well. The  LQR  problem  is  briefly  discussed  for  both  the  continuous-  and  discrete-time cases. In  Section 4.3 full-order  full-state  and partial-state  observers  for  continuous-and  discrete-time  systems  are  studied.  Also reduced-order  and  optimal  observers are addressed. In the discrete-time case current state estimators are also introduced. Optimal  Linear  Quadratic  Gaussian  (LQG)  estimators  are  briefly  discussed.  The duality  of  the  state  feedback  and  the  state  observer  problems  is  emphasized.  It  is pointed out that the main factor that limits the magnitude of the gains in observers is noise. This is in contrast to the limiting factor  in the magnitude of the control gains which is limitations of the control actuators and of the linear model used to describe the  system. In  Section  4.4  dynamic  state  observers  are  used  together  with  static  state feedback  controllers  to  derive  dynamic  output  controllers.' The  Separation  Prin ciple  is  discussed  and  the  degradation  of  performance  in  state  feedback  control when  an  observer  is  used  to  estimate  the  state  is  explained.  The  analysis  is  ac complished  in  both  state-space  and  transfer  function  frameworks.  Furthermore additional  output  controller  configurations  are  also  derived  in  terms  of  state-space representations. 326 Linear Systems C.  Guidelines  for  the  Reader In this chapter Unear state feedback  controllers  and linear state observers for  linear time-invariant  systems  are  studied.  Subsection  4IB  describes  the  contents  of  the chapter. At a first reading one can cover selected topics from the material on state  feed back and state observers in Sections 4.2 and 4.3 respectively. Regarding Section 4.2 on  state  feedback  after  studying  the  issues  associated  with  open-  and  closed-loop control  in  Subsection  4.2A  one could  concentrate  on eigenvalue  assignment  using linear state feedback. In particular one could study Theorem 2.1 in Subsection 4.2B which  shows  that  only  the  controllable  eigenvalues  can be  arbitrarily  assigned  via state feedback;  then  one could  study the methodologies  to assign  desired  eigenval ues (and in part the corresponding  eigenvectors  as well). Regarding  Section 4.3 on state observers at a first reading  one could concentrate  only on full-order  full-state observers  for  continuous-time  systems  in  Subsection  4.3A.  Then  one  could  study dynamic output feedback  controllers that are based on state observers in Section 4.4. The degradation  of performance  when  an observer is used to estimate the state in a state feedback  controller is also discussed in that  section. 4.2 LINEAR  STATE  FEEDBACK A.  Continuous-Time  Systems We consider linear time-invariant continuous-time  systems described by  equations of the  form x  =  Ax  + Bu y  =  Cx  + Du (2.1) where A  G /?"><^ B  G /e'^x^ C  G /^^x^ and D  G /?^><^. DEFINITION  2.1.  The linear  time-invariant state feedback control law is defined by where F  G R^^^  is a gain matrix and r{t) G R^ is an extemal input vector. u  = Fx  + r (2.2) • Note  that  r{t)  is  an  external  input  also  called  a  command  or  reference  input (see Fig. 4.1). It is used to provide an input to the compensated  closed-loop  system and  is  omitted  when  such  input  is  not  necessary  in  a given  discussion  [r{t)  =  0]. This  is  the  case  e.g.  when  the  Lyapunov  stability  of  a  system  is  studied.  Note that  the  vector  r(t)  in  (2.2)  has  the  same  dimension  as  u(t).  If  a  different  number of  inputs  is  desired  then  an  input  transformation  map  may  be  used  to  accomplish this. The compensated  closed-loop  system  of Fig. 4.1 is described by the  equations i  =  (A +  BF)x  +  Br y  =  {C + DF)x  +  Dr (2.3) which  were determined  by  substituting  u  =  Fx  ^- r into the description  of the un compensated  open-loop  system  (2.1). 327 CHAPTER 4: State Feedback and State Observers The state feedback  gain  matrix  F affects  the closed-loop system behavior.  This is accomplished by altering the dynamic effects  of the matrices A  and  C of (2.1). In fact the main influence of Fis exercised through the matrix A resulting in the matrix A + BF  of the closed-loop system. The matrix F affects  the eigenvalues of A +  BF and  therefore  the  modes  of  the  closed-loop  system.  The  effects  of  F  can  also  be thought of as restricting the choices for w (=  Fx  for r  =  0) so that for appropriate F certain properties such as asymptotic Lyapunov  stability of the equilibrium  x  =  0 are obtained. Open- and closed-loop  control The  linear  state  feedback  control  law  (2.2)  can  be  expressed  in  terms  of  the initial  state  x(0)  =  XQ. In  particular  working  with  Laplace  transforms  we  obtain u  =  Fx-^f  =  F[(^/-A)~^xo  + (^/-A)~^Bw]-t-f inviewof  ^-x-xo  =  Ax  +  Bu derived  from  x  =  Ax  + Bu.  Collecting  terms  we  have  [/  -  F{sl  -  A)~^B^u  = F(sl  -  A)~^xo  +  r. This yields u  =  F[sl  -  (A +  BF)r^xo  +  [/  -  F(sl  -  Ay^BV^r (2.4) where the matrix identities  [/  -  F(sl  -  A)-^Br^F(sI BF(sI  -  A)-i]-i  ^  F[sl  -  (A +  BF)]-^  have been  used. -  A)"^  =  F(sl  -  Ay^[I -Expression  (2.4)  is  an  open-loop  (feedforward)  control  law  expressed  in  the Laplace transform  domain. It is phrased in terms of the initial conditions x(0)  =  XQ and if it is applied to the open-loop system (2.1) it generates exactly the same control action  u(t)  for  r >  0  as  the  state  feedback  u  =  Fx  -\-  r  in  (2.2). It  can  readily  be verified  that the descriptions  of the compensated  system are exactly the same when either control expressions (2.2) or (2.4) are used (verify  this). In practice however these two control laws hardly behave the same as explained in the  following. First notice that in the open-loop  scheme  (2.4) the initial conditions  XQ  are as sumed to be known exactly. It is also assumed that the plant parameters in A and B are known  exactly.  If  there  are  uncertainties  in  the  data  this  control  law  may  fail miserably even when the differences  are small since it is based on incorrect  infor mation  without  any way  of knowing  that these data are not valid. In contrast to the above  the  feedback  law  (2.2)  does  not  require  knowledge  of  XQ. Moreover  it  re ceives feedback  information  from  x(t)  and adjusts  u(t)  to reflect  the current  system parameters and consequently is more robust to parameter variations. Of course the feedback  control law (2.2) will also fail when the parameter variations are too large. In  fact  the  area  of  robust  control  relates  feedback  control  law  designs  to  bounds on  the  uncertainties  (due  to  possible  changes)  and  aims  to  derive  the  best  design possible under the circumstances. The  point  we  wish  to  emphasize  here  is  that  although  open-  and  closed-loop control laws may appear to produce identical effects  typically  they do not the rea son being  that  the mathematical  system  models  used  are  not  sufficiently  accurate by  necessity  or  design.  Feedback  control  and  closed-loop  control  are  preferred  to accommodate ever-present modeling uncertainties in the plant and the environment. The purpose of controlling a system is to achieve certain control goals. Examples include tracking a given trajectory  regulating the state so that it returns to the origin if it is disturbed  and stabilizing a system. Stabilization is discussed at length in this chapter.  Regulation  and  tracking  together  with  other  control  problems  are  briefly discussed  in  Chapter  7  where  output  feedback  compensation  in  addition  to  state feedback  compensation is used. 328 Linear Systems At this point a few observations are in order. First we note that feeding back the ^^ate in synthesizing a control law is a very powerful  mechanism since the state con tains all the information  about the past history of a system that is needed to uniquely determine the future system behavior given the input. We observe that the state feed back control law considered presently is linear resulting in a closed-loop system that is also linear. Nonlinear  state feedback  control laws are of course also possible. No tice that when a time-invariant  system is considered  the state feedback  is typically static  unless  there  is  no  choice  (as  in  certain  optimal  control  problems)  resulting in a closed-loop  system that is also time-invariant. These comments justify  to a cer tain extent the choice of linear time-invariant  state feedback  control to compensate linear time-invariant  systems. The problem of stabilizing  a system by using state feedback  is considered  next. Stabilization The problem  we wish  to consider now  is to determine  a state feedback  control law  (2.2)  having  the  property  that  the  resulting  compensated  closed-loop  system has  an  equilibrium  x  =  0 that  is  asymptotically  stable  (in  the  sense  of  Lyapunov) when  r  =  0. (For a discussion of asymptotic  stability refer  to Chapters 2 and 6.) In particular we wish to determine a matrix F  G R^^^  so that the  system i:  =  (A +  BF)x (2.5) where A G R^^^midB  E  7?'^^'^ has equilibrium x  =  0 that is asymptotically stable. Note that (2.5) was obtained from  (2.3) by letting  r  =  0. One method of deriving  such  stabilizing F is by formulating  the problem  as an optimal  control  problem  e.g.  as  the  Linear  Quadratic  Regulator  (LQR)  problem. This  is discussed  at the end  of this  section. We point  out that  an LQR  formulation can also be used to derive stabilizing gains F(t)  in the time-varying  case so that the equilibrium  x  =  0  of  the  system  x  =  [A(t)  +  B(t)F(t)]x  is  asymptotically  stable. However this will not be pursued here. Alternatively  in view  of Chapter  2 (and  Chapter  6) the equilibrium  x  =  0 of (2.5)  is  asymptotically  stable  if  and  only  if  the  eigenvalues  A/ of  A +  BF  satisfy Re  A/ <  0 /  =  1...  n. Therefore  the stabilization  problem  for  the  time-invariant case reduces to the problem of selecting F in such a manner that the eigenvalues of A +  BF  are shifted  into desired locations. This will be studied in the following  sub section. Note that stabilization is only one of the control objectives although a most important  one that can be achieved  by  shifting  eigenvalues.  Since the  eigenvalues of  a linear  system  determine  its  qualitative  dynamic  behavior  (refer  to the  discus sion  of  modes  in  Chapter  2)  one  can  attain  a number  of  control  goals  by  shifting of eigenvalues in addition to stability. Control  system design  via eigenvalue  (pole) assignment is a topic that is addressed in detail in a number of control books. B.  Eigenvalue  Assignment Consider again the closed-loop system i  =  (A + BF)x  given in (2.5). We shall show that if (A B) is fully  controllable (-from-the-origin  or reachable) all eigenvalues of A + BF  can be arbitrarily assigned by appropriately  selecting F. In other words "the eigenvalues of the original system can arbitrarily be changed in this case." This last statement  commonly  used  in  the  literature  is  rather  confusing:  The  eigenvalues of  a  given  system  x  =  Ax  +  Bu  are  not  physically  changed  by  the  use  of  feed back.  They  are  the  same  as  they  used  to  be  before  the  introduction  of  feedback. Instead  the  feedback  law  u  =  Fx  ~\-  r  r  =  0  generates  an  input  u(t)  that  when fed  back  to  the  system  makes  it behave  as  if the  eigenvalues  of  the  system  were  at different  locations  [i.e.  the  input  u(t)  makes  it  behave  as  a  different  system  the behavior  of  which  is  we  hope  more  desirable  than  the  behavior  of  the  original system]. 329 CHAPTER 4: State  Feedback and  State Observers THEOREM2.1.  GivenA  G /?"><" a n d5  G Z?"^'" there exists F  G /?""><" such that then eigenvalues of A + BF  can be assigned to arbitrary real or complex conjugate  locations if and only if (A B) is controllable  (-from-the-origin  or reachable). Proof.  (Necessity)  Suppose  that  the  eigenvalues  of  A +  BF  have  been  arbitrarily  as signed  and  assume that  (A B)  in (2.1) is not fully  controllable. We shall  show that  this leads  to a contradiction.  Since  (A B)  is not fully  controllable  in view  of the results  in Section  3.4  in  Chapter  3 there  exists  a similarity  transformation  that  will  separate  the controllable  part from  the uncontrollable  part in  (2.5). In particular  there  exists  a non-singular matrix  Q such that Q-\A +  BF)Q  =  Q-'AQ +  (Q-'B)(FQ) = Ai 0 An Ai. {FF2\ Ai  +  BiFi Ai2 + BxF2 0 A2 (2.6) where  [Fi F2]  =  FQ  and  {A\Bi) is controllable.  The  eigenvalues  of A +  BF  are  the same  as  the  eigenvalues  of  Q~^{A  +  BF)Q  which  implies  that  A  +  BF  has  certain fixed  eigenvalues  the  eigenvalues  of  A2 that  cannot  be  shifted  via  F.  These  are  the uncontrollable  eigenvalues  of  the  system.  Therefore  the  eigenvalues  of  A +  BF  have not  been  arbitrarily  assigned  which  is  a  contradiction.  Thus  (A B)  is  fully  control lable. (Sufficiency)  Let  (A B)  be fully  controllable. Then by using  any of the  eigenvalue assignment algorithms presented later in this section all the eigenvalues of A +  BF  can be arbitrarily  assigned. • LEMMA 2.2.  The uncontrollable eigenvalues of (A B) cannot be shifted  via state  feed back. Proof  See the necessity part of the proof  of Theorem  2.1. Note that the  uncontrollable eigenvalues  are the eigenvalues of A2. • EXAMPLE  2.1.  Consider  the  uncontrollable  pair  (A 5)  where  A  = 0 -2 1  -3   B  = . This pair can be transformed  to a standard form for uncontrollable systems namely A  = -2 0 1 -1 B  =  from which it can easily be seen that -1  is the uncontrollable eigenvalue while  -2  is the controllable eigenvalue (see also Example 4.3 in Chapter 3). NowifF  =  [ff2lihendet(sI-(A + BF))  =  det\ \~  ^\ ]_~^^\ L -1  -  Jl  S + 3- f2_ -f2  + 3) + (-f-f2 s(-f\ eigenvalue  -1  cannot be shifted  via state feedback.  The controllable eigenvalue  -2  can • be shifted  arbitrarily  to (f  +  /s  -  2) by F  =  [/i  /2]. + 2)  =  (s+  l)(s  + ( - /i  -  /2 + 2)). Clearly the uncontrollable 330 Linear Systems It is now quite clear that a given system (2.1) can be made asymptotically  stable via the state feedback  control law (2.2) only when all the uncontrollable  eigenvalues of  (A B)  are  already  in  the  open  left  part  of  the  s-plane.  This  is  so because  state feedback  can alter only the controllable  eigenvalues. DEFINITION 2.2.  The pair (A B) is called stabilizable if all its uncontrollable eigen values are stable. • Before presenting methods to select F for eigenvalue assignment it is of interest to examine  how  the linear  feedback  control  law  u  =  Fx  + r given  in  (2.2)  affects controllability  and observability. We write si  -{A  +  BF)  B D -{C  +  DF) si-A -c B\ D\ \  ^  ^ [-F I (2.7) and note that rank [A/ -  (A +  BF)  B]  =  rank [XI -  A B] for  all complex  A (show this). Thus if (A B) is controllable then  so is (A +  BF  B) for  any F. Further notice that in view of %F =  [B (A  +  BF)B  (A +  BFfB... (A +  BF^'^  ^ 'B] I  FB  F(A  +  BF)B 0 / FB I = [BABA^B...A''~'B] (2.8) gi(^/r)  =  ^([B  AB...  A^-i^])  =  S/l(^) (why?). This shows that F does not alter the controllability  subspace of the system  (see Section 3.2.). This in turn proves the following  lemma. LE M M A 2.3.  The controllability subspaces of x are the same for any F. Ax + Bu and i: =  (A + BF)x + Br Although the controllability  of the system is not altered by linear state  feedback u  == Fx  + r this is not true for the observability property. Note that the observability of the closed-loop system (2.3) depends on the matrices (A + BF)  and (C + DF)  and it is possible to select F to make certain eigenvalues unobservable from the output. In fact this mechanism is quite common and is used in several control design methods. It is  also possible  to make  observable  certain  eigenvalues  of  the  open-loop  system that were unobservable; for an example  see Example 2.8 below. Several methods are now presented to select F so to arbitrarily assign the closed-loop eigenvalues. Methods for eigenvalue assignment by state  feedback In view of Theorem  2.1 the eigenvalue  assignment  problem  can now be  stated as follows. Given a controllable pair (A B) determine F to assign the n eigenvalues of A + BF  to arbitrary real and/or complex conjugate locations. This problem is also known  as the pole  assignment  problem  where by the term  "pole" is meant  a "pole of the system" (or an eigenvalue of the "A" matrix). This is to be distinguished  from the "poles of the transfer  function"  (see Section 3.5 for the appropriate  definitions). Note that all matrices A B and F are real so the coefficients  of the polynomial det [si  -  (A + BF)}  are also real. This imposes the restriction that the complex roots of  this  polynomial  must  appear  in  conjugate  pairs.  Also  note  that  if  (A B)  is  not fully  controllable then (2.6) can be used together with the methods described a little later to assign all the controllable eigenvalues; the uncontrollable  ones will remain fixed (see Theorem 2.1 and Lemma  2.2). It is assumed in the following  that B has full  column rank i.e.. 331 CHAPTER 4: state Feedback and State Observers rank  B  =  m. (2.9) This means  that the  system  x  =  Ax  + Bu  has m independent  inputs. If  rank  B  = r  <  m this would imply that one could achieve the same result by manipulating only r inputs  (instead  of m >  r). To assign eigenvalues  in this case one can proceed  by writing A +  J5F  -  A +  (BM)(M-^F)  =  A  + [5i 0] A +  5iFi (2.10) where M  is chosen  so that  BM  =  [Bi 0]  with  Bi  G  T?"^'* and  rank  Bi  =  r.  Then Fi  G R^^^  can be determined to assign the eigenvalues of A +  BiFi  using any one of the methods  presented  next.  Note that  (A B)  is controllable  implies  that  (A  Bi) is controllable (why?). The state feedback  matrix F is given in this case by F  =  M (2.11) where F2 G R(^-^)^n  is  arbitrary. /.  Direct  method.  Let F  =  [fij]  i  =  1...  m j  =  \.. .n  and  express  the coefficients  of the characteristic polynomial of A +  BF  in terms of fij  i.e. det {si  -  (A +  BF))  =  s-  + gn-iifiX'' +  • • • +  goifijl Now if the roots of the polynomial (^d(s) =  ^" +  dn-lS""  ^ +  • • • +  Ji^  +  Jo are the n desired  eigenvalues then the  fij  i  =  \.. termined  so that .m  j  =  1  n must be de-gk(fij)  =  dk A: =  0 1 . . .  n-  1. (2.12) In  general  (2.12)  constitutes  a nonlinear  system  of  algebraic  equations;  how ever it is linear in the single-input  case m  =  \.  The main difficulty  in this method is not so much in deriving a numerical solution for the nonlinear system of equations but in carrying out the symbolic manipulations needed to determine the  coefficients g]^  in  terms  of  the  ftj  in  (2.12). This  difficulty  usually  restricts  this  method  to  the simplest cases with n  =  2 or 3 and m  =  1 or 2 being typical. EXAMPLE  2.2.  For  A [B we have det (si  -  A)  = s(s -  5/2) and therefore the eigenvalues of A are 0 and  |.  We wish to determine F so that the eigen values of A + BF are at -1  ± 7. 332 Linear Systems IfF  =  [fu f2lihQndet(sI-(A  + BF))  = det -1 s-2 -1 [flf2]]  = det s-\-fi - l - /i - 1 - /2 s-2-f2_ =  s^ + s{-l  -  fi  -  f2) + f\  -  Ifi.  The desired eigen-values are the roots of the polynomial aM  =  (^ -  (-1  + Ms  -  (-1  -  j))  ^  s^ + 2s + 2. Equating  coefficients  one obtains  -\  —  fi  -  fi  = 2 fi  -  \f2  =  2 a. linear  system of equations. Note that it is linear because m  =  1. In general one must solve a set of nonlinear algebraic equations. We have as the appropriate state feedback matrix. • F = [fuf2]  = [-l-'i] 2.  The use of controller forms.  Given that the pair (A B) is controllable there exists an equivalence transformation  matrix P  so that the pair (Ac  =  PAP~^Bc  = PB)  is  in  controller  form  (see  Section  3.4).  The  matrices  A  +  BF  and  P(A  + BF)P~^  =  PAP~^  +  PBFP-^  =  Ac  -^ BcFc have  the  same eigenvalues  and  the problem is to determine Fc so that Ac + BcFc has desired eigenvalues. This problem is  easier  to  solve than  the  original  one because  of  the  special  structures  of  Ac  and Be. Once Fc has been determined then the original feedback  matrix F is given by FrP (2.13) We shall now^ assume that (A B)  has already been reduced  to (A^ Be) and  describe methods of deriving Fc for eigenvalue  assignment. Consider first the single-input case (m  =  1). We let Fc  -  [fo>  "  -y  fn-l]' (2.14) In view^ of Section 3.4 since A^  Be are in controller form  v^e have AcF  =  Ac +  BcFc 1 0 0 -ao 0 -ax 0 0 0 0 1 Oin-\ "o" [/O. •••> fn-\\ + 0 1 (2.15) -(<^o  -  /o) - ( «i  -  /i) -{pin-\  - fn-\) where  a/  /  =  0  . . .  n  -  1 are the coefficients  of the  characteristic  polynomial  of Ac i.e. det{sl  -  Ac)  =  s""  + an-is""'^  +  • • • 4- a:i^  +  ao- (2.16) Notice that  ACF  is also in companion  form  and its characteristic  polynomial  can be written directly  as det{sl  -  ACF)  =  S-  + {dn-i  ~  fn-Ds""-'  +  ' ''  +  (CQ  "  /Q). (2.17) If  the  desired  eigenvalues  are  the  roots  of  the  polynomial aais)  =  s"" +  dn-is""-^  +  • • • +  ^o (2.18) then  by  equating  coefficients  fii  =  0  1  . . .  n  -  1 must  satisfy  the relations  dt  = at  -  fiJ  =  01.. -  1 from  which  we  obtain .n 333 CHAPTER 4: State Feedback and  State Observers fi «/  -  di. 0...n-h (2.19) Alternatively  note  that  there  exists  a matrix  A^  in  companion  form  the  charac teristic  polynomial  of  which  is  (2.18).  An  alternative  way  of  deriving  (2.19)  is  then to  set  AcF  =  Ac  +  BcFc  =  A^  from  which  we  obtain Fc  = B-'[A.-Aml (2.20) where  Bm  =  1 Aj^  =  [-do... - a „ - i ].  Therefore Bm  Aj^  and  A^  are  the  nth  rows  of  B^  A^  and  A^ respectively  (see  Section  3.4). Relationship  (2.20) which  is an  alternative  formula  to (2.19) has  the  advantage  that it is  in  a form  that  can  be  generalized  to  the  multi-input  case. and  Am  =  [-OCQ  ... -d^-i] EXAMPLE2.3.  Consider the matrices  A B  = of Example 2.2. Deter mine F  so that the eigenvalues  of A +  BF  are  -1  ± the polynomial ad(s)  =  s^ + 2s  +  2. To reduce (A B)  into the controller form  let j  i.e. so that they  are the roots of ^  -  [B AB]  = 1 3 1  3 and^ -1  _ 3 -1 from which P qA and -2 [see (4.44) in Chapter 3]. Then P'^  = -1 i 2 Ar  =  PAP-^  = Br  = Thus Am  =  [0 f ] and B^  =  I.  Now Aj  = 1 -2 and Ad^  =  [ - 2  - 2]  since the characteristic polynomial of A^ is ^^ +  2^ +  2  =  ad(s).  Applying  (2.20) we obtain that Fc  =  B-'[Ad^-Am] =  [ - 2  - |] and  F  =  FcP  = [-2 r  2 3 1 3 - 2J 21 3 2 3-[-- y]  assigns  the  eigenvalues  of  the closed-loop  system  at  -1  ±  j. This is the  same result  as the one obtained  by the  direct method given in Example 2.2 If  ad(s)  =  s^ + dis  +  do then Aj^^  =  [-do  -d\\  Fc  = B-J{Ad^  -  Am]  =  [-^0  -dx  -  5/2] and F  =  FcP  = \[2do-di -2do  -  2di  -  5]. In general the larger the difference between the coefficients  of ad{s) and Q:(5') (A^^  -Am) the larger the gains in F. This is as expected since larger changes require in general larger • control action. Note  that  (2.20)  can  also be  derived  using  (4.63)  of  Section  3.4  in  Chapter  3. To see  this  write AcF  =  Ac  +  BcFc  =  (Ac  +  BcAm)  +  {BcBm)Fc  =  A  +  5  ( A^  +  BmFd 334 Linear Systems where A^  Be are defined in (4.63). Selecting Aj  =  A^ + BcAd^ and requiring ACF  = Aj  implies Bc[Am +  BfnFc] =  BcAd^ from  which A^  +  5m^c  =  ^dm^ which in turn implies (2.20). After Fc has been found to determine F so that A + BF  has desired eigenvalues one  should  use F  =  FcP  given  in  (2.13). Note that P  which  reduces  (A B)  to the controller form has a specific form in this case [see (4.44) of Section 3.4]. Combining these results it is possible to derive a formula for the eigenvalue assigning F in terms of  the  original  pair  (A B)  and  the  coefficients  of  the  desired  polynomial  a^is).  In particular the  1 X n matrix F that assigns the n eigenvalues  of A +  BF  at the roots of a^is)  is given by F  =  -el%-^aM\ (2.21) where  Cn =  [0...  0 1]^ G  R""  and ^  =  [5 A 5  . . .  A^'-^B]  is the  controllability matrix.  Relation  (2.21)  is  known  as Ackermann's  formula  and  its  validity  will  be verified  shortly. Notice that the F that assigns the n eigenvalues of A + BF  is unique when m  = 1.  This  is  not  difficult  to  see  from  the  preceding.  In  particular  notice  that  Fc  = [foy' "y  fn-\\  is uniquely determined by (2.19) and that the transformation  matrix P is also unique in this case (m  =  1) and is in fact  given by (4.44) in Section  3.4. Verification  of Ackermann's  Formula Given  (A^ Be) in controller form  it was shown that the matrix Fc  =  [o^Q  -  do.  ..an-\ -  dn-\\  =  5^^[Aj^  -  Ajn] assigns the eigenvalues of Ac + BcFc to the desired locations the roots of a^is)  given in (2.18). The matrix Fc is related to Fby F  =  FcP where Ac  =  PAP-\  Be  =  PB and  P  =  ^c^~^  and  where ^  and %c are the controllability  matrices  of  (A B)  and (Ac Be) respectively. These relations will now be combined to derive  Ackermann's formula. First  note that if  (2.21)  were used  for  a given  pair  (Ac Be) in controller  form then Fc  =  -el%-^aMc\ (2.22) We now show that this Fc assigns the n eigenvalues at the desired locations. In doing so  we  note  that  aj(Ac)  =  A^  +  ^„_iA"~^  +  • • • +  d\Ac  +  d(^l which  in  view  of the Cayley-Hamilton Theorem  [namely a{Ae)  =  A^ + a„_iA^"^  H V aol  =  0] assumes the  form n-l otd(Ac)  =  ^(di i = 0 -  ai)A[ since  A^  =  -ZfJo^/AJ.  Also  note  that  ef^c  =  e^  or  that  ^J^^~^  =  e\.  Now ~el%-^ad{Ac)  =  -el{{do [ao-do... oin-\  -  dn-i]  =  Fe  of  (2.20)  that  is  (2.22)  is  verified.  Note  that  the  last  relation was derived using the fact  that -  ao)I  +  •••  +  (d^-i  -  a^-M^^) = ejAc  =  el  {e\ Ac) Ac  =  el Ac  =  4  . . .  ^f A^^'^  =  e^. from  which  -e^^do  -  ao)I  H  + {dn-i  -  an-i)A^  ^] =  (o^o -  <io)^f  + {oci -di)el  H h {an-i  -  dn-i)el  = [ao -  dQai -  Ji...a^_i  -  dn-i].  In view of (2.22) we now have F =  FcP=-el^-'aMc)P 335 CHAPTER 4: State Feedback and State Observers which is Ackermann's  formula  (2.21). EX AMPLE 2.4. To the system of Example 2.3 we apply (2.21) and obtain -el^t 'ad{A) -[01] 2 2 L  3 -1] 2 3J / ( 2 il W 2 1  2 + 2 •1 2 1 1" 2  + 2 "1 0 0" 1 2  2 I] 3J " 3' 3 "17 4 9 2 9 2 11 [1 -~  L  6' 13] 3 J' which is identical to the F found in Example 2.3. • • Now  consider  the  multi-input  case  (m >  1).  We proceed  in a way  completely analogous to the single-input case. Assume  that Ac  and Be are in the controller  form  (4.62) given  in  Section  3.4. Notice  that  ACF = A^ + BcFc is also in (controller)  companion  form  with  identical block structure as A(^ for any FQ. In fact the pair  {AQF^BQ) has the same controllability indices  /i// =  1... m as {Ac^Bc). This was  shown in Section  3.4 of Chapter  3 and can also be seen directly using (4.63) since Ac  +  BcFc  —  {Ac  + BcAm)  +  {BcBm)Fc  — Ac + Bc{Am  +  BmFc) (2.23) where Ac  and Be are defined  in  (4.63). We can now  select  an n x n matrix Aj  with desired characteristic  polynomial det{sI  — Ad) = OJj(^)  =^^ + J^_i^^ -Jo (2.24) and in companion  form  having the  same block  structure  as ACF  or A^  that is Aj = Ac+BcAd^.  Now if AcF = Aj then in view of (2.23) Bc{Am+BmFc)  = BcA^^.  From this it follows  that (2.25) where Bm^A^^ and A^  are the m Gjth rows of 5cAj  and A^ respectively and Gj = E/=i l^iij  = ^i---i^'  Note  that this  is a generalization of (2.20)  of  the  single-input case. F=B-'[A^„-A^] We shall now show how to select an n x n matrix A j  in multivariable  companion form to have the desired characteristic  polynomial. One choice is "0 1 ••• 0 Ad 0 -Jo 0 -di 1 -dn-l 336 Linear  Systems the  characteristic  polynomial  of  which  is  a^{s).  In  this  case  the  m  x  n  matrix A^^  is given  by '  0 ••• 0 1 ••• 0 ••• ^d^ 0 -do 0 0 0 0 -dn-1 where  the  ith  row  / = l  . . .  m — l  is  zero  everywhere  except  at  the  (7/ +  1  column location  where  it is  one. Another  choice  is  to  select  Aj  =  [Aij]iJ  =  1  . .. m  with  Aij  =  0  for  /  ^  7 i.e. [All 0 0 A22 ••• ••• 0 0 Ad-0 0 noting  that  det  {si  — A^)  =  det  {si  — An)"- det  {sI — Amm).  Then "0 1 ••• 0" A where the last row is selected  so that det  {si—An)  has desired roots. The  disadvantage of  this  selection  is that  it may  impose  unnecessary  restrictions  on  the  number  of  real eigenvalues  assigned.  For  example  if  n  =  4m  =  2  and  the  dimensions  of  An  and A22 which  are equal  to the controllability  indices  are di=3 and  d2 =  l  then  two  of the  eigenvalues  must  be  real  (why?). There  are of course  other  selections  for A j  and the reader is encouraged  to  come up  with  additional  choices.  A  point  that  should  be  quite  clear  by  now  is  that  Fc  (or F)  is  not  unique  in  the  present  case  since  different  Fc  can  be  derived  for  differ ent  A j ^  all  assigning  the  eigenvalues  at  the  same  desired  locations.  In  the  single-input  case  Fc  is  unique  as  was  shown.  Therefore  the  following  result  has  been shown. LEMMA  2.4.  Let  (A5)  be  controllable  and  suppose  that  n desired  real  and  complex conjugate  eigenvalues for A + BF  have been  selected. The state feedback  matrix F  that assigns  all eigenvalues  of A +  BF  to  desired  locations  is not unique  in the  multi-input case  (m >  1). It is unique in the single-input  case m=  I. • EXAMPLE  2.5.  Consider the controllable pair  (A 5)  where 0 0 0 1 0 2 and "0 1 0 r 1 0 It was shown in Example 4.14 of Chapter 3 that this pair can be reduced to its controller form PAP-0 2 1 1 -1 0 "0 P  = 0 .1 0' 0 0. 0 1 0 Be =  PB  = '0  0" 1  1 .0  1. 11 2 1 2 1 2 J where Suppose  we desire to assign  the eigenvalues  of A +  BF  to the  locations  {-2  -1  ±  j} i.e. at the roots of the polynomial ad(s)  =  (s + 2)(s'^ + 2s + 2)  =  s^ + As^ +  65' + 4. A choice for Ad is 337 CHAPTER 4: State  Feedback and  State Observers Adi  = 1 0 0 0 -4 -and  F ci B-^Ad 0 1 - 4. A  J 1 .0 --11 ij r-2 L-5 1 -6 leading to A^^.  = 0 . -4 0 -6 1  1 0  1 r -4 1 -4 0 -6 7 -6 ively Ad2  = "  0 -2 ( 3 -1 -2 0 0 0 -2 and 5 ; ^ [ A .  - AJ   from  which Ad ^ = ' -2 0 -2 0 0' -2_ '1 0 - 11 Ij r-4 [-1 -1 0 0" -2 -1 0 2 -2 5 1  --4 -6 Both  Fi  =  FciP  = and  F2  =  FciP  = 2 -2 assign  the eigenvalues  of A +  5F  to the locations { 2  - l ± j }. The reader  should plot the  states  of the equation  x  =  {A  + BF)x  for  F  =  Fi  and F  =  F2 when  x(0)  =  [1  1  1]^ and  should  comment  on the  differences  between  the trajectories. • Relation  (2.25)  gives  all feedback  matrices  Fc  (or F  =  FcP)  that  assign  the  n eigenvalues  of A^ +  BcFc  (or A +  BF)  to desired  locations. The  freedom  in  selecting such  Fc  is expressed  in terms  of the  different  A j  all in companion  form  with  A^  = [Aij]  and A/y of dimensions  fit  X fij  which have the same characteristic  polynomial. Deciding  which  one  of  all  the  possible  matrices  A^  to  select  so  that  in  addition to  eigenvalue  assignment  other  objectives  can  be  achieved  is  not  apparent.  This flexibility in  selecting  F  can  also  be  expressed  in  terms  of  other  parameters  where both  eigenvalue  and  eigenvector  assignment  are  discussed  as  will  now  be  shown. 3.  Assigning  eigenvalues  and  eigenvectors. Suppose  now  that F  was  selected so that A +  BF  has  a desired  eigenvalue  Sj  with  corresponding  eigenvector  v^.  Then 338 Linear  Systems [sjl  — (A +  BF)]Vj  =  0 which  can be written  as [Sjl-AB] -Fvj_ 0. (2.26) To  determine  an F  that  assigns  Sj  as a closed-loop  eigenvalue  one could  first  deter mine  a basis  for the right  kernel  (null  space)  of  [sjI — A^B] i.e. one could  determine a  basis Mj -Di such  that [Sjl-AB] Mj 0. (2.27) Note  that  the dimension  of this basis is (n + m)-rank [sjl —AB]  =  {n + m)—n  =  m where  rank  [sjl —AB]  =  n  since  the pair  {A^B)  is  controllable.  Since  it is  a  basis there  exists  a nonzero  m x  1 vector  Uj so that Mj-aj  = _-Fvj_ Combining  the relations  —DjUj —FVj  and Mjaj --Vj  one obtains FMjaj  = DjUj. (2.28) (2.29) This  is  the  relation  that  F  must  satisfy  for  Sj  to  be  a  closed-loop  eigenvalue.  The nonzero  m x  1 vector  aj  can be chosen  arbitrarily.  Note  that Mjaj  =  Vj is the eigen vector  corresponding  to  Sj.  Note  also  that  aj  represents  the  flexibility  one  has  in selecting  the corresponding  eigenvector  in addition  to assigning  an eigenvalue.  The nxl eigenvector  Vj cannot be arbitrarily  assigned;  rather  the m x  1 vector aj  can be (almost)  arbitrarily  selected.  These mild conditions on aj  are stated next as a theorem. THEOREM  2.5.  The pair  (sj Vj) is an (eigenvalue eigenvector)-pair  of A + BF if and only if F  satisfies  (2.29) for some nonzero  vector Uj such that  Vj = MjUj  with Mj a basis of the null space of [sjl —AB\  as in (2.27). Proof  Necessity has been shown. To prove sufficiency  postmultiply Sjl — ( A + 5 F)  by Mjaj  and use (2.29) to obtain  (sjI — A)Mjaj  —BDjaj  =  0 in view of (2.27). Thus [sjI-{A  + BF)]Mjaj  = 0. which  implies  that Sj is an eigenvalue  of A + BF  and MjUj  = Vj is the  corresponding • eigenvector. If relation  (2.29) is written for n desired  eigenvalues Sj where the aj  are  selected so  that  the  corresponding  eigenvectors  Vj =  Mjaj  are  linearly  independent then FV  =  W (2.30) where  V  =  [ M i ^ i  . .. M^a^]  and  W  =  [ D i ^ i  .. .^Dnan]  uniquely  specify  F  as  the solution  to these  n linearly  independent  equations.  When  Sj  are distinct  it is  always possible  to select  aj  so that  V  has full  rank;  in fact  almost  any set of nonzero  aj  suf fices.  When  Sj have  repeated  values  it may still be possible  under  certain  conditions to  select  aj  so that  Mjaj  are linearly  independent;  however  in  general  for  multiple eigenvalues  (2.30)  needs  to be modified  and the details  for this  can be found  in the literature  (e.g.  [16]). 339 CHAPTER 4: State  Feedback and  State Observers It can be shown that for distinct eigenvalues Sj  the n vectors Mjaj .n are  Hnearly  independent  for  almost  any  nonzero  aj.  For  further  discussion  on  this see  the  remarks  following  Example  2.6  and  Subsection  A.4  on  Interpolation  in the  Appendix.  Also  note  that  if  Sj+i  =  s*  the  complex  conjugate  of  Sj  then  the corresponding  eigenvector  V/+1  =  v*  = j  =  1.. ^)^)' Relation  (2.30)  clearly  shows  that  the  F  that  assigns  all  n  closed-loop  eigenval ues  is  not  unique  (see  also  Lemma  2.4).  All  such  F  are  parameterized  by  the  vec tors aj  that in turn  characterize  the corresponding  eigenvectors. If the  corresponding eigenvectors  have  been  decided  upon—of  course  within  the  set  of  possible  eigen vectors  Vj  =  MjUj—then  F is uniquely  specified.  Note that in the  single-input  case (2.29)  becomes  FMj  =  Dj  where  Vj  =  Mj.  In  this  case  F  is  unique. EXAMPLE  2.6.  Consider the controllable pair (A B) of Example 2.5 given by 0 0 0 1 0 2 0 1 -1 ro 1 .0 11 1 0. Again it is desired to assign the eigenvalues of A + BF  at - 2  -1  ±  j.  Let ^-i  =  -2S2  = -I  + j  and S3 =  -I -  j. Then in view of (2.27) Ml] = 1 -1 2 r 0 0 ' -1 1 -2 2_ J 2 M2 -D2. = 1 0 0 2 +  J 1 -1 +  7 1 -J and M3 -D3 Ml  the complex conjugate  since S3 Each  eigenvector  v^  =  Miai  i  =  1 2 3 is  a linear combination  of the columns of Mi.  Note that V3 =  V2. If we select the eigenvectors  to be r '1 y  =  [vi V2 V3]  =  0 1 j 0  2 I.e. ai  = a2  and a3 then  (2.30) implies  that 1  1 0 j 0  2 --2-j -1 -2  +  j -1 from  which we have '^rj -2-j -1 -2  +  J -1 4y 0 0 0 2 -2 - 2/ j j  . 2 -2 -1 0 -2 ^ This matrix F is such that A +  BF  has the desired eigenvalues  and eigenvectors  (verify this). • 340 Remarks Linear Systems At this point several comments  are in order. 1.  In Example 2.6 if the eigenvectors were chosen to be the eigenvectors of A + BFi (instead of A + BF2) of Example 2.5 then from FV  =  W it follows that F would have been Fi  (instead of F2). 2.  When  st  =  ^*^j  then  the  corresponding  eigenvectors  are  also  complex  conju gates i.e. Vi =  v*^j. In this case we obtain from  (2.30) that FV  =  Fl.. ViR +  pii  ViR - jvii...] =  [...  WiR +  jwii  WiR -  jwii ...]  =  W. Although  these  calculations  could  be  performed  over  the  complex  numbers  (as was  done  in  the  example)  this  is  not  necessary  since  postmultiplication  of FV  =  Why -J' + 7 shows that the above equation FV  =  W  is equivalent to Fl.. ViR Vii...] =  [...  WiR  Wii...] which involves only reals. 3.  The bases -Dj\' U j  =  1 • • • n in (2.27) can be determined in an alternative way and the calculations  can be  simplified  if the controller  form  of the pair  (A B)  is ons can be  simplified  if  tl known. In particular note that  [si  -  AB]\ =  0 where the n X m ma D{s) trix S(s)  is given by S(s)  =  block  diag  [1 >y... 5^'"^] and the ^c^ /  =  1...  m are the controllability  indices of (A B).  Also the m X m matrix D{s) is given by D{s)  =  B^^ [diag [s^^\ ...  s^"'] -  AmS{s)l  Note that S{s) and D{s) were  defined in the Structure Theorem  (controllable version) in Chapter  3 (Subsection  3.4D). It was shown there that {si  -  Ac)S{s)  =  BcD(s)  from which it follows that (si  -A)P~^S(s)  =  BD(s)  where P  is a similarity  transformation  matrix that  reduces (A B) to the controller form  (Ac  =  PAP-\  Be  =  PB).  Since P~^S(s)  and  D(s) are right  coprime  polynomial  matrices  (see  Section  7.2  of  Chapter  7) we  have rank D(sj) =  m for any Sj and  therefore P-'S{Sj) D(sj) qualifies  as a basis for  the null  space of the matrix  [sjl  -  A B](P  =  I  when  A B  are in  controller form  i.e. A  =  Ac  and B  =  Be.) We note that this approach is discussed  further in Subsection A.4 of the Appendix. Returning to Example 2.6 the controller form of (A B) was found  in Exam ple 2.5 using 341 CHAPTER 4: State  Feedback and  State Observers p-' — "1  0 r 1  1  0 2  0  0 Here S(s) '\  0" s  0 0  1 D{s) = ^2 '-+  s-  1 -1 M{s) -Dis) and Then Ml] -Di\ '  Mi-2) -Di-2)_ M2 -£>2 Mi-l  +  j) -D(-l  +  j) -s s 1 0 0 s -s 1 s + 1 2 -(-$2 +  5 - 1) 1 -1 2 r 0 0 -1 1 -2 2_ 1 0 0 J 2 2 +  j 1 1 +  ; 1 -7 Ms and which are precisely the bases used in the example. 4.  If in Example 2.6 the only requirement were that (^'i vi)  =  (-2  (1 0 0)^) then F(h  0 Of  =  (2 - 2 f i.e.  any F  =  \l  {'^  ^}' will  assign the desired  values to an eigenvalue of A +  BF  and its corresponding  eigenvector. L^/22 723. 5.  All possible eigenvectors  vi  and V2(v3  =  v^) in Example 2.6 are given by v\  =  M\a\  = 1  1] -1  0 2  Oj - |- an [ai2_ and V2 = M2a2 /I ail  +  JC131 [a22 +  JC132 where  the  atj  are  such  that  the  set  {vi V2 V3} is  linearly  independent  (i.e. y  =  [vi V2 V3] is nonsingular) but otherwise arbitrary. Note that in this case  (sj 342 Linear Systems distinct)  almost  any  arbitrary  choice  for  atj  will  satisfy  the  above  requirement (see Appendix  Subsection A.4). C.  The Linear  Quadratic  Regulator  (LQR): Continuous-Time  Case A linear state feedback  control law that is optimal in some sense can be  determined as a solution to the so-called  Linear  Quadratic  Regulator  (LQR) problem  (more re cently  also  called  the  H2  optimal  control  problem).  The  LQR  problem  has  been studied extensively and the interested reader should consult the extensive  literature on  optimal  control  for  additional  information  on  the  subject.  In  the  following  we give a brief  outline of certain  central results  of this topic to emphasize the fact  that the  state  feedback  gain  F  can  be  determined  to  satisfy  in  an  optimal  fashion  re quirements  other than  eigenvalue  assignment  discussed  above. The LQR  problem has  been  studied  for  the  time-varying  and  time-invariant  cases. Presently  we  will concentrate on the time-invariant  optimal regulator  problem. Consider the time-invariant  linear system given by X =  Ax-\-  Bu z  =  Mx (2.31) where the vector z(t)  represents the variables to be regulated—to  be driven to zero. We wish to determine  u{t) t  ^  0 which minimizes the quadratic  cost J(u)  = Jo [z' (t)Qz(t)  +  u'  (t)Ru(t)]  dt (2.32) for any initial state x{0).  The weighting matrices  Q R are real symmetric and pos itive definite  i.e.  Q  =  Q^ R  =  R^  and  Q>  0 R  >  0. This  is the most  common version  of the LQR  problem.  The term  z^Qz  =  x^ {M^ QM)x  is nonnegative  and minimizing  its integral forces  z{t)  to approach  zero as t goes to infinity.  The  matrix M^QM is  in  general  positive  semidefinite  which  allows  some  of  the  states  to  be treated  as  "do  not  care"  states.  The  term  u^Ru  with  R  >  0  is  always  positive  for w 7^ 0  and  minimizing  its integral  forces  u(t)  to remain  small. The relative  "size" of Q and R enforces  tradeoffs  between the size of the control action and the speed of response. Assume that  (A B Q^'^^M) is controllable  (-from-the-origin)  and observable. It turns out that the solution  w*(0 to this optimal control problem can be expressed  in state feedback  form which is independent of the initial condition x(0). In particular the optimal control  w*  is given by u{t)  -  F*x(0  =  -R~^B^Plx{t\ (2.33) where  P* denotes the  symmetric  positive-definite  solution  of the algebraic  Riccati equation A^Pc  +  PcA  -  PcBR'^B^Pc  +  M^QM  =  0. (2.34) This equation may have more than one solution but only one that is  positive-definite (see  Example  2.7).  It  can  be  shown  that  w*(0  "=  F*x(t)  is  a  stabilizing  feedback control law and that the minimum cost is given by Jniin  =  /(w*)  =  x^(0)P*x(0) The  assumptions  that  (A B Q^'^^M)  are  controllable  and  observable  may  be relaxed  somewhat.  If  (A B Q^'-^M)  is  stabilizable  and  detectable  then  the  uncon-trollable and unobservable eigenvalues respectively are stable and P* is the unique symmetric but now positive-semidefinite  solution of the algebraic Riccati equation. The matrix  F* is  still  a stabilizing  gain but it is understood  that the  uncontrollable and unobservable  (but stable) eigenvalues  will not be affected  by  F*. Note that if the time interval  of interest in the evaluation  of the cost goes  from 343 CHAPTER 4: State Feedback and State Observers 0 to ^1 <  00 instead of 0 to oo that is if J(u)  = [z^(t)Qz(t)  +  u^(t)Ru(t)]  dt (2.35) then the optimal control law is time-varying  and is given by u{t)  =  -R~^B^P\t)x{t) Q^t^ti (2.36) where P*(0 is the unique symmetric and positive-semidefinite  solution of the Ric cati equation which is a matrix differential  equation of the  form - — P{t)  =  A^P(t)  +  P(t)A  -  P(t)BR-^B^P(t)  +  M^QM dt (2.37) where  P(ti)  =  0.  It  is  interesting  to  note  that  if  (A B Q^'^M)  is  stabilizable  and detectable (or controllable and observable) then the solution to this problem as ti  —> 00 approaches the steady-state value P* given by the algebraic Riccati equation; that is when  ti^ ^  the optimal control policy is the time-invariant  control law  (2.33) which is much easier to implement than time-varying  control policies. EXAMPLE 2.7.  Consider the system described by the equations i  =  Ax+Buy  =  Cx where A  "0  r 0  0. and C{sl -A)  ^B =  \ls^. We wish to determine the optimal control M*(0 ^ ^  0 which minimizes the performance index C  =  [1 0]. Then (A B C) is controllable and observable "0" .1_ B  = J  = (/(O  + pu^iO) dt where p is positive and real. Then R  = p>  0 zit)  = y(t) M  =  C and Q the present case the algebraic Riccati equation (2.34) assumes the form 1 >  0. In A^Pc  + PcA -  PcBR-^B^Pc  + M^QM 0  0 1  0 0  0 1  0 Pr  +  Pr Pi P2 Pi P3 1 0 Pi IP2 p P2 P3. 0  0 0  0 [0 l]Pc + [10] Oj 1  Pi PIP2 P2] P3\ ro  0] [0 ij \pi [P2 P2 P3 where Pr =  Pi  P2 IP2  P3j =  P^. This implies that 1 ~-P2  +  1 = 0 1 pi  -  -P2P3  = 0 2p2 P P :PI  = 0. P Now Pc is positive definite if and only if pi  >  0 and p\p3  — p\>  0. The first equation above implies that p2 =  ± J^.  However the third equation which yields p\  = 2pp2 344 Linear Systems implies that pi  =  + yp. Then p\  = 2p J~p and p^  =  ±  2p J^.  The second equation yields pi  = {\lp)p2P?>  and implies that only p^  =  +  lip  J~p is acceptable since we must have p\  >  0 for P^ to be positive definite. Note that pi  >  0 and P3-p\  = 2p-p  = p >  0 which shows that > J^P/P. VP is the positive definite solution of the algebraic Riccati equation. The optimal control law is now given by u\t)  = F*x(t)  =  -R'^B^PlxQ)  =  - - [ 0  IjPXO. The eigenvalues of the compensated system i.e. the eigenvalues of A + BF* can now be determined for different  p. Also the corresponding u*(t) and y{t) for given x(0) can be plotted. As p increases the control energy expended to drive the output to zero is forced to decrease. The reader is asked to verify  this by plotting  u*(t)  and  y(t) for  different values of p when  x(0)  =  [1 1]^. Also the reader is asked to plot the eigenvalues of • A + BF* as a function  of p and to comment on the results. It  should  be  pointed  out  that  the  locations  of  the  closed-loop  eigenvalues  as the  weights  Q  and  R  vary  have  been  studied  extensively.  Briefly  for  the  single-input  case  and  for  Q  =  ql  and  R  =  r  in  (2.32)  it  can  be  shown  that  the  opti mal  closed-loop  eigenvalues  are  the  stable  zeros  of  1 +  (q/r)H^(-s)H(s)  where H(s)  =  M(sl  -  Ay^B.  As  q/r  varies  from  zero  (no  state  weighting)  to  infinity (no  control  weighting)  the  optimal  closed-loop  eigenvalues  move  from  the  sta ble  poles  of  H'^(-s)H(s) to  the  stable  zeros  of  H'^(-s)H(s).  Note  that  the  stable poles  of  H^(-s)H(s) are  the  stable  poles  of  H(s)  and  the  stable  reflections  of  its unstable  poles  with  respect  to  the  imaginary  axis  in  the  complex  plane  while  its stable  zeros  are  the  stable  zeros  of  H(s)  and  the  stable  reflections  of  its  unstable zeros. The solution of the LQR problem relies on solving the Riccati equation. A num ber of numerically stable algorithms exist for solving the algebraic Riccati equation. The  reader  is  encouraged  to  consult  the  literature  for  computer  software  packages that implement these methods. A rather straightforward  method for determining  P* is to use the Hamiltonian  matrix  given by H^ A -M^QM -BR-^B^ -A^ (2.38) Let  [V\  V2V  denote  the  n  eigenvectors  of  H  that  correspond  to  the  n  stable [Re (A)  <  0]  eigenvalues.  Note  that  of  the  2n  eigenvalues  of  H n  are  stable  and are the mirror images reflected  on the imaginary  axis of its n unstable  eigenvalues. When  (A B Q^''^M)  is  controllable  and  observable  then  H  has  no  eigenvalues  on the  imaginary  axis  [Re (A)  =  0]. In  this  case  the  n  stable  eigenvalues  of H  are  in fact the closed-loop eigenvalues of the optimally controlled system and the solution to the algebraic Riccati equation is then given by P*  =  V^V^K (2.39) Note that in this case the matrix  Vi  consists of the n eigenvectors of A +  i5F* since for  Ai  a stable eigenvalue  of H  and  vi  the  corresponding  (first)  column  of  Vi  we have [Ai/  -  (A +  5F*)]vi  =  [Ai/  -  A +  BR-^B^  V2yr^]vi 345 CHAPTER 4: State Feedback and State Observers {A-BR-'B''[ Vi'v Vx  vi [Ai/0] 0  X 0  X 0  X 0  X where the fact  that Vi are eigenvectors  of H  was used. It is worth reflecting  for a moment on the relationship between (2.39) and (2.30). The optimal control F derived by (2.39) is in the class of F  derived by (2.30). D.  Input-Output  Relations It is useful  to derive the input-output relations for  a closed-loop system that is com pensated by linear state feedback  and several are derived in this subsection.  Given the uncompensated  or open-loop  system  x  =  Ax  +  Bu y  =  Cx  + Du  with  initial conditions  x(0)  =  XQ we have y(s)  =  C(sl  -  Ay^xo  +  H(s)u(sl (2.40) where the open-loop transfer function H(s)  =  C(sI—A)~^B-\-D.  Under the feedback control  law  u  =  Fx  -\-  r  the  compensated  closed-loop  system  is  described  by  the equations  x  =  (A  + BF)x  -h 5r j  =  (C  +  DF)x  +  Dr from  which we obtain y{s)  =  (C  +  DF)[sI  -  (A +  BF)r^xo  +  HF(s)r{sl (2.41) where the closed-loop transfer  function  Hf(s)  is given by HF(S)  =  (C +  DF)[sI  -  (A +  BF)Y^B -f-  D. Alternative expressions for  HF{S)  can be derived rather easily by substituting  (2.4) namely u{s)  =  F[sl  -  (A +  BF)r^xo  +  [/  -  F(sl  - Ay^Br^r(sl into  (2.40). This  corresponds  to working  with  an  open-loop  control  law  that  nomi nally  produces  the  same results  when  applied  to the  system  [see the discussion  on open- and closed-loop control that follows  (2.4)]. Substituting we obtain y{s)  =  [C(sl  -  A)"i  +  H(s)F[sI  -  (A -t-  BF)r^]xo +  H(s)[I  -  F(sl  - Ay^Br^r(s). (2.42) 346 Linear Systems Comparing with (2.41) we see that (C + DF){sI  -  (A +  BF)Y^  =  C(sl  -  A)~^  + H(s)F[sI  -  (A  + BF)]~\  and that HF(S)  =  (C  + DF)[sI  -  (A +  BF)r^B  +  D =  [C(sl  -  Ay^B  +  D][I  -  F(sl  -  Ay^B]-^ =  H(s)[I  -  F(sl  -  Ay^Br^^ (2.43) The last relation points out the fact that y(s)  =  HF(s)r(s)  can be obtained  from y(s)  =  H(s)u(s)  using the open-loop control  u(s)  =  [/  -  F{sl  - Ay^B]~^r(s). Relation  (2.43) can easily be derived in an alternative manner using  fractional matrix descriptions for the transfer function introduced in Section 3.4 (see the Struc ture  Theorem).  In  particular  the  transfer  function  H{s)  of  the  open-loop  system {A B C D} is given by H(s)  - N(s)D-\s\ where A^(^)  =  CS(s)  +  DD{s)  with S(s)  and D(s)  satisfying  (si  -  A)S(s)  =  BD(s) (refer to the proof of the controllable version of the Structure Theorem given in Sec tion  3.4).  Notice  that  it has  been  assumed  without  loss  of  generality  that  the  pair (A B) is in controller  form. Similarly the transfer function  HF(S)  of the compensated system {A+BF  BC+ DF  D] is given by HF{S)  = NF(S)D^\S\ where  NF(S)  =  (C  + DF)S(s)  + DDF(S)  with S(s)  and  DF(S)  satisfying  [si  -  (A  + BF)]S(s)  =  BDF(S).  This relation  implies  that  (si  -  A)S(s)  =  B[DF(S)  +  FS(s)l from  which  we obtain  DF(S) +  FS(s)  =  D(s).  Then  NF(S)  =  CS(s)  +  D[FS(s)  + DF(S)]  =  CS(s)  +  DD(s)  =  N(s\ that is HF(S)  =  N(s)Dp\s\ (2.44) where DF(S)  =  D(s)  -  FS(s).  The full justification  of the validity  of these expres sions  will  be  given  in  Subsection  7.4B  of  Chapter  7  where  feedback  systems  de scribed in terms of polynomial matrix representations  are  addressed. Note  that  /  -  F(sl  -  Ay  ^B  in  (2A3)  is  the  transfer  function  of  the  system -FS(s)+ID(s). {AB  - F  /} and can be expressed as D/7(5')D"^(^) where D/7(^)  = LetM(^)  =  (Z)/7(^)^~k^))"^. Then (2.44) assumes the  form HF(S)  =  N(s)Dp\s)  =  (N(s)D-\s))(D(s)D^\s)) =  H(s)M(s). (2.45) Note  that  relation  HF(S)  =  N(s)D^^(s)  also  shows  that  the  zeros  of  H(s)  [in N(s)  see also  Subsection  7.3B]  are invariant  under  linear  state feedback;  they  can be changed only via cancellations with poles. Also observe that M(s)  =  D(s)Df^(s) is the transfer  function  of the system {A + BF  B F I] (show this). This implies that HF(S)  in (2.43) can also be written as HF(S)  =  H(s)[F(sI  -  (A +  BF)y^B  + / ] a result that could also be shown directly using matrix  identities. EXAMPLE 2.8.  Consider the system x  = Ax  + Bu y  = Cx where ro 1 .0 1 -1 0 B  =^  Be 0 2 1 0 0 0 and 0] 1 1. as in Example  2.5 and let  C  =  Q  =  [1 1 0]. Hf(s)  will now be determined.  In  view of  the  Structure  Theorem  developed  in  Section  3.4  the  transfer  function  is  given  by H(s)  =  N(s)D-\sX  where N(s)  =  CcS(s)  =  [110] n s 0 01 0 1. =  [^ +  10] 347 CHAPTER 4: State Feedback and  State Observers -1  01 0  Oj ri  01" \s  0 Lo i j. and  D{s)=^  B-'[A{s)-AmS(s)] = 1  1 0  1 s^  0 0 s "2 1 1 0 -1 1 s^  -\-s-2 -1 0 s s^  + s- I Then H{s)  =  N(s)D-'is)  =  [s+  10] s'^ + s-  1 -1 [s +  1 0] s 1 1 s s^ +  S-1  ^3 +  52 -  25 s(s^  +  5 -  2 )'  ' '' ' '' [5(5+1) 5(5+1)]  = - ^ ± i - [ l  l ]. 52 +  5 -2 I f F  = 3  7  5 -5  -6  -4 (which is Fci  of Example 2.5) then D^(5)  =  Z)(5) -  FcS(s)  = 5^ +  5 —  1  —5 -1 3 -5 7 -6 51 -4j ri  0" 5  0 Lo  1. 52 -  65 -  4 65 +  4 - 5 -5 5 +  4 Note that detDpis)  =  5^ +  452 +  65 + 4  =  (5 +  2)(52 +  25 +  2) with roots  - 2  -1  ±  7 as expected.  Now HF{S)  =  N{s)D}\s) =  [5 +  10] 5 +4 -65  -  4 5 +5 1 5 2 - 6 5 -4 (5 +  2)(52 +  25 +  2) 5+  1 (5 +  2)(52  +  25 +  2) [5 +  4 5 + 5]. Note  that  the  zeros  of  H{s)  and  Hp{s)  are  identical  located  at  - 1. Then  Hpis) H(s)M(s)  where -M(s)  =  D(s)Dp\s)  = 52 +  5 -1 5 +4 - 6 5 -4 5 +5 1 5 2 - 6 5 -4 53 +452  +  65 +  4 5^ +  I I 5 2 + 7 5 -4 -652 -  55 -  4 1252 +  8 5 -5 6 5 2 - 5 5 -5 1 53 +452  +  65 +  4 = [I~F(sI-Acr'Bc]-K Note that the open-loop uncompensated  system is unobservable with 0 being the unob-servable eigenvalue (why?) while the closed-loop system is observable i.e. the control • law changed the observability  of the system. 348 Linear Systems E.  Discrete-Time  Systems Linear  state  feedback  control  for  discrete-time  systems  is  defined  in  a way  that  is analogous to the continuous-time case. The definitions are included here for purposes of completeness. We consider  a linear  time-invariant  discrete-time  system  described  by  equa tions of the  form x(k  +  1)  -  Ax(k)  +  Bu(kl  y(k)  =  Cx(k)  +  Du(k) (2.46) where A  G /?"><^ B  E  Z^^^^ C  G /?^><^ D  G 7?^x^ and  k  >  ko with  k  ^  ko  =  0 being typical (see Section 2.7). DEFINITION2.3.  The linear (discrete-time time-invariant) state feedback control law is defined by where F  G T?'"^" is a gain matrix and r(k) G R^ is the external input vector. u(k)  = Fx(k)  + r(k) (2.47) • This  definition  is  similar  to  Definition  2.1  for  the  continuous-time  case.  The compensated  closed-loop system is now given by x(k  +  1)  -  (A 4- BF)x(k)  +  Br(k) y(k)  =  (C  +  DF)x(k)  +  Dr(kl (2.48) In  view  of  Section  2.7  of  Chapter  2 the  system  x(k  +  1)  =  (A  +  BF)x(k) is asymptotically  stable if and only if the eigenvalues of A + BF  satisfy  |A/| <  1 i.e. if they lie strictly within the unit disc of the complex plane. The stabilization  problem for the time-invariant  case therefore  becomes  a problem of shifting  the  eigenvalues of  A +  BF  which  is precisely  the problem  studied  before  for  the  continuous-time case. Theorem 2.1 and Lemmas 2.2 and 2.3 apply without change and the methods developed before for eigenvalue assignment can be used here as well. The only dif ference  in this  case is the location  of the desired  eigenvalues: they  are  assigned  to be within the unit circle to achieve  stability. We will not repeat here the details  for these results. Input-output  relations  for  discrete-time  systems  which  are  in  the  spirit  of  the results  developed  in the preceding  subsection  for  continuous-time  systems  can  be derived in a similar fashion  this time making use of the z-transform  of x(k  +  1)  = Ax(k)  +  Bu(k)  x(0)  =  XQ to obtain x(z)  =  z(zl  -  Ay^xo  +  (zl  -  Ar^Bu(z). (2.49) The reader [Compare expression (2.49) with x(^)  =  (si-  A)~^xo-^(sI-A)~^Bu(s).] is asked to derive formulas  for the discrete-time case that are analogs to expressions (2.40) to (2.45) for the continuous-time  case. F.  The Linear  Quadratic  Regulator  (LQR): Discrete-Time  Case The  formulation  of  the  LQR  problem  in  the  discrete-time  case  is  analogous  to  the continuous-time LQR problem. Consider the time-invariant linear  system x(k  +  i)  =  Ax(k)  +  Bu(kl  z(k)  =  Mx(k\ (2.50) 349 CHAPTER 4: State Feedback and State Observers where  the  vector  z(t)  represents  the  variables  to  be  regulated.  The  LQR  problem is to determine  a control  sequence  {u*(k)} /: >  0  which  minimizes  the  cost  func tion J(u)  =  ^[z^(k)Qz(k) +  u^(k)Ru(k)] (2.51) k = 0 for  any initial  state x(0) where the weighting  matrices  Q and R are real  symmetric and positive  definite. Assume that (A B Q^'^M) is reachable and observable. Then the solution to the LQR problem is given by the linear state feedback  control law u{k)  =  F*x(^)  -  -[R  +  B^PlBr^B^PlAx(k) (2.52) where  P* is the unique  symmetric  and  positive-definite  solution  of  the  (discrete-time) algebraic  Riccati  equation  given by Pc  =  A'^lPc  -  PcB[R  +  B^PcBr^B^PM +  M^QM. (2.53) Theminimum  valueof/is/(w*)  =  /min  =  x^(0)P*x(0). As  in  the  continuous-time  case  it  can  be  shown  that  the  solution  P*  can  be determined  from  the  eigenvectors  of  the  Hamiltonian  matrix  which  in  this  case is H A  + BR-^B^A-^M^QM -BR-^B^A-^ -A-^M^QM A-^ (2.54) where  it is assumed  that A~^  exists. Variations  of the above method  that relax  this assumption  exist  and  can  be  found  in  the  literature.  Let  [V\  Vj]^  be  n  eigenvec tors  corresponding  to the  n  stable  (|A|  <  1) eigenvalues  of H.  Note  that  out  of  the In  eigenvalues  of //  n  of  them  are  stable  (i.e. within  the  unit  circle)  and  are  the reciprocals of the remaining n unstable eigenvalues  (located outside the unit circle). When (A B Q^'^M) is controllable (-from-the-origin)  and observable then if has no eigenvalues  on the unit circle (|A|  =  1). In fact  the n stable eigenvalues  of//  are in this case the closed-loop eigenvalues of the optimally  controlled  system. The solution to the algebraic Riccati equation is given by -1 K  -  ^2vr (2.55) As  in  the  continuous-time  case  we  note  that  Vi  consists  of  the  n  eigenvectors  of A +  5F*  (show this). EXAMPLE  2.9.  We consider the  system  x{k +  1)  =  Ax{k)  +  Bu{k) y(k)  =  Cx(k) where A  = "0  1" .0  0^ B  = "0" .1_ C  =  [1 0] and we wish to determine the optimal control sequence {u*(k)} /: >  0 that minimizes the performance index J(u)  = Y.(y\k)  + puHk)) k = 0 where p >  0. In (2.51) z(k)  = y(k) M  = CQ  =  1 and /? =  p. The reader is asked to determine u'^k) given in (2.52) by solving the discrete-time algebraic Riccati equation (2.53) in a manner analogous to the solution in Example 2.7 (for the continuous-time algebraic Riccati equation). • 350 Linear Systems 4.3 LINEAR  STATE  OBSERVERS Since the states of a system contain a great deal of useful information there are many applications  where knowledge  of the  state vector  over  some time  interval  is  desir able. It may  be possible  to measure  states  of  a system  by  appropriately  positioned sensors. This was in fact assumed in the previous section where the state values were multiplied by appropriate gains and then fed back to the system in the state  feedback control law. Frequently however it may be either impossible or simply  impractical to  obtain  measurements  for  all  states.  In  particular  some  of  the  states  may  not  be available for measurement  at all (as in the case for example with temperatures  and pressures  in  inaccessible  parts  of  a jet  engine). There  are  also  cases  where  it  may be impractical to obtain state measurements from otherwise available states because of economic reasons  (e.g. some of the sensors may be too expensive) or because of technical  reasons  (e.g. the  environment  may  be too noisy  for  any  useful  measure ments). Thus there is a need to be able to estimate the values of the state of a system from  available measurements typically  outputs and inputs (see Fig. 4.3). Given the system parameters A B C D and the values of the inputs and outputs over a time interval it is possible to estimate the state when the system is observable. This problem a problem in state estimation is discussed in this section. In particu lar we will address  at length the so-called full-order  and reduced-order  asymptotic estimators which are also called full-order  and reduced-order  observers. u [A B  C D} y 1 f state observer • ^ FIGURE 4.3 A.  Full-Order  Observers:  Continuous-Time  Systems We consider  systems described by equations of the  form X =  Ax  +  Bu y  =  Cx  +  Du (3.1) where A  G /^"><^ B  G T^^^^ C  G  RP''''  and D  G  RP'''^. Full-state observers: The identity  observer An  estimator  of the full  state  x(t)  can be constructed  in  the following  manner. We consider the  system 'x =  Ax  + Bu^K(y- yl (3.2) ^  A where y  =  Cx  -\-  Du.  Note that (3.2) can be written  as 'x =  {A-  KC)x  +{B-  KD  K] (3.3) which  clearly  reveals  the role  of  u and y  (see Fig. 4.4). The  error between  the  ac tual  state  x{t)  and  the  estimated  state  x{t) e(t)  =  x(t)  -  x(t)  is  governed  by  the differential  equation e(t)  =  x(t)  -  jc(t)  =  [Ax  +  Bu]  -  [Ax  + Bu  + KC(x  -  x)] or e(t)  =  [A-  KC]e(tl Solving (3.4) we obtain (3.4) 351 CHAPTER 4: State Feedback and State Observers e(t)  =  exp [(A (3.5) Now  if the eigenvalues  of A -  KC  are in the left  half-plane  then  e(t)  ->  0 as ^  ^ 00 independently  of the initial condition  ^(0)  =  x(0)  -  x(0).  This asymptotic  state estimator  is known as the Luenberger  observer. KC)t]e(0). Li B-KD Ht) :§>-A-KC FIGURE 4.4 LEMMA 3.1.  There exists K  E  R'^^P  SO that the eigenvalues of A -  KC  are assigned to arbitrary real or complex conjugate locations if and only if (A C) is observable. Proof The eigenvalues of (A -  KCY  =  A^ -  C^K^  are arbitrarily assigned via K^ if and only if the pair (A^ C^) is controllable (see Theorem 2.1 of the previous section) or equivalently if and only if the pair (A C) is observable. • If  (A C)  is  not  observable  but  the  unobservable  eigenvalues  are  stable  i.e. (A C)  is detectable  then  the error  e{t)  will  still tend  to zero  asymptotically.  How ever the unobservable eigenvalues will appear in this case as eigenvalues of A -  KC (show this) and they may affect the speed of the response of the estimator in an unde sirable way. For example if the unobservable eigenvalues  are stable but are located close  to the imaginary  axis then  their  corresponding  modes  will  tend  to  dominate the response most likely  resulting  in  a state estimator  that converges  too slowly  to the actual value of the state. Where  should the eigenvalues  of A -  KC  be located? This problem  is dual to the problem  of closed-loop  eigenvalue placement  via  state feedback  and is  equally difficult  to resolve.  On  one  hand  the  observer  must  estimate  the  state  sufficiently fast  which  implies  that  the  eigenvalues  should  be placed  sufficiently  far  from  the imaginary  axis  so that the error  e{t) will tend to zero sufficiently  fast.  On the other hand  this  requirement  may  result  in  a high  gain  K  which  tends  to  amplify  exist-352 Linear Systems ing  noise thus  reducing  the  accuracy  of  the  estimate.  Note  that  in this  case  noise i^ the only Hmiting factor  of how fast  an estimator may be since the gain K is real ized by  an algorithm  and is typically  implemented  by means  of a digital  computer. Therefore  gains  of  any  size  can  easily  be  introduced.  Compare  this  situation  with the  limiting  factors  in  the  control  case  which  is  imposed  by  the  magnitude  of  the required  control action (and the limits of the corresponding  actuator). Typically the faster  the compensated  system the larger the required control  magnitude. One may  of course balance the trade-offs  between  speed of response of the es timator and effects  of noise by formulating  an optimal  estimation  problem  to derive the best K.  For this one commonly  assumes  certain probabilistic  properties  for  the process. Typically  the measurement  noise  and the initial  condition  of the plant  are assumed to be Gaussian random variables and one tries to minimize a quadratic per formance  index. This problem is typically referred  to as the Linear Quadratic Gaus sian (LQG) estimation problem. This optimal estimation or filtering problem can be seen to be the dual of the quadratic optimal control problem of the previous section a fact that will be exploited in deriving its solution. Note that the well-known  Kalman filter  is such an estimator. In the following  we shall briefly  discuss the optimal esti mation problem. First however we shall address the following  related issues. 1.  Is it possible to take ^  =  Ointheestimator  (3.2)? Such a choice would eliminate the information  contained in the term y — y from the estimator which would now be of the  form X =  Ai  +  Bu. (3.6) In  this  case  the  estimator  would  operate  without  receiving  any  information  on how accurate the estimate x  actually is. The error e{t)  =  x(t)  —  x(t)  would go to zero only when A  is stable. There is no mechanism  to affect  the speed by  which x(t)  would approach  x(t)  in this case and this is undesirable. One could perhaps determine x(0) using the methods in Section 3.3 of Chapter 3 assuming that the system is observable. Then by  setting  x(0)  =  x(0)  presumably  x(t)  =  x(t)  for all t  >  0 in view of (3.6). This of course is not practical for several reasons. First the calculated  x(0)  is never  exactly  equal  to the  actual  x(0) which  implies  that ^(0) would be nonzero. Therefore the method would rely again on A being stable as before  with  the  advantage  here  that  ^(0)  would  be  small  in  some  sense  and so  e(t)  ->  0 faster.  Second  this  scheme  assumes  that  sufficient  data  have  been collected in advance to determine (an approximation to) x(0)  and to initialize the estimator which may not be possible. Third it is assumed that this  initialization process is repeated whenever the estimator is restarted which may be impractical. 2.  If  derivatives  of  the  inputs  and  outputs  are  available  then  the  state  x(t)  may be  determined  directly  (see  Exercise  3.14  in  Chapter  3).  The  estimate  x(t)  is in this  case produced  instantaneously  from  the values  of the inputs  and  outputs and  their  derivatives.  Under  these  circumstances  x(t)  is  the  output  of  a  static state  estimator  as  opposed  to  the  above  dynamic  state  estimator  which  leads to a state estimate  x(t)  that  only  approaches  the  actual  state  x(t)  asymptotically as r -^  00 [e(t)  =  x(t)  -  x(t)  -^  0 as r ^  oo]. Unfortunately  this  approach  is in general not viable since noise present in the measurements of u(t) and y(t)  makes accurate calculations of the derivatives problematic and since errors in u(t)  y(t) 353 CHAPTER 4: State  Feedback and  State Observers and  their  derivatives  are not smoothed  by the algebraic  equations  of the static es timator (as opposed to the smoothing  effects  introduced by integration in  dynamic systems). It follows  that  in this  case  the state  estimates  may be  erroneous. M P L E 3.1.  Consider the observable 0^ "0  1 0  0 1 ' .0  2  - 1_ A = pair C = =  [100]. We  wish  to assign  the eigenvalues  of A -  ^C  in a manner  that  enables  us to  design a full-order/full-state  asymptotic  observer. Let the desired  characteristic  polynomial be a J (5*) = s^ + dis^  + d\s  + do and consider AD  ^  AT  ^ 0 1 0 0 0 1 0 2 -1 and Bn  = C T  _ To reduce  (AD BD) to controller form we consider % [BDADBDAIBD] = Then P  =  UAD IqAl from  which we obtain AD  =  PADP~ ro 0 Ll 0 1 -1 1" -1 3. 0 0 0 1 0 2 "1  0  0" 0  1  0 .0  0  1. =  ^ and P-^  = -2  1  1 1  1  0 1  0  0 and BD  =  PBE The state feedback  is then given by FD  =  B^^[Ad^  -  Am]  =  [-do  -di  -  2 -d2  + I] and  FD  =  FDCP  =  [~d2  +  I  d2  — d\  — 3 d\  — do  — 3d2  +  5].  Then K  =  -Fl  =  [d2 -hdi-d2 + 3 do-di+  M2 -  5]^ assigns the eigenvalues  of A -  KC  at the roots of 0Ld{s)  = s^ + d2S^ + d\s  + do. Note that the same result could also have been derived using the direct method for eigenvalue assignment using  |^/ -  (A -  (^0 h  fe)^C)|  =  ad{s).  Also the result could have been derived using the observable  version  of Ackermann's  formula  namely K  =  -Fl= ad(A)€~'en where FD  =  -el^^^ad(AD) from  (2.21). Note  that the given  system has eigenvalues at 01  -2  and is therefore unstable. The observer derived in this case will be used in the next  section  (Example  4.1) in combination  with  state  feedback  to stabilize  the system X = Ax  + Bu y  =  Cx  where 0' 1 - 1. "0  1 1  1 .0  0 C  =  [1 0 0] 1 0 2 0 0 0 B  = and (see Example 2.5) using only output  measurements. EXAMPLE 3.2.  Consider the system i:  =  Axy  =  Cx where A  = andC  = [01] and where (A C) is in observer form. It is easy to show that K  =  [do-  2  d\-2Y 354 Linear Systems assigns the eigenvalues of A -  KC at the roots of s^ + d\s  + d^. To verify this note that det{sI-{A-KC)) =  det\ s  0 0  s 0 .1 -do -d\\ s  +  d\s  + d{). The error e{t)  = x(t) -  x(t) is governed by the equation e(t)  = (A -  KC)e(t) given in (3.4). Noting that the eigenvalues of A are -1  ±  j select different  sets of eigenvalues for the observer and plot the states x(t) x(t)  and the error  e(t) for  x(0)  =  [2 2]^  and x(0)  =  [0 0]^. The further  away the eigenvalues of the observer are selected from the imaginary axis (with negative real parts) the larger the gains in K will become and the faster x(t) -^  x(t) (verify this). • Partial or linear functional  state  observers The  state estimator  studied above is a full-state  estimator or observer  i.e.  x(t) is  an  estimate  of  the  full-state  vector  x(t).  There  are  cases  where  only  part  of  the state vector or a linear combination of the states is of interest. In control problems for example Fx(t)  is used and fed back instead of Fx(t)  where F is an m X ^ state feedback  gain  matrix  (see  also  Section  4.4).  An  interesting  question  that  arises  at this point is: is it possible to estimate directly  a linear combination  of the state say Tx  where T  E  R^'^^^ h^  nl  This problem is considered  next. We consider the  system z  =  Az  + Bu  + Ky (3.7) where A  G  R^^"" B  G /^^x^ and K  G  R^'^P are to be determined. The error equation is given by ^  =  Tx- z  =  TAx  +  TBu  -  Az-  Bu-  K{Cx  +  Du) =  Ae  + {TA  -  AT  -  KC)x  +  {TB  -  KD  -  B)u. Now if and r.  A and K  satisfy B  =  TB-  KD TA-  AT  =  KC (3.8) (3.9) then  e  =  Ae.  If  in  addition  A is  stable  then  e{t) -^  0  as t ^  oo  and  z(t)  will  ap proach  Tx(t)  asymptotically.  A key  issue is clearly  the existence  (and  calculation) of appropriate  solutions of (3.9). This has been  studied extensively  in the  literature (see for  example  O'Reilly  [18]). Here we  simply  wish to point to a special  case of (3.9) namely the case of the identity observer (T  =  I)  or of the full-state  estimator: T  =  I  and  A  =  A  -  KC  where  for  (A C)  observable  there  always  exists  K  that renders A stable as was shown above. Note that in general  a solution  (A K)  of  (3.9) with A stable may not exist  i.e. there may not exist an observer (3.7) of order n(A  G R^^^)  to estimate n linear func tions  of the  state  Tx  (T  G  R^^^).  It is possible however  to decouple  the order of the observer from  the number of linear functions  of the state to be estimated  in the following  manner. Let w  =  Tx  w  ^  R^  and  consider z  =  Az-^  Bu-\-  Ky w  =  Tx{y-Du)  +  T2Z (3.10) where  z  G  i?^ A  G  R'""'  B  G  i^'^^  K f  G W""  and  write G  i?'^^^^ Ti  G  /?^><^ and  72  G  R^""'.  Let 355 CHAPTER 4: State Feedback and State Observers z -  ri  =  Az +  5w +  i^(Cx  +  Du)  -  r(A;c  +  5w) =^  Az  + (KC  -  tA)x  + {B-fB +  KD)u. -  TA  =  -AT  where A is stable then z-fx = -  fx(0))  i.e.  z(t)  -^  fx(t).  We  are  interested Now  if  B  =  TB  -  KD  mdKC A(z  -  tx)  or z - tx  =  e^\z(0) however  in estimates of w  =  Tx.  Consider therefore  w —  w  =  [TiCx  + Tiifx  + e^\z{Qi) -  fx(0))]  -Tx  =  {TiC  + T2f  -  T)x  +  T2e^\z{0)  -  fx(0)).  Now if T  -TiC  +  T2f  then w-w  =  T2e^\z{^)  -  fx(0))  and w(t)  -^  w(t)  =  Tx{t\  since A is stable. Therefore  an observer  (3.10)  of  w  =  Tx  T  G  R^^^  of  order  r exists  if  there exist  f  G T?''^" Ti  G /?"><^ T2 G /?"><^ and ^  G i?^><^ such that for A stable. TA  -  AT  -  /TC and TiC  +  727  =  7. (3.11) We thus have B  =  TB  — KD  We note that it can be shown that for (A C) detectable and  r sufficiently  large there  will  always  exist  a solution.  An  example  is the  case when 7  =  / a n dr  =  ^ - p. This is the case of the reduced-order observer discussed next in Subsection B. Another simple case of interest is when 7 is a row vector {n  =  1). In this case it can be shown (Luenberger [14]) that an observer of order i^ -  1 [i^ is the observability index  of  (A C)]  can  always  be  constructed  with  its  eigenvalues  freely  assignable which will asymptotically  estimate the linear function  of the state Tx.  In general it can be shown that an observer of order h{v  -  1) can be constructed that will asymp totically  estimate  Tx.  Note  that  the  problem  of  determining  an  observer  of  Tx  of minimum order is a very difficult  problem except in special cases. B.  Reduced-Order  Observers:  Continuous-Time  Systems Suppose thatp  states out of the n state can be measured  directly. This  information can then be used to reduce the order of the full-state estimator from nton-p. Similar results are true for the estimator of a linear function  of the state but this problem will not be addressed here. To determine a full-state estimator of order n-p  first  consider the case when  C  =  [Ip 0]. In particular  let Xi = "All A21 An A22. Ixi [X2_ + z = [lp0] Xil X2y (3.12) where  z  =  xi  represents  the p  measured  states. Therefore  only  X2(t) G  R^  ^ is to be estimated. The system whose state is to be estimated is now given by X2  =  A22X2  +  [A2b B2] — ^22-^2  +  Bu (3.13) 356 Linear Systems where B  == [A21 B2] and  u  = is a known signal. Also ^  A  . y  =  xi Anxi -  Biu  = A12X2 (3.14) where y is known. An estimator for X2 can now be constructed. In particular in view of (3.2) we have that the  system X2 =  A22X2  + Bu  + K(y  -  A12X2) =  (A22 -  KAn)x2  +  (A21Z +  B2U)  +  m -  Anz  -  Biu) (3.15) is an asymptotic  state estimator for  X2. Note that the error e satisfies  the equation ^  =  i:2  -  ^2  =  (A22 -  KAx2)e (3.16) and if (A22 A12) is observable then the eigenvalues of A22 -  ^Ai2 can be arbitrarily assigned making use of K.  It can be shown that if the pair (A  =  [Aij\  C  =  [Ip 0]) is  observable  then  (A22. A12) is  also  observable  (prove  this  using  the  eigenvalue observability test of Section 3.4 of Chapter 3). System (3.15) is an estimator of order n-  p  and therefore  the estimate of the entire state x is z X2 . To avoid using z  =  xi in y given by (3.14) one could use X2 =  w + Kz  and obtain from  (3.15) an estimator in terms of w z and u. In particular w  =  (A22 -  KAn)w  +  [(A22 -  KAn)K  +  A21 -  KAix\z  +  [B2 -  KBi]u. (3.17) Then w is an estimate of X2  -  Kz  and of course w + ^z  is an estimate for X2 (verify this). In  the  above  derivation  it  was  assumed  for  simplicity  that  a part  of  the  state xi  is  measured  directly  i.e.  C  =  [Ip 0]. One  could  also  derive  a  reduced-order estimator for the  system X =  Ax  + Bu y  =  Cx. \c To see this let rank  C  =  p  and define  a similarity transformation  matrix  P  =  \^ where  C is such that P is nonsingular.  Then k  =  Ax  + Buy  =  Cx  =  [Ip 0]x (3.18) where  x  =  PxA  =  PAP'K  B  =  PB  and  C  =  CP'^  =  [Ip 0]  (show  this). The transformed  system  is  now  in  an  appropriate  form  for  an  estimator  of  order  n  -p  to  be  derived  using  the  procedure  discussed  above.  The  estimate  of  x  is and  the  estimate  of  the  original  state  x  is  P .  In  particular  X2 =  w  +  Ky where  w  satisfies  (3.17)  with  z  =  y [Atj]  =  A  =  PAP  ^  and The interested reader should verify  this result. Bi B2 B  =  PB. EXAMPLE  3.3.  Consider the system  x Ax  + Bu y  =  Cx  where A  = 0 1 -2 -2  and C  =  [0 1]. We wish to design a reduced n-  p  = n-  I  = 2-  I  =  first-ro [i. B = order asymptotic state estimator. 357 CHAPTER 4: State Feedback and State Observers The  similarity  transformation  matrix  P X =  PxdindA  =  PAP~^  = "0  1 1  0 0 1 -2 -2 C C. "0  1 1  0 "0  1 .1  0 -2  1 -2  0 leads  to  (3.18) where B  = PB = and C  =  CP  ^  =  [10].  The  system  {A B C} is now  in  an  appropriate  form  for  use of (3.17). We have A the form All An [A21  A22 -2  1 -2  0 B L  and  (3.17)  assumes w  = (-K)w  + [-K^  + (-2)  -  K(-2)]y  +  (-K)u a system observer of order 1. For K  =  -10  we have w  =  lOw -  I22y  +  lOw and w + Ky  =  w -  lOy is an estimate for X2. Therefore y w -  lOy is an estimate of x and y [w —  lOy 0  1 1  0 y w -  lOy w -  lOy y is an estimate of x(t) for the original system. C.  Optimal  State Estimation:  Continuous-Time  Systems The  gain  K  in  the  estimator  (3.2)  above  can  be  determined  so that  it is  optimal  in an appropriate  sense. This is discussed very briefly  in the following.  The  interested reader should consult the extensive literature on filtering theory  for additional  infor mation in particular the literature on the Kalman-Bucy  filter. In  addressing  optimal  state  estimation  noise  with  certain  statistical  properties is introduced  in  the model  and  an  appropriate  cost functional  is  set up that  is  then minimized.  In  the  following  we  shall  introduce  some  of  the  key  equations  of  the Kalman-Bucy  filter  and  we  will  point  out  the  duality  between  the  optimal  control and estimation problems. We concentrate on the time-invariant case although as in the LQR control problem discussed earlier more general results for the time-varying case do exist. We consider the linear time-invariant  system X =  Ax  + Bu  + Tw  y  =  Cx  +  V (3.19) where  w  and  v represent  process  and  measurement  noise  terms. Both  w  and  v are assumed to be white zero-mean  Gaussian  stochastic processes i.e. they are uncor-related in time and have expected values £[w]  =  0 and £[v]  =  0. Let E[ww^]  =  W E[vv^]  =  V (3.20) denote their covariances where  W and  V are real symmetric  and positive  definite matrices  i.e.  W  =  W^  W  >  0  and  V  =  V^  V >  0. Assume  that  the  noise pro cesses w and V are independent  i.e. E[wv^]  =  0. Also assume that the initial  state ;c(0) of the plant is a Gaussian random variable of known mean E[x(0)]  =  XQ and known covariance E[(x(0)  -  jco)(-^(0) ~  -^o)^]  =  PeO-  Assume also that x(0)  is in dependent of w and v. Note that all these are typical assumptions made in practice. Consider now the estimator  (3.2) namely X =  Ax  + Bu  + K(y  -  Cx)  =  (A  -  KC)x  + Bu  +  Ky (3.21) 358 Linear Systems and let (A F W^^^ C) be controllable  (-from-the-origin)  and observable. It turns out that  the  error  covariance  E[(x  -  x){x  -  x)^]  is  minimized  when  the  filter  gain  is given by K*  =  PlC^V-\ (3.22) where  P* denotes  the  symmetric  positive  definite  solution  of  the  quadratic  (dual) algebraic  Riccati  equation PA^  +  APe  -  PeC^V'^CP +  TWT^ 0. (3.23) Note that P* which is in fact the minimum error covariance is the positive semidef-inite  solution  of the  above Riccati  equation  if  (A TW^'^ C)  is  stabilizable  and  de tectable. The optimal estimator is asymptotically  stable. The  above  algebraic  Riccati  equation  is the dual  to the Riccati  equation  given in  (2.34)  for  optimal  control  and  can  be  obtained  from  (2.34)  making  use  of  the substitutions A^B C^M and R  VQ^W. (3.24) Clearly methods that are analogous to the ones developed by solving the control Ric cati equation  (2.34) may be applied to solve the Riccati equation  (3.21) in  filtering. These methods are not discussed here. EXAMPLE 3.4  Consider the system i  =  Axy  =  CJC whereA  =  1  0  X  =  [0  1] and  let  T  = V  =  p  > 0W  =  1.  We  wish  to  derive  the  optimal filter K* ro  01 P*C'^V~^  given  in  (3.22).  In  this  case  the  Riccati  equation  (3.23)  is  precisely  the Riccati  equation  of  the  control  problem  given  in  Example  2.7.  The  solution  of  this equation was determined to be P: We note that this was expected since our example was chosen to satisfy  (3.24). There fore r  =  P: 1 i jp Jp J^p D.  Full-Order  Observers: Discrete-Time  Systems We consider described by equations of the  form x{k  +  1)  =  Ax{k)  +  Bu{k) y  =  Cx{k)  +  Du(k) (3.25) where A G 7^'^><^ B  G i?^><'^ C  G  RP"""^ and D  G  RP"""^. The construction  of  state estimators  for  discrete-time  systems  is mostly  analo gous to the continuous-time  case and the results that we established  above for  such systems are valid in here as well subject to obvious adjustments  and  modifications. There are however some notable differences.  For example in discrete-time systems it is possible to construct a state estimator that converges to the true value of the state in finite time instead  of infinite  time  as in the case  of  asymptotic  state  estimators. This is the estimator known as the deadbeat  observer.  Furthermore in discrete-time systems it is possible to talk about current  state estimators in addition to prediction state estimators. In what follows a brief description of the results that are analogous to  the  continuous-time  case  are  given.  Current  estimators  and  deadbeat  observers that are unique to the discrete-time case are discussed  at greater  length. 359 CHAPTER 4: State Feedback and State Observers Full-state observers: The identity  observer As in the continuous-time  case following  (3.2) we consider  systems  described by equations of the  form x{k  +  1)  =  Ax{k)  +  Bu{k)  +  K[y{k)  -  y{k)l (3.26) where y{k)  =  Cx(k)  -h Dx{k).  This can also be written  as x{k  +  1)  =  (A -  KC)x(k)  +  [5  -  KD  K] u(k) (3.27) It  can  be  shown  that  the  error  e(k)  =  x(k)  -  x(k)  obeys  the  equation  e(k  +  1) == (A -  KC)e(k).  Therefore if the eigenvalues of A -  KC  are inside the open unit disc of the complex plane then  e(k)  ^  0 as /: —>  oo. There exists K  so that the  eigenvalues of A -  KC  can be arbitrarily assigned if and only if the pair (A C) is observable (see Lemma  3.1). The discussion following Lemma 3.1 for the case when (A C) is not completely observable  although  detectable  is  still  valid.  Also the remarks  on appropriate  lo cations  for  the  eigenvalues  of A -  KC  and  noise being  the  limiting  factor  in  state estimators are also valid in the present case. Note that the latter point should seriously be considered when deciding whether or not to use the deadbeat observer  described next. To  balance  the  trade-offs  between  speed  of  the  estimator  response  and  noise amplification  one  may  formulate  an  optimal  estimation  problem  as  was  done  in the  continuous-time  case  the  Linear  Quadratic  Gaussian  (LQG)  design  being  a common  formulation.  The Kalman  filter  (discrete-time  case) which is based on the "current  estimator"  described  below  is  such  a quadratic  estimator.  The  LQG  opti mal estimation  problem can be  seen to be the dual  of the quadratic  optimal  control problem discussed  in the previous  section. As in the continuous-time  case  optimal estimation  in the discrete-time  case will be discussed  only briefly  in the  following. First however  several other related issues are  addressed. Deadbeat  observer.  If the pair (A C) is observable it is possible to select K so that all the eigenvalues of A -  KC  are at the origin. In this case e{k)  =  x(k)-  x{k)  = (A -  KC)^e(0)  =  0 for some k^  n\ i.e. the error will be identically zero within at most n steps. The minimum value of k for which (A -  KC)^  =  0 depends on the size of the largest block on the diagonal of the Jordan canonical form of A -  KC.  (Refer to the discussion on the modes of discrete-time systems in Section 2.7 of Chapter 2.) EXAMPLE 3.5.  Consider the system x(^ +  1) =  Ax(k)  y(k)  =  CJC(^) where A = C  p  1 0  1 360 Linear Systems is in observer form. We wish to design a deadbeat  observer. It is rather easy to show (compare with Example 2.5) that " Aj  -'  2  11" -1  0 0" K r-1 [  1 -1 am 0  Oj. which was determined by taking the dual AD  = A^BD  =  C^ in controller form using FD  = B;'[Ad^ -  Am] and K  = -F^ The  matrix  Aj  consists  of  the  second  and  third  columns  of  a  matrix  A^  = 0  X 1  X 0  X in observer (companion) form with all its eigenvalues at 0. For Aj^ "0  0  0" 1  0  0 .0  1  0_  we have Ki  =  1 ro  0' p  0 Li  0. -"  2  1 -1  0 0  0 -1 1 0 -1 1  1 -1  0 -1  0 and for A^2  ^  we obtain K2 1 -1 0 Note that A -  /^iC  =  A^  A 2 _ and  Al 0 and A -  K2C =  A^^  and Ai 0. Therefore  for the observer gain Ki  the error e(k) in the deadbeat observer will become zero in n  =  3 steps since e(3)  =  (A -  KiC)^e(0)  = 0. For the observer gain K2 the error e(k) in the deadbeat observer will become zero in2  < n steps since e(2) =  (A -  K2C)^e(0) = 0. The reader should determine the Jordan canonical forms of Ad^  and A^^ ^^^ verify that the dimension of the largest block on the diagonal is 3 and 2 respectively. • The  comments  in  the  discussion  following  Lemma  3.1  on  taking  ^  =  0  are valid  in the discrete-time  case  as well. Also the  approach  of determining  the  state instantaneously  in the continuous-time  case using  the  derivatives  of the input  and output  corresponds  in the  discrete-time  case  to determining  the  state from  current and future  input and output values  (see Exercise  3.14 in Chapter  3). This  approach was in fact  used to determine  x(0) when  studying  observability  in Section  3.3. The disadvantage  of this method is that it requires future  measurements  to calculate  the current  state. This issue of using future  or past measurements  to determine the cur rent state is elaborated upon next. Current  estimator The  estimator  (3.26)  is  called  3. prediction  estimator.  The  state  estimate  x(k) is  based  on  measurements  up  to  and  including  y(/: -  1). It  is  often  of  interest  in 361 CHAPTER 4: state Feedback and State Observers applications  to determine  the state estimate  x(k)  based  on measurements  up to and including  y(k).  This may  seem rather odd at first; however if the computation time required to calculate  x(k)  is short compared to the sample period in a sampled-data system then it is certainly possible practically to determine the estimate x(k)  before x(k  +  1) and  y(k  +  1) are generated  by  the  system.  If this  state estimate which  is based on current measurements  of y(k)  is to be used to control the system then the unavoidable computational  delays  should be taken into  consideration. Now  let  x(k)  denote  the  current  state  estimate  based  on  measurements  up through y(k).  Consider the current  estimator where x(k)  =  x(k)  +  Kc(y(k)  -  Cx{k)) x(k)  =  Ax(k  -  1) +  Bu(k  -  1) (3.28) (3.29) i.e.  x(k)  denotes  the  estimate  based  on  model  prediction  from  the  previous  time estimate x(k  -  1). Note that in (3.28) the error is y(k)  -  y(k)  where y(k)  =  Cx(k) (D  =  0) for  simplicity. Combining the above we obtain x(k)  =  (I  -  KcC)Ax(k  -  1) +  [(/  -  KcC)B  -Kc] \u(k -  1)1 y(k) (3.30) The relation to the prediction estimator (3.26) can be seen by substituting (3.28) into (3.29) to obtain x(k  +  1)  =  Ax(k)  +  Bu(k)  +  AKc[y(k)  -  Cx(k)l (3.31) Comparison with the prediction estimator  (3.26) (with D  =  0) shows that if K  -  AKc (3.32) then (3.31) is indeed the prediction estimator and the estimate x(k)  used in the cur rent estimator (3.28) is indeed the prediction state estimate. In view of this we expect to obtain for the error e(k)  =  x(k)  -  x(k)  the difference  equation e(k  +  1)  =  (A -  AKcC)e(k) (3.33) (show  this).  To  determine  the  error  e{k)  =  x{k)  -  x(k)  we  note  that  e{k)  = e(k)  -  (x(k)  -  x(k)).  Equation  (3.28)  now  implies  that  x(k)  -  x(k)  =  KcCe(k). Therefore e(k)  = (I-KcC)e(kl (3.34) This establishes the relationship between errors in current and prediction estimators. Premultiplying  (3.31) by /  -  KcC  (assuming  |/  -  KcC\  ¥- 0) we obtain e(k^  1)  - {A-KcCA)e(k\ (3.35) which is the current estimator error equation. The gain Kc is chosen so that the eigen values  of A -  KcC A  are within  the  open  unit  disc  of the  complex  plane. The  pair (A CA)  must be observable for  arbitrary  eigenvalue  assignment. Note that the two error equations  (3.33) and (3.35) have identical eigenvalues  (show this). EXAMPLE  3.6.  Consider the  system  x{k +  1)  =  Ax(^) y{]i) =  Cx(k)  where A  = 0 1 C  =  [0  1] which is in observer form  (see also Example 3.2). We wish to -21 362 Linear Systems design a current estimator. In view of the error equation (3.35) we consider det{sl  -  (A -  KcCA))  = det l\s  0" |o  s_ s + ko ki  -  1 po Ui. [1  - 2] - 2. _/ I "0  -2" .1 2 -  Iko  ] s +  2-2y^iJ = det =  /  + 5(2 -  2y^i +  ko) + (2 -  2y^i) =  5^ + dis  -\- do = ad(s) a desired polynomial from  which Kc  =  [ko ki]^  =  [di -  do  \{2  -  Jo)]^- Note that AKc  =  [do  -2di -  2f  = K found in Example 3.2 as noted in (3.32). The  current  estimator  (3.30)  is  now  given  by  x{k)  =  {A -  KcCA)x{k  -  1)  -KcCBu(k -  1) + Kcy(kl  or x(k) — ko  — 2 + 2ko I -  ki -2  + 2ki_ x(k  -  1) + y(k). Partial or linear functional  state  observers The problem of estimating a linear function  of the state Tx{k)  T  E  R^^^^ where ft ^  n using a prediction estimator is completely  analogous to the  continuous-time case and w^ill therefore  not be discussed further  here. E.  Reduced-Order  Observers: Discrete-Time  Systems It is possible to estimate the full  state x(k)  using an estimator of order n-  p  where p  =  rank  C. If a prediction  estimator is used for that part of the state that needs to be estimated then the problem in the discrete-time  case is completely  analogous to the continuous-time  case discussed before. We will omit the details. F.  Optimal  State Estimation: Discrete-Time  Systems The formulation  of the Kalman filtering problem in discrete-time is analogous to the continuous-time  case. Consider the linear time-invariant  system given by x{k  +  1)  =  Ax{k)  +  Bu{k)  +  Tw{k\ y(k)  =  Cx(k)  +  v (3.36) where  the  process  and  measurement  noises  w v  are  white  zero-mean  Gaussian stochastic processes i.e. they are uncorrected in time with £'[vv]  =  0 andjE'[v]  ==  0. Let the covariances be given by E[ww^]  =  W E[vv^]  =  V (3.37) where  W  =  W^  W  >  0  and  V  =  V^  V >  0.  Assume  that  w v  are  independent that the initial state x{0) is Gaussian of known mean {E[x{0)\  =  XQ) that E[(x(0) — xo)(x(0)  -  XQ)^]  =  Peo and that x(0)  is independent  ofw  and v. Consider now the current estimator (3.26) namely x(k)  =  x(k)  +  Kc[y(k)  -  Cx(k)l where  x(k)  =  Ax(k  -  1) +  Bu(k  -  1) and  x(k)  denotes  the prior  estimate  of  the state at the time of a measurement. It turns out that the state error covariance is minimized  when the filter gain is Kl  =  PlC^(CPlC^  +  V)~\ (3.38) where P* is the unique symmetric positive definite  solution of the Riccati equation 363 CHAPTER 4: State Feedback and State Observers Pe  -  A[Pe -  PeC^iCPeC^ +  W^CPeU^ +  TWT^. (3.39) It  is  assumed  here  that  (A FW^^^ C)  is reachable  and  observable.  This  algebraic Riccati  equation  is the dual to the Riccati equation  (2.53) that arose in the discrete-time LQR problem and can be obtained by  substituting A^B C^M and R-^  VQ  W. (3.40) It is clear that as in the case of the LQR problem the solution of the algebraic Riccati equation can be determined using the eigenvectors of the (dual)  Hamiltonian. The  filter  derived  above  is  called  the  discrete-time  Kalman  filter.  It  is  based on  the  current  estimator  (3.28). Note  that  AKc  yields  the  gain  K  of  the  prediction estimator  [see (3.32)]. 4.4 OBSERVER-BASED  DYNAMIC  CONTROLLERS State  estimates  derived  by  the methods  described  in the previous  section  may  be used  in  state feedback  control  laws  to compensate  given  systems. This  section  ad dresses that topic. In  Section  4.2  the  linear  state  feedback  control  law  was  introduced.  There  it was implicitly  assumed that the state vector  x{t)  is available for measurement.  The values of the states x{t) for t  >  t^ were fed back and used to generate a control input in accordance with the relation  u{t)  =  Fx(t)  + r(t).  There are cases however  when it  may  be  either  impossible  or impractical  to measure  the  states  directly.  This  has provided the motivation to develop methods for estimating the states. Some of these methods  were considered  in  Section 4.3. A natural  question  that  arises  at this  time is  the  following:  what  would  happen  to  system  performance  if  in  the  control  law u  =  Fx-\-r  the state estimate x were used in place of x as in Fig. 4.5? How much if any would the compensated system response deteriorate? What are the difficulties  in designing such estimator- (observer-) based linear state feedback  controllers? These questions  are  addressed  in this  section. Note that  observer-based  controllers  of  the type described in the following  are widely  used. '  Itr 1 v^* - {-• 1 ^  u 1 * System  [ State observer U F  1-FIGURE Observer-4.5 3ased contro Her 364 Linear Systems In  the  remainder  of  this  section  we  will  concentrate  primarily  on  full-state/ full-order  observers  and  (static)  linear  state  feedback  as  applied  to  linear  time-invariant  systems. The analysis of partial-state and/or reduced-order  observers  with static  or dynamic  state  feedback  is  analogous;  however  it is  more  complex.  Such control  schemes  are not considered  at this point.  Instead  they  will be  discussed  in the exercise section (Exercise 4.8). In this section continuous-time  systems are ad dressed. The analysis of observer-based  output controllers in the discrete-time  case is completely  analogous and will be  omitted. A.  State-Space  Analysis We consider  systems described by equations of the  form x  =  Ax-\-Bu y  =  Cx  + Du (4.1) where A  E  i?"^^'^ B  G T^^^^^ C  G i^^^^^ and D  G  RP'^'^.  For such systems we de termine  an  estimate  x{t)  G  R^  of  the  state  x{t)  via  the  (full-state/full-order)  state observer  (3.2) given by =  Ax^Bu-\- K(y  -  y) =  (A-  KC)x  +  [J5 -  KD  K] u = X (4.2) where  y  =  Cx  + Du.  We now compensate  the  system by state feedback  using  the control law u  =  Fx  +  r (4.3) where  x  is the output of the state estimator  and we wish to analyze the behavior of the compensated  system. To this end we first eliminate y in (4.2) to obtain The state equations of the compensated  system are then given by X =  {A-  KC)x  +  KCx  +  Bu X =  Ax-^  BFx  +  Br k  =  KCx  + (A-KC + BF)x  +  Br and the output equation assumes the  form y  =  Cx  + DFx  + Dr (4.4) (4.5) (4.6) where u was eliminated from  (4.1) and (4.4) using (4.3). Rewriting in matrix  form we have X X =  A KC  A BF 1 KC  +  BF\ \x IX +  B B y  =  [QDF]  X X + Dr (4.7) which is a representation of the compensated closed-loop system. Note that (4.7) con stitutes a 2Azth-order system. Its properties  are more easily  studied if an  appropriate 365 CHAPTER  4: State Feedback and State Observers similarity  transformation  is used to simplify  the representation.  Such  a  transforma tion is given by X PK  = = X 7 / 01 -l\ \x [x  =  X e where the error e(t)  =  xit)  -  x(t).  Then the equivalent representation  is X e = A  + BF 0 -BF A-KC \x [e  + B 0 y = [C +  DF  -DF] x\ +  Dr (4.8) (4.9) It is now quite clear that the closed-loop system is not fully  controllable with respect to r (explain this in view of Subsection 3.4A). In fact  e{t) does not depend on r at all. This is of course as it should be since the error  e{t)  =  x(t)  -  x(t)  should  converge to zero independently  of the externally  applied input  r. The closed-loop eigenvalues  are the roots of the polynomial \sln -  (A +  BF)\\sIn  -  (A -  KC)\. (4.10) Recall  that  the  roots  of  \sln -  (A  +  BF)\  are  the  eigenvalues  of  A +  BF  that  can arbitrarily  be assigned  via F provided  that the pair  (A B)  is controllable. These  are in  fact  the closed-loop  eigenvalues  of the  system  when  the  state x  is  available  and the linear state feedback  control law u  =  Fx-\-  ris  used (see Section 4.2). The roots of |^/„ -  (A -  KC)\  are the eigenvalues of (A -  KC)  that can arbitrarily be assigned via K  provided  that  the pair  (A C)  is observable.  These  are  the eigenvalues  of  the estimator (4.2). The above discussion  points out that the design  of the control  law  (4.3)  can  be carried  out  independently  of the design  of the estimator  (4.2).  This is referred  to as the  Separation  Property  and  is  generally  not  true  for  more  complex  systems.  The separation  property  indicates  that  the linear  state feedback  control  law  may  be de signed as if the state x were available and the eigenvalues of A + BF  are assigned at appropriate locations. The feedback  matrix F can also be determined by  solving  an optimal control problem (LQR). If state measurements are not available for  feedback a state estimator is employed. The eigenvalues of a full-state/full-order  estimator are given by the eigenvalues  of A -  KC.  These are typically  assigned  so that the error e(t)  =  x(t)  -  x(t)  becomes  adequately  small in a short period  of time. For this the eigenvalues of A -  KC  are (empirically) taken to be about 6 to 10 times further  away from  the  imaginary  axis  (in  the  complex  plane  for  continuous-time  systems)  than the eigenvalues of A + BF.  The behavior of the closed-loop system should be verified since the above is only a rule of thumb. (Refer to any good book on control for  further discussion—see  Section 4.6 Notes.) The estimator  gain K  may  also be  determined by solving an optimal estimation problem  (the Kalman filter). In fact  under the as sumption  of Gaussian  noise and initial conditions  given  earlier  (see Section 4.3) F and i^ can be found by solving respectively optimal control and estimation problems with quadratic performance  criteria. In particular the deterministic LQR problem is first solved to determine the optimal control gain F* and then the stochastic  Kalman filtering  problem  is  solved  to  determine  the  optimal  filter  gain  i^*.  The  separation property  (i.e..  Separation  Theorem—see  any  optimal  control  textbook)  guaran tees  that  the  overall  (state  estimate  feedback)  Linear  Quadratic  Gaussian  (LQG) 366 Linear Systems control design  is optimal in the sense that the control law  w*(0  =  F''x{t)  minimizes the quadratic performance  index E[\^{z^Qz  +  u^Ru)  dt].  As was discussed in pre vious sections the gain matrices F* and K* are evaluated in the following  manner. Consider X =  Ax-\-  Bu-l-Tw y  =  Cx-\-vz  =  Mx (4.11) with E[ww^]  =  W >  0 and E[vv'^]  =  V >  0 and with  Q >  0 R  >  0 denoting  the matrix weights in the performance index E[\^(z^Qx  + u^Ru)  dt].  Assume that both (A B Q^'^M) and  (A TW^''^  C)  are controllable  and  observable.  Then the  optimal control law is given by u{t)  =  F'^m  =  -R-^B^Pim (4.12) where P*  >  0 is the solution of the algebraic Riccati equation  (2.34) given by A^Pc  +  PcA  -  PcBR-^B^Pc  +  M^QM  =  0. (4.13) The estimate  x is generated by the optimal  estimator where k  =  Ax^  Bu  + K\y ^*  =  Plc^y--  Cx) (4.14) (4.15) in which  P*  >  0 is the  solution  to the dual  algebraic  Riccati  equation  (3.21)  given by PeA^  +  APe -  PeC^V'^CPe +  TWT^ =  0. (4.16) Designing  observer-based  dynamic  controllers  by  the  LQG  control  design method  has  been  quite  successful  especially  when  the  plant  model  is  accurately known. In this approach the weight matrices  Q R and the covariance matrices  W V are used as design parameters. Unfortunately  this method does not necessarily  lead to robust designs when uncertainties  are present. This has led to an enhancement of this method called the LQR/LTR  (Loop Transfer  Recovery) method where the de sign parameters  W and  V are selected  (iteratively)  so that the robustness  properties of the LQR  design are recovered  (refer  to Section 4.6). Finally as was mentioned the discrete-time case is analogous to the continuous-time case and its discussion will be  omitted. EXAMPLE 4.1.  Consider the system x  = Ax  + Bu y  =  Cx where 0  1 0  0 0 0 1 2 -1 ro 1 .0 11 1 0. C  =  [1 0 0]. This is a controllable and observable but unstable system with eigenvalues of A equal to 0 -2 1. A linear state feedback  control u  = Fx  -\-  r was derived in Example 2.5 to assign the eigenvalues of A + BF at -2 -1  ± j.  An appropriate F to accomplish this was shown to be F  = 2 -2 -1 0 -2 \ If the state x(t) is not available for measurement then an estimate x(t) is used instead i.e. the control law  u  =  Fx  + r is employed.  In Example  3.1 a  full-order/full-state observer given by X =  (A-  KC)x  +  [B K^ was derived  [see (3.3)]  with the eigenvalues  of A -  KC  determined  as the roots of  the polynomial  ad{s)  =  s^  + d2S^ + dis  + do. It  was  shown  that  the  (unique)  K  is in  this case K  =  [d2~  1 Ji  -  ^2  +  3 do-di+ 3d2 -  5]^ 367 CHAPTER 4: State  Feedback and  State Observers and the observer is given by 1  -d2 -di +d2-3 _-do  + di  -  3^2  +  5 1 0 2 0" 1 - 1. x + 0  1 1  1 0  0  do-di+ d2-l ^ 1 - ^ 2 +3 3d2 -  5 Using  the  estimate  x  in place  of the  control  state x  in  the  feedback  control  law  causes some deterioration in the behavior of the system. This deterioration can be studied experi mentally. (See the next subsection for analytical results.) To this end let the eigenvalues of  the  observer  be  at  say  -10  -10  - 1 0  let  x(0)  =  [1 1 1]^  and  x(0)  =  [0 0 0]^ plot x(t)  x(t)  and e(t)  =  x(t)  — x(t)  and compare these with the corresponding plots of Example 2.5 where no observer was used. Repeat the above with observer  eigenvalues closer to the eigenvalues of A + BF  (say at - 2  -1  ± j)  and also further  away. In general the faster the observer the faster  e(t)  -^  0 and the smaller the deterioration of response; however in this case care should be taken if noise is present in the system. • B.  Transfer  Function  Analysis For  the  compensated  system  (4.9)  [or  (4.7)]  the  closed-loop  transfer  function  T(s) between  y  and  r is  given  by 3;(^)  =  T(s)r(s)  =  [(C  +  DF)[sI -  (A  +  BF)Y^B +  D]r{s) (4.17) where  y{s)  and  r{s)  denote  the Laplace  transforms  of  y{t)  and  r ( 0  respectively.  The function  T{s)  was  found  from  (4.9) using  the  fact  that  the  uncontrollable  part  of  the system  does  not  appear  in  the  transfer  function  (see  Section  3.4).  Note  that  T{s)  is the  transfer  function  of  {A  +  BF  BC  +  DF  D]  i.e.  T{s)  is  precisely  the  transfer function  of  the  closed-loop  system  Hp{s)  when  no  state  estimation  is  present  (see Section  4.2).  Therefore  the  compensated  system  behaves  to  the  outside  world  as if  there  were  no  estimator  present.  Note  that  this  statement is  true  only  after  suffi cient  time  has  elapsed  from  the  initial  time  allowing to  become  negli gible.  (Recall  what  the transfer  function  represents  in  a system.)  Specifically  taking Laplace  transforms  in  (4.9)  and  solving  we  obtain the  transients = [si  -(A  +  BF)] -1 _ -[si (A  +  BF)]-^BF[sI -  (A  -  KC)]-^] 0 [si  -  (A-h  BF)]-^ + [si  -(A  +  BF)]-^B 0 ris) Ks)  =  [C +  DF  -DF]  Ms)] _e(s)\ + Dris). \x(0) \\e(0) (4.18) 368 Linear Systems Therefore y(s)  - (C  +  DF)[sI  -  (A +  BF)r^x(0) -  [(C  +  DF)[sI  -  (A +  BF)r^BF[sI +  DF[sI  -  (A +  BF)r^]e(0)  +  T(s)r{s\ -  (A KC)y (4.19) which  indicates  the  effects  of  the  estimator  on  the  input-output  behavior  of  the closed-loop system. Notice how the initial conditions for the error ^(0)  =  x(0) -  x(0) influence  the  response.  Specifically  when  ^(0)  T^ 0  its  effect  can  be  viewed  as  a disturbance that will become negligible  at steady  state. The speed by which the  ef fect of ^(0) on y will diminish depends on the location of the eigenvalues of A +  BF and A -  KC  as can be easily  seen from relation (4.19). Two-input  controller In the following we will find it of interest to view the observer-based  controller discussed  previously  as  a  one-vector  output  (u)  and  a  two-vector  input  (y  and  r) controller.  In  particular  from  x  =  (A  -  KC)x  -\-  (B  -  KD)u  +  Ky  given  in  (4.2) and  u  =  Fx  -\-  r given in (4.3) we obtain the  equations X =  {A-KC  + BF  -  KDF)x  + [KB-  KD] u  =  Fx  +  r. (4.20) This is the description  of the (nth order) controller  shown in Fig. 4.5. The state x  is of  course  the  state  of  the  estimator  and  the  transfer  function  between  u and  y r  is given by u(s)  =  F[sl  -{A-KC +  [F[sl  -{A- + BF  - KC  + BF  ~  KDF)Y\B KDF)Y^Ky{s) -  KD)  4- I]r{s\ (4.21) If we are interested only in "loop properties" then r can be taken to be zero in which case (4.21) (for r  =  0) yields the output feedback  compensator which  accomplishes the same control objectives  (that are typically only "loop properties") as the original observer-based controller. This fact is used in the LQG/LTR design approach. When r  7^ 0 (4.21) is not appropriate for the realization of the controller since the  transfer function  from  r which must be outside the loop may be unstable. Note that an ex pression  for  this controller that leads  to a realization  of a stable closed-loop  system is given by u{s)  =  [F[sl  -{A-KC + BF  -  KDF)Y^[K  B  -  KD]  -I- [0 /]] (4.22) r(s) (see Fig. 4.6). This was also derived from  (4.20). The stability of general  two-input controllers  (with two degrees  of freedom)  is discussed  at length in Section 7.4D  of Chapter 7. r Controller u System y FIGURE 4.6 Two-input controller At this point we find it of interest to determine the relationship of the observer-based controller and the conventional one-and-two block controller configurations of Fig. 4.7. Here the requirement is to maintain the same transfer functions between in puts y and r and output u. (For further  discussion of stability and attainable response maps  in  systems  controlled  by  output  feedback  controllers  refer  to  Section  7.4  of Chapter 7.) We proceed by considering  once more (4.2) and (4.3) and by  writing 369 CHAPTER 4: State Feedback and State Observers u(s)  =  F[sl  -  (A -  KC)r\B -  KD)u(s) +  F[sl  -  (A -  KC)r^Ky(s) +  r(s)  =  Guu(s)  +  Gyy(s)  +  r(s). This yields u(s)  =  (I-  Gur'[Gyy(s)  +  r(s)] (4.23) (see Fig. 4.7). Notice that Gy  =  F[sl  -  (A -  KOr^K (4.24) i.e. the controller  in the feedback  path  is  stable  (why?). The matrix  (/  -  Gu)~^  is not necessarily  stable; however it is inside the loop and the internal  stability  of the compensated  system is preserved.  Comparing with (4.21) we obtain (/  -  G J ~i  =  F[sl  -{A-KC + BF  -  KDF)Y\B -  KD)  +  /. (4.25) Also as expected we have (/  -  GuT^Gy  =  F[sl  -{A-KC^-BF -  KDF)Y^K (4.26) (show this). These relations  could have been  derived  directly  as well by the use of matrix identities; however  such derivation is quite  involved. r '  ' ^0  • 1  + 1 1 1 FIGURE 4.7 (1 -  Gu)-' '  u System y Gy EXAMPLE 4.2.  For the system x  = Ax  + Bu y  =  Cx with A = B = and C  =  [0  1] we have H(s)  =  C(sl  -  A)-^B  = s/(s^ + 2s + 2). In Example 3.2 it was shown that the gain matrix K  =  [do  -  2 di  -  2]^ assigns the eigenvalues of the asymptotic observer (of A -  KC) at the roots of s^ + dis + do. In fact si  -  (A-  KC)  = do s -1  s + di_ . It is straightforward  to show that F  =  [\ao —  12 — ai\  will assign the eigenvalues of the closed-loop system (of A + BF) at the roots of ^^ + ^i^ + ao. Indeed si  -{A  + BF) 2 s kao  s -\- a\ . Now in (4.23) we have Gy{s) = F(sI-(A-KC))-^K s((do -  2)(iao  -  1) + (di -  2)(2 -  aQ) + ((do  -  d){ao  -  2) + {do  -  2)(2 -  a)) s^ + dis  + do 370 Linear Systems 1 s(2 — a\) — doilao  —  I) G(s)  =  FisI  -  (A -  KOr'B  =  ^ (1 -  G„)-'  = ''+ds  + do _ s^ + s(di  + a\  -  2) +  haodo ^'' °^^  ° s"-+ dis  + do '- 4.5 SUMMARY In this chapter Hnear state feedback controllers and state observers were studied with an  emphasis  on  time-invariant  continuous-time  and  discrete-time  systems  (Sec tions 4.2 and 4.3). State feedback controllers and state observers were then combined (in Section 4.4) to develop observer-based dynamic output feedback  controllers. We note that output feedback  controllers are studied further  in Chapter 7. Linear  state  feedback  was  studied  in  Section  4.2. First  the  need  for  feedback was explained by discussing open- and closed-loop control in the presence of uncer tainties. System  stabilization  was then  addressed  which  for  the time-invariant  case leads  to the eigenvalue  or pole  assignment  problem.  It was pointed  out that  an op timal control formulation  such  as the Linear  Quadratic  Regulator  (LQR)  problem will  also  lead  to  stable closed-loop  control  systems  while  attaining  additional  con trol  goals  as well. Furthermore  the LQR  problem  formulation  can  also be used  in the time-varying case. The eigenvalue assignment problem was studied at length by introducing several such methods. Analogous results for the discrete-time case were established  in Subsections 4.2E and 4.2F. In  Section  4.3 full-order  observers  for  the  entire  state  or for  a linear  function of  the  state  were  presented.  Reduced-order  observers  and  optimal  observers  were also addressed in Subsections 4.3B  and 4.3C respectively. The duality between the state feedback controller and state observer problems was explored and emphasized. Analogous results for the discrete-time case were also described. Current  state esti mators were introduced  and developed in Subsection 4.3D. In Section 4.4 state observers together with state feedback controllers were used to derive dynamic output feedback  controllers and the Separation Principle was dis cussed. The degradation of performance  in state feedback  control when an observer is used  to estimate  the  state  was  explained.  An  analysis  of  the  closed-loop  system was carried out using both state-space and transfer  function  matrix  descriptions. 4.6 NOTES The fact that if a system is (state) controllable then all its eigenvalues can arbitrarily be  assigned  by  means  of  linear  state  feedback  has  been  known  since  the  1960s. Original  sources  include  Rissanen  [19] Popov  [17] and  Wonham  [23]. (See  also remarks in Kailath  [10] pp.  187  195.) The present approach for eigenvalue assignment via linear state feedback  using the controller form follows the development in Wolovich [22]. Ackermann's  formula first appeared in Ackermann  [1]. The development of the eigenvector formulas for the feedback matrix that assign all the closed-loop eigenvalues and (in part) the corresponding  eigenvectors  follows 371 CHAPTER 4: State Feedback and State Observers Moore [16]. The corresponding development that uses (A B) in controller  (compan ion) form  and polynomial matrix descriptions follows  Antsaklis  [3]. Related  results on static output feedback  and on polynomial and rational matrix interpolation can be found  in Antsaklis and Wolovich  [4] and Antsaklis and Gao  [5]. Note that the  flexi bility in assigning the eigenvalues  via state feedback  in the multi-input  case can be used  to  assign  the invariant  polynomials  of si  -  {A  -\r  BF)\  conditions  for  this  are given by Rosenbrock [20]. The Linear Quadratic Regulator (LQR) problem and the Linear Quadratic Gaus sian  (LQG)  problem  have  been  studied  extensively  particularly  in  the  1960s  and early  1970s. Sources for these topics include the books by Anderson and Moore [2] Kwakemaak  and Sivan  [11] Lewis  [12] and Dorato et al. [9]. Early optimal control sources include Athans  and Falb  [6] and Bryson  and Ho  [8]. A very powerful  idea in optimal  control  is the Principle  of  Optimality  Bellman  [7] which  can be  stated as follows:  "An  optimal  trajectory  has  the property  that  at  any  intermediate  point no matter how it was reached the remaining part of a trajectory  must coincide  with an optimal trajectory  computed from the intermediate point as the initial point". For historical remarks on this topic refer  e.g. to Kailath  [10] pp.  240-241. The most influential work on state observers is the work of Luenberger. Although the asymptotic observer presented here is generally  attributed to him  Luenberger's Ph.D. thesis work in  1963 was closer to the reduced-order observer presented above. Original  sources on state observers include Luenberger  [13] [14] and  [15]. For an extensive overview of observers refer  to the book by O'Reilly  [18]. When  linear  quadratic  optimal  controllers  and  observers  are combined  in  con trol design  a procedure  called  LQG/LTR  (Loop  Transfer  Recovery)  is used  to  en hance  the  robustness  properties  of  the  closed  loop  system.  For  a treatment  of  this procedure see Stein and Athans  [21] and contemporary  textbooks on  multivariable control. 4.7 REFERENCES 1.  J. Ackermann "Der Entwurf linearer Regelungssysteme im Zustandsraum" Regelungs-technik und Prozessdatenverarheitung Vol. 7 pp. 297-300 1972. 2.  B.  D.  O.  Anderson  and  J.  B.  Moore  Optimal Control  Linear  Quadratic  Methods Prentice-Hall Englewood Cliffs NJ 1990. 3.  R J. Antsaklis "Some New Matrix Methods Applied to Multivariable System Analysis and Design" Ph.D. Dissertation Brown University May 1976. 4.  P. J. Antsaklis  and W. A. Wolovich  "Arbitrary  Pole Placement  Using Linear  Output Feedback Compensation" Int. J. Control Vol. 25 No. 6 pp. 915-925 1977. 5.  P. J. Antsaklis and Z. Gao "Polynomial and Rational Matrix Interpolation: Theory and Control Applications" Int. J. of Control Vol. 58 No. 2 349-404 August 1993. 6.  M. Athans and P. L. Falb Optimal Control McGraw-Hill New York 1966. 7.  R. Bellman Dynamic Programming Princeton University Press Princeton NJ 1957. 8.  A. E. Bryson and Y. C. Ho Applied Optimal Control Hoisted Press New York 1968. 9.  P. Dorato  C.  Abdallah  and  V  Cerone  Linear-Quadratic  Control:  An Introduction Prentice-Hall Englewood Cliffs NJ 1995. 10.  T  Kailath Linear Systems Prentice-Hall Englewood Cliffs NJ 1980. 11.  H. Kwakemaak and R. Sivan Linear Optimal Control Systems Wiley New York 1972. 12.  F  L. Lewis Optimal Control Wiley New York 1986. 372 Linear  Systems 13.  D. G. Luenberger "Observing the State of a Linear System" IEEE  Trans. Mil.  Electron 14 15 16 17 MIL-8 pp. 74-80  1964. D. G. Luenberger "Observers for Multivariable Systems" IEEE  Trans on Auto.  Control AC-llpp.  190-199  1966. D.  G.  Luenberger  "An  Introduction  to  Observers"  IEEE  Trans  on  Auto.  Control AC-16 pp. 596-603 December 1971. B.  C. Moore  "On  the Flexibihty  Offered  by  State  Feedback  in  Multivariable  Systems Beyond  Closed  Loop Eigenvalue  Assignment"  IEEE  Trans  on Auto.  Control  AC-21 pp. 689-692  1976; see also AC-22 pp.  140-141  1977 for the repeated eigenvalue case. V. M. Popov "Hyperstability and Optimality of Automatic Systems with Several Control Functions" Rev. Roum.  Sci. Tech. Sen ElectrotechEnerg.  Vol. 9 pp. 629-6901964. See also V. M. Popov Hyperstability  of Control  Systems  Springer-Verlag New York 1973. J. O'Reilly  Observers for  Linear  Systems  Academic Press New York 1983. 18 19.  J. Rissanen "Control System Synthesis by Analogue Computer Based on the Generalized Linear  Feedback  Concept"  in  Proc.  of  Symp.  on Analog  Comp.  Applied  to  the  Study of  Chem.  Processes  pp.  1-13  Intern.  Seminar  Brussels  1960.  Presses  Academiques Europeennes Bruxelles 1961. H. H. Rosenbrock  State-Space  and Multivariable  Theory  Wiley New York  1970. G. Stein and M. Athans "The LQG/LTR Procedure for Multivariable Feedback  Control Design" IEEE  Trans on Automatic  Control  Vol. AC-32 pp.  105-114 February  1987. W. A. Wolovich Linear  Multivariable  Systems  Springer-Verlag New York  1974. W.  M.  Wonham  "On  Pole  Assigment  in  Multi-Input  Controllable  Linear  Systems" Vol. AC-12 pp. 660-665  1967. 22 23 20 21 4.8 EXERCISES 4.1.  Consider  the  system  x  =  Ax  +  Bu  where  A -0.01 0 0 -0.02 and  B 1 1 0.25  0.75J with  u  =  Ex. (a)  Verify  that the three different  state feedback  matrices given by -1.1 0 -3.7 0 0 -1.1 0 1.2333 -0.1 0 0 - 0 .1 all assign the closed-loop eigenvalues at the same locations namely at —0.1025 ± jO.04944. Note that in the first control law  (Fi)  only the first input is used  while in the second law (F2) only the second input is used. For all three cases plot x(t)  = [xi(t)  X2(t)]^  when  x(0)  =  [0 1]^  and  comment  on  your  results.  This  example demonstrates how different  the responses can be for different  designs even though the eigenvalues  of the compensated  system are at the same locations (b)  Use the eigenvalue/eigenvector assignment method to characterize all F that assign the closed-loop  eigenvalues  at  -0.1025  ±  70.04944. Show how to select the  free parameters  to obtain Fi F2 and F3 above. What  are the closed-loop  eigenvectors in these  cases? 4.2.  For the system  x  =  Ax  -\-  Bu  with A  E  /^"><" and B  E  7?"^"" where  (A B)  is control lable  and  m  >  I  choose  u  =  Fx  dis  the  feedback  control  law. It is possible to  assign all eigenvalues  of A +  BE  by first reducing this problem to the case of eigenvalue as signment for single-input  systems (m  =  1). This is accomplished by first reducing the system to a single-input  controllable  system.  We proceed  as  follows. Let  F  =  g  ' f  where  g  G  R^  and  /^  G  i?" are vectors  to be  selected.  Let g  be 373 CHAPTER  4: State  Feedback and  State Observers chosen  such that (A 5^)  is controllable. Then/  in A +  5F  =  A +  ( 5 g )/ can be viewed  as the state feedback  gain vector for  a single-input  controllable  system (A Bg)  and  any  of  the  single-input  eigenvalue  assignment  methods  can  be  used  to select/  so that the closed-loop eigenvalues  are at desired locations. The only question that remains to be addressed is whether there exists g such that (A Bg)  is  controllable.  It  can  be  shown  that  if  (A B)  is  controllable  and A is  cyclic then  almost  any  g  G  R^  will  make  (A Bg)  controllable.  (A matrix A is  cyclic  if  and only if its characteristic  and minimal polynomials are equal.) In the case when A is not cyclic it can be shown that if (A B C) is controllable  and observable then for  almost any real output feedback  gain matrix //  A +  BHC  is cyclic. So initially by an almost arbitrary choice of H or F  =  HC  the matrix A is made cyclic and then by  employing a g (A Bg)  is made  controllable.  The  state feedback  vector  gain/  is then  selected  so that the eigenvalues  are at desired  locations. Note that F  =  gf  is always  a rank  one matrix  and this restriction  on F  reduces the applicability of the method when requirements in addition to eigenvalue assignment are to be met. For  the  present  approach  see  W.  M.  Wonham  "On  Pole  Assignment  in  Multi-Input  Controllable  Linear  Systems"  IEEE  Trans.  Autom.  Control  Vol.  AC-12 pp.  660-665  December  1967  and  F.  M.  Brasch  and  J.  B.  Pearson  "Pole  Place ment  Using  Dynamic  Compensators" IEEE  Trans. Autom.  Control  Vol. AC-15 pp. 34-43  February  1970.  For  a  discussion  of  cyclicity  of  matrices  see  Chapter  2  and also P. J. Antsaklis "Cyclicity  and Controllability  in Linear Time-Invariant  Systems" IEEE  Trans. Autom.  Control  Vol. AC-23 pp. 745-746 August  1978. (a)  For A B as in Exercise 4.4 use the method described above to determine F so that the closed-loop eigenvalues are at —1 ±  jand—2  ±  /Comment on your choice for ^. (b)  For  A 0  11 1  ij and  B eigenvalues  are at - 1. characterize  all  g  such  that  the  closed-loop 4.3.  Consider the system  x(k  + 1) A  = Ax(k)  • 4  0 0 0  1 -1 Bu(k)  where B  = '  0 1 . -1 0" 0 1. Determine  a linear  state  feedback  control  law  u(k)  =  Fx{k)  such  that  all  the  eigen values of A  + BE  are located  at the origin. To accomplish  this use (a)  reduction to a single-input  controllable  system (b)  the controller form  of (A  B) (c)  det  (zl  -  (A +  BF))  and the resulting nonlinear  system of equations. In each case plot x(k)  with x(0)  =  [1 1 1]^ and comment on your results. In how many  steps does your compensated  system go to the zero  state? 4.4.  For the system  x  =  Ax  + Bu  where [O 1 0 'l 0 0 0 o" 0 0 1 determine F so that the eigenvalues of A + BF  are at -different  methods to choose F as you can. -1 ±  J  and  -2  ±  /  Use as many 374 Linear  Systems 4.5.  Consider the system  x  =  Ax  + Bu  where -1 -1 1 0 1 -1 (a)  Show that  -1  is an uncontrollable  eigenvalue. (b)  For  the  control  law  u  =  Fx  determine  F  so that  the  controllable  eigenvalues  of 0 1 0 0 -1 0 3 -1 -2 0 2 0 -6 0 6 0 0 -2 r  0 -1 0 0 1 0 r -2 -1 0 2 0 0 2 1 0 2 0 B  = A  + BF  are - . l  - . 2  - l ± j  - 2. 4.6.  Consider  the  SISO  system  Xc  =  AcXc + BcUy  =  CcXc +  DcU where  {Ac Be)  is  in controller form  with 1 0 ••• 0  1 [01 Ac  = 0 l-ao 0 -ai ••• ••• 1 -an-ij Be  = Cc  =  [Co  Ci . . .  Cn-ll and let w =  FcX + r  =  [/o  / i  . . .  fn-i]x  +  r be the linear state feedback  control law. Use the Structure Theorem of Chapter 3 to show that the open-loop transfer  function  is H(s)  =  Cc{sI-Ac)-'Bc + Dc _  n{s)^ d(s) -is-n-l • • • +  CiS  +  Co s^  +  an-is^  ^ +  • • • +  ais  +  ao + Dc and the closed-loop transfer  function  is HF(S)  =  (Cc +  DcFc)[sI  -  (Ac  +  BcFc)]-'Bc  + Dc _ (Cn-l  +  Dcfn-l)s''-' +  • • •  +  (CiS  +  Dcfx)s  +  (CQ  +  Z )  / o) s^  +  (a„-i /„_!>«-!  +  •••  +  ( « !-  / i>  +  (ao  -  /o) -hD ^ FW Observe  that  state  feedback  does  not  change  the numerator  n(s)  of the transfer  func tion but it can arbitrarily  assign any desired (monic) denominator polynomial dF(s)  = .  Thus  state  feedback  does  not  (directly)  alter  the  zeros  of d(s)-Fells H(s)  but it can  arbitrarily  assign the poles  of H(s).  Note that these results  generalize to the MIMO case  [see (2.44)]. 4.7.  Consider the system  x  =  Ax  + Buy  =  Cx  where [01 ro A  =  \o 1 1 0 0 01 1   - ij B  = C  =  [1 2 0]. (a)  Determine an appropriate linear state feedback  control law  u  =  Fx  + Gr(G  E  R) so that the closed-loop transfer function  is equal to a given desired transfer  function ^ -W  =  s^  + \s  + 2-We note that this is an example of model  matching  i.e. compensating a given sys tem so that it matches the input-output behavior of a desired model. In the present case state feedback  is used; however  output feedback  is more common in model matching. (b)  Is the compensated  system in  (a) controllable?  Is it observable? Explain  your  an swers. (c)  Repeat  (a)  and  (b)  by  assuming  that  the  state  is  not  available  for  measurement. Design an appropriate  state observer if possible. 375 CHAPTER 4: State  Feedback and  State Observers 4.8.  Consider the nth order system  x  =  Ax  + Bu  y  =  Cx  -\- Du. (a)  Let  z  =  Az  + Ky  +  [FB  -  KD]u  A  E  R^'^^  be  an  asymptotic  estimator  of  the linear function  of the state Fx where F  G R^^"- is the desired state feedback  gain. Consider the control law u  =  z + r and determine the representation and properties of the closed-loop  system. (b)  Consider a reduced-order  observer of order  n -  p  (as in Subsection 4.3B)  and the control law u  =  Fx  + r. Determine the representation and properties of the closed-loop  system. Hint: The analysis is analogous to the full-state  observer case given in Section 4.4. 4.9.  Design an observer for the oscillatory  system i (0  =  v{t)v{t)  =  -COQX(0 using mea surements of the velocity v. Place both observer poles at 5* =  -COQ. 4.10.  Consider  the  undamped  harmonic  oscillator  xi(t)  =  X2(t) X2(t)  =  -(olxi(t)  +  u(t). Using  an observation  of velocity  y  =  X2 design  an observer/state  feedback  compen sator to control the position xi.  Place the state feedback  controller poles at ^  =  —(Oo± j(x)o  and both observer poles a.ts= -COQ. Plot x(t)  for  x(0)  =  [1 1]-^ and  COQ  =  2. 4.11.  A servomotor that drives a load is described by the equation  (d^O/dt^)  +  (dOldt)  =  w where  6 is the  shaft  position  (output)  and  u is the applied  voltage. Choose  u so that 6 and  (dO/dt)  will  go  to  zero  exponentially  (when  their  initial  values  are  not  zero). To accomplish this proceed as  follows. (a)  Derive a state-space representation  of the  servomotor. (b)  Determine linear  state  feedback  u  =  Fx  +  r  so  that  both  closed-loop eigenvalues  are  at  - 1.  Such  F  is  actually  optimal  since  it  minimizes  J  = IQW^  +  (dO/dtf  +  u^] dt  (show this). (c)  Since only 9 and u are available for measurement design an asymptotic state esti mator (with eigenvalues at say  - 3)  and use the state estimate x in the linear state feedback  control law. Write the transfer  function  and the state-space description of the overall  system and comment on stability controllability  and  observability. (d)  Plot 0 and dSldt  in (b) and (c) for  r  =  0 and initial conditions equal to  [11]^. (e)  Repeat  (c) and (d) using a reduced-order  observer of order  1. 4.12.  Consider the LQR problem for the system  x  =  Ax  + Bu  where (A B) is  controllable and the performance  index is given by J{u)  =  \ Jo e^'''[x^(t)Qx(t)-\-u^(t)Ru(t)]dt where a  G /? a  >  0 and  g  >  0 /? >  0. (a)  Show  that  u* that  minimizes  J(u)  is  a  fixed  control  law  with  constant  gains  on the  states  even  though  the  weighting  matrices  Q  =  e^^^Q R  =  e^^^R are  time varying. Derive the algebraic Riccati matrix equation that characterizes this control law. (b)  The performance  index given above has been used to solve the question of relative stability. In the light of your solution how do you explain  this? Hint:  Reformulate  the  problem  in  terms  of  the  transformed  variables  x  =  e^^x. 376 Linear  Systems 4.13.  Consider the system x --x + u and the performance  indices / i J2 given by Ji= /»oo Jo {x\+X2  + u^)dt and J2= /»oo Jo (900ix\+X2)  +  u^)dt. Determine  the  optimal  control  laws  that  minimize  7i  and  J2.  In  each  case  plot  u{t) xi{t)X2{t)  for x(0)  =  [11]^  and comment on your results. 4.14.  Consider the discrete-time  system x{k-\-l)  =Ax{k)  -\-Bu{k)y{k)  = C{k)x{k)  where 1x2 2^ C = [ 1  0] and where  T  is the sampling period. This is a sampled-data  system obtained from  (the double  integrator)  A .B and C =  [10]  via  a zero-order  hold  and  an ideal sampler both of period  T  (see Chapter 2). Consider  the  cost  functional  J{u)  =  Yk=o(y^(^)  +  2w^(^))-  This  represents (2.51)  with  z{k)  =  y{k)M  =  CQ  =  1  and  R  =  2.  Determine  the  control  sequence {u*{k)}k>0 that  minimizes  the  cost  functional  as  a function  of  T.  Compare  your results with Examples  2.7  and 2.9. 4.15.  Consider the system x = (a)  Use  state  feedback  u x + to  assign"the  eigenvalues  of  A+  BF  at  —0.5zbj0.5. Plot  x(t)  =  [xi(t)X2{t)]'^  for  the  open-  and  closed-loop  system  with  x(0)  = [-0.60.4]^. [l0]x. uy= 1 0 1 0 =  Fx (b)  Design an identity observer with eigenvalues at — a  ±  y where  a  >  0. What is the observer gain K in this case? (c)  Use the state estimate x from  (b) in the linear feedback  control law u = Fx  where F  was found  in  (a). Derive the  state-space  description  of the closed-loop  system. If w =  Fx +  r what is the transfer  function  between y and  r? (d)  For  x(0)  =  [-0.60.4]^  and  x(0)  =  [00]^  plot  x\t)x{t)y{t) and  u{t)  of  the closed-loop  system obtained  in (c) and comment  on your results. Use  a  =  125 and  10 and comment on the effects  on the system response. Remark:  This  exercise  illustrates  the  deterioration  of  system  response  when  state observers  are used  to  generate  the  state  estimate  that  is used  in  the  feedback  control law. 4.16.  Consider the  system X(^+1):  --Ax{k)+Bu{k)+Eq{k) y{k)=Cx{k) where  q{k)  G R^  is  some  disturbance  vector.  It  is  desirable  to  completely  eliminate the  effects  of  q(k)  on the  output  y{k).  This  can happen  only  when  E  satisfies  certain conditions. Presently it is assumed that q{k)  is an arbitrary  r x  1 vector. (a)  Express  the  required  conditions  on  E  in  terms  of  the  observability  matrix  of  the system. (b)  If A = C =  [11] characterize  all E  that satisfy  these conditions. (c)  Suppose E  eR^^^C  eRP^^  and q{k)  is a step and let the objective be to asymp totically reduce the effects  of q on the output. Note that this specification  is not as strict as in (a) and in general it is more easily  satisfied.  Use z-transforms  to derive conditions for this to happen. Hint:  Express the conditions in terms of poles and zeros of  {A  EC}. 4.17.  Consider  the  system  {A B C D} given  hy  x  =  Ax  + Bu  y  =  Cx  + Du  where  A  G 377 i?'^><« B  e  R^^'P C  G 7?^><^ D  G RP^'P  and detD  #  0. (a)  ShowthatjA^  C 5}  =  {A -  BD~^CBD-\ -D'^Q  D~^} is the inverse  system of{ABCD}i.e. C(5/  -  A)~^5  + D  =  H(s)-\ CHAPTER 4: State  Feedback and  State Observers where //(i-)  =  CC^*/ -  A) ~ ^ 5 +D  is the transfer function matrix of {A B C D). Ver ify this  result  using  a  state-space  representation  of  the  system  H{s)  = (s^  +  l)/(^2  +  2). (b)  It is possible  to  show  (a) using results  involving  state  feedback.  In particular  let (A B) be controllable let u  =  Fx+Gr  and choose the linear state feedback control gain matrices (FG)  so that//FGW  =  (C+  DF)(sI-(A  +BF)y^BG  + DG  =  L Note that this choice for F makes  all eigenvalues  unobservable. Use this result to prove  (a). Hint:  It  can  be  shown  using  matrix  identities  that  HFG(S)  =  H(s)  X [I-F(sI~A)-^C]-^G BF))-^B]G  [see (2A3)  to  (2A5)]. =  H(s)[I+  F(sI-(A+ Suppose now that det  D  =  0 but there exists a diagonal polynomial  matrix X(s)  =  diag(pi(s)) i  =  I...  p with pi{s)  monic  stable polynomials  of degree  fi  such that \imX{s)H{s)  =  D where det  D  9^ 0. (c)  Show that a realization  of X(s)H(s)  is x  =  Ax  + Bu  y  =  Cx  + Du  i.e. the A B are the same as above and  C D are some new matrices. (d)  Determine a control law u  =  Fx  + Gr that when applied to the system {A B C D} yields HPG{S)  = X-\sl a diagonal transfer function  matrix with stable poles at the zeros of Pi(s). Note that this  is  the  problem  of  diagonal  decoupling  via  state feedback  with  stability  (see below  and refer  to Subsection  7.4D  and the Notes of Chapter 7; see also  Exercise 4.20). In  view  of  the  hint  in  (b) determine  a matrix  HR{S) SO that  H(S)HR(S)  = X-\s). The diagonal  decoupling  problem  via state feedback  is to determine a control law  u  =  Fx  -\- Gr  that  when  applied  to  the  system  {A B C D]  yields  a  closed-loop transfer  function  HFG(S)  that is diagonal and nonsingular. The conditions  for existence  of  solutions  can  be  expressed  as  follows:  let X(s)  be  the  diagonal  ma trix X(s)  =  diag  [5^]  where  the  nonnegative  integers  {f}  are  so that  all rows of lim^^oo X(s)H(s)  are constant and nonzero and let \imX(s)H(s)  =  B\ Then  the  system  can  be  diagonally  decoupled  via  state  feedback  if  and  only  if rank  B*  =  p.  The  integers  {f}  are  called  the  decoupling  indices  of  the  system. (Note  that  5*  =  D.)  It  can  be  shown  that  the  p  X p  matrix  5*  can  also  be  con structed  from  {A B C D} as follows:  if the /th row of D is nonzero this  becomes the  /th  row  of  B*.  Otherwise  if  f is  the  lowest  integer  for  which  the  /th  row of CA^i'^B  is nonzero then this row becomes the /th row of B*. (e)  Consider  the  system  {A B C}  of  Exercise  4.20  and  determine  whether  it  can be  diagonally  decoupled  via  state  feedback.  Use  both  the  state  space  matrices A B C  and  the  transfer  function  H(s)  and  verify  that  they  result  to  the  same matrix  B*. 378 Linear  Systems 4.18.  Consider  the  system  {A B C D}  given  by  x  =  Ax  +  Bu  y  =  Cx  +  Du  with detD  ^  0  where  (A B)  is  controllable  and  (A C)  is  observable.  If  {A B C D}  = {A  -  BD~^C  BD~^  -D'^C  D'^} is  its  inverse  system  show  that  the  zeros  of {A B C D} are thepoles  of {A 5 C 5}. Also show  that the poles of {A B C D} are the zeros of {A 5 C 5}. 4.19.  Consider the system  x  =  Ax  + Bu  y  =  Cx  + Du  where c =  1  0 1  2 1  0 0  1 0  0 0  0 B = A  = D = 1  0 0  1 Let u  =  Fx  + rbea.  linear state feedback  control law. (a)  Determine F so that the eigenvalues  of A +  BF  are - 1 -2  and are  unobservable from  y.  What  is  the  closed  loop  transfer  function  Hf(s)(y  =  Hpf)  in  this  case? Hint:  Select the eigenvalues  and eigenvectors  of A +  BF. (b)  Using the Structure Theorem of Chapter  3 verify  that  N{s)D~^{s) 5 +  1 0 1 5 +  2 5  0 0 s -s+  1 0 5 +  2 =  H(s)  =  C(sl  ~  A)-^B  +  D. Express your results in (a) as//(5)M(5)  =  iN(s)D-\s))(D(s)D^\s)) Hf(s)  (see Subsection  4.2D). =  N(s)Dp\s) 4.20.  Consider the system  x  =  Ax  + Bu  y  =  Cx  where r  0  1  0] 0  0  0 . -1  0  1. B = ro  0] 1  0 Lo  1. c = fl 1 1 .1  0 0 -1 Note that H{s) =  N{s: )D-\s)  =  \ 5+  1 1 -1 0 1 -1 [1 0 -1 5 Is it possible to determine the pair (F G)mu  =  Fx  + Gr  so that HFAS) 5 +1 (5 +  2)(5 +  3) 0 5+  IJ If your answer is yes determine  such a pair (F  G). Note that if it is required that Hf^cis)  be diagonal with poles at any stable locations then this is the problem of diagonal  decoupling  via state  feedback. Hint:  Write H(s)  =  \ f. H(s)  and  work with H(s)  to determine  (F G)  so that Hf^cis)  is diagonal with poles at desired locations. 4.21.  Consider  the  controllable  and  observable  SISO  system  x  =  Ax  + Buy  =  Cx  with H(s)  =  C(sl  -  A)~^B. (a)  If  A  is not  an  eigenvalue  of A  show  that  there  exists  an  initial  state  XQ  such  that the response to u(t)  =  e^\  r >  0 is y{t)  =  H{X)e^\  t  >  0. What happens if A is a zero  of//(5)? (b)  Assume that A has distinct eigenvalues. Let A be an eigenvalue of A and show that there exists an initial state XQ such that with "no input" {u{t)  =  0) y{t)  =  ke^\  t  > 0 for  some  k  B  R. 4.22. 4.23. 4.24. 379 CHAPTER  4: State  Feedback and  State Observers For the  discrete-time  case  derive expressions  that  correspond  to formulas  (2.40)  to (2.45) of Subsection  4.2D. Consider  the  controllable  and  observable  SISO  system  x  = Ax-\-bu-\-  bwy  =  ex where  w is  a  constant  unknown  disturbance  modelled  by  w =  0.  If  an  estimate  of ww  is  available  then  we  may  attempt  to  cancel  out  the  disturbance  by  selecting u  =  —w. For  this  consider  w  to  be  an  additional  state  for  the  original  system  and determine  an observer for this augmented  system. (a)  Show that the augmented  system is observable if  and only if 5 =  0 is not a zero of the system  [or of H{s)  =c{sl—A)~^b].  Hint:  Use the eigenvalue criterion  for observability. (b)  Assume  that  {Abc} is  stable  (or  has  been  stabilized)  and  select  the  gain  of the  observer  to  be  K^  =  [0^]  where  k  e  R.  Show  that  the  compensator  u  = —w which  asymptotically  cancels  out  the  constant  disturbance  is  an  integral feedback  compensator. (c)  Consider  the  original  system  H{s)  =  c{sl  —A)~^b  and  use  the  results  of  Sub section  4.4B  to  design  an  output  compensator  that  compensates  for  the  above constant  disturbance.  For  simplicity  assume  that  H{s)  is  stable.  Hint:  There must be a pole ais  = 0. Note that the conditions in (a) must be  satisfied. Show that  {A + BHCBC} only if  {ABC} is controllable  and observable. is controllable  and observable  for  any H  G  RP"" if  and 4.25. Consider the  system x{k+l)= 2 I  0 0 x{k) + "1 0 0 0" 1 0 u{k)y{k)-0 0 x{k). 4.26. Is  it  possible  to  determine  a  linear  state  feedback  law  u{k)  =  Fx{k)  +  r{k)  so  that the eigenvalues of A-\-BF  remain at exactly the same locations while  {A-\-BFB  C} becomes  observable? If the answer is affirmative  determine  such  F. Static  or constant  output feedback  u = Hy-\-rH  G R'^^P^  can be used to compensate a system  X = Ax-\-Bu  y = Cx where A e  R^^^C  e  R^^^  and assign the eigenvalues of A + BHC  of the closed-loop  system i  =  (A + BHC)x  -\-Br y = Cx. In contrast  to state  feedback  compensation  in  general  the  closed-loop  eigenvalues  cannot  be  ar bitrarily  assigned using H  even when  {ABC} is controllable and observable. It can be  shown that one can arbitrarily  assign "almost  always" at least min  {m-\-p  —  ln) eigenvalues  using  //;  note  that  when  p  =  m  =  1  only  one  eigenvalue  can  be arbitrarily  assigned using H.  To illustrate (a)  Let  AB  be  as  in  Exercise  4.1  let  C  =  [11]  and  determine  H  so  that  the eigenvalues of A + BHC  are at  -0.1025 ±y  0.04944. (b)  Let AB  be  as  in  Exercise  4.3  let  C =  [101]  and  determine  H  so  that  the eigenvalues of A-\-BHC  are all at zero. (c)  For an example (of a "nongenetic" case) where fewer  than p-\-m—l eigenval ues can be arbitrarily  assigned  consider A  = "0 0 0 0 10 0 0 0 0 0 0" 0 10 1 0 "0 1 0 0 0" 0 0 1 c 0 1 and  show  that  only  2  <  p-\-m aeR. 1 =  3 eigenvalues  can be  assigned  to  i ^ / a^ 380 Linear  Systems Remark:  When only some of the n eigenvalues are assigned as was the case when using  constant  output  feedback  care  should  be taken  to guarantee  that the  remaining eigenvalues  will also be stable since H  will typically  shift  all eigenvalues. 4.27.  (Inverted pendulum)  Consider  the inverted  pendulum  described  in Exercise  1.20  of Ax  + Bu(dibouiy  =  0) assuming the Chapter  1. The linearized state-space model  x friction  is zero is given by Xi X2 X3 = "0 1 0 .0  g ^8 L  ML 0 mg M 0 0 0 0 1 r^i \X2 + 1x4 0] 0 0 oj r 1 ML 0 1 M 0 u where  xi  — 6 X2 =  9 X3 =  s X4 =  s  and  u  =  ^ii the force  applied  to the cart.  Let L  =  1 m m  =  0.1 kg M  =  1 kg and ^  =  10 m/sec^ to obtain Xi ol \xi 0 \x2 Us 0 OJ \M (a)  Determine the eigenvalues  and eigenvectors  of A. (b)  Plot the  states when  x(0)  =  [0.01 0 00]^. Repeat for  zero initial conditions  and - l" 0 1 0 11 0 -1 0 0 0 0 1 0 1 0 0 X2 X3 X4 + u the unit step. Comment on your results. (c)  Determine the linear state feedback control law u  =  Fx  + r so thai the closed-loop system eigenvalues  are at  -10  - 3  - 1  and  - 0 . 5. (d)  Use the LQR  formulation  to determine  a stabilizing  linear  state feedback  control law  u  =  Fx  + r. Comment on your choices for the weights. (e)  Letx(O)^  =  [0.5 0 0 0]. Repeat (b) for the closed-loop system derived in (c) and (d). 4.28.  (Armature  voltage-controlled  dc  servomotor)  Consider  the  armature  voltage-controlled  dc servomotor of Exercise 2.71 in Chapter 2. Let y  =  xi. (a)  Design  a full-order  state observer with eigenvalues  at  - 5  - 1  and  -0.5  to derive an estimate x(t)  of the state x(t).  For x(Of  =  [77/6 00] x(Of  =  [00 0] plot the error e(t)  =  x(t)  -  x(t)  for  r >  0 and comment on your results. (b)  Assume  that  the  system  is  driven  by  a  zero-mean  Gaussian  white-noise  w  and the measurement  noise v is also zero-mean  Gaussian  white noise where w and v are uncorrected  in time with covariances  W  =  10"^ and  V  =  10~^ respectively. The state-space description of the system is now x  =  Ax  + Bu  + Fw y  =  Cx + v where F  =  [11 0]^. Design an optimal observer and compare with (a). 4.29.  (Automobile  suspension system)  Consider the automobile  suspension  system of Ex ercise 2.74 in Chapter 2. Assume that the state-space description  of the system is i:  = Ax  + Bu  + Twy  =  Cx  + v with A B from  Exercise 2.74  C  =  [00 1 0] and F  = [ - 1  0 0 0]"^. Both process noise w and measurement noise v are assumed to be uncor-related zero-mean Gaussian stochastic processes with covariances W  =  Ix  10""^ and V  =  10~^ respectively.  Let the damping  constant  c  =  750 N sec/m  and the  velocity of the car V =  18 m/sec. (a)  Design  an  optimal  LQG  observer-based  dynamic  controller.  Comment  on  your choice of the weights. (b)  Using the controller from (a) plot the states x(t)  and the control input u(t) fort  ^  0 when  x(0)  =  [1 0 0 0]^ w  =  0 v  =  0 and the reference  input  of the  system is r(t)  =  I  sin(27rvr/20). 4.30.  (Aircraft  dynamics)  Consider  the  systems  describing  the  aircraft  dynamics  in  Exer 381 cise 2.76 in Chapter 2. (a)  For the  state-space representation  of the longitudinal  motion  of the  fighter  AFTI-16 design  a linear  state feedback  control law  u  =  Fx  + r so that the  closed-loop system eigenvalues  are at  -1.25  ±  7*2.2651 and  -0.01  ±  j0.095. (b)  Let  y  =  Cx  with  C  =  [00  1 0]. Design  a full-order  state  observer  with  eigen CHAPTER 4: State  Feedback and  State Observers values at 0  -0.421  -0.587  and - 1. (c)  Let  the  system  be  compensated  via  the  state  feedback  control  law  u  =  Fx  +  r where  x  is the output  of the  state estimator.  Derive  the  state-space  representation and the transfer function  between y and r of the compensated system. Is the system fully  controllable from  r? Explain. (d)  Use the LQR  formulation  to determine  a stabilizing  linear  state feedback  control law  u  =  Fx  + r. Comment on your choices for the weights. (e)  Assume  that  process  noise  w  and  measurement  noise  v are present  and  that  both are uncorrelated zero-mean Gaussian stochastic processes with covariances  W  = 10""^ and  V  =  10~^ respectively. Let T  =  [0 1 1 0]^ and design an optimal ob server. (f)  Design  an  optimal  LQG  observer-based  dynamic  controller  and  determine  the eigenvalues  of the closed-loop  system. Discuss your answer in view  of the results in (c). 4.31.  (Chemical  reaction  process)  [C. E.  Rohrs  J.  M.  Melsa  and  D.  G.  Schultz  Linear Control  Systems  McGraw-Hill  1993 p. 70.] Consider the process  depicted  schemat ically  in  Fig.  4.8.  A  reaction  tank  of  volume  V  =  5 000  gallons  accepts  a  feed  of reactant  that contains  a substance A  in concentration  CAO- The feed  enters  at a rate of F gallons per hour and at a temperature  TQ. In the tank some of the reactant A is turned into  the  desired  product  B.  The  output  product  is removed  from  the  tank  at the  same rate that the feed  enters the tank. The mixture in the tank has a uniform  concentration of A CA and a uniform  temperature  T. Temperature Product FIGURE  4.8 Chemical reaction  process 382 Linear  Systems The temperature of the water in the jacket is assumed uniform  at Tj.  The  temperature of the water flowing  into the jacket  is  TIQ. The  system is controlled  by measuring  the temperature  T  in  the  tank  and  controlling  the  flow  of  the  water  in  the jacket  Fj  by activating a valve. The equations describing the evolution of  CA  T and  Tj  are CA t CAQ • r-y^iCA^~^^2/n V To--T-hhCAC-^^^'^^-hiT-Tj) TJ  =  TT'^J'^  ' yj -TJ  +  k5(T  -  TJ) V where  CAO  =  0.5  TQ  =  70°F Tj^o =  70°F and  k\  k2 fe k4 ks  are appropriate  con stants. A  linearized  state-space  model  x  =  Ax  + Bu  y  =  Cx  around  the  equilibrium point  CA  =  0.245 f  =  140 fj  =  93.3 is given by 'xi X2 > 3. = '-1.1 696 0 -2 2.13  X 10-4 2.9 6.5 'xi' y  =  [010] X2 .•^3. 0  1 2.4 -19.5J pr U2 + L-^3. " "  0 0 .-0.16_ where x\  =  8CA X2  =  ST  X3 =  STj  and  u  =  8Fj. (a)  Determine  the  eigenvalues  and  eigenvectors  of A.  Is  the  system  asymptotically stable? Explain. (b)  Let  the  input  be  the  unit  step  indicating  that  the  cooling  flow  is  increased  and held  at  the  new  value.  Plot  the  states  for  /^ >  0  assuming  zero  initial  conditions (equilibrium  values). (c)  Plot the states for f >  0 for zero input but with an initial concentration of substance A slightly larger than the equilibrium value namely  x(0)  =  [0.1 0 0]^. (d)  Determine the linear state feedback  control law u  =  Fx  + r so that the closed-loop system eigenvalues  are at  - 5  - 1 0  and  - 1 0. (e)  Use the LQR  formulation  to derive  a stabilizing  linear  state feedback  control  law u  =  Fx  + r. Comment on your choices of the weights. (f)  Repeat  (b) and (c) for the closed-loop  system derived in (c) and (d). 4.32.  (Economic  model  for  national  income)  Consider  the  economic  model  for  national income in Exercise 2.68 in Chapter 2. (a)  In which cases (i) (ii) or (iii) is the system  reachable? (b)  For  case  (i) design  a linear  state feedback  control  law  to place  both  eigenvalues at  zero. This  corresponds  to  a  strategy  for  government  spending  that  will  return deviations in consumer expenditure  and private investment to zero. 4.33.  (Read/write head of a hard disk) Consider the discrete-time model of the read/write head  of a hard disk described  in Exercise 2.77 of Chapter 2. (a)  Find a linear state feedback  control law to assign both eigenvalues at zero. Plot the response  of  the  discrete-time  closed-loop  system  to  a unit  step  and  comment  on your results. (b)  Let  y(k)  =  6(k)  be the position  of the head  at time  k.  Design  an  appropriate  ob server  of the  state and use it together  with the control law  determined  in  (a). Plot the response to a unit step and compare your results to the results in (a). C H A P T E RS Realization Theory and Algorithms When  a linear  system  is  described  by  an  internal  description  it is  straightforward to  derive  its  external  description.  In  particular  given  a  state-space  description  for a linear  system  the impulse  response  and  also  the transfer  function  in the  case of time-invariant systems were readily expressed in terms of the state-space  coefficient matrices in previous chapters. In this chapter the inverse problem is being addressed: given an external description of a linear system specifically  its transfer  function  or its  impulse  response  determine  an  internal  state-space  description  for  the  system that  generates  the  given  transfer  function.  This  is  the  problem  of  system  realiza tion. The name reflects  the fact that if a (continuous-time)  state-space description is known  an operational  amplifier  circuit  can be built in  a straightforward  manner  to realize (actually  simulate) the system response. The ability of reaUzing systems that exhibit desired input-output behavior is very important in applications. In the design of a system (such as a controller or a filter) the desired  system behavior is frequently  specified  in terms of its transfer  function which  is  typically  obtained  by  some  desired  frequency  response.  One  must  then implement  a system by hardware  or software  that exhibits  the desired  input-output behavior described by the transfer  function.  This in effect  corresponds to building a system  by  combining  typically  less  complex  systems  in parallel  feedback  or cas cade configurations.  In terms of block diagrams this corresponds to building  a sys tem by combining simpler blocks. There are of course many ways an infinite number in fact of realizing a given transfer  function.  Presently we are interested in realiza tions that contain the least possible number of energy  or memory  storage elements i.e.  in  realizations  of  least  order  (in  terms  of  differential  or  difference  equations). To accomplish  this the concepts  of  controllability  and  observability  play  a  central role. Indeed it turns out that realizations of transfer  functions  of least order are both controllable  and  observable.  The  theory  of  realizations  presented  in  this  chapter also sheds light on the behavior of systems that are built by interconnecting  several other systems as for example in feedback control systems. In such systems possible 383 384 Linear Systems pole-zero  cancellations  between  transfer  functions  of  different  subsystems  can  be studied using internal descriptions and the notion of controllability and observability (see also Subsections  7.3B  and 7.3C in Chapter 7). 5.1 INTRODUCTION The  goal  of  this  chapter  is  to  introduce  the  theory  of  realization  to  establish  fun damental  existence  and  minimality  results  and  to  develop  several  realization  al gorithms.  The  emphasis  is  on  time-invariant  continuous-time  and  discrete-time systems. In what follows  we first provide  a glimpse of the contents of this  chapter. This is followed  by some guidelines for the reader. A.  Chapter  Description In Section 5.2 the problem of system realization is introduced both for continuous-time  and  discrete-time  systems.  State-space  realizations  of  impulse  and  pulse  re sponses  for  time-varying  and  time-invariant  systems  and  of  transfer  functions  (in the case of time-invariant  systems) are  discussed. In  Section  5.3 the existence  of  state-space  realizations  is considered  first  and results are presented for both time-varying  and time-invariant cases. The remainder of our development of realization theory and algorithms in this chapter  concentrates primarily  on  time-invariant  continuous-time  and  discrete-time  systems.  Minimal or irreducible realizations are discussed. For the time-invariant  case it is shown that a  state-space  realization  is irreducible  if  and  only  if  it is both  controllable  and  ob servable.  Also it is  shown  that  if  two  realizations  are minimal  then  they  must  be equivalent. The order of minimal realizations is considered next and it is shown that it can be determined directly from a given transfer function  matrix without first find ing a realization. This is accomplished by use of the pole polynomial of the  transfer function that determines its McMillan degree and also by use of the Hankel matrix of the Markov parameters of a system. In addition it is shown that in any minimal real ization the pole polynomial  of the transfer  function  is the characteristic  polynomial of the matrix A. In Section 5.4 a number of realization algorithms are presented. The use of dual ity in obtaining realizations is also highlighted. Algorithms for obtaining realizations in controller  and observer form  are introduced. The SISO case is treated first in the interest  of  clarity.  Realizations  with  the  matrix A  in  diagonal  or block  companion form  are also derived. Finally singular-value decomposition  is used to obtain trans fer function  realizations such as balanced realizations in a computationally  efficient manner. B.  Guidelines  for  the  Reader In  this  chapter  state-space  realizations  of  input-output  descriptions  which  are pri marily  in  transfer  function  matrix  form  are  developed.  In  the  present  treatment 385 CHAPTER  5: Realization Theory and Algorithms fundamental  results  are emphasized.  We point  out that realization  theory  is  among the first important principal topics studied in system theory. Existence and minimal ity  results  of  state-space  realizations  are  established  and  several  realization  algo rithms are developed. We note that detailed summaries of the contents of the sections are given at the beginning  of Sections 5.3 and  5.4. Existence of time-invariant state-space realizations of a transfer function  matrix H(s)  is  addressed  in  Subsection  5.3A  Theorem  3.3 while  minimality  is  fully  ex plored  in two results Theorems  3.9  and  3.10  in  Subsection  5.3B. It is useful  to be able  to determine  the  order  of  a minimal  realization  directly  from  H(s)  and  this  is discussed  in Subsection  5.3C; we note that the pole polynomial  of H{s)  introduced in Section 3.5 of Chapter 3 is required in one of the approaches presented. In Section 5.4  several realization  algorithms  are developed. The use of duality  in  realizations is emphasized  in Subsection  5.4A. At  a first reading  the reader  may  concentrate  on minimality  of realizations  in Subsection  5.3B  and on the order of minimal realizations  in  Subsection  5.3C;  then study one or two realization  algorithms e.g. the one that leads to a realization  with A  diagonal  in  Subsection  5.4C  and  to a realization  with  A B  in controller  form  in Subsection  5.4B.  It  is  also  important  to  study  Subsection  5.4A  on  realizations  us ing duality. If realization  algorithms with good numerical properties  are of primary interest  then  the reader  should  concentrate  on Subsection  5.4E  where  realizations using singular-value  decomposition  are presented. 5.2 STATE-SPACE  REALIZATIONS  OF EXTERNAL  DESCRIPTIONS In  this  section  state-space  realizations  of  impulse  responses  for  time-varying  and time-invariant  systems  and  of transfer  functions  for  time-invariant  systems  are  in troduced. Continuous-time  systems are discussed first in Subsection 5.2A  followed by discrete-time  systems in Subsection  5.2B. A.  Continuous-Time  Systems Before formally  defining  the problem of system realization we first review  some of the relations that were derived in Chapter 2. We consider a system described by equations of the  form X =  A(t)x  +  B(t)u y  =  C{t)x  +  D{t)u (2.1) where  A{t)  E  T^^^^^ B{t)  G  R'''''^  C{t)  G  i^^^"  and  D{t)  G  /^^^^  are  continuous matrices  over  some open  time  interval  {a b).  The response  of  this  system  is  given by y{t)  =  C{t)(S^{ttQ)xo  + H{tT)u{T)dT (2.2) where ^{t  to) is the  nX  n state transition matrix of i:  =  A(t)x  x(to)  =  XQ  (the ini tial condition) and H(t  r) is the p X m impulse response matrix of this system given 386 Linear Systems by H{tT)  = C(t)^(t  T)B(T)  +  D(t)8(t  -  T) for t  >  r 0 for  t  < T. In the time-invariant  case (2.1) assumes the  form X =  Ax  +  Bu y  =  Cx  +  Du and the system response is in this case given by y(t)  =  Ce^^xo  +  H(t  T)u(r)  dr (2.3) (2.4) (2.5) where without loss of generality  ^o was taken to be zero. The impulse response  is now given by the expression H{t  T) Ce^^'-^^B  +  DS(t  -  T) for  t  ^  r 0 for  t  < T. (2.6) Recall that the time invariance of system (2.4) implies that H(t  r)  =  H(t  -  r 0) and therefore r which is the time at which a unit impulse input is applied to the system can be taken to equal zero  (r  =  0) without loss of generality to yield H(t  0). The transfer function matrix of the system is the (one-sided) Laplace transform of H(t  0) namely. H(s)  =  iE[H(t 0)]  =  C(sl  -  A)~^B  -h D. (2.7) Let {A(0 B(t)  C{t\  D(t)}  denote the system description  (2.1) and let H(t  r)  be a /? X m matrix with real functions  of arguments  t and r  as entries. DEFINITION 2.1.  A realization ofH(t  r) is any set{A(0 B(t\  C{t) D{t)} the impulse response of which is H{t r). That is {A{t\ B(t\  C(t\  D(t)} is a reahzation of H(t r) if (2.3) is satisfied.  (See Fig. 5.1.) • D{t) C{t) U(fo) J n x{t) + ys>^ u{t) B{t)  +  ^7^ '^^\  \ + \  A{t) FIGURE 5.1 Block diagram reahzation of {A(t\  B{t) C{t) D(t)} Note that it is not necessary that any given pX  m matrix H(t  r)  be the impulse response to some system of the form  (2.1). The conditions on H(t  r)  under which a realization {A(t) B{t) C{t) D(t)}  exists are given in the next  section. In  the time-invariant  case  a realization  is commonly  defined  in terms  of the transfer  function  matrix. We let {A B C D] denote the system description  given in (2.4) and we let H{s) be a p  X m matrix with entries that are functions  of s. DEFINITION  2.2.  A realization of H{s) is any set {A B C D} the transfer  function matrix of which is H{s) i.e. {A B C D] is a realization of H{s) if (2.7) is satisfied. • 387 CHAPTERS: Realization Theory and Algorithms As  will  be  shown  in the next  section  given  H{s)  a  condition  for  a  realiza tion {A B C D} of H{s)  to exist is that all entries in H{s) are proper rational  func tions. Alternative  conditions  under  which  a given  set {A B C D] is a realization of some H{s) can easily be derived. To this end we expand ^(5") in a Laurent series to obtain H{s)  -  //o +  ^ 1 ^"  + ^2^~  + (2.8) DEFINITION 2.3.  The terms Hu i =  01 2 the system. in (2.8) are the Markov parameters of The Markov parameters can be determined by the formulas HQ  =  lim H(sl Hi  =  lim s(H(s)  -  HQI H2  =  lim s^(H(s)  -Ho- His'^l and so forth.  Recall that relations involving the Markov parameters were alluded to earlier in Exercise 2.63 of Chapter 2. THEOREM 2.1.  The set {A B C D) is a realization of ^(^) if and only if if0  -  ^ and  Hi = CA^'^B i  =  12.... (2.9) Proof H(s) = D + C(sl -  Ay^B  = D + Cs'^I  -  s'^Ay^B  =  D+ Cs-'{Y.U{s-'Am (2.8). = D +  Y.U[CA^-'B]s- from which (2.9) is derived in view of By  definition  the impulse  response  description  of a linear  system  contains no information  about  the initial  conditions  or the initial  energy  stored  in the system. In  fact  H{t T)  is determined  by assuming  that  the system  is at rest  before  r the time  when  the impulse  input  8{t -  T) is applied.  It is therefore  apparent  that  dif ferent  state-space  realizations  of H{t r)  will  yield  the same  zero-state  response while  their  zero-input  response  which  depends  on initial  conditions  can be quite different. Note that if a realization of a given H{t r) exists then there are infinitely  many realizations.  Given {A{t) B(t) C{t) D(t)}  a realization of H(t r) other realizations with the same dimension n of the state vector can readily be generated by means of equivalence  transformations.  Recall from  Chapter 2 that equivalent  representations generate the same impulse response. To illustrate in Example 3.1 of Section 5.3 the system i:  -  P(t)N(t)u(t)  mdy(t)  = M(t)P~\t)x(t)  with N(t)  = ' and M(t) -t [t 1] is a realization of H(t r)  =  t -  r for any P(t) such that P  ^(t) and P(t) exist and are continuous. 388 Linear Systems B.  Discrete-Time  Systems The problem of realization in the discrete-time case is defined  as in the continuous-time  case:  given  an  external  description  either  the  unit  pulse  [(discrete)  impulse] response  H{k  €) or in  the  time-invariant  case  typically  the  transfer  function  ma trix  H{z)  (see  Chapter  2) determine  an  internal  state-space  description  the  pulse response of which is the given H{k £). The  realization  theory  in  the  discrete-time  case  essentially  parallels  the continuous-time  case.  There  are  of  course  certain  notable  differences  because  in the  present  case  the  realizations  are  difference  equations  instead  of  differential equations. We point to these differences  in the subsequent  sections. Some of the relations derived in Section 2.7 of Chapter 2 will be recalled  next. We consider systems described by equations of the  form x(k  +  1)  -  A(k)x(k)  +  B(k)u(k\ y(k)  =  C(k)x(k)  +  D(k)u(k) (2.10) where A(k)  G  7?"><" B(k)  G  J^^X'^ C(k)  G  /?^x^  and  D(k)  G  RP'''^  The  response of this system is given by the  expression y(k)  =  C(k)<i>(k ko)xQ  +  ^  H(k  i)u{i) k  >  k^ (2.11) k-i where ^{k  ko) denotes the  n  X n state transition  matrix  of the  system  x(k  +  1)  = A(k)x(kX  x(ko)  =  xo is the initial condition and H(k  i) is the p  X m pulse response matrix given by C(k)<^(k i +  l)B{i\ H(k  i)  =  < D(k) 0 k  >  / k  =  / k< i. The state transition matrix can readily be determined to be ^{k^  =  \ { A{k- [ / l)A(fc-2)---A(€) yt>€ k =  t (2.12) (2.13) In the time-invariant case (2.10) assumes the  form x{k  +  1)  -  Ax{k)  +  Bu{k) y(k)  =  Cx(k)  +  Du(kl (2.14) and the system response of (2.14) is given by k-l y(k)  =  CA^xo  +  X  ^ ( ^' ^*)"(^*)^ ^  ^  0 (2.15) where without loss of generality ^o was taken to be zero. The pulse response is now given by Hik i)  =  < -(i+i)B CA'' D 0 k>  i k  =  / k<  i. (2.16) 389 CHAPTER  5: Realization Theory and Algorithms Recall  that  since  the  system  (2.14)  is  time-invariant  H{k  i)  =  H(k  -  i 0)  and  / the time the pulse  input  is  applied  can be taken  to be  zero to yield  H(k0)  as  the external system description. The transfer  function  matrix for (2.14) is now the (one sided) z-transform  of H(k  0). We have. H(z)  =  %{H{k 0)}  =  C(zl  -  Ay^B  +  D. (2.17) Now  let  {A{k) B(k)  C(k)  D(k)}  denote  the  system  description  (2.10)  and  let H(k  /) be a /7 X m matrix with real function  entries of arguments k and / defined  for k^ /. DEFINITION  2.4.  A  realization  of H(k i) is  any  set  of  matrices  {A(k) B(k) C(k) • D(k)} the pulse response of which is H(k i). Let H(z)  be a /? X m matrix with functions  of z as entries. DEFINITION  2.5.  A realization  of H{z) is any set {A B C D] the transfer  function matrix of which is H{z). • A result that is analogous to Theorem  2.1 is also valid in the discrete-time  case [with H{s)  replaced by H{z)\  (show this). The remarks following  Theorem 2.1 con cerning the zero-state response of a system and the uniqueness of realizations are also valid in the present  case. Thus all realizations  of the pulse response or the  transfer function  will  yield  the  same  zero-state  response  while  their  zero-input  response which  depends  on initial conditions can be quite different.  Also if  a realization  of H{k  i) or H{z)  exists then there exists an infinite number of realizations (show this). 5.3 EXISTENCE AND MINIMALITY OF REALIZATIONS In this  section the existence  and minimality  of internal  state-space realizations  of a given external description  are  determined. The  existence  of realizations  is  examined  first.  Given  3. p  X  m  matrix  H(t  r) conditions  under  which  this  matrix  is  the  impulse  response  of  a linear  system  de scribed by equations of the form i:  =  A(t)x  + B(t)uy  =  C(0^ + ^ ( 0^  are given in Theorem  3.1. Theorem 3.2 provides the corresponding result for time-invariant  sys tems.  For  such  systems H(s)  is typically  given  in place  of H(t  r)  and  conditions for H(s)  to be the transfer  function  matrix of a system described by equations of the form  X  =  Ax  + Bu y  =  Cx  -\-  Du  are given in Theorem  3.3. It is shown that  such realizations  exist if and only if H(s)  is a matrix  of rational functions  with the prop erty  that lims-^oo H(s)  is finite. The corresponding  results  for  discrete-time  systems are then developed  and presented  in Theorems  3.5 3.6 and 3.7. Realizations  of  least  order  also  called  minimal  or irreducible  realizations  are of  interest  to  us  since  they  realize  a  system  using  the  least  number  of  dynamical elements  (minimum number of elements with memory). The main emphasis in this section  is  on  time-invariant  systems  and  realizations  of  transfer  function  matrices H(s).  The principal  results  are given  in Theorems  3.9  and  3.10  where  it is  shown that  minimal  realizations  are  controllable  (-from-the-origin)  and  observable  and that  all  minimal  realizations  of  H(s)  are  equivalent  representations.  The  order  of any minimal realization can be determined directly without first determining  a min imal realization and this can be accomplished by using the characteristic polynomial 390 Linear Systems and the degree of H(s)  (Theorem 3.11) or from the rank of a Hankel matrix (Theorem 3.13). All the results on minimality of realizations apply to the discrete-time case as well with no substantial changes. This is discussed  at the end of the  section. A.  Existence  of  Realizations Continuous-time  systems Let the /? X m matrix H(t  r)  with tT  E  (a b) be given. A realization of H(t  r) was  defined  in the previous  section  as an internal  description  of the form  (2.1)  de noted by {A{t) B{t) C{t) D(t)}  the impulse response of which is H(t  r). THEOREM 3.1.  H(t T) is realizable as the impulse response of a system described by (2.1) if and only if H(t r) can be decomposed into the form H(t T)  = M(t)N(T) + D(t)8(t -  r) (3.1) for r >  T where M A'' and D SLYQ p X n n X m and p X m matrices respectively with continuous real-valued entries and with n  finite. Proof. (Sufficiency) Assume the decomposition (3.1) is true and consider the realization {0 A^(0 M(t\  D(t)} which yields the system x  = N(t)u(tX y(t)  = M(t)x(t)  + D(t)u(t). The state transition matrix is ^(t  T)  =  / since the homogeneous equation is in this case X = A(t)x  = 0. Applying (2.3) we obtain as the impulse response H(t r)  =  M(t) • / • N(T)  + D(t)8(t -  r) for t >  r and H(t T)  = Ofovt  < r which equals H(t r). (Necessity)  Let  (2.1) be a realization  of H(t r). Then  (2.3) is true and in view of the identity <!>(/ r)  =  <|)(/  (T)^((7  T) H(t r) can be written for r >  r  as H(t r)  = [C(t)<^(t a)][<i>(a  T)B(T)]  + D(t)8(t -  T)  = M(t)N(T) + D(t)8(t -  r) where M(t) = C(t)(!?(t a)  and N(r)  = (t>(a  T)B(T)  (a fixed). Therefore  the decomposition  (3.1) is necessary. • Since the matrices  in  (2.1)  are taken  to be  continuous  we require  in  Theorem 3.1 that M(t)  N(t)  and D(t)  be continuous although this restriction is not necessary. EXAMPLE 3.1.  Let H(t T)  = t -  T  t >  T. Then H(t r)  =  [t  1] Therefore x 1 -t u(t) y(t)  =  [t  l]x(t) is a reahzation. n =  M(t)N(T). EXAMPLE 3.2.  Let H(t r)  =  l/(t  -  T). In this case a decomposition of the form (3.1) • does not exist and therefore H(t r) does not have a realization of the form (2.1). If  a system  is time-invariant  and  is described  by  (2.4) its impulse  response  is given by  (2.6). The following  result establishes  necessary  and  sufficient  conditions for the existence of time-invariant  realizations. THEOREM 3.2.  H(t T) is realizable as the impulse response of a system described by (2.4) if and only if H(t r) can be decomposed for ^ >  r into the form H(t T)  = M(t)N(T) + D(t)8(t -  T) (3.2) where M(t) and N(t) are differentiable  and (3.3) The first part of this result is identical to Theorem 3.1. For time invariance we require in addition relation (3.3) and  differentiability. H(t T)  = H(t  -  T 0). 391 CHAPTER 5: Realization Theory and Algorithms Proof  {Necessity)  Let (2.4) be a realization.  Then  in view  of (2.6) H{t r)  =  H(t  -T0)  =  Ce^^'-'^B  +  D8{t  -  r)  =  {Ce^'Xe'^-'B)  +  Dd{t  -  r).  Let  M(t) = Ce^\  N(T) = e'^^B  and note that both M(t)  and N(t)  are  differentiable. (Sufficiency)  The proof  of  this  part  is  much  more  involved.  The complete  proof can be found  for example  in Brockett  [1] p. 99. In the following  we give an outline of  the proof  (a proof  by  construction).  First  it  can be  shown  that  given  H(t r)  with decomposion  (3.1) a realization  of the form  x  =  N(t)u(t\  y(t)  =  M(t)x(t)  +  D(t)u(t) can be found where n the dimension of the state vector is the smallest possible. Note that this  system  is controllable  and observable.  Now define  W(to ti)  =  j /^ N(a)N^(a)da and  Wi(to h)  =  j^^^  [(d/da)N(a)]  N^(a)da.  Then  using H(t r)  =  H(t  -  T 0) it can be  shown  that {A B C D)  -  {-Wi(tQ  h)W~\tQ  h) N(0) M(0) D} is a realization of H(t  T). Note that W~H^> ^i) exists because {0 N{t) M{t) D] was taken to be of minimal dimension. Therefore  this system is controllable. • EXAMPLE  3.3.  Consider  again H{t r)  =  t -  T given  in Example  3.1 where  a time-varying  realization  was derived. This H(t r) certainly  satisfies  the conditions  of Theo rem 3.2 and thus a time-invariant realization  x  =  Ax  + Bu y  =  Cx  + Du also exists. Unfortunately  the proof  of Theorem  3.2 does not provide us with  a means  of deriving such realizations. In general it is easier to consider the Laplace transform  of H(t r) and use the algorithms  that we will develop in Section  5.4 to show that a time-invariant re alization of H(t T)  =  t -  T is given by i:  = and also see Example 3.4.) "0  1 .0  0. X + "0" 1. u y  =  [1 0]x. (Verify  this In the remainder of this chapter we shall concentrate primarily  on  time-invariant realizations  of  impulse  responses.  In  fact  we  shall  assume  that  the  system  transfer function  matrix  H(s)  which  is  the  Laplace  transform  of  the  impulse  response is  given  and  we  shall  introduce  methods  for  deriving  realizations  directly  from H(s). Given  a pX  m  matrix  H(s)  the following  result  establishes  necessary  and  suf ficient  conditions  for the existence  of time-invariant  realizations. THEOREM  3.3.  H(s)  is reahzable  as the transfer  function  matrix  of a  time-invariant system described by (2.4) if and only if H(s)  is a matrix of rational functions  and satisfies \im  H(s)  < CO (3.4) i.e. if and only if H(s) is a. proper  rational  matrix. (Necessity)  If the system x  = Ax  -\- Bu y  =  Cx  + Du is Si reahzation  of  H(s) Proof then C(sl  -  A)~^B -\- D  = H(s) which shows that H(s) must be a rational matrix. Fur thermore \im H(s)  = D (3.5) which is a real finite matrix. (Sufficiency)  lfH(s)  is a proper rational matrix then any of the algorithms discussed • in the next section can be applied to derive a realization. EXAMPLE  3.4.  Loi H(s)  =  l/s^(y(s)  =  //(^)w(^)) which is the transfer  function of the  double  integrator.  Then  using  the controller  form  realization  algorithm  of  Section 5.4  a realization  of H(s)  is  given  by  i:  = "0  1" .0  0. X + 0 1. uy  =  [10]x.  Notice  that ^-^[H(s)]  =  ^-^[l/s^]  =  t  = H(t 0) the same as in Examples 3.1 and 3.3. 392 Linear  Systems Theorem  3.3  can  be  used  to  show  what  types  of  entries  are  required  for  H{t  0) for  it  to  be  reaUzable  as  a  Hnear  time-invariant  continuous-time  system  of  the  form given  in (2.4). In particular  H{t  0)  =  X~^[H{sy\  and the fact  that all entries  of  H{s) are proper  rational  functions  (Theorem  3.3)  implies  the  next  result. COROLLARY  3.4.  A  pX  m matrix H{i){H{t  0)) is realizable as the impulse  response of a system described by equations of the form  (2.4) if and only if all entries of H{t)  are sums of terms of the form at^e^^  and I38(t)  where a  /3 are real numbers k is an integer (k  >  0) and A is a complex  scalar. Proof.  The  proof  is  left  as  an  exercise  for  the  reader.  (Refer  to  Chapter  2  Subsection 2.4B  for  a  review  of  Laplace  transforms.  Also  refer  to  the  discussion  of  modes  and • asymptotic behavior of a system in that chapter.) EXAMPLE  3.5.  Let H(t)  =  W  +  e~^ +  ^(0. e%  In view  of the  above  corollary  H{t) is  realizable  as  the  impulse  response  of  a  system  described  by  (2.4). In  fact  H{s)  = ^{H(t)}  = 's^  + 2s-l ^ 2 -1 1  " '  s -l and the system i:  =  Ax  + Buy  =  Cx + DwwithA  = r -1  2" L  0 i_ with Theorem  3.2 we  write "2  1 .1  1. B = C  =  [10] D  =  [1 0] is a reahzation  (verify  this).  Comparing C  =  [l H(t  -T0)  =  [e'- +  e-^'-^^ +  8(t  -  T)  e'-'] [e-\  e'  -  e-n e^ + e~ +  [ l  0 ] 5 ( r - T) M(t)N(T)  +  D(t)8(t T) which shows that the given H{t)  [resp. H{t  0)] is indeed realizable by a system described by  (2.4).  Actually  in  this  case  M{t)  and  N{T)  were  chosen  so  that  M{t)  =  Ce^^  and N(T)  =  e  ^^B  where e^^ 0 e' (verify  this). Discrete-time  systems Results  for  the  existence  of  realizations  of  discrete-time  systems  that  are  anal ogous  to  the  continuous-time  case  can  also  be  established.  In  the  following  result which  corresponds  to  Theorem  3.1 H(k  i)  k  >  /  denotes  a. p  X  m  matrix. THEOREM  3.5.  H(ki)is (2.10) if and only if H(k  i) can be decomposed  into the  form realizable  as  the  pulse  response  of  a  system  described  by H(k  i) M(k)N(i) k> / D(k) k  =  i. (3.6) Proof  {Sufficiency)  We consider the reahzation  {/ N{k)  M(k)  D(k)}  i.e.  x(k  +  1)  = x(k)  +  N(k)u(k)  and  y(k)  =  M(k)x(k)  +  D(k)u(k).  In  this  case  ^(k  ^  =  Ik> t Applying  (2.12) it is immediately  verified  that the pulse response  of this  system is the given H(k  i). {Necessity)  For  any  realization  (2.10)  C(k)<^(ki  +  l)B(i)  =  C(k)A(k  -  1)... +  l)B(i)  =  (C(k)^(ka))  X • i  <  a  ^  k which in view of (2.10) implies  (3.6). A(i  +  l)B(i)  =  C(k)A(k  - (0(o- / +  l)B(i))  =  M(k)N(i) l)...A(a)A(a l)...A(i - The discrete-time  system result corresponding  to Theorem  3.2 for  time-invariant systems  is  considered  next. THEOREM  3.6.  H(kJ) (2.14) if and only if H(k  i) can be decomposed  as in (3.6) and is  realizable  as  the  pulse  response  of  a  system  described  by H(k  i)  =  H(k  -  i 0). (3.7) Proof  (Necessity)  This part of the proof is the same as the necessity proof of the previous theorem. Also in view of (2.16) it is clear that H(k  i)  =  H(k  -  i 0). (Sufficiency)  Let H(k  i) satisfy  (3.6) and (3.7). The proof is by construction consid ering a least-order realization x(k  + 1)  =  x(k)  + N(k)u(k)  y(k)  =  M(k)x(k)  +  D(k)u(k) and proceeding  along similar lines as in the proof of Theorem  3.2. • 393 CHAPTER  5: Realization Theory  and Algorithms EXAMPLE  3.6.  Let  H(k  i)  =  k- /  k  >  /.  Here  H(k  i)  =  [k 1] =  M(k)N(i) k  >  /  and H(k  i)  =  0  =  D(k)  k  =  i. In  view  of Theorem  3.5 there  exists  a realiza tion the pulse response  of which  is the  given  H(k  i).  A  particular  reahzation  is  given by the system equations  x(k  +  1)  =  x(k)  + 1 -k u(k)  y(k)  =  [k l]x(k)  (see the proof of  Theorem  3.5).  (Verify  this.)  This  is  of  course  a  time-varying  reahzation.  However here H(k  i)  =  H(k  -  i 0) which in view of Theorem 3.6 implies that a time-invariant realization  also exists. Such  a realization  is x(k  +  1)  =  Ax(k)  +  Bu(k)  y(k)  =  Cx(k) where  A B  = C  =  [0 1]. [Verify  that  in the present  case H(k  0)  = CA^  ^Bk>  0.] This realization  was determined using H(z)  =  ^H(k  0) and the controller form realization  algorithm in Subsection  5.4B. zl(z -  If • Given  apXm  matrix H(z)  the next theorem establishes necessary  and  sufficient conditions for time-invariant realizations. This result corresponds to Theorem  3.3  for the  continuous-time  case.  Notice  that  the  conditions  in  these  results  are  identical. THEOREM  3.7.  H(z)  is  realizable  as  the  transfer  function  matrix  of  a  time-invariant system described by  (2.14) if and only if H(z)  is a matrix  of rational functions  and  sat isfies  the condition  that lim  H(z)  <  00. (3.8) Proof  Similar to the proof  of Theorem 3.3. COROLLARY  3.8.  A  pX  m  matrix  H(k)  [resp. H(k  0)] A:  >  0  is realizable  as  the pulse response of a system described by (2.14) if and only if all entries of H(k)  are sums of polynomial  terms of the form  a\^  I3k(k  — 1)- • -(A: -  € +  1)A^~^ and y  where a  p y  denote real numbers  k £ are nonnegative integers and A is a complex  scalar. Proof  The  details  of  the  proof  are  left  to  the  reader.  Note  that  H(k  0)  = ^-^[H(z)\ where in view of Theorem 3.7 all the entries of H(z)  are proper rational functions.  Refer to Section  2.7  of Chapter  2 for  a review  of z-transforms  and  a discussion  of the  modes • and the asymptotic behavior of discrete-time  systems. EXAMPLE  3.7.  Let H(z)  =  zl(z  -  1)^. In view  of Theorem  3.7 H(z)  is realizable  as the  transfer  function  of  a  system  described  by  (2.14).  Such  a  realization  is  given  by x(k  +1)  =  Ax(k)  + Bu(k)  y(k)  =  Cx(k)  + Du(k)  with A  = Ic = [01]  D  =  0  (refer  to  Example  3.6).  Note  that  in  this  case  H(k  0)  =  ^-^[H(z)]  = k k>  0 which in view  of Corollary  3.8 imphes  that H(k  0)  =  kis  realizable  as  the pulse response of a system described by (2.14). Such a reahzation  is given above. • 2r-[i: -1 394 Linear Systems B.  Minimality  of  Realizations As was discussed  in Section 5.2 realizations  of an impulse response H{t  r)  can be expected  to  generate  only  the  zero-state  response  of  a  system  since  the  external description H{t  r) has by definition  no information  about the initial conditions and the zero-input response of the  system. A second important point to take note of is the fact that if a realization  of a  given H(t  T)  exists  then  there exist  an  infinite number  of realizations.  It was pointed  out that if  (2.1) denoted by {A{t\  B(t)  C{t\  D{t)} is a reaUzation  of the /? X m matrix H{t  T)  then  realizations  of  the  same  order  n  i.e.  of  the  same  dimension  n  of  the state vector can readily be generated by an equivalence transformation.  Recall that in  Subsection  2.6C  of  Chapter  2  it  was  shown  that  equivalent  state-space  (inter nal) representations generate identical impulse responses (external representations). There are of course other ways of generating  alternative realizations. In particular if (2.1) is a reahzation  of H(t  r) then for example the  system X =  A{t)x  +  B(t)u y  =  C(t)x  +  D(t)u z  =  F(t)z  +  G(t)u is  also  a  realization.  This  was  accomplished  by  adding  to  (2.1)  a  state  equation z  =  F(t)z  + G(t)u  that does not affect  the system output. The dimension ofF  dim F and consequently the order of the realization n + dim F can be larger than any given finite number. In other words there may be no upper bound  to the order of the real izations  of a given  H(t  r). There  exists however  a lower bound  and  a realization of such lowest order is called a least-order minimal or irreducible  realization. DEFINITION 3.1.  A realization X = A(t)x  + B(t)u y  = C(t)x + D(t)u (3.10) of the impulse response H(t r) of least order n (A(t) E /?"><") is called a least order  or a minimal order  or an irreducible realization of H(t r). • If  the  given  impulse  response  H(t  r)  satisfies  the  conditions  of  Theorem  3.2 so that  a time-invariant  realization  exists then  one usually  talks  about  minimal  or irreducible  realizations  of  tho  p  X  m  transfer  function  matrix  H{s)  =  i£[H(t 0)] and this is the case on which we shall concentrate in the remainder of this chapter. It should be noted that results corresponding  to Theorems  3.9 and 3.10 exist for time-varying systems as well. The interested reader is encouraged to consult for instance Brockett  [1] for  such results. Briefly  as will be  shown  for  the time-invariant  case system  (3.10) is a minimal realization  of H(t  r)  if  and only if the representation  is controllable (-from-the-origin  or reachable) and observable. Note that in this chapter the term controllable is used in place of reachable to conform  with accepted use in the literature. By controllability  we will really mean  controllability-from-the-origin or reachability.  This  distinction  is not important  in continuous-time  systems but it is important in discrete-time  systems. DEFINITION 3.2.  A realization x  = Ax  + Bu y  = Cx + Du (3.11) of the transfer function matrix H{s) of least order n{A G /?«><«) is called a least-order or a minimal  or an irreducible realization ofH{s). • 395 CHAPTERS: Realization Theory and Algorithms Theorems  3.9  and  3.10 completely  solve the minimal realization  problem.  The first of these results  shows  that  a realization  is minimal  if  and  only  if  it is  control lable (-from-the-origin  or reachable)  and observable while the second result  shows that  if  a  minimal  realization  has  been  found  then  all  other  minimal  realizations can  be  obtained  from  the  determined  realization  using  equivalence  of  representa tions. Controllability  (-from-the-origin  or reachabihty)  and observabiHty play  an im portant  role  in  the  minimality  of  realizations.  This  is  to  be  expected  since  these properties as was discussed  at length in Chapter  3 characterize  the strength  of the connections  between  input  and  state  and  between  state  and  output  respectively. Therefore  it is reasonable  to expect  that they  will play  a significant  role in the re lation between  internal  and  external  descriptions  of  systems. Indeed  it was  shown in  Subsection  3.4C  that only  that part  of  a system  that is both  controllable  and ob servable  appears  in  H(s).  In  other  words  H(s)  contains  no  information  about  the uncontrollable  and/or  unobservable  parts  of the system.  To illustrate this  consider the following  specific  case. 1/ "0 .1 -1 EXAMPLE  3.8. t( s)  = is +  1). Four different  re L e t/ "0  1 .1  0. "0  1 .1  0. "1 0 " P  - 1. -IB = (i)  {A  = (ii)  {A  = (iii)  {A  = (iv)  {A  = B  = IC  = [-HID  =  0}. B  =  C  =  [0 1] D  = 0}. B  =  C  =  [0 1] D  = 0}. 1. 0 1 D  =  0}. 1 ic The eigenvalue +1 that in (i) is unobservable in (ii) is uncontrollable and in (iii) is both uncontrollable and unobservable does not appear in H(s) at all. Realization (iv) which is of order 1 is a minimal realization. It is controllable and observable. • THEOREM  3.9.  An /2-dimensional  realization  {A B C D} of H(s)  is minimal  (irre ducible of least order) if and only if it is both controllable and observable. Proof (Necessity) Assume that {A B C D} is a minimal realization but is not both con trollable and observable. Then using Kalman's Canonical Decomposition of Subsection 3.4A one may find another realization of lower dimension that is both controllable and observable. This contradicts the assumption that {A B C D) is a minimal realization. Therefore it must be both controllable and observable. {Sufficiency) Assume that the realization {A B C D} is controllable and observable but there exists another realization say {A B C D} of order n < n. Since they are both realizations of H{s) or of the impulse response H{t 0) then Ce^'B  + D8{t) = Ce^'B  + D8(t) (3.12) for all r >  0. Clearly D  = D  = \ims-^ccH(s). Using the power series expansion of the exponential and equating coefficients  of the same power of t  we obtain CA^B  = CA^B k  =  012. (3.13) i.e. the Markov parameters of the two representations are the same (see Theorem 2.1 in Section 5.2). Let %n  =  [B AB...  A'^-^B] E Z^^^^'"" 396 Linear  Systems and C CA CA n-l r:  Dpnxn (3.14) Then the pn  x mn matrix product  ^n^n  assumes the  form CB CAB CAB CA^B • CA^'-^B CA^'B CA^'-^B CA^'B CB CAB CAB CA^B CA^^'-^B CA^'-^B CA^'B CA'^-^B CA'^B CA^^'-^B (3.15) In view of Sylvester's  rank inequality  which relates the rank of the product of two matrices to the rank of its factors  we have rank  0^  +  rank ^n  — ^^  rank  {^n^n)  ^  rnin {rank  ^„ rank  ^n) (3.16) and we obtain that rank dn = ^ci^k "^n = ^  ^^^^ {^n^'^n) = ^- This result however con tradicts our assumptions  since n =  rank  {dn'^n)  ^  min(ran^  dn.rank  ' ^)  <  n because n is the order of  {A5 C  5 }.  Therefore  n<n.  Hence n cannot be less than n and they • can only be equal. Thus n = n and  {ABCD} is indeed  a minimal realization. Theorem  3.9  suggests  the following  procedure  to realize H{s).  First  we  obtain  a controllable  (observable)  realization  ofH{s).  Next  using  a similarity  transformation we  obtain  an  observable  standard  form  to  separate  the  observable  from  the  unob-servable  parts  (controllable  from  the  uncontrollable  parts)  using  the  approach  of Subsection  3.4A.  Finally  we  take  the  observable  (controllable)  part  that  will  also  be controllable  (observable)  as  the  minimal  realization.  We  shall  use  this  procedure  in the  next  section. Is  the  minimal  realization  unique?  The  answer  to  this  question  is  of  course  no since  we  know  that  equivalent  representations  which  are  of  the  same  order  give the  same  transfer  function  matrix.  The  following  theorem  shows  how  to  obtain  all minimal  realizations  of  H{s). If THEOREM  3.10.  Let  {ABCD} {ABCD} is  also  a  minimal  realization if  and  only  if  the  two  realizations  are  equivalent  i.e.  if  and  only  if  D  = D  and  there exists a nonsingular matrix P such that and  {A5C5}  be  realizations  of  H(s). is  a  minimal  realization  then  {ABCD} A = PAP-\ B = PB and c = cp-Furthermore if P exists it is given by P  =  ^ ^ ( ^ ^ ^ ) -i or P  = (3.17) (3.18) is  minimal Proof  (Sufficiency)  Let  the  realizations  be  equivalent.  Since  {ABCD} it  is  controllable  and  observable  and  its  equivalent  representation  {ABCD} is  also controllable  and  observable  and  therefore  minimal.  Alternatively  since  equivalence preserves the dimension of A the equivalent realization  {A5C5}  is also minimal. {Necessity) Suppose {A B C D] is also minimal. We shall show that it is equivalent to {A B C D}. Since they are both realizations of H{s) they satisfy  D  = D and (3.19) as was  shown  in the proof  of Theorem  3.9. Here both realizations  are minimal and therefore they are both of the same order n and are both controllable and observable. CA^B  = CA^B k  =  012.. Define ^  =  ^„  and 0  =  ©„ as in (3.14). Then in view of (3.15) 0^  =  0^  and premultiplying by 0^ we obtain 0^0^  =  0^0^. Using Sylvester's inequality we obtain rank 0^6  =  n and therefore 397 CHAPTER  5: Realization Theory and Algorithms % =  [(O^O)"^©^©]^  =  P% (3.20) where P  =  (O^©)'^©^© G 7?"><\ Note that rank P  =  n since rank ©^© is also equal to n as can be seen from  rank ©^©^  =  n and from  Sylvester's inequaUty. Therefore P qualifies  as a similarity  transformation.  Similarly ©^  =  ©^ implies that ©^^^  =  and 0  = ©[^^^(^^^)-i]  = ©P (3.21) where P  = %%^{%%^y^  G T?"^'^ with rank P  = n. Note that P = P.  To  show  that  P  is  the  equivalence  transformation  given  in  (3.17)  we  note  that ©A^  =  ©A^ from (3.15). Premultiplying by ©^ and postmultiplying by ^^ we obtain PA  = AF in view of (3.20) and (3.21). To show that PB  =  fiandC  =  CP we simply use the relations P^  = % and © =  ©P respectively. • C.  The  Order  of Minimal  Realizations In the next  subsection  algorithms  to derive minimal realizations  of H{s)  are devel oped. One could ask the question whether the order of a minimal realization of  H(s) can be determined  directly without having to actually derive a minimal  realization. The answer to this question  is yes and in the following  we will  show how this  can be accomplished. When  deriving realizations it is of advantage to know  at the out set what the order of a minimal realization  ought to be since in this way  erroneous results can be avoided by cross checking the validity  of the results. Determination  via the characteristic or pole polynomial  of  H(s) The characteristic  polynomial  (or pole  polynomial)  PH(S)  of a transfer  func tion matrix H(s)  was defined in Section 3.5 using the Smith-McMillan form of  H(s). The polynomial PH(S)  is equal to the monic least common denominator of all nonzero minors of H(s).  The minimal polynomial  of a transfer function  matrix H(s)  mnis) was defined as the monic least common denominator of all nonzero first-order minors (entries) of  H(s). DEFINITION 3.3.  The McMillan degree of H(s) is the degree of PH(S). • The number of poles in H(s)  which are defined  as the zeros of PH(S)  in  Defini tion 5.1 in Section 3.5 is equal to the McMillan degree of H{s).  The degree of  H{s) is in fact the order of any minimal realization of H(s)  as the following  result shows. THEOREM 3.11.  Let {A 5 C D} be a minimal realization of H(s). Then the charac teristic polynomial of H(s\  PH{S)  is equal to the characteristic polynomial of A a{s) = \sl -  A| i.e. PH{S)  = a(s). Therefore the McMillan degree of H(s) equals the order of any minimal realization. 398 Linear  Systems Proof.  The proof outlined here is based in part on results that will be established in Sec tions 7.2 and 7.3 of Chapter 7. (The reader may wish to postpone reading the proof  until Sections 7.2 and 7.3 have been covered.) First we note that  '\fH{s)  = N(s)D(s)~^  with N(s)  D(s) right coprime polynomial matrices then the characteristic or pole polynomial of H(s) is given  by pnis)  =  k  det  D(s) where  the real  k is such  that  k  det  D(s) is monic.  This  can be seen  for example  by reducing  H(s)  to its Smith-McMillan  form [see (5.3) in Chapter  3]. We have. Ui(s)H(s)U2(s)  = SMH(S) diagi-j^... -^) 0 ^p—rm—r diag{ijJx...\lJr) (ei...€r) 0 = N{s)D-\s\ where  r  =  rank  H(s)  and  Ui U2 are unimodular  matrices.  Then  H  =  (U^^Ns)  X (U2Ds)~^  = A^D~\whereMZ)arerightcoprime.  Notethat  A:(i^/^Z)(5)  =  iAi^2***^r  = PH(S)  by definition.  Now this is true for any right coprime factorization  since these are related by unimodular postmultiplication  (see Subsection 7.3A in Chapter 7). The  controllable  and observable  realization  {A B C D} is now equivalent  to any other  controllable  and observable  realization  of the form  Dz  =  uy  = Nz  with  D N right  coprime  (see Subsection  7.3A).  This  implies  that  15"/ -  A|  =  k\D{s%  since  such equivalence  relation  preserves  the system  eigenvalues.  Note  that  the same  result can be  derived  using  the Structure  Theorem  of Chapter  3 (show  this). Therefore  the pole polynomial of H{s)  is given by PH{S)  =  \sl ~ A\. • It can also be shown  that the minimal  polynomial  ofH{s)  mnis) is equal to the minimal  polynomial  of A am(s)  where  {A B C D] is any controllable  and observ able  realization  of H(s).  This  is illustrated  in the following  example. EXAMPLE  3.9.  LQI  H(s)  = [1/^ 0 2/s^ -\/s\ . The first-order minors the entries  of H(s) have denominators s s and s and therefore  mnis)  =  s. The only second-order minor is -  l/s^ and PH(S)  = s^ with deg PH{S)  =  2. Therefore the order of a minimal realization 2" c =  "1  0" 0  1_ is 2. Such a realization is given hy x  = Ax  + Bu and y  =  Cx with A  =  L  AB  = "1 .0 then we verify  that it is controllable and observable and therefore  minimal. Notice that the characteristic  polynomial of A is a(s)  = s^  =  PH{S)  and its minimal polynomial is • a^{s)  = s  =  rriHis). It can be verified first that this system is a realization of H(s) and 0  0 - 1. The  above  example  also  shows  that  when  H{s)  is  expressed  as  a  polynomial then  the roots of matrix  N{s)  divided  by a polynomial  i.e. H{s)  =  (l/mH(s))N(s) niH are not necessarily  the eigenvalues  of a minimal  realization  of H{s).  They  are in general  a subset  of those  eigenvalues  since the minimal  polynomial  always  divides the  characteristic  polynomial.  In the case  when  H(s)  is a scalar  however  the roots of  rriH =  PH  are the eigenvalues  of  any minimal  realization  of H(s)  as the  next example  shows. EXAMPLE  3.10.  Let H(s)  =  n(s)/d(s)  be a scalar proper rational  function.  Applying Definition  5.1 of Section 3.5 we obtain  PH(S)  =  rnnis)  = d(s) and the order of a mini mal realization is deg PH(S)  =  degd(s).  Thus given H(s)  =  l/(s^  +  3^ + 2) we know that  a minimal  realization  is of second  order  since  deg (s^  +  3^ +  2)  =  2. A  minimal reahzation  in controller  form  is  given  by  A B C  =  [10]. No 399 CHAPTER 5: Realization Theory and Algorithms tice that the minimal polynomial of A is am(s)  = a(s)  = s^ + 3s + 2 the characteristic polynomial of A which is equal to d(s)  =  PH(S)  =  rnnis)  as expected. • The  observations  in Example  3.10 can be formalized  as the following  result. COROLLARY  3.12.  Let  H(s)  =  n(s)/d(s)  be  a  scalar  proper  rational  function.  If {A B C D} is a minimal realization of H(s)  then (3.22) where a(s)  =  det (si  — A) and a^is)  are the characteristic and minimal polynomials of A respectively and A: is a real scalar so that  kd(s) is a monic polynomial. kd(s)  = a(s)  =  ani(s\ Proof  The characteristic  and minimal  polynomials  of H{s) pnis)  and mnis)  are by definition  equal to d(s) in the scalar case. Applying Theorem 3.11 proves the result. • Determination  via the Hankel  matrix There  is an alternative  way of determining  the order  of a minimal  realization of H(s).  This  is accomplished  via the Hankel  matrix  associated  with  H(s). Given  H(s)  we express  H(s)  as a Laurent  series  expansion  to  obtain H(s)  ^  Ho  + H(s)  =  Ho+  His-^  +  H2S~^  +  H^^s'^  +  • • • (3.23) where H{s)  is strictly  proper  and the real pXm  matrices  HQ  / / i  . ..  are the Markov parameters  of the system.  They  can be determined  by the  formulas Ho  = limH(s) Hi  = lims(H(s)-Ho\ H2  =  lim  s\H(s) -Ho- His~'l and  so  forth. DEFINITION  3.4.  The Hankel  matrix  MHH j)  of  order  (/ j)  corresponding  to the (Markov parameter)  sequence H\ H2... is defined  as the ip  X jm  matrix given by MH(iJ)  = Hi  H2 H2  H3 Hj+i Hi  Hi+i Hi+j-i\ (3.24) THEOREM  3.13.  The order  of a minimal  realization  of H(s)  is the rank  of Mnir  r) where r is the degree of the least common  denominator  of the entries of H(s) i.e. r  = degmnis). Proof  Let {A B C D] be any realization  of H{s) of order  n. The Markov  parameters satisfy  the relationships Hi  =  CA'-^B /  =  12... (3.25) 400 Linear  Systems (refer to Exercise 2.63 in Chapter 2). Therefore the Hankel matrix Mnir  r) can be writ ten as C CA Mnir  r) [BAB...A'-^Bl 026) Using Sylvester's Inequahty we have rank Mnir  r) <  n since the common  dimension in the product given by (3.26) is n. This imphes  that any reaHzation of H(s) is of order higher  than  or equal  to rank  Mnin  r). We now must  show  that  there  exists  a realiza tion of order exactly  equal to rank  Mnir  r) where r is the degree of the least  common denominator of the entries of H{s).  Let {A B C} be given by 0„ 0„ A = -dolp ^p -dil^ Up ^p -dr-\Ip Hi H2 B = and  C =  Up 0 0] -h  ••• -h where  A  E  RP'^'P^  B  E  RP''''^  and  C  G  RP^'P'  with  mnis)  =^ s'  + dr-is''^ d\s  + d{)  the minimal  polynomial of H{s). We have  that {A B C) is an  observable re alization  of H{s)  (see Section  5.4). Furthermore  note  that  |^/ -  A|  =  (mH(s))P. Now Hr rankMH{rr)  =  rank C CA CA' [B...  A'-^B]  = rank  [B...  A'-^B].  This is true be-=  Ipr as can easily  be seen  from  above.  Note  now that  mniA)  = 0. C CA CA'-^ This  is true  since  (m//(A))^  =  0 in view  of the  Cay ley-Hamilton  Theorem.  There fore  A\  i >  r can be expressed  as a linear  combination  of / A  . . .  A'""^  which im plies that rank  Mnin  r)  = rank[B  ...A'-^B] -  rank  [ 5  . . .  AP^'^BI  Therefore the rank  of the controllability  matrix  of the  above  realization  is equal  to rank  Mnir  r). Hence if {A B C} is reduced by means of a transformation  to a standard  uncontrollable form Ai 0 An Ai. [Ci C2] \ (see Subsection 3.4A) with (Ai Bi) controllable then {Ai Bi Ci} will be a controllable and observable (minimal) realization of H(s) of order equal to rankMnir  r) since this is the rank of the controllablity matrix of (A B) and the dimension of Ai. • EXAMPLE  3.11.  Let H{s)  = 1 s+  1 -1 s+  1 1 {s + l){s + 2) s + 2 Here the minimal polynomial is muis)  = (s+l)(s  + 2) and therefore r = deg  mnis) 2. The Hankel matrix M//(r r) is then Mnir  r)  = MH(2 2) Hi  H2 H2  H3 an r/7 X  rm  =  4 X 4 matrix and H\  = \im sH(s)  =  lim s^  °° S^  00 5+  1 —s Is s+  1 s 1  2 0  1 {s +  1)(^ + 2)  ^ + 2 and 401 CHAPTERS: Realization Theory and Algorithms H2  =  lim 5->oo y^(i/(5)  -  His-') r .2 2s' o =  lim 5—»oo [(s+ - .2 l)(s  + 2) s^ s +  2 -s s+  1 =  lim [(s+ l)(^  + 2) -2s  -s+  1 -2s s +  2. = Similarly H^ = 1  2 3  4 .No> N rank MH (2 2)  = rank 1 0 1 1 2 1 -2 -2 -1 -1 1 3 -2 -2 2 4 =  3 which is the order of  any minimal  realization  in  view  of Theorem  3.12. The reader • should verify this result using Theorem 3.11. EXAMPLE  3.12.  Consider  the  transfer  function  matrix  H(s)  = Ills 21s LO  - 1 /^ as m Example 3.9. Here r  = degmnis)  = degs  =  1. Now the Hankel matrix Mnir r) Example 3.9. Here r  =  degmnis) n MH(1  I)  =  HI  =  lim^-^oo sH(s)  = [0 2 -1 .  Its  rank  is  2  which  is  the  order  of  a minimal realization of H(s). This agrees with the results in Example 3.9. D.  Minimality  of Realizations: Discrete-Time  Systems All  definitions  and  theorems  given  thus  far  in  this  section  for  the  continuous-time case apply directly to the discrete-time case with no substantial changes. Thus mini mal or irreducible realizations are defined as in Definitions  3.1 and 3.2 where H(k  i) and H{z)  should be used in place of H{t  r)  and H(s)  respectively.  The main crite ria for  establishing  minimality  are given by results  that  are essentially  the  same  as Theorems  3.9  and  3.10.  The  McMillan  degree  of  H(z)  is  as  defined  in  Definition 3.3 while results that are essentially the same as Theorems  3.11 and 3.13 provide a means of determining the order of the minimal realizations. 402 Linear Systems The fact  that the results  on minimality  of realizations  in the discrete-time  case ^^^ essentially  identical to the corresponding  results for the continuous-time  case is not surprising  since we are concentrating here on the time-invariant cases for  which the transfer function  matrices have the same forms: H(s)  =  C(sl  -  A)~^B  + D and H(z)  =  C(zl  -  A)~^B  + D.  Accordingly  the results  on how  to  generate  4-tuples {A B C D] to  satisfy  these relations  are of  course  the  same. The  realization  algo rithms developed in Section 5.4 apply directly to the discrete-time case as well since they  are  algorithms  for  time-invariant  realizations  of  transfer  matrices. The  differ ences in the realization theory between continuous- and discrete-time  systems  arise primarily  in  the  time-varying  case  (compare  Theorems  3.1  and  3.5  for  example). However these differences  are rather  insignificant. 5.4 REALIZATION  ALGORITHMS In  this  section  algorithms  for  generating  time-invariant  state-space  realizations of  external  system  descriptions  are  introduced.  In  particular  it  is  assumed  that  a proper  rational  matrix  H{s)  of  dimensions  p  X  m  is  given  for  which  a  state-space reahzation  {A B C D}  of  H{s)  given  by  i  =  Ax  +  Bu y  =  Cx  + Du  such  that C{sl  -  A)~^B  -\-  D  =  H(s)  has  been  derived  (see  Theorem  3.3).  This  problem  is equivalent  to realizing H(t  0)  =  5£~^[H(s)] A  brief  outline of the contents  of this section  follows. Realizations of H(s)  can often be derived in an easier manner if duality is used and this is demonstrated first in this section. Realizations  of minimal order are both controllable and observable as was shown in the previous  section. To derive a min imal reahzation  of H(s)  one typically  derives  a realization  that is controllable  (ob servable)  and then  extracts  the part that is also observable  (controllable) using  the methods  of Subsection  3.4A  of Chapter  3. This involves in general  a two-step pro cedure. However  in certain  cases a minimal realization  can be derived in one step as for  example when H(s)  is a scalar transfer  function.  Algorithms  for  realizations in a controller/observer  form  are discussed  first.  In the interest  of clarity the  SISO case  is  presented  separately  thus  providing  an  introduction  to the  general  MIMO case. Realization  algorithms where A is diagonal  or in block companion  form  are introduced  next. Finally balanced realizations are  addressed. It  is  not  difficult  to  see  that  the  above  algorithms  can  also  be  used  to  derive realizations described by equations of the form  x(k  +  1)  =  Ax(k)  +  Bu{k)  y{k)  = Cx(k)  +  Du(k)  of  transfer  function  matrices  H(z)  for  discrete-time  time-invariant systems.  Accordingly  the  discrete-time  case  will  not  be  treated  separately  in  this section. A.  Realizations  Using  Duality If the system described by the equations x  =  Ax-\- Bu y  =  Cx  + Du is a realization of if (^) then H{s)  =  C(sl  -  AY^B  +  D. (4.1) 403 CHAPTERS: Reahzation Theory and Algorithms If  H(s)  =  H^{s) C^  C  =  B^  and D  =  D^  is a reahzation  of H(s)  since in view of (4.1) then  x  =  Ax  + Bu  dind  y  =  Cx  + Du  where  A  =-  A^B  = H(s)  =  H^(s) =  B^{sl-  A^y^C^ +D^ =  C(sl  -Ay^B + D. (4.2) The  representation  {A B C D]  is  the  dual  representation  to  {A B C D}  and if  {A B C D}  is  controllable  (observable)  then  {A B C D}  is  observable  (con trollable)  (see  Chapter  3).  In  other  words  if  a  controllable  (observable)  real ization  {A B C D}  of  the  p  X m  transfer  function  matrix  H{s)  is  known  then an  observable  (controllable)  realization  of  the  m  X />  transfer  function  matrix H{s)  =  H^(s)  can  be  derived  immediately:  it  is  the  dual  representation  namely {A B C D}  =  {A^ C^ B^  D^}.  This fact is used to advantage in deriving  realiza tions in the MIMO case since obtaining first a realization  of H^(s)  instead  of  H(s) and then using duality leads sometimes to simpler lower order realizations. Duality  is  very  useful  in  realizations  of  symmetric  transfer  functions  which have  the property  that H(s)  =  H^(s)  as e.g.  in the  case  of  SISO  systems  where H(s)  is a scalar. Under these conditions if {A B C D} is a controllable  (observable) realization of H(s)  then {A^ C^  B^  D^}  is an observable (controllable)  realization of the same H{s).  Note that in this case H{s)  =  C(sl  -  Ay^B  + D  =  H^{s)  -  B^{sl  -  A^y^C^ -h  D^. In realization  algorithms  of MIMO  systems a realization that is either control lable or observable  is typically  obtained  first.  Next  this realization  is reduced  to a minimal  one by  extracting  the part  of  the  system  that  is both  controllable  and  ob servable using the methods of Subsection 3.4A. Dual representations may  simplify this  process  considerably.  In  the  following  we  summarize  the process  of  deriving minimal realizations for the reader's  convenience. transfer function  matrix  H(s)  with lims-^ooH(s) <  00 (see  Theorem  3.3)  we  consider  the  strictly  proper  part  H(s)  = H(s)  -  lirris-^a: H(s)  =  H(s)  -  D  [noting that working with H(s)  instead of H(s)  is optional]. Given  a  proper  rational  p  X  m 1.  If  a realization  algorithm  leading  to  a  controllable  reahzation  is  used  then  the following  steps are taken H(s)  -^  (H(s)  =  H^(s))  -^  {A B C}->  {A  =  A^ B  =  C^C  =  B^l (4.3a) where {A B C] is a controllable realization of H{s)  and {A B C} is an observable realization of  H(s). 2.  To obtain a minimal  realization. {A B C} An A2 0 [Ci C2] (4.3b) where {A B C} is an observable realization  of H(s)  obtained  from  step  (1) and (Ai Bi)  is controllable  (derived  by using  the method  of  Subsection  3.4A)  then {Ai Bi  Ci} is a controllable and observable and therefore a minimal realization of H(s)  and furthermore  {Ai Bi  Ci D} is a minimal realization  of  H{s). 404 Linear Systems B.  Realizations  in  Controller/Observer  Form We shall first consider realizations  of scalar transfer  functions  H(s). Single-input/single-output  (SISO) systems (p  =  m  =  1) Let H{s)  = n(s) d(s) bnS^ +  • • • +  bis  +  Z?o s^  +  an-\s^  1 +  • • • +  a\s  +  UQ (4.4) where  n{s)  and  ^(5*) are  prime  polynomials.  This  is  the  general  form  of  a  proper transfer  function  of (McMillan)  degree n. Note that if the leading coefficient  in the numerator  n{s) is zero i.e. bn  =  0 then H(s)  is strictly proper. Also recall that bnu^""^  +  '"  + biu^'^  + bou (1) (4.5a) or d{q)y{t)  =  {q""  + an-iq""  '  +  •••+  ^ i^ +  ao)y{t) =  (bnq^  +  -"bxq  +  bo)u(t)  =  n(q)u(t) 7 2 -1 (4.5b) where  q  =  d/dt  the  differential  operator.  This  is the  corresponding  nth-order  dif ferential  equation that directly gives rise to the map y(s)  =  H(s)u(s)  if the Laplace transform  of both sides is taken assuming that all variables and their derivatives  are zero 3tt  =  0. Controller form  realizations Given  n(s)  and d(s)  we proceed  as follows  to derive a realization  in  controller form. 1.  Determine  Cj  G  R""  and Dc  ^  R  so that where S(s)  =  [ls... implies that n(s)  =  CcS(s)  +  Dcd(sX (4.6) s^~^]^  is an n X 1 vector of polynomials. Equation  (4.6) Dc  =  limH(s)  =  bn. (4.7) Then n(s)  — bnd(s)  is in general a polynomial of degree n- a real vector  Q  that satisfies  (4.6) always exists. I  which shows that [bo...  bn-i]  i.e. Cc consists of the coefficients  of the n- If  bn  =  0  i.e.  if  H(s)  is  strictly  proper  then  from  (4.6)  we  obtain  Q  = I degree  numerator. If  bn #  0 then  (4.6) implies  that the entries  of  Q  are a combination  of the coefficients  bt and a/.  In particular Cc  =  [bo -  bnao bi  -  bnUi...  Z?„_i  -  Z?«a„-i]. (4.8) 2.  A realization of H(s)  in controller form  is given by the equations yi'C -^^C "^ C JJ Q Id' 1 0 0 0 -flo 0 -a„-\ Xc + y  =  CcXc + DcU. (4.9) The  n  states  of  the  reaUzation  in  (4.9)  are related  by -^/+i  = Xj or  X i +i .(0 1 n—1 and -aoxi n-l ^atXi^i^u i=\ -aoxi .(i) n-\ i=l It can  now  be  shown  that xi  satisfies  the  relationship d{q)xi{t)  =  u{t) y{t)  = n{q)xi{t) (4.10) where  q  =  d/dt the  differential  operator.  In  particular  note  that d{q)xi{t) 405 CHAPTER  5: Realization Theory  and Algorithms 1...W u(t)  because  Xn =  — E/Lo  ^iH view  of  Xn =  -^i"^ derived  from  Xn =  x\' ^\  implies  that  —d{q)xi  +  w  =  0.  The relation y{t)  =  n{q)xi  {t)  can easily be verified  by multiplying both  sides ofn{q)  = CcS{q)  -\-Dcd{q)  given  in  (4.6)  by  xi. Sn) x^\  which  in -d{q)xi » ^» > - l) In) ~^ LEMMA  4.1.  The representation  (4.9) is a minimal realization of H(s)  given in (4.4). Proof  We must  first  show  that  (4.9) is indeed  a realization  i.e.  that  it  satisfies  (4.1). This is of course true in view of the Structure Theorem in Subsection 3.4D of Chapter 3. Presently this will be shown directly using  (4.10). Relation  d{q)x\{t)  =  u{t)  implies  that  x\{s)  =  (d(s))~^u(s).  This  yields  for =  S(s)(d(s))~^i2(s). the  state  that  x(s)  =  [xi(s)...Xn(s)]'^  =  [ls...s''~^]'^xi(s) However we also have x{s)  =  {si — Ac)~^Bcu{s).  Therefore Now  CcisI-Ac)-^Bc+Dc n{s)/d{s)  =H{s) i.e. (4.9) is indeed  a realization. {sI-Ac)S{s)=Bcd{s). (4.11) =CcS{s){d{s))-^+Dc =  (QSis)  + Dcd{s)){d{s))-^  = System (4.9) is of order n and is therefore  a minimal controllable and observable realization.  This is because the degree of H{s)  is n which in view of Theorem  3.11 is the order of any minimal realization. Controllability and observability can also be estab lished  directly  by  forming  the controllability  and  observability  matrices.  The reader  is encouraged to pursue this approach. • According  to Definition  3.3  given in Section  5.3 the McMillan  degree  of  a ratio nal  scalar  transfer  function  H{s)  =  n{s)/d{s) is  n  only  when  n{s)  and  d{s)  are  prime polynomials;  if  they  are  not  all  cancellations  must  first  take  place  before  the  degree can be determined.  If n{s)  and  d{s)  are not prime then the above algorithm  will  yield a  realization  that  is  not  observable.  Notice  that  realization  (4.9)  is  always  control lable  since it is in controller  form.  This can  also be  seen  directly  from  the  expression ro  0 •••  1" [5cAc5c...A^  Be 1 X (4.12) which is of full  rank.  The realization  (4.9) is observable if and only if the  polynomials d{s)  and  n{s)  are  prime. In  Fig.  5.2  a  block  realization  diagram  of  the  form  (4.9)  for  a  second-order transfer  function  is  shown.  Note  that  the  state  xi{t)  and  X2{t)  are  taken  to  be  the voltages  at  the  outputs  of  the  integrators.  This  is  common  when  realizing  transfer functions  using  analog  computer  circuits. 406 Linear  Systems _b2_ u .^ . X2 Efi ^ b-\ —  b23-\ LU * ^'i bo -  ^231  + + lifi FIGURE 5.2 Block realization of H(s) in controller form of the system 0 -ao »  = \x2 +  0 1 1 1 -ai\ b2S^ + bis  + bo .2  + ais-\-ao wy =  [bo-b2aobi  - M l] -\-b2U; Observer form  realizations Given  the  transfer  function  (4.4)  the  nth-order  realization  in  observer  form  is given by XQ  ^ /\QXQ -\-IJQU • • 0 1 0 0 -ao —ai bo -  bnao bi  -  bnai ^0  + 0 • • 1 —Cln-l bn-l -K^n-1 y =  CoXo + DoU =  [00... 0 l]xo + bnU. (4.13) This realization was derived by taking the dual of realization  (4.9). Notice that A^  = AlBo=ClCo=Bl midDo=Dl. LEMMA 4.2.  The representation (4.13) is a minimal realization of H{s) given in (4.4). Proof Note that the observer form realization  {AoBoCoDo} described by (4.13) is the dual of the controller form realization  {AcBcCcDc} described by (4.9) used in • Lemma 4.1. The  realization  (4.13)  can  also  be  derived  directly  from  H{s)  using  defining relations  similar  to  (4.6).  In  particular.  Bo  and  Do  can  be  determined  from  the expression n{s)=S{s)Bo^d{s)Do (4.14) where 5(^)  =  [1^.  ..^^~^]. It can be  shown  (by taking transposes)  that the corresponding  relation to  (4.11) is now given by and that corresponds to (4.10). S{s){sI-Ao)=d{s)Co d{q)z{t)  = n{q)u{t)y{t)  = z{t) (4.15) (4.16) Figure  5.3  depicts  a  block  realization  diagram  of  the  form  (4.13)  for  a  second-407 order  transfer  function. CHAPTERS: Realization Theory  and Algorithms FIGURE  5.3 Block realization  of H(s)  in observer form  of the  system Xi 0 -^o] 1 -ai\ b2S^  +  bx \x{ U2. 5 + + bo-Pi--  b2ao -  bzai = X2_ H{s )  = u.y  = [01] +  b2U\ EXAMPLE  4.1.  We  wish  to  derive  a  minimal  realization  for  the  transfer  function H{s)  =  (s^  + s  -  l)/(s^  -\-  2s'^ -  s  -  2).  Consider  a reahzation  {A^  Be Cc Dc} where (Ac Be)  is  in  controller  form.  In  view  of  (4.6)  to  (4.9)  Dc  =  lims^oo H(s)  =  1  and n(s)  =  s^  -\-  s  — 1  =  CcS(s)  +  Dcd(s)  from  which  we have  CcS(s)  =  (s^  + s  — I)  -(s^  + 2s^  -  s  -2)  =  -2s^  +  2^ +  1  =  [1 2 -2][1 s s^f.  Therefore  a realization  of H{s)  is Xc  =  AcXc +  BcU y  =  CcXc +  DcU where Ac  = 0 0 2 1 0 1 0" 1 -2 Be  = ro 0 1 Cc  =  [12-2] Dc This is a minimal realization  (verify  this). Instead of solving n(s)  =  CcS(s)  +  Dcd(s)  for  Cc as was done above it is possible to derive  Q  by inspection  after  H(s)  is written  as H(s)  =  H(s)  +  lim^(^)  =  H(s)  +  Dc (4.17) where  H(s)  is now  strictly  proper.  Notice that  if  H(s)  is  given  by  (4.4) then  Dc  =  bn and H(s) +  •••  +  Ci5  +  Co Cn-lS'' •  Un-is^  1 +  •••  +  ais  +  ao (4.18) where in fact  c/  =  hi -  bnUi  i  =  0  . . .  /t -  1. The realization {Ac Be Cc) of H(s)  has (Ac Be) precisely the same as before; however  Cc can now be written directly  as Cc  = [Co  Ci ...Cn~ll (4.19) i.e. given H(s)  there are three ways of determining  Q:  (i) using formula  (4.8) (ii) solv ing CcS(s)  — n(s) — Dcd(s)  as in (4.6) and (iii) calculating H(s)  =  H(s)  -  lim^^^oo  H(s). The reader should verify  that in this example (i) and (iii) yield the same Q  =  [1 2  - 2] as in method  (ii). Suppose now that it is of interest to determine a minimal reahzation {Ao Bo Co Do} where  (Ao Co)  is  in  observer  form.  This  can  be  accomplished  in  ways  completely 408 Linear  Systems analogous  to  the  methods  used  to  derive  realizations  in  controller  form.  Alternatively one could use duality directly  and show that AQ  —  A„  — 0 0 1 Co  =  Bl  =  [001] 0 1 0 2 1 -2 Bo  -  Cc -Do  =  Di  =  I is a minimal realization where the pair (A^ Co) is in observer form. • EXAMPLE  4.2.  Consider now the transfer  function  H(s)  =  (s^ -  l)/(s^  +2s^  - s-  2) where the numerator is n(s)  =  ^^ -  1 instead of ^-^ + s -  1 as in Example 4.1. We wish to  derive  a  minimal  realization  of  H(s).  Using  the  same  procedure  as  in  the  previous example it is not difficult  to derive the realization Ac 0 0 2 1 0 1 0^ 1 -2 Be  = ro 0 1 Cc  =  [11  - 2 ] Dc This realization  is controllable  since  (A^  Be) is in controller  form  (see Exercise  5.11); however  it is not observable  since rank  €  =  2  <  3  =  n where 0  denotes the  observ ability matrix given by Cc  ' C-c-^c QA?. = r  1 -4 10 1 -1 1 -2 5 -11 (show  this).  Therefore  the  above  is  not  a  minimal  realization.  This  has  occurred  be cause  the  numerator  and  denominator  of H(s)  are not prime polynomials  i.e.  ^ -  1 is a  common  factor.  Thus  strictly  speaking  the  H(s)  given  above  is  not  a transfer  func tion  since it is  assumed  that in  a transfer  function  all cancellations  of common  factors have  taken  place.  (See  also  the  discussion  following  Lemma  4.1.)  Correspondingly  if the algorithm for deriving an observer form would be applied to the present case the re alization {Ao Bo Co Do] would be an observable realization but not a controllable one and would therefore  not be a minimal  realization. To obtain a minimal realization of the above transfer function  H{s)  one could either extract the part of the controllable realization {Ac Be Cc Dc) that is also observable or simply cancel the factor 5 -1  in H{s)  and apply the algorithm again. The former  approach of reducing a controllable realization will be illustrated when discussing the MIMO case. The latter approach is perhaps the easiest one to apply in the present case. We have His)  = 1 2^2 s^  + s  +  \ s^  + 3s  + 2 -2s-  1 s^ +  3s-\-2 +  1 and a minimal realization  of this is then determined  as 0 -2 1 - 3. Be Cc [ - 1  - 2 ] Dc The reader  should verify  this. Multi-Input-Multi-Output  (MIMO)  Systems  (pm  >  1) Let  Si(p  X  m)  proper  rational  matrix  H(s)  be  given  with  lim^^oo H(s)  <  oo (see Theorem  3.3).  We  now  present  alogrithms  to  obtain  realizations  {Ac  Be  Cc Dc}  of H(s) in  observer  form. in  controller  form  and  realizations  {Ao  Bo  Co Do}  of  H(s) 409 CHAPTER  5: Realization Theory and Algorithms Minimal realizations can then be obtained by separating the observable  (controllable) part of the controllable (observable)  realization. Controller form  realizations Consider  a  transfer  function  matrix  H{s)  =  [nij{s)/dij{s)]i  =  l...p7  = 1... m  and  let  ij (s)  denote  the  (monic)  least  common  denominator  of  all  entries in the  jth  column  of H{s).  The ij{s)  is the least degree polynomial  divisible by  all dij {s)J=  1... p. Then H{s)  can be written as H{s)=Nis)D-\s) a ratio of two polynomial matrices where N{s)  =  [nij{s)]  and D{s)  = .im{s)].  Note that  nij{s)/ij{s)  = nij{s)/dij{s)  for  / =  1 diag[ii{s).. j =  1... m. Let dj  =  deg ij  (s) and assume that dj  >1.  Define A(^)  =  diag  (y 4i (4.20)  p  and  all and S{s)  =  block diag (4.21) and note that S{s)  ism\n(= E7=i ^j)  ^^  polynomial matrix. Write \  s'lj-' / D{s)  = DhA{s) + DiS{s) (4.22) and note that D^ is the highest column degree coefficient  matrix of D{s).  Here  D{s) is  diagonal  with  monic  polynomial  entries  and  therefore  D/^ =  Im- If.  for  exam- then the highest column degree coefficient  matrix D^  = [3^2 j^ ^  2s] pie D{s)  =  \  ^ 2s  and DiS{s)  given in (4.22) accounts for the remaining lower column degree terms in D{s)  with D^ being a matrix of  coefficients. Observe that  \Dh\ 7^ 0 and define the mxm  and mxn  matrices respectively. Also determine Cc and Dc such that -Dn'Di and note that N{s)=CcS{s)^DcD{s) Dc =  lim  H{s). (4.23) (4.24) (4.25) We have H{s)  = N{s)D-^  (s) = CcS{s)D-^  (s) +Dc  with CcS{s)D-^  {s) being  strictly proper (show this). Therefore  only Cc needs to be determined from  (4.24). A controllable  realization  of H{s)  in controller  form  is now given by the  equa tions Here Cc and Dc were defined  in (4.24) and (4.25) respectively. ^c-^mi Be — BcByy (4.26) 410 Linear Systems where Ac  =  block  diag  [Ai A2... A^]  with ^j = \' 0  0 Id ... 0 j^djXdj Be  =  block  diag G R'^J  j  = I.. .  m and Am Bm were defined  in (4.23). Note that if dj  =  /JLJ j  =  1...  m the control-labiHty indices then (4.26) is precisely the relation  (4.63) given in Section  3.4. LEMMA 4.3.  The system {Ac Be Cc Dc} is an w( =  2  J.. 1 ^;)-th-order controllable re alization of H(s) with (Ac Be) in controller form. Proof. First to show that {Ac Be Cc Dc} is a realization of H(s) we note that in view of the Structure Theorem given in Subsection 3.4D we have Cdsl  -  Ae)~^Be  + Dc = N(s)D(s)-\whQYQ D(s)  ^  B;'[A(S) -  AmS(s)l N(s)  ^  CeS(s) + DcD(s). However 5(5)  =  D(s)mdN(s)  =  A^(5)in view of (4.22) to (4.24). Therefore  Q ( 5 /-AcT^Bc  + De = N{s)D-\s)  = //(5) in view of (4.20). It is now shown that (Ac Be) is controllable. We write [si  Ac Be] =  [sl -Ac-  BcAm BcB^] =  [si  -  Ac  Be] I 0 Brr (4.27) and notice that rank [sjl  -  Ac Be] =  n for any complex Sj. This is so because of the special form of A^ Be. (This is in fact the Brunovski canonical form.) Now since \Bm\  ^ 0 Sylvester's Rank Inequality implies that rank [sjl  -  Ac Be] =  n for any complex Sj which in view of Subsection 3.4B implies that {Ac Be) is controllable. In addition since Bm = Im^ it follows  that (Ac Be) is of the form (4.69) given in Subsection 3.4D. With dj  =  )LiJ the pair (Ac 5c) is in controller form. • An alternative way of determining  Q  is to first write H(s)  in the  form H(s)  =  H(s)  +  lim H(s)  =  H(s)  + Dc (4.28) where  H(s)  =  H(s)  -  Dc  is  strictly  proper.  Now  applying  the  above  algorithm  to H(s)  one  obtains  H(s)  =  N(s)D~^(s)  where  D(s)  is  precisely  equal  to  the  ex pression  given  in  (4.20). We note however  that N(s)  is  different.  In  fact  N(s)  = N(s)  -  DcD(s).  The matrix  Q  is now found  to be of the  form A^(^)  -  CcS(sl (4.29) Note  that  this  is  a generalization  of  the  scalar  case  discussed  in  Example  4.1  [see (4.17) to (4.19)]. In  the  above  algorithm  the  assumption  that  dj  >  1 for  all  j  =  1...  m  was made.  If  for  some 7 dj  =  0  this  would  mean  that  the jth  column  of H(s)  will  be a  real  m  X  1 vector  that  will  be  equal  to  the jth  column  of  Dc  [recall  that  Dc  = 411 CHAPTERS: Realization Theory and Algorithms lim^-^oo H(s)].  The strictly proper H(s)  in (4.28) will then have its jth  column  equal to zero and this zero column can be generated by a realization where theyth column of Be is set to zero. Therefore  the zero column  (thejth  column)  of H{s)  is  ignored in this case and the algorithm is applied  to obtain  a controllable realization. A zero column is then added to Be. (See Example  4.4.) Finally we note that given H(s)  =  N(s)D~^{s)  with A^(^) D(s)  not  necessarily from  (4.20) the above algorithm  leads to a controllable realization  if dj  =  column degrees  of D(s)  provided  that  D^  the highest  column  degree  coefficient  matrix  is nonsingular  (\Dh\ #  0). This  in  fact  means  that  D(s)  is  column  proper  (see  Sub section 7.2B). The resulting pair (Ac Be) is in controllable companion form but not necessarily  in controller  form  since Bm  =  D^^  is not necessarily  upper  triangular with ones on the diagonal. (See Example  4.5.) Observer form  realizations These realizations are dual to the controller form realizations and can be obtained by duality  arguments  [see  (4.2)  and Example  4.3]. In the following  observer  form realizations  are obtained directly for completeness  of exposition. We  consider  the  transfer  function  matrix  H{s)  =  [nij{s)ldij{s)\ i  =  1...  p j  =  1...  m and let £i{s) be the  (monic) least common  denominator  of  all  entries in the /th row of H(s).  Then H(s)  can be written as H(s)  =  D'\s)N(s\ (4.30) where  N(s)  =  [nij(s)]  and  D(s)  =  diag  [ti{s\  . ..Jp{s)l  Note  that  nij{s)l'ii{s)  = nij(s)/dij(s)  for  7  =  1...  m and all /  =  1...  p. Let di  =  deg  (((s)  assume that J^- >  1 define A(^)  =  diag  ( /s  . . .  / ^ ) S(s)  =  block  diag  ([I ^...  / ~ ^ ]  /  =  1 and note that S(s)  is a p  X n (=  Xf= i di) polynomial matrix. Now write D(s)  =  Ms)Dh  +  S(s)De (4.31) (4.32) and note that Dh is the highest row degree coefficient  matrix of D(s).  Note that  D(s) is diagonal with entries monic polynomials so that Dh  =  Ip the pX  p identity ma-trix. If  for example D(s)  = \3s^  +  1  2^1 2s  then the highest row degree  coefficient matrix  is Du and S(s)D^  in  (4.32)  accounts  for  the remaining  lower  row degree terms of D(s)  with D^ a matrix of  coefficients. Observe that  \Dh\ y^ 0; in fact Dh  =  Ip. Define  the p  X p  and nX  p  matrices Cp  =  Dh^ and Ap  =  -DeDh\ respectively. Also determine Bo and Do such that N(s)  =  S(s)Bo  +  D(s)Do. Note that and therefore  only Bo needs to be determined from  (4.34). Do  =  lim H(s\ (4.33) (4.34) (4.35) 412 Linear Systems An observable realization of H(s)  in observer form is now given by Xo  =  AQXO  +  BQU y  —  CQXO  +  DQU where Bo and Do were defined  in (4.34) and (4.35) respectively and where Ao  =  block  diag  [Ai A2... Ap\  with (4.36) Ai  = 0  . ..  0  0 0 ^di-\ 0 j^diXdi Co  =  block  diag  ([0.. .01]  E  R^^^\  i  =  1...  p)  and  Ap Cp  is  defined  in (4.33).  Note  that  (4.36)  is  exactly  relation  (4.85)  of  Section  3.4  if  di  =  vt  i  = 1.../? the observability  indices. LEMMA 4.4.  The system {Ao Bo Co Do} is an n (=  2f=i  <iJ)th-order observable re alization of H{s) with (Ao Co) in observer form. Proof This is the dual result to Lemma 4.3. The proof is completely analogous and is omitted. • We  conclude  by noting  that  results  dual  to the results  discussed  after  Lemma 4.3 are also valid here i.e. results  involving  (i) a strictly proper H(s)  (ii) an  H(s) with di  =  0 for some row / and (iii) H(s)  =  D~^(s)N(s)  where D(s) N{s) are not necessarily determined using (4.30) (refer to the following  examples). The reader is encouraged to explicitly  state these results. EXAMPLE  4.3.  Let  H(s)  = S^ +  I  S +  I . We wish to derive a minimal realization for H(s). To this end we consider realizations {Ac Be Cc Dc) where {Ac Be) is in con troller form. Here €1(5) =  s^ €2(5) =  s^ and H{s) can therefore be written in the form (4.20) as H{s)  =  N{s)D~\s)  = {s^  -^\s+  1] Here di  = 2d2 = 3 and A(s) 0  S(s) 1 ^ 0 00 0  0 I  s  s^ . Note that n = di  + d2 = 5 and therefore the realization will be of order 5. Write D{s) =  DhA(s)-\-DeS(s) and note that Dh = h D^ = . Therefore in view of (4.23) ro  0  0  0  01 0  0  0  0  0 Bm — 1  0 0  1 and Am  =  -DP  = 0  0  0  0  0 0  0  0  0  0 Here Dc = lims^oo H(s)  =  [1 0] and (4.24) implies that CcS(s) = N(s) -  DcD(s) = [s^ + 1 5 + 1] -  [s^ 0]  -  [ls  + 1] from which we have Q  =  [1 0 1 1 0]. A control lable  realization  in controller  form  is therefore  given  by x  -  AcXc  + BcU  and y = ^~^ C  C -^-^C  9 413 CHAPTERS: Realization Theory  and Algorithms where Ac  = 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 Be '0 1 0 0 0 0" 0 0 0 1 Cc [1  011  0] and I =  [10]. Note  that  the  characteristic  (pole)  polynomial  of  H{s)  is  s'^ and  the  McMillan  degree of H{s)  is  3. The  order  of  any  minimal  reaUzation  of H{s)  is therefore  3 (see  Theorem 3.11). This  implies  that the controllable  fifth-order  realization  derived  above cannot  be observable  [verify  that  (A^ Cc) is not observable]. To derive  a minimal realization  the observable  part  of  the  system  [Ac Be Cc Dc) needs  to be  extracted  using  the  method described in Subsection 3.4A. In particular a transformation  matrix P needs to be deter mined  so that A  =  PAcP  -1  _  Ai 0 A21  A2 and C  =  CcP~ [Ci0] where  (Ai Ci)  is  observable.  If  B  =  PBc   then  {Ai 5i  Ci Di}  is  a  minimal realization of H{s).  To reduce (Ac Cc) to such standard form for unobservable  systems we let AD  =  AJ  BD  =  C j  and  Co  -  B^  and  we reduce  (AD BD) to a standard  form for uncontrollable  systems. Here the controllability  matrix  is 10 0  10 10 1 10 0  1 10 0  0  0 0  0 0  0  0 0  0 0 Pj)^  are taken to be the Note that rank ^^  =  3. Now if the first three columns of  QD first three linearly independent columns of ^ D  while the rest are chosen so that  \QD\  ¥=  0 (see Subsection  3.4A)  then QD  = and QD'-0 1 0 1 1 0 0 0 0 1 —; -] 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 -1 - 1 0 0 1 0 0 414 Linear  Systems This implies  that AD  =  QDAUQD = ADI [  0 ADU AD2 • '  1 0 0 0 0 BD  — QD  ^D  — BDI _BDI\ Then 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 -1 0 0 1 -1 -1 -1 1 1 CD  —  CDQD -0  10 0  1 10 0  0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 -1 -1 -1 1 1 C  =  Bl  =  [Ci 0]  =  [10 0: 00]. A  = 0 Ai .A21  A2. ^  AT  ^ B  =  Cl  = "0 1 0 0 0 0" 1 1 0 0 Clearly A  =  A^ C  =  Bj  is in standard form. Therefore  a controllable and  observable realization  which  is  a minimal  realization  is given  by  Xco =  Aco^co +  BcoU  and  y  = Cco^co +  J^coU where 0 0 0 1  0" 0  1 0  Oj Bco -"0 1 .0 0 1 1 Ceo =  [100] Dc [10]. A  minimal  realization  could  also  have  been  derived  directly  in  the  present  case if  a realization  {A^ Bo Co Do} of  H(s)  where  (Ao Bo)  is  in  observer  form  had  been considered  first  as  is  shown  next.  Notice  that  the  McMillan  degree  of  H(s)  is  3  and therefore  any realization of order higher than 3 will not be minimal. Here however the degree of the least common denominator of the (only) row is 3 and therefore it is known in advance that the realization in observer form which is of order three will be minimal. A realization {Ao Bo Co Do} of H(s)  in observer form  can also be derived by con sidering H^(s)  and deriving  a realization  in controller  form.  Presently {Ao Bo Co Do} is  derived  directly.  In  particular  we  write  H(s)  =  JD ^(s)N(s)  =  (s^)  ^[s(s^  +  1) s  +  1]. Then  di  =  3[=  deg  \{s)  =  deg  s\  and  A^  =  s^S{s)  =  [lss^l  Then D(s)  =  s^  =  Ms)Dh  +  S(s)Di  implies  that  Dh  =  1 and  De  =  [0 0 0]^.  In  view  of (4.33) we have Cp  =  1 Ap  =  [00 0 ]^ 415 CHAPTERS: Realization Theory  and Algorithms Do  =  liiRs^^  H(s)  =  [10]  and  (4.34)  implies  that  S(s)Bo  =  N(s)  -  D(s)Do  = [s(s^  +  1) 5 +  1]  -  [s^ 0]  =  [ss  -\-  1] from  which  we  have  Bo 0  1  0 [1  1  OJ .  An observable realization  of H(s)  is the system  x  =  AoXo +  BoU y  =  CoXo +  DoU where Ao  = 0  0  01 1  0  0 0  1  0 Bo = ro  r 1  1 0  0 Co =  [001] Do =  [10] with  (Ao Co) in observer  form  (see Lemma  4.4). This realization  is minimal  since it is of order 3 which is the McMillan  degree of H(s).  (The reader  should verify  this.)  Note how much easier it was to derive a minimal realization using the second approach. • EXAMPLE  4.4.  Let  H(s)  = s + 1 1 s .  We  wish  to  derive  a  minimal  realization. Here €1(5)  =  s(s+I)  with  di  =  2and€2(^)  =  lwith(i2  =  0. In view of the discussion following  Lemma  4.3 we  let  Dc  = lims^oH(s) [0  1 0  0 ] 2 and  H(s) We now  consider  the transfer  function  H(s) ization. s+  1 1 and  determine  a minimal  real Note that the McMillan degree of H(s)  is 2 and therefore any realization of order 2 will be minimal. Minimal realizations are now derived using two alternative approaches: 1.  Via  a  controller  form  realization.  Here  £\(s)  =  s(s  +  1)  Ji  = 2  and  H(s)  = 2 s+  1 1 s [s(s  +  l)]-i  =  N(s)D-\s). Then  A(s)  =  s^  and  S(s)  =  [hsf  D(s) "  2s 5 +1 s(s  +  1)  -  Is^  +  [0 1][15 0  2 1  1 -[01].  Also Cc CcS(s).  Then  a  minimal  realization  for  H(s)  is  Ac  = ro 0 DhA(s)  +  D^S(s).  Therefore  B^r 1   which  follows  from  N(s) " 2s .5+ 1 1] - i j' rr [s 1  and  Am  = ro  21 1  1 ro] .ij' Cc Be  = Adding a zero column to 5^ a minimal realization of H(s)  is now derived as 0  2 1  1 A  = B  = 0  0 1  0 c =  0  2 1  1 D  = 0  1 0  0 We  ask  the  reader  to  verify  that  by  adding  a  zero  column  to  B^  controllability  is preserved. 2.  Via an observer form  realization.  We consider H  (s)  =  [2/(s +1)  l/s]  and derive a realization in controller form.  In particular  ^i  =  s  +  I £2 =  s H^(s)  = 416 Linear  Systems 0 s 1  0 0  1 [21] s+l 0 5 + 1 0^ s 1  0 0  0 0 [21] 1  0 0  1 1  0" 0  0 namely Ao  = tion of H(s)  is 1  0 0  1 -1 di  =  d2  =  I A(s)  = s  0 0 s 1  0 0  0 s  0 0 s 1  0 0  1  and S(s)  = 1  0 0  1 . Then D(s)  = =  DhA{s) + DiS(s)mdB^ = 1  0 0  1 Also Cc  =  [2 1] from  which we obtain A^(^)  =  [2 1]  = =  CcS(s).  Therefore  a minimal realization {A B C} of H^(s)  is  [2 1] [.  The dual of this is a minimal realization  of  H(s) 0 0  0 Bo  and Co 1  0 0  1 . Therefore  a minimal  realiza-r -1 0 0  0 B  = 2  0 1  0 C  = 1  0 0  1 D  = 0  1 0  0 EXAMPLE  4.5.  Let H(s)  = 1 ^2 +  1 1 ^ 2 +1 0 I ^ s+  1 \l \s s -l 0 s+  1 0 S 5^ +  1 N(s)D'\s). We  wish  to  derive  a  minimal  realization  based  on  the  given  factorization  N(s)  D{s). Since  the McMillan  degree  of H(s)  is  3 (show  this) any realization  of  order  3 will  be minimal. Note that in the present  case di  =  I  and  J2  =  2 the degrees  of the  first  and second  columns  of D(s)  respectively.  Then  A(^) D(s)  = 1  0 1  1 0 0 + 1  0  0 0  1  0 1  0  01 0  1  ^J 0 s' 0 S(s) 1  0  0 0  1 s   and =  DhA(s)  +  D^S(s).  Here  \Dh\ #  0 and therefore  the  algorithm  above  still  applies. Note that Dh is not in upper  triangular form with ones on the diagonal and therefore  the resulting controllable realization  will not  be  in  controller  form;  A^  however  will  be  in  companion  form.  Let  B^  =  Dj^^  = 1  0 1 -1  A -D-'D -1 1 0  0" 0. -1 and = N(s)  =  CcS(s)  +  DcD{s)  = 1 . -1 -1 0 11 OJ ri  0" 0  1 LO  S_ +  "0  01 .1  OJ s+l s 0 s^ + I A minimal realization is now given by Ac  = Cc 0 0 0 -1   Be -1 0" 0 -1 0 1 -1  r 0  0 Dc  = 0  0 1  0 Verify  this. 417 CHAPTER  5: Realization Theory  and Algorithms C.  Realizations  with Matrix A  Diagonal When  the  roots  of  the  minimal  polynomial  mnis)  of  H(s)  are  distinct  there  is  a realization  algorithm  due to Gilbert  [2] that provides  a minimal realization  of  H(s) with A  diagonal. Let mnis)  =  /  +  dr-is'' '-{-'"+dis -^ do (4.37) be the (monic) least common denominator of all nonzero entries of the p  X m matrix H(s)  which  in view  of Section  3.5 is the minimal polynomial  of H(s).  We  assume that its r roots A/ are distinct and we write i = i (4.38) Note  that  the  pole  polynomial  of  H(s\  PH(S)  will  have  repeated  roots  (poles)  if PH(S)  ¥=  mnis)  (see  Section  3.5  and  Example  4.6).  We  now  consider  the  strictly proper  matrix  H(s)  = H(s)  -  lims-^oo H(s)  =  H(s)  -  D  and  expand  it  into  partial fractions  to obtain H(s) 1 rriHis) N(s)  ±7^^.''-! = 1 The pX  m residue matrices R  can be found  from  the relation Ri  =  lim(s  -  \i)H(s). We write Ri  =  CiBi /  - l  . . .  r (4.39) (4.40) (4.41) where  C/  is  a /? X pi  and  Bi  is  a  p/  X m matrix  with  p/  =  rank  Ri  <  min  (p m). Note that the above expression is always possible. Indeed there is a systematic pro cedure  of  generating  it namely  by  obtaining  an LU  decomposition  of  Ri  (refer  to the Appendix). Then All Upi A2/; P2 ) B = ^rlpr. Bi B2 Br C  =  [CiC2...C] D = lim/ i(s) (4.42) is a minimal realization  of order n  =  XL I  Pi LEMMA  4.5.  Representation  (4.42) is a minimal realization  of  H(s). Proof  It can be  verified  directly  that  C(sl  -  A)~^B  + D  =  H(s)  i.e. that  (4.42)  is  a realization of H(s).To  verify  controllability  we write =  [B^AB 'B]  = B2 X\Im Aj Im Br Xrlm> '•• 418 Linear Systems The second matrix in the product is a block Vandermonde matrix of dimensions mr X mn. It can be shown that this matrix has full  rank mr since all A/ are assumed to be distinct. Also note that the (n  = %pi)  X mr matrix block diag [Bi\ has rank equal to 2/^=1 rank Bt  =  2[=i  P/  =  n ^  mr. Now in view of Sylvester's Rank Inequality as applied to the above matrix product we have n + mr -  mr ^  rank ^  <  min (n mr) from which rank % = n. Therefore {A B C D} is controllable. Observability is shown in a similar way. Therefore representation (4.42) is minimal. • EXAMPLE  4.6.  Let H(s)  = Is+l s(s+l)i Herem/zW  =  ^(5+1) with roots Ai  =  0 A2 =  -1 distinct. We write//(5)  =  (l/s)Ri + [l/(s  +  l)]/?2 where Ri  = lims^o sH(s)  = lim^^o 1 2s s+  1 0 1 s+l\ n  01 0  1  .Ri — \ims^-i(s-\-l)H(s)  = lim^ L 5 - » -l 0 2 0 -1  pi  =  rank Ri  =  2 and p2  = rank R2 =  I i.e. the order of a minimal realization is n  =  pi  + p2  =  3. We now write Ri  = Ri 1  0 0  1 0 2 -1  0 0  1 1  0 0  1  =  CB [2 -  1] =  C2B2. Then A = A1/2 0 01 A2J C [Ci C2]  = _ ro 0 [0 ri  0 [0  1 0 0 0 01 0 - ij 01 1 B  = «1 kJ = n 0 .2 01 1 - 1. is a minimal realization with A diagonal (show this). Note that the characteristic polyno mial of H{s) is PH{S)  = s'^(s +1) and therefore the McMillan degree which is equal to the order of any minimal realization is 3 as expected. • D.  Realizations  with Matrix A in Block  Companion  Form The realizations  derived using the algorithms  described below  are in general  either controllable or observable and of order mr or pr  where r is the degree of the minimal polynomial  of  ^(.s*). Most  often  it is  also  necessary  to use  the  methods  of  Subsec tion 3.4A  to reduce these realizations  to the standard forms  for unobservable or un controllable  systems and in this way derive minimal realizations. 419 CHAPTER 5: Realization Theory  and Algorithms Using the numerator polynomial  matrix Consider a proper pxm  matrix H{s)  and let mnis) -dr-lS  r-l -dis^do be its minimal  polynomial  as in (4.37).  The  polynomial  mnis)  is the monic  least common denominator of all entries of  H{s). Let Nh{s) =  mH{s)H{s)  be a polynomial matrix and write N}y{s) = CbcSb{s)  ^Dbcmnis) (4.43) where  St{s) =  [Im^slm^ ...^s^~^Im].  Note  that  Dj^c  =  lim^^ooH(^)  and  therefore C}jc  is the only  unknown  in (4.43).  A solution  Q^ always  exists  since the highest possible  degree  in  N}y{s)  — Df^c^nis)  is  r —  1  because  H{s) is  proper.  Expres sion  (4.43)  is  analogous  to  (4.24).  In addition  we can also  write  mnis)  = s^ — -dQ...-dr-i][\s r-UT . . y  from  which we have mH{s)Im = S^im —  [—dolm^  • • •   —dr-lIm\Sb{s)^ (4.44) which corresponds to D{s) in (4.22). A controllable realization in block controllable companion form is given by x --AicX + BicU^y = ChcS -\-DfycU with Q^  and Df^c  specified  in (4.43) and o„ A be 0„ 0. B be  • (4.45) -dolm -d\Im -dr-]Ir l^m Note  that if the  strictly  proper  matrix H{s)  =  H{s) — lim^^ooH(^)  =  H{s) — D^c is used  then Nt{s)  ^  mH{s)H{s)  = Rr-is'-^  +  • • • +/^o where /^/ / =  0  . . . r — 1 are real pxm  coefficient  matrices. It is now  not  difficult to see that in this  case Q  =  [/^o/^i.../^r-i]. (4.46) The realization  {A/^c? ^/?C5 Qc? ^/?c} in (4.45) is of order mr and is a direct  generaliza tion of the  SISO  realization  (4.9) to the MIMO case. In general it is only  controllable but  not observable.  It is  also  observable  when  m =  1 and therefore  it is  minimal for  m =  1.  This  is  true  because  of  Theorem  3.11 and the  fact  that  in  this  case r  =  deg  mH{s)  =  deg  PH{S) the  McMillan  degree  of  H{s). L E M M A 4.6.  The representation  (4.45) is a controllable realization of H(s). Proof  The proof is similar to the proof of Lemma 4.1. First to show that (4.45) is a real ization  we consider the  rmxm  matrix X{s) = [X[ (s).  ..Xj^{s)]'^  = {si — Aic)~^Bic. Then sX —A^cX  = B^c and sXi = Xi^i/  =  1...r —  1 or X^+i =  s^Xii=  1...r — 1. Also  sXf — [—dQlm---—dr-\Im\X  = Im  which  implies  in  view  of  X^+i  = s^Xi that  mH{s)X\  = I^.  Thus  Xi =  {mH{s))~^s^~^Imi  =  1... r  and in view  of X{s) = {sI-Abc)-^Bbc {sI-Abc)Sb{s)=BbcmH{s) (4.47) 420 Linear  Systems which is of course analogous to (4.11). (Refer also to the algorithm for the controller  form realization  in the MIMO  case  and the  Structure  Theorem  in  Subsection  3.4D  where  a similar  relation  is  valid.)  Note  that  \sl  —  A^J  =  (mnis))^ (show  this).  To  show  that (Ab^ Bbc) is controllable we observe that [Bbc AbcBbc He  Bbc .Air'Bbc] = Om  0 X X (4.48) which has full  rank mr  since the first mr  columns are linearly  independent. We  may  also  easily  obtain  an  observable  realization  of  H{s)  using  duality.  In particular On 0 0„ -dolp -d\Ip Abo  = LOP In -dr-llpj Bbo Ri R r -l (4.49) Cbo  =  [0/7. Dbo  =  lim  H(s) is  an  observable  realization  of  order  pr  (use  duality  arguments  to prove  this). In  both  of  the  above  cases  of  controllable  realization  {Abo  Bbc  Cbo  Dbc}  or  ob servable  realization  {Abo Bbo  Cbo Dbo}  the  methods  of  Subsection  3.4A  may  be used  to  obtain  minimal  realizations  [see  (4.3b)]. EXAMPLE  4.7.  Let H(s) . 5 +1 5(5-+  1). as in Example 4.6. We wish to determine an observable realization with A in block com panion form. To this end duality will be used and the procedure described by (4.3a) will be  followed. In  this  case  we  have  mnis)  =  s^  + s  =  s^  + dis  + do from  which  we  conclude that r  =  2  Ji  =  1 and Jo  =  0. Note that H(s)  is stricdy proper. Let H(s)  =  H^(s)  = 1 s(s  +  1) s +  1  2s 1 0 =  Nb(s)(mH(s))  ^ and  write Nb(s)  = 1  2 0  0  s + 1  0 0  1 =  Ris  + /?o. (4.50) Then a controllable realization  of H(s)  is given by Ahr  = O2 -doh h -dih. 0 0 0 0 0 0 0 0 1 0 0 1 -1 0 0 -1 Bbc  — [Ol [h\ "0 0 1 0 0" 0 0 1 Cbc  — [^0>  ^ l] 1 0 : 12 0 1 : 00 and an observable realization of H(s) is given by 421 CHAPTERS: Realization Theory and Algorithms Abo  —  ^Z?c 0 0 1 0 0 0 0 1 : : : : 0 0 0 0 -1 0 0 -1 Bbo  — Cz?c " 1 0 1 _2 0" 1 0 0 Cbo  -  Bi  — ^bc 0  0 0  0 1  0 0  1 Note  that  the system  {Abo^ Bboy Cbo) is not controllable.  We ask the reader  to use  the procedure  shown  in (4.3b)  to obtain  a minimal  realization.  The order  of any minimal realization is 3 (why?). • Using the Hankel  Matrix We consider  a proper p  X  m transfer  function  matrix H(s)  and let muis)  =  s^ + dr-is^ r -l +  • • • +  dis  + do be its minimal polynomial  as in (4.37). We write H{s)  =  HQ+  His-^  +  H2S~^ +  • • • (4.51) where the Hi  are the Markov parameters  of the system. Let Op Ip .. • 0 A  = Op -dolp 0 -dilp .. .. • • ip -~dr-llp_ C  =  [lp{)p...Opl D  =  Ho. B  = Hi H2 Hr (4.52) LEMMA  4.7.  The representation  (4.52) is an observable realization of H(s). Proof  To show  that  (4.52) is a realization  we note that {A B C D} is a realization of H{s)  if  and only  if  D  =  HQ and CA^~^B  =  /// /  =  1 2  . ..  (prove  this  referring  to Exercise 2.63). Here C CA ICA'-^ (4.53) and therefore CA^'^B  = Hi i  =  1...  r. To show that this is also true for /  =  r + 1  . . . a relationship between Hi and di is required. In particular we let H{s)  = H{s) -  HQ and we write 422 Linear  Systems mH(s)H(s)  =  (s'  +  dr-is'  +  '"  +dis  + doXHis  ^ +H2S  ^ +  • • •) Equating coefficients  of equal powers of s we obtain '+Ro .r-2 . Hi H2 Rr-ly Rr-2  ~  dr-\H\ (4.54) (4.55) s '.  Hr  =  RQ ~  dr-iHf-i  — • • • —  d\Hiy and Hr+i  =  —dr-\Hr+i-\  —  •••  —  d{)Hi i  =  1 2  Using the above relations it can be shown that CA^~^B  =  Hi fori  =  1 2 Now in view of (4.53) the observability matrix has rank pr  which is equal to the order of the system. Therefore  the system is observable. • We  can  now  use  duality  arguments  to  show  that 'Om ^m •• .  Om .  On -dolm -dllm A  =  B  = (4.56) % .. • c  =  mH2 Im —dr-\Im_ ....Hrl D = -H^ is a controllable  realization. EXAMPLE  4.8.  Let H(s)  = -  1 5' 2 0 1 L^ +  l ^(5 +  1) as  in  Example  4.7.  We  wish  to  determine  an  observable  realization.  Here  muis)  = s^ + s  =  5'^ + (ii5 + (io.fromwhichr  =  2 Ji  =  landdo -  0. Let//(5)  =  /fo+^i^~^  + H^s'^  +  • • •. The Markov parameters  can be found  directly  from  //Q  =  lim^y^oo H{s)  = 0  Hi  =  lims-.^s(H(s) -Ho) ^^  H2  =  lims-..o s\H(s) ^ 2  0 -  (Ho  +  His-'))  = and  so forth  or from  (4.55)  since  /?o • • • > ^r-i  are  already  known  from  Ex 0  0 -2  1 ample  4.7  (after  the  transpose  is  taken).  We  have  Hi  =  Ri  = diHi = 1  0 0  1 n  0 [2  0. 0  0 -2  1   and  H2+i -diH2+i [2  0 -  doHi  = H2  —  RQ — —Hi+i  i  = 12...; that is ro [2 0 - 1. =  -H2  =  H3  =  -H4  =  Hs  =  •••. Therefore  an observable reahzation  (4.52) is given by A  = O2 -doh h -dih. 0 0 0 0 0 0 0 0 1 0 -1 0 0 -1 B  = \Hi] [H2\ 1 2 0 -2 0" 0 0 1 C  =  [/202]  =  1  0 0  1 0  0 0  0 D  = ro  01 0  0 This realization is not controllable since a minimal realization is of order 3. We ask the reader to use Theorem 3.13 to show this. • 423 CHAPTERS: Realization Theory and Algorithms E.  Realizations  Using  Singular-Value  Decomposition Internally balanced  realizations Given  a proper  p  X  m  matrix  H(s)  we  let  r denote  the  degree  of  its  minimal polynomial  mnis)  we write H(s)  = Ho^His-^  +//: 2^-^  +  ... to obtain the Markov parameters Hi  and we  define T  ^  Mnir  r) \Hi ... Hr Hr H 2r-\ f'  A Ht H r+l H r +1 H2r (4.57) where Mnir  r) is the Hankel matrix  (see Definition  3.4)  and  T f  are real  matrices of dimension  rp  X  rm. Using singular-value  decomposition  (see the Appendix) we write T  =  K  s  0 0  0 (4.58) where  X  =  diag  [Ai... A„]  £  R"^"  with  n  =  rank  T  =  rank  Muir  r)  which in view  of Theorem  3.13 is the order of a minimal reahzation  of H(s).  The A with Ai  s  A2 ^  • • • >  A„ >  0 are the singular values of T i.e. the nonzero  eigenvalues of T^T.  Furthermore KK^  =  K^K  =  Ipr and LL^  =  L^L  =  Inr- We write T  =  KXU =[KX"^[X"^LA=VU (4.59) where Ki  denotes the first n columns of K L\  denotes the first n rows of L  K\K\  = In and LiL[  =  /„. Also V  G  R'P''''  and  U  E  R^''''^. We  let  V^  and  Lf^  denote  pseudoinverses  of  V  and  U respectively  (see  Ap pendix) i.e. V  =  ^-"^Kl and ^^+  - U^  =  L [ S-^^^-"^ (4.60) where  V^V  =  In and  UU^  =  In. Now  define A  =  V^fu^ B  = UIL ^ppr  V D  =  Ho. (4.61) where 4^  =  Uh ^e-kl  k  <  ^ i.e. Ik^^ is a fc X € matrix  with its first k columns  de termining  an identity  matrix  and the remaining  (-  k columns being  equal to zero. Thus B is defined  as the first m columns of  C/ and C is defined  as the first/? rows of V. Note that A  G 7?"X^ B  G Z^^^^'" C  G T^^^" and D  G  RP'''^ LEMMA 4.8.  The representation (4.61) is a minimal reahzation of H(s). 424 Linear Systems Proof It can be shown that CA^'^B  = Hi i  =  1 2...  (see also the proof of Lemma 4.7). Thus {A B C D} is a reahzation. We note that V and U are the observabiHty and controllabihty  matrices respectively  and that both  are of full  rank  n. Therefore  the reahzation is minimal. Furthermore we notice that V'^V =  UU^  = 2.  Realizations of this type are called internally balanced realizations. • The term  internally  balanced  emphasizes  the fact  that realizations  of this  type are  "as  much  controllable  as  they  are  observable"  since  their  controllability  and observability  Gramians are equal and diagonal (see Exercise 5.20). Using such rep resentations it is possible to construct reasonable reduced-order  models of  systems by deleting that part of the state space that is "least controllable" and therefore  "least observable" in accordance with some criterion. In fact the realization procedure de scribed can be used to obtain a reduced-order  model for a given system.  Specifically if the system is to be approximated  by a ^-dimensional  model with  q  <  n then  the reduced-order model can be obtained  from T  =  Kqdiag[Xi...\q\Lq (4.62) where Kq denotes the first q columns of K in (4.58) and Lq denotes the first q rows ofL. 5.5 SUMMARY The theory of state-space realizations of input-output descriptions given by  impulse responses and the time-invariant case by transfer function descriptions were studied in this  chapter. In  Section  5.2  the  problem  of  state-space  realizations  of  input-output  descrip tions  was  defined  and  the  existence  of  such  realizations  was  addressed.  In  Sub section  5.3A  time-varying  and  time-invariant  continuous-time  and  discrete-time systems  were  considered.  Subsequently  the  focus  was  on  time-invariant  systems and  transfer  function  matrix  descriptions  H{s).  The  minimality  of  realizations  of H{s)  was  studied  in  Subsection  5.3C  culminating  in two results Theorem  3.9  and Theorem  3.10  where  it was first shown  that  a realization  is minimal  if  and  only if it is controllable  and observable and next that if a realization  is minimal all other minimal realizations of a given H{s)  can be found  via similarity transformations.  In Subsection  5.3C  it  was  shown  how  to determine  the  order  of minimal  realizations directly  from  H{s).  Several  realization  algorithms  were  presented  in  Section  5.4 and the role of duality was emphasized  in Section  5.4A. 5.6 NOTES A  clear  understanding  of  the  relationship  between  external  and  internal  descrip tions  of  systems  is one of  the principal  contributions  of  systems  theory.  This  topic was  developed  in  the  early  sixties  with  original  contributions  by  Gilbert  [2]  and Kalman [4]. The  role  of  controllability  and  observability  in  minimal  realizations  is  due  to Kalman  [4]. See also Kalman Falb and Arbib [5]. The  first  realization  method  for  MIMO  systems  is  attributed  to  Gilbert  [2]. It was  developed  for  systems  where  the  matrix A  can  be  taken  to  be  diagonal.  This method  is presented  in this  chapter.  For extensive  historical  comments  concerning this topic see Kailath  [3]. Additional information  concerning realizations for the time-varying case can be found  for  example  in  Brockett  [1] Silverman  [9] Kamen  [6] Rugh  [8] and  the literature cited in these  references. Balanced realizations were introduced in Moore [7]. 425 CHAPTER 5: Realization Theory and Algorithms 5.7 REFERENCES 1.  R. W. Brockett Finite Dimensional Linear Systems Wiley New York 1970. 2.  E. Gilbert "Controllability  and Observability in Multivariable Control Systems" SIAM J. Control  Vol. 1 pp. 128-151 1963. 3.  T  Kailath Linear Systems  Prentice Hall Englewood Cliffs NJ 1980. 4.  R. E. Kalman "Mathematical Description of Linear Systems" SIAM J. Control  Vol. 1 5.  R.  E.  Kalman  R  L.  Falb  and  M.  A.  Arbib  Topics in Mathematical System  Theory pp. 152-192 1963. McGraw-Hill New York 1969. 6.  E. W. Kamen"New Results in Realization Theory for Linear Time-Varying Analytic Sys tems" IEEE Trans  on Automatic Control  Vol. AC-24 pp. 866-877 1979. 7.  B.C. Moore"Principal Component Analysis in Linear Systems: Controllability Observ ability and Model Reduction" IEEE Trans on Automatic Control Vol. AC-26 pp. 17-32 1981. 8.  W. J. Rugh Linear System Theory  Second Edition Prentice-Hall England  Cliffs  NJ 1996. 9.  L. M. Silverman "Realization of Linear Dynamical Systems" IEEE Trans on Automatic Control  Vol. AC-16 pp. 554-567 1971. 5.8 EXERCISES 5.1.  Given the transfer function matrix H{s) s-  1 s 0 5+ 1 s-2 s + 2 0 determine the McMillan degree of H{s) and find a minimal realization for H{s). Verify your results. 5.2.  Consider the transfer function matrix H{s) \s-  1 s+\ 1 1 s^  -\ 0 (a)  Determine the pole polynomial  and the McMillan degree of II{s)  using both the Smith-McMillan form and the Hankel matrix. 426 Linear  Systems (b)  Determine an observable realization  of  H(s). (c)  Determine  a minimal realization  of  H(s). Hint:  Obtain realizations  for 1 1 s +  1  s^ 1 5.3.  Consider the transfer function  matrix  H(s) for H(s)  a minimal realization in controller  form. (s +  1)(-^  +  5) (s -  l)(s^  -9)' s s-1 and determine 5.4.  Consider the transfer  function  matrix His)  =  I -3s^ -6s-2 3 ^-  1 1 (^ + 1)^ s (S +  1)3 (s -  2)(s  +  1)3 (s -  2)(s  +  1)2 s (s-2)(s+ s 1)3 ( ^ - 2 ) ( ^+  1)2 J (a)  Determine the pole polynomial  of H(s)  and the McMillan  degree of  H(s). (b)  Determine a minimal realization  of H(s)  in observer  form. 5.5.  Consider the scalar proper rational transfer function//(>s')[})(5')  =  //(5')M(^)] and assume -. Note that this can be done when  the that H(s)  can be written  as H(s)  =  2 Li  -s  -  Ai poles A/ /  =  1...  w are distinct  (see Subsection  5.4C  on realizations  with A  in diag onal  form). (a)  Show that H(s)  can be realized as the sum or the parallel combination of realizations of terms  in the form  —^-—; i.e. H(s)  is realized  by  the  system represented  in  the block  diagram  of Fig.  5.4  where  r/  =  QZ?/ /  =  1...  n  with  cj  G  /?" bi  E  /?" with q-  Xi  and q = didt  denotes the integrator circuit shown in Fig. 5.5. Note that this realization of H{s)  has advantages with respect to sensitivity to parameter  vari ations. "' ^ 1 (7-X1 1 q-K <=1 ^n FIGURE  5.4 Block diagram of the system in Example  5.5a ")"" ) c Vr ^T 1 I ^i FIGURE  5.5 Block diagram of an integrator (b)  Show that when poles are repeated as e.g. in the transfer  function H(s)  = -"TT + + ;r  then  a  parallel  realization  is  as  shown  in  the (5+1)2 ^ +1 5 +  2 block diagram given in Fig. 5.6. ^ 427 CHAPTER  5: Reahzation Theory  and Algorithms 1 qr+2 u 1 q+^ 1 (7+1 W \ J FIGURE  5.6 Block diagram of the system in Example  5.5b (c)  When  there  are complex  conjugate  poles  as e.g.  in the  transfer  function  H(s) as  + b J then H(s)  can be written as  H(s) (S +  C)2 +  ^2^—-^-^-- and can be realized  as indicated in the block diagram given in Fig. 5.7. --^-^ ^ al{s +  c) (b  -  ac)/(s  +  cf I  +  (j2/(^  +  c)2)  •' 1 +  (dVis  +  C)2) a " k 1 q  + c 1 g  + c ^ b-  ac  1 ^ i d^ 1 • c^ k/ FIGURE  5.7 Block diagrams of the system in Exercise  5.5c Note that in addition to sum or parallel  realizations  discussed in (a) to (c) there 5 +1 (5 +  2)(5 +  3) are also product  or cascade  realizations  where for example  H(s) is realized  as 5+  1 5 +  2  5 +  3 1 1 5+  1 5 +  2  5 +  3' 5.6.  Consider the transfer  function H(s)  = 5 +  3 5+  1 5 •5 +  3 5 + 1-(a)  Determine the pole polynomial of H(s)  and the McMillan  degree of  H(s). (b)  Determine a minimal realization {A B C D] of H(s)  where A is a diagonal matrix. 5.7.  Consider  a system described  by equations  of the  form 0 Xi AIJL^IJ [0 Bi\ y  =  [Ci Ci]  where Ai is the Jordan block associated with the eigenvalue A and Ai 428 Linear  Systems is the complex conjugate of Ai associated with A. Show that the similarity  transformation matrix P given by P  =  . . can be used to reduce the representation given above to one that involves only real system parameters namely Xi X2\ ReAi -ImAi Im  A\ ReAi 2 Re  Bi -llmBi y  =  [ReCi ImCi\ Note that this is a way of obtaining representations involving only real coefficients  from realizations  with A  in diagonal or in Jordan form  that may contain complex numbers. 5.8.  Determine an observable reaHzation oiH{s)  given in Exercise 5.1 where the matrix A is in block companion form by using (a)  the numerator polynomial  matrix (b)  the Hankel  matrix. 5.9.  Show that if the system {A B C D} reaHzes H{s)  =  -^—;r and  \sl  -  A\  =  s^ + 2 ^ + 1  then  (A B)  must  be  controllable  and  (A C)  must  be  observable.  Hint:  Use Theorem 3.11. s  + 2 s^  + 2s  -\-  \ 5.10.  Show  that  if  the  system  {A B C D]  realizes  the  transfer  function  matrix  H{s)  and 1^/ -  A|  =  mnis)  the  monic  least  common  denominator  of  the  entries  of  H(s)  then {A B C D} is a minimal realization of H(s).  Can one always find a realization of  ^(^) that satisfies  this property? Hint:  Use Theorem 3.11. 5.11.  Consider  a  scalar  proper  rational  transfer  function  H(s)  =  n(s)/d(s)  and  let  x  = AcXc +  BcU y  =  CcXc +  DcU be a realization  of H(s)  in controller form  (see Subsec tion  5.4B). (a)  Show that the realization {A^ B^  Cc Dc} is always  controllable. (b)  Show  that  {Ac Be Cc Dc] is  observable  if  and  only  if  n{s)  and  d{s)  do not  have any factors  in common i.e. they  are prime polynomials. (c)  State the dual results to (a) and (b) involving  a realization  in observer  form. 5.12.  Consider  3. p  X  m proper rational transfer  function  matrix H(s)  and the algorithm that leads  to  a  reaHzation  {Ac Be Cc Dc} of  H(s)  =  N(s)D(s)~^  in  controller  form  [see (4.22) and (4.24)]. (a)  Show that (Ac Cc) is observable if and only if rank  ' N(\)\ ' =  m for any A complex scalar. Hint:  Use the eigenvalue/eigenvector  tests for  observability.  Note that  this rank condition is a necessary  and sufficient  condition for N(s)  and D(s)  to be right coprime (see Theorem 2.4 in Subsection  7.2D). (b)  State  the  dual  result  to  (a)  that  involves  a  realization  in  observer  form  {Ao Bo Co Do) of His)  =  D-\s)N(s). ^  -i^  X 5.13.  Let H(s)  =  ^K  =  -^ rr^  X ^(S) d(s) z T ^5 -  ^4 +  s^ S'^ -  S  +  1 +  s-  1 form. Is your realization minimal? Explain your answer. Hint:  Use the results of Exer cise 5.11. .  Determine  a realization  in  controller 5.14.  For the transfer  function  H(s)  = s+  1 ^2 +  2' find (a)  an uncontrollable  realization (b)  an unobservable  realization (c)  an uncontrollable  and unobservable  realization (d)  a minimal  realization. 429 CHAPTERS: Realization Theory  and Algorithms 5.15.  Find minimal discrete-time  state-space realizations  of the transfer  function  matrix -z  +  2 H(z)  =  z +1 z - z +1 1 z +  3 z +1 z +  2-using all the realization  methods described  in this  chapter. 5.16.  Given  is  the  system  depicted  in  the  block  diagram  of  Fig.  5.8  where  H(s)  = s^  +  1 —  —  —. Determine  a minimal  state-space representation  for the  closed-{s +  1)(^ +  2){s +  3) loop system using two approaches. In  particular: F F u His) y ^ :^ L FIGURE  5.8 Block diagram of the system in Exercise  5.16 (a)  First determine  a state-space realization  for H(s)  and then determine  a minimal state-space representation  for the closed-loop  system. (b)  First find the closed-loop transfer  function  and then determine  a minimal  state-space representation  for the closed-loop  system. Compare the two  approaches. 5.17.  Consider  the  system  depicted  in  the  block  diagram  of  Fig.  5.9  where  H(s)  = s  +  I —  —  and  G(s)  = ^(^ +  3) system to be controlled  and G{s) could be regarded  as a feedback  controller. with  ka  EL R.  Presently  H{s)  could  be  viewed  as  the s  + a k r^ ". V -J t H{s) G(s) y FIGURE  5.9 Block diagram of the  system in Exercise  5.17 (a)  Obtain a state-space representation  of the closed-loop system by (i)  first  determining realizations  for H{s)  and G{s) and then combining  them; (ii)  first  determining Hds)  the closed-loop transfer  function. (b)  Are there any choices for the parameters k and a for which your closed-loop state-space  representation  is  uncontrollable  and  unobservable?  If  your  answer  is  yes specify. 5.18.  For H{s)  as in Exercises 5.1 5.2 and 5.3 determine internally balanced minimal real izations using  singular-value  decomposition. 430 Linear  Systems 5.19.  Let H{s) 1 ^2 +  1 5^ +  1 ^4 5^ +  1 7T3 5^ +  1 ^3 +  2 5 +1 1 5 +  2 ^3 +  1 be a transfer  function  matrix. (a)  Find a minimal realization  for H(s)  in controller  form. (b)  Find an internally balanced minimal realization for  H{s). 5.20.  Consider  the  controllable  (-from-the-origin)  and  observable  system  given  by  i  = Ax-\-Bu  y  =  Cx-\- Du  and its equivalent representation x  = Ax-\-Bu  y  = Cx-\-  Du where  A  =  PAP-\  B  =  PBC  =  CP~\  and  D  =  D.  Let  Wr  and  Wo denote  the reachability  and observability  Gramians  respectively. (a)  Show that Wr = PWrP* and Wo  =  {p-^yWoP~\  where P* denotes the  complex conjugate  transpose  of P.  Note  that P* =  P^  when  only  real  coefficients  in  the system equations are involved. Using  singular-value  decomposition  (refer  to the Appendix)  write where  WU  =IVV values of W.  Define Wr = Ur^rV; and Wo =  Uo^oV; /  and  E  =  diag  (EiE2... S„)  with  E/  the  singular H=iY}J'yu:Ur{Y}J').. and using  singular-value  decomposition  write where U^UH '-1J VHVH  = ^' Prove the  following:  l / 2 x -l (b)  IfP  = Pin =  y//(E;/^)">;thenW;=/W^: (c)  If P  =  Pout =  Ufj(E^)  V* then Wr =  ^]iWo-(d)  If P  =  Pib  =  Pin^ll^  =  ^]i^Pouu  then Wr = Wo=  S//.  Note  that the  equivalent in  (b)  (c)  and  (d)  are  called  respectively  input-representations  {ABCD} normal  output-normal  and internally  balanced  representations. = 1. 5.21.  Show that the representation  (4.61) in Section  5.4  is internally  balanced  in view of Exercise  5.20(d). 5.22.  Consider  the  transfer  function  H{s) realization for  H{s). 4^3 +  3 5 +1 and  determine  a  minimal 5.23.  Transform  the  system "  -1 0 1 2 1 0 - 2" 1 -1 x + "2" 0 1 u };=[ll0]x into Xc = AcXc -\-BcU  y  =  CcXc  an equivalent  representation  in  controller  form  uti lizing the following  two methods: (a)  Use of a similarity  transformation. (b)  Determination  of the transfer  function  H{s)  and use of a realization  algorithm. 5.24.  Consider  the  two  system  5*1  and  5*2  in  series  as  depicted  in  the  block  diagram given  in  Fig.  5.10.  Suppose  that  5*1  is  described  by  yi{s)  =  Hi{s)Ui{s)  where ^i('^)  ^ ^  +  1'  ^^^  similarly  that  5*2  is described  by 3^2('^) =  ti2(s)u2(s)  where 5 +  2 U2{s)=yi{s)  and7/2(5)  =  ——-. 431 CHAPTER  5: Realization Theory  and Algorithms ^1 1 1 V 1 l_ y  =u^ ^1 ^2 "  ~ i^ >2 * FIGURE  5.10 Block diagram of the system in Exercise  5.24 (a)  Determine controllable and observable state-space descriptions for the individual subsystems 5*1  and 5*2. (b)  Determine a state-space representation for the entire  system S using your answer in (a). (c)  Is the  state-space  description  of  S in  (b)  controllable?  Is  it  observable?  What  is the transfer  function  H{s)  of 5*? 5.25.  Consider a system described by 1 ( 5 + 1 )2 0 2 5 +1 ^1(5) U2{s) (a)  What is the order of a controllable  and observable realization of this  system? (b)  If  we  consider  such  a realization  is  the  resulting  system  controllable  from  the input W2? Is it observable from  the output yi ? Explain your  answers. 5.26.  Consider  the  system  described  by  H{s) 1 5 - (l  +  e) (y(s)  =H(s)u(s)) and  C(5) 5 -1 5 +  2 (^(5)  =  C{s)f{s))  connected in series  (e G /?). (a)  Derive  minimal  state-space  realizations  for  H{s)  and  C{s)  and  determine  a (second-order)  state-space description  for the system y{s)  =  H{s)C{s)f{s). (b)  Let e =  0 and discuss the implications regarding the overall transfer  function  and your  state-space  representations  in  (a).  Is  the  overall  system  now  controllable observable  asymptotically  stable?  Are the poles  of  the  overall transfer  function stable?  [That  is  is  the  overall  system  BIBO  stable?  (See  Chapter  6.)]  Plot  the states and the output for  some nonzero initial condition  and a unit step input  and comment on your results. (c)  In  practice  if  H{s)  is  a  given  system  to  be  controlled  and  C{s)  is  a  controller it  is  unlikely  that  e  will  be  exactly  equal  to  zero  and  therefore  the  situation  in (a)  rather  than  (b)  will  arise.  In  view  of  this  comment  on  whether  open-loop stabilization can be used in practice. Carefully  explain your reasoning. C H A P T ER  6 Stability 432 Dynamical  systems either occurring  in nature or manufactured  usually  function in  some specified  mode. The most common  such modes are operating points that frequently  turn out to be equilibria. In this chapter we will concern ourselves primarily with the qualitative behavior of equilibria. Most of the time we will be interested in the asymptotic stability of an equilibrium (operating point) which means that when the state of a given system is displaced (disturbed) from its desired operating value (equilibrium) the expecta tion is that the state will return to the equilibrium. For example in the case of an automobile under cruise control traveling at the desired constant speed of 50 mph (which determines the operating point or equilibrium condition) perturbations due to hill climbing (hill descending) will result in decreasing (increasing) speeds. In a properly designed cruise control system it is expected that the car will return to its desired operating speed of 50 mph. Another qualitative characterization of dynamical systems is the expectation that bounded system inputs will result in bounded system outputs and that small changes in inputs will result in small changes in outputs. System properties of this type are referred  to as input-output stability. Such properties are important for example in tracking  systems where the output  of the  system is expected  to follow  a desired input. Frequently it is possible to establish a connection between the input-output stability properties and the Lyapunov stability properties of an equilibrium. In the case of linear systems this connection is well understood. 6.1 INTRODUCTION In this chapter we present a brief introduction to stability theory. We are concerned primarily with linear systems and systems that are a consequence of linearizations finite-  433 CHAPTER 6 Stability of  nonlinear  systems.  As  in  the  other  chapters  of  this  book  we  consider dimensional  continuous-time  systems  and  finite-dimensional  discrete-time  systems described  by  systems  of  first-order  ordinary  differential  equations  and  systems  of first-order ordinary difference  equations respectively. We will consider both internal descriptions and external descriptions of systems. For the former  we present results for  various types  of Lyapunov  stability  of an equilibrium  (under  assumptions  of no external  inputs)  while  for  the  latter  we  develop  results  for  input-output  stability (under  assumptions  of zero initial  conditions). We also present  results  that  connect these two  stability  types. By  considering  both Lyapunov  stability  and  input-output stability we are frequently  able to conduct  a more complete  qualitative  analysis of a system than can be accomplished by applying only one type of stability  analysis. Recall that in Subsections 2.4C and 2.7E we have already encountered  stability properties  of  an equilibrium  for  time-invariant  continuous-time  and  discrete-time systems respectively. A.  Chapter  Description This  chapter  is  organized  into  three  parts. In  Part  1 (Sections  6.3  through  6.8)  we address  the  Lyapunov  stability  of  an  equilibrium  in  Part  2  (Section  6.9)  we  con sider  input-output  stability  and  in  Part  3  (Section  6.10)  we  treat  both  Lyapunov stability  and input-output  stability  of (time-invariant)  discrete-time  systems. Part 1 is preceded  by  Section  6.2  which  contains  some background  material  from  linear algebra. In Section 6.2 we provide additional background in linear algebra dealing  with bilinear  functional  and  congruence  Euclidean  vector  spaces  and  linear  transfor mations  on Euclidean  vector  spaces. This material  constitutes  a continuation  of the material presented in Subsections  I.IOA and I.IOB and Section 2.2 and the notation used in those sections will also be employed in the second  section of this  chapter. In Section 6.3 we introduce the concept of equilibrium of dynamical systems de scribed by  systems  of  first-order  ordinary  differential  equations and in Section  6.4 we give definitions  of various types of stability in the sense of Lyapunov  (including stability uniform  stability  asymptotic  stability uniform  asymptotic  stability expo nential stability and instability). In  Section  6.5  we  establish  conditions  for  the  various  Lyapunov  stability  and instability  types  enumerated  in  Section  6.4  for  linear  systems  {LH)  (L)  and  (P) Most  of  these  results  are  phrased  in  terms  of  the  properties  of  the  state  transition matrix for  such  systems. In  Section  6.6  we  state  and  prove  necessary  and  sufficient  conditions  for  the exponential  stability  of the equilibrium  of nth-order  ordinary  differential  equations with constant coefficients  and systems of linear first-order ordinary differential  equa tions  (L). These  involve  geometric  criteria  (the interlacing  theorem)  and  algebraic criteria (the Routh-Hurwitz  criterion). In  Section  6.7  we  introduce  the  Second  Method  of  Lyapunov  also  called  the Direct Method of Lyapunov to establish necessary and sufficient  conditions for var ious Lyapunov stability types of an equilibrium for linear systems (L). These results which  are phrased  in  terms  of  the  system  parameters  [coefficients  of  the  matrix  A for  system (L)] give rise to the Lyapunov matrix  equation. 434 Linear Systems In Section 6.8 we use the Direct Method of Lyapunov in deducing the asymptotic stabihty and instabiUty of an equihbrium of nonhnear autonomous systems (A) from the stabihty properties of their hnearizations. In  Section  6.9  we  estabhsh  necessary  and  sufficient  conditions  for  the  input-output  stabihty  (more  precisely  for  the  uniform  bounded  input/bounded  output stability) of continuous-time linear time-varying systems and linear time-invariant systems. These results involve the  system impulse  response  matrix. In this  section we also establish  a connection  between  the bounded input/bounded  output  stability of linear systems and the exponential  stability of an equilibrium of linear  systems. The stability results presented in Sections 6.3 through and including Section 6.9 pertain  to continuous-time  systems. In  Section  6.10  we present  analogous  stability results  for  discrete time  systems; however  in the interests  of economy  we  confine ourselves in this section to time-invariant systems. Also to give the reader a glimpse into the qualitative  theory  of dynamical  systems  described  by  nonlinear  equations we present in this section Lyapunov stability results for  finite-dimensional  dynamical systems described by systems of nonlinear first-order ordinary difference  equations. We  conclude  the  chapter  with  comments  concerning  some  of  the  existing  lit erature  dealing  with  the  present  topic.  As  in  all  the  other  chapters  problems  are provided  at the end of the chapter to further  clarify  the subject  at hand. B.  Guidelines  for the  Reader In  a first reading  the background  material  on linear  algebra  given  in  Section  6.2 can be reviewed rather quickly as needed. In a first course on linear  systems the reader needs to acquire familiarity  with the  notion  of  an  equilibrium  (Section  6.3)  and  various  stability  concepts  of  an equilibrium  (Section  6.4).  Such  a course  may  be confined  to  studying  the  stability properties of an equilibrium for time-invariant  systems  (L) (refer  to Theorem 5.6 in Section 6.5). This may be followed  by coverage of Section 6.7 where the principal Lyapunov  stability  results  for  time  invariant  systems  (L)  are  established  in  terms of  the  properties  of  the  Matrix  Lyapunov  Equation.  In  a  first  reading.  Section  6.8 should  also be covered  in its entirety  where  conditions  are established  that  enable one to deduce the stability properties of a time-invariant  nonlinear  system  (A)  from the hnearization  of  (A). The  reader  should  concentrate  on  the  input-output  stability  results  for  linear time-invariant  continuous-time  systems  given  in  Theorems  9.4  and  9.5  in  a  first course on linear  systems. Finally  in  a first reading  the  reader  may  consider  some  or  all  of  the  counter parts of the above results for the case of linear time-invariant discrete-time  systems developed in Section  6.10. 6.2 MATHEMATICAL  BACKGROUND  MATERIAL In this  section we provide  additional background  material  from  linear  algebra  and matrix  theory.  As in previous  sections where we present  such material we  assume that the reader has  some background  in these areas and therefore  our presentation will be in the form  of a summary rather than a development of the subject  at hand. This  section  consists  of  three  subsections.  In  the  first  subsection  we  address bilinear functionals  and congruence in the second subsection we present material on Euclidean vector spaces and in the third subsection we address some issues dealing with linear transformations  on Euclidean  spaces. 435 CHAPTER 6: Stability A.  Bilinear Functionals  and  Congruence We consider here the representation of bilinear functionals  on real  finite-dimensional vector spaces. Throughout this subsection  V is assumed to be an /i-dimensional vec tor space over the field of real numbers  R. We define  a bilinear functional  on  V as a mapping  f  : V  X  V -^  R having  the properties f(av^  +  jSv^ w)  =  af(v\  w)  +  )8/(v^  w) /(v yw^  +  8w^)  =  yf(v  w^)  +  8f(v  w^) (2.1) (2.2) for  all  a  I3yd  G  R  and  for  all  v^ v^ w^ w^  in  Y.  A  direct  consequence  of  this definition  is the more general  property for  all aj  ^^  G R and v^ w^  ^  V j  =  1...  r-and  k  =  1... ^. Now let {v^ . . .  v"} be a basis for the vector space  V and let fij  =  f(y\vJ) ij  =  l  . . .  n. (2.3) The matrix F  =  [ftj^  is called the matrix  of the bilinear functional  f  with  respect to The  characterization  of  bihnear  functionals  on  real  finite-dimensional  vector spaces  is given  in the following  result  which  is a direct  consequence  of the  above definitions  and  the  properties  of  bases:  l e t/  be  a bilinear  functional  on  V  and  let {v^ . . .  v"} be  a basis  for  V.  Let  F  be  the  matrix  of  the  bilinear  functional/  with respect  to the basis {v^ ...  v'^}. If x  and y  are arbitrary  vectors in V  and if ^  and 7] are their coordinate representations  with respect to the basis {v^ ...  v"} then f{x y) = eFv  = XJlM^^J' / -I  7 = 1 (2.4) Conversely if we are given  any  nX  n matrix F we can use (2.4) to define  the bilinear  functional/  whose matrix  with respect  to the given basis {v^ . . .  v"} is in turn F again. In general it therefore follows that on  finite-dimensional  vector spaces bilinear  functionals  correspond  in  a one-to-one  fashion  to matrices.  The  particular one-to-one correspondence  depends on the particular basis  chosen. A bilinear functional/  on  V is said to be symmetric  if f(x  y)  =  f(y  x)  for  all xyGV and skew  symmetric  if f(x  y)  =  — f{y  x)  for  all x y  E  V. 436 Linear Systems For symmetric and skew symmetric bilinear functional  the following results are easily proved: let {v^ . . .  v^} be a basis for  V and let F be the matrix for  a bilinear functional  with respect to {v^ . . .  v'^}. Then 1. /  is symmetric if and only if F  =  F^. 2.  f  is skew symmetric if and only if F  = 3.  For  every  bilinear  functional/  there  exists  a  unique  symmetric  bilinear  func -F^. tional /i  and a unique skew symmetric bilinear functional  fz  such that /  =  /i  +  fi. (2.5) We call /i  the symmetric  part  off  and /2 the skew  symmetric  part  of/. The above result motivates the following  definitions:  an n  X n matrix F is  said to ht  symmetricif  F  =  F^  dind skew  symmetric  if F  =  —F^. Using  definitions  the  following  result  is  easily  established:  l e t/  be  a  bilinear functional  on  V and let /  and  /2 be the  symmetric  and  skew  symmetric parts  of/ respectively.  Then /i(vw)  =  ^[/(vw)  +  /(wv)] and /2(v w)  =  \ [/(v  w)  -  f{w  v)] (2.6) (2.7) for all V w G  V. Next  we  define  the quadratic form  induced  by  a bilinear  functional/  on  V as f(y)  =  /(^^ v) for all V  E  y.  It is easily verified  that in terms of matrices we have fix)  =  fF^  = ^JlfiMp (2-8) where  ^  denotes  the  coordinate  representation  of  x  with  respect  to  the  basis {v^...v"}. For quadratic  forms  we have the following  result:  l e t/  and g be bilinear  func tional  on  V. The quadratic  forms  induced  b y/  and g are equal if and only iff  and g have  the  same  symmetric  part.  In other  words  /(v)  =  g(v)  for  all v E  V if  and only if k [/(v  w)  +  /(w V)]  =  i [g(v w)  +  g(w^ V)] (2.9) for all vw  EiV.  From this result we can conclude that when treating quadratic  func tional it suffices  to work  with  symmetric  bilinear  functionals. It is also easily verified  from  definitions  that a bilinear functional/  on a vector space V is skew  symmetric if and only if /(v v)  =  0 for  all v G  V. Next let/ be a bilinear functional  on a vector space V let {v\  . . .  v'^} be a basis for y and let Fbe the matrix off  with respect to this basis. Let {v^ . . .  v'^} be another basis whose matrix with respect to {v^ . . .  v'^} is P. It can readily be verified that the matrix F  off  with respect to the basis {v^ ...  v"} is given by F  =  P^FP. (2.10) This result  gives rise to the following  concept:  annX  n matrix  F  is  said to be congruent  to an nXn  matrix F if there exists a nonsingular matrix P such that (2.10) holds. We express  congruence  of  two  matrices  F F  by  writing  F--F.lt is  easily shown that ~  is reflexive symmetric and transitive and as such it is an equivalence relation. For practical reasons  we are interested  in determining  the simplest matrix  con gruent  to  a given  matrix  or what  amounts  to the  same thing the  most  convenient basis  to use  in  expressing  a given  bilinear  functional.  If  in particular  we  confine our interests to quadratic functional  then it suffices  as observed earlier to consider symmetric bilinear functionals. The following result called Sylvester's  Theorem ad dresses this issue. The proof of this result is rather lengthy and the interested  reader should consult one of the references  on linear algebra cited at the end of this chapter for details. Let/  be any symmetric bilinear functional  on a real n-dimensional vector  space such that the matrix  of/  with  respect V. Then there exists  a basis {v^...v^}ofV to this basis is of the  form 437 CHAPTER  6: Stability 1 \n. (2.11) -1 0 The integers  r and p  in this matrix are uniquely determined by the bilinear  form. Sylvester's  Theorem  allows  the  following  classification  of  symmetric  bilinear functionals:  the integer r in (2.11) is called the rank of the symmetric bilinear  func tional/  the  integer p  is  called  the  index  of/  and  n  is  called  the  order  off.  The integer s  =  2p-r (i.e. the number of  + I's minus the number of  -1 's) is called the signature  of/. A  bilinear  functional /  on  a  vector  space  V  is  said  to  be positive  (or  positive semidefinite)  if  /(v v)  >  0 for  all v E  V and  strictly  positive  (or positive  definite) if  /(v v)  >  0  for  all  V  7^ 0 V  G  y  [note  that  /(v v)  =  0  for  v  =  0]. It  is  readily verified  that  a  symmetric  bilinear  functional  is  strictly  positive  if  and  only  \i  p  = r  =  n in (2.11) and positive if and only if  p  =  r. Finally we say that a bilinear functional/  is negative  (or negative  semidefinite) if  -/  is positive and strictly  negative  (or negative  definite)  if  -/  is strictly positive. Also a bilinear functional/  is said to be indefinite  if in (2.11) r  >  /? >  0. B.  Euclidean  Vector  Spaces As in the preceding subsection we assume throughout this subsection that Visa real vector space. A  bilinear  functional  /  defined  on  V  is  said  to  be  an  inner product  if  (i) /  is symmetric  and  (ii) /  is  strictly  positive.  A  real  vector  space  V  on  which  an  inner product is defined  is called a real inner product  space  and a real  finite-dimensional vector space on which an inner product is defined  is called a Euclidean  space. Since we will always be concerned with a given bilinear functional  on V we will write (v w) in place of f(v  w) to denote the inner product of v and w.  Accordingly 438 the axioms of a real inner product are given as Linear Systems ^  (v v) >  0 for all v 7^ 0 and (v v)  =  0 when v  =  0. 2.  (v w)  =  (w v) for  all v w e  V. 3.  (av  +  ^w  u)  =  a(v  u)  +  ^{w  u) for all uvw  ^V  and all a  (3 ^  R. 4.  (w av  +  j8w)  =  a(M v) +  J8(M W) for all uvw  G  V and all a  (3 G  R. In the following  we enumerate several results on Euclidean spaces all of which are easily  proved: 1.  The inner product (v w)  =  0 for all v E  F  if and only if w  =  0. 2.  Let ^  G L(V V) Then (v ^w)  =  0 for  all v^wGVif and only if ^  =  0. 3.  Let ^  gS G L(K  V). If (v siw)  =  (v ^w)  for  all v w G  V then ^  =  ^. 4.  Let A G /^"x^ If ^^AT;  =  0 for  all ^ r^ G  7?^ then A  =  0. We now define  the  function ||v||  =  (vv)^/2 (2.12) for  all  V  G  y.  It  is  easily  verified  (using  definitions  and  the  properties  of  bilinear functionals)  that  this  function  satisfies  the  axioms  of  a  norm  (refer  to  Subsection I.IOB) i.e. for all v w in  V and for  all scalars a  the following  hold: L  ||v||  >  0 for all V  7^ 0 and ||v||  =  0 when v  =  0. 2.  ||a:v||  =  |«|  • ||v|| where  \a\ denotes the absolute value of the scalar  a. 3.  ||v +  w\\ < ||v|| +  ||w||. In the usual proof of 3 use is made of the Schwarz  Inequality  which states that \{yw)\  <  IMI'IMI (2.13) for all V w G  V and |(v w)|  =  ||v|| •  IHI if and only if v and w are linearly  dependent. Another  result  for  Euclidean  spaces  that is easily  proved  is the  parallelogram law which asserts that for all vw  G  V the equality ||v +  wf  +  ||v -  w|p  =  2||v|p  +  2|Hp. (2.14) Before  proceeding  we recall that a vector  space  V on which  a norm is  defined is called a normed  linear space.  It is therefore clear that Euclidean vector spaces are normed  linear  spaces  with norm defined  by  (2.12)  (refer  to Subsection  l.lOB). We also recall that any norm can be used to define  a distance function  called  a  metric on a normed vector space by  letting d{uv)  =  \\u-v\\ (2.15) for all w V  G  V (refer to Subsection  l.lOB). Using thepropertiesofnorm  it is readily verified  that for  all w v w G  V it is true that 1.  d(u  v)  =  d(v  u). 2.  d{u v) >  0 and d{u v)  =  0 if and only if  w == v. 3.  d{u v) <  d{u w) +  d{w v). Without making use of inner products or norms one can define  a distance func-  439 CHAPTER 6: Stabihty tion having  the properties  enumerated  above  on  an  arbitrary  set.  Such  a set  along with the distance function is then called a metric space. It is thus clear that Euclidean vector spaces are metric  spaces as well. The concept of inner product enables us to introduce the notion of orthogonality: two vectors vw  ^V  are said to be orthogonal  (to one another) if  (v w)  = 0. This is usually written as v ± w. With  the  aid  of  the  above  concept  we  can  immediately  establish  the  famous Pythagorean  Theorem:  for v w G  V if v _L w then ||v + w|p  =  ||v|p + ||w|p. A  vector  v E V is  said  to be a unit  vector  if ||v|| =  1. Let  w  T^ 0  and  let u  = (l/||w||)w.  Then  the  norm  of  u is  ||w|| = (l/||w||)||w||  ^  1 i.e.  w  is a unit  vector.  We call the process of generating  a unit vector from  an arbitrary  nonzero vector w nor malizing  the vector  w. Now  let  {w^...  w^} be  an  arbitrary  basis  for  V and  let F = [fij]  denote  the matrix of the inner product with respect to this basis i.e. fij = (w^ w^) for all / and j.  More specifically  F denotes the matrix of the bilinear functional/  that is used in determining  the inner product on  V with respect to the indicated basis. Let ^  and rj denote the coordinate representation of vectors x and y respectively with respect to {w^ . . .  w""}. Then we have by (2.4) (X  y)  =  ^Fv  = V^F^  =  XXfiJ^J^i' (2.16) Now  by  Sylvester's  Theorem  [see  (2.11)  and  the  discussion  following  that  theo rem]  since the inner product is symmetric  and  strictly positive there exists a basis {v^ .. .yV^} for  V such that the matrix of the inner product with respect to this basis is the nX n identity matrix /  i.e. (v\vJ)  = Sij  = '  0 1 1 .if  /  7^ J .lii  =  J. .. (2.17) This  motivates  the  following  definition:  if {v\  . . .  v"} is a basis  for  V such  that (v^ v^)  = 0 for  all / T^ J i.e. if  v^ J_ v^ for  all / T^ j then {v^ ...  v'^} is called  an orthogonal  basis.  If in addition  (v^ v^  =  1 i.e. ||v^|| = 1 for all / then {v^ . . .  v"} is said to be an orthonormal  basis for  V [thus {v^ . . .  v"} is orthonormal if and only if{v\vJ)^8ijl Using  the properties  of inner product  and  the definitions  of orthogonal  and  or thonormal bases we can easily establish  several useful  results: 1.  Let {v^ ...  v^} be an orthonormal basis for  V. Let x and y be arbitrary vectors in y  and  let the  coordinate  representation  of x  and y with  respect  to this basis  be ^^  = (^1 ...^n)  and  7]^ = (171... 7]n) respectively.  Then (xy) = ev  = V^^ =X^iVi (2.18) and ll^ll = (e^f^  =  7^f  + - - - +e (2.19) 2.  Let {v^ . . .  v^} be an orthonormal basis for  V and let x be an arbitrary vector. The coordinates of x with respect to {v^ . . .  v"} are given by the  formula ^.  =  (xv') i = l...n. (2.20) 440 Linear Systems 3.  Let {v^ . . .  v"} be an orthogonal basis for  V. Then for  all x  E  V we have /^  ^l\ /^  ^n\ ( ^ v'  +  -  +  i ^ v «. .= (2.21) 4.  {ParsevaVs identity)  Let {v^ ...  v"} be an orthogonal basis for  V. Then for  any xy  ^V  we have („)  = X ^ ^. (2.22) 5.  Suppose that x^... x^  are mutually orthogonal nonzero vectors in V i.e. x^  ± ;c^ /  ^  7. Then  x^  . . .  x^ are linearly  independent. 6.  A  set  of  k  nonzero  mutually  orthogonal  vectors  is  a basis  for  V  if  and  only  if k  =  dimV  =  n. 7.  For  V there exist not more than n mutually  orthonormal  vectors  (called  a  com plete  orthonormal  set of  vectors). 8.  {Gram-Schmidtprocess)  Let {w^...  w"} be an arbitrary basis for  V. Set 11 ^  ^  ^ 1 ^ M n il (2.23) « -i V == i i— Then {v^ ...  v^} is an orthonormal basis for  V. 9.  If v^  ...  v^ k  <  n are mutually orthogonal nonzero vectors in  V then we can find  a  set  of  vectors  v^"*"^ . . .  v^  such  that  the  set  {v\  . . .  v"} forms  a  basis forV. 10.  {BesseVs inequality)  If {w^ ...  w^} is an arbitrary  set of mutually  orthonormal vectors in  V then k ^ / =  i |(w wOP ^  IMP (2.24) for all w  E.V.  Moreover the vector k u  =  w — ^_^(w w^)w^ is orthogonal to each w\i  =  1...  k. 11.  Let  ]¥ be a linear subspace of  V and let W^  =  {v^V :(vw)  =  0 for  all w  G  W}. (2.25) (i)  Let  {w^...  w^}  span  W.  Then  v  G  W-^  if  and  only  if  v  ±  w^  for  j  = l...yt. (ii)  W^  is a linear subspace of  V. (iii)  ^  =  dimV  =  dimT^  4- dimW^. (iv)  (W^)-^  =  W. (v)  V  =  WeW^. (vi)  Let  uv  ^  V.  If  u  =  u^  -\-  u^  and  v  =  v^  +  v^  where  u^ v^  G  W  and u^ v^  G  W^  then {uv)  =  (u\v^)  + (u^v^) and ||M||  =  V||wi|P  +  \\uY. (2.26) (2.27) 441 CHAPTER 6: Stability We conclude this subsection with the following  definition: let W be a linear sub-space of V. The subspace W^  defined in (2.25) is called the orthogonal  complement ofW C.  Linear  Transformations  on Euclidean  Vector  Spaces In this  subsection  we  consider  some of the properties  of three important  classes  of linear transformations  defined  on Euclidean  spaces: orthogonal transformations  ad joint  transformations  and  self-adjoint  transformations.  Unless  otherwise  explicitly stated  V will denote an n-dimensional  Euclidean vector  space. Let  {v^...v"}  be  an  orthonormal  basis  for  V  let  v^  =  ^1=\  PjiV^i  = 1...  /2 and  let  P  denote  the  matrix  determined  by  the  real  scalars  pij.  The  fol lowing question arises: when is the set {v^ . . .  v"} also an orthonormal basis for  VI To determine the desired properties of P we consider (v^ vO  =  (X  Pki^'^ S  PuA  =  X  PkiPiM.  v^ (2.28) \k=i 1 = 1 J ki So that {v\  v^)  =  0 for  /  T^ j  and (v^ v^)  =  1 for  i  =  j  we require that n n {v\vJ)  =  X  PkiPijSki  =  ^PkiPkj =  Sij (2.29) ki =  i k=i i.e. we require that P^P  =  I (2.30) where as usual /  denotes the n X n identity  matrix. The  above discussion  is summarized  in the following  result:  let {v^ ...  v"} be an orthonormal basis for  V and let v^  =  X}= i Pji^jy  i  =  \.. .n.  Then {v^ ...  v"} is an orthonormal basis for  V if and only if P^  =  P~^.  This result in turn gives rise to the concept of orthogonal matrix. Thus a matrix  P  G R^^^  such that P^  =  P~^ i.e. such that P^P  =  P~^P  =  /  is called an orthogonal  matrix. It is not difficult  to show that if P is an orthogonal matrix then either det  P  =  \ or det  P  =  -I.  Also if P  and  Qsire nX  n orthogonal matrices then so is  PQ. The nomenclature used in the next definition  will become clear shortly. We say that a linear transformation  si  from  V into  V is an orthogonal  linear  transformation if (^v  siw)  =  (v w) for  all vw  G  V. We now enumerate several properties of orthogonal transformations. The proofs of these statements are  straightforward. 1.  Let  si  G L(V V).  Then  ^  is orthogonal  if  and only if  \\siv\\ =  \\v\\  for  all v G  V. [Note that if si  is an orthogonal linear transformation then v _L  w for all v w G V if and only if siv  1  siw  since (v w)  =  0 if and only if (^v  siw)  =  0.] 442 Linear Systems 2.  Every orthogonal linear transformation  of  V into  V is  nonsingular. 3.  Let {v^ ...  v"} be an orthonormal basis for  V. Let ^  G L(V V)  and let A be the matrix  of  ^  with  respect  to this basis. Then M^  is orthogonal  if  and  only  if A is orthogonal. 4.  Let d  e  L(V V). If si  is orthogonal then det  d  =  ± 1. 5.  Let si^  E. L(V V). If M and 2^ are orthogonal linear transformations  then 64SS is also an orthogonal linear  transformation. The next result  enables  us to introduce  adjoint  linear transformations  in  a nat ural  manner.  Let ^  G L{V V)  and  define  g  : V  X  V -^  Rby  g(v w)  =  (v ^w)  for all vw  GV.  Then g is a bilinear functional  on V Moreover if {v^ . . .  v'^} is an or thonormal basis for  V then the matrix of g with respect to this basis denoted by  G is the matrix of ^  with respect to {v^ . . .  v"}. Conversely given an arbitrary  bilinear functional  g  defined  on  V there  exists  a unique  linear  transformation  ^  G L(V  V) such that (v ^w)  =  g(v w) for  all vw  E.  V It should be noted that the correspondence between bilinear functionals  and lin ear  transformations  determined  by  the  relation  (v ^w)  =  g(v w)  for  all  v w  G  V does not depend on the particular basis chosen for  V; however it does depend on the way the inner product is chosen for  V at the outset. Now  let  ^  G  L(V V)  set  g(v w)  =  (v ^w)  and  let  h(v w)  =  g(w v)  = (w ^v)  =  (^v w).  By  the  result  given  above  there  exists  a  unique  linear  trans formation denote it by ^* such that h(v w)  =  (v Ww)  for all vw  G  V. We call the hnear transformation  ^*  G L(V V) the adjoint  of^.  We have the following  results: 1.  For  each  ^  G  L(V V)  there  is  a  unique  «*  G  L(V V)  such  that  (v^*w)  = (^v w) for  all V w G  y. 2.  Let {v\  . . .  v^} be an orthonormal basis for V and let G be the matrix of the Hnear transformation  ^  G L(V V) with respect to this basis. Let G* be the matrix of ^* with respect to {v^ . . .  v"}. Then G*  =  G^. The above results allow the following  equivalent definition  of the adjoint  linear transformation:  let  ^  G  L(V V).  The  adjoint  transformation  ^*  is  defined  by  the formula (v^*w)  =  (^vw) (2.31) for  all vw  EV. In the following we enumerate some of the elementary properties of the  adjoint of linear transformations.  The proofs  of these assertions  follow  readily  from  defini tions. Let ^  gS G L(V V)  let d*  9^* denote their respective  adjoints  and let a  be a real scalar. Then 1.  (^y  = M. 2.  (^  + my  -  64* +  m\ 3.  (asiT  =  aM\ 4.  (^SS)*  =  a*^*. 5.  ^*  =  S> where ^  denotes the identity  transformation. 6.  ©*  =  0 where 0  denotes the null  transformation. 7.  ^  is nonsingular if and only if ^4* is nonsingular. 8.  If^  is nonsingular then (6^*)-^  = (si-^. The following  results (the proofs  of which are straightforward)  characterize or- thogonal transformations  in terms of their adjoints.  Let si  E  L(V V).  Then si  is or- thogonal if and only if 6^*  =  si~^.  Furthermore si  is orthogonal if and only if si~^ is orthogonal and si~^  is orthogonal if and only if si* is orthogonal. Using adjoints we now introduce two additional important types of linear trans formations.  Let si  E  L(V V).  Then si  is said to be self-adjoint  if si*  =  si  and it is said to be skew-adjoint  if si*  =  -^.  Some of the properties of such  transformations are as  follows. 1.  Let si  E  L(V V)  let {v^ . . .  v'^} be an orthonormal basis for  V and let A  be the matrix of si  with respect to this basis. The following  are  equivalent: 443 CHAPTER 6: Stability (i)  si  is  self-adjoint (ii)  ^  is symmetric (iii)  (^v  w)  =  (v siw)  for all vw  E. V. 2.  Let si  E  L(V V) let {v\  . . .  v'^} be an orthonormal basis for  V and let A  be the matrix of si  with respect to this basis. The following  are equivalent: (i)  ^  is  skew-adjoint (ii)  si  is  skew-symmetric (iii)  (^v  w)  =  -(v siw)  for  all v w E  V. The next result follows  from  part  (iii) of the above result. Let ^  E  L(V V) let {v^ . . .  v'^} be an orthonormal basis for  V and let A be the matrix of si  with respect to this basis. The following  are equivalent: (i)  si  is  skew-symmetric (ii)  (v siv)  =  0 for all v E  V. (iii)  ^v  1  V for all v E  V. The  following  result  enables  one  to  represent  arbitrary  linear  transformations as the sum of self-adjoint  and skew-adjoint  transformations.  Let si  E  L(V V).  Then there  exist  unique  ^di ^2  ^  L(VV)  such  that  ^  =  sii  + ^2  where  ^i is  self-adjoint  and si2 is skew-adjoint.  This has the direct consequence that every real  nXn matrix can be written in one and only one way as the sum of a symmetric and skew-symmetric  matrix. The next result is applicable to real as well as complex vector spaces. (We will state it for complex  spaces.) Let y  be a complex vector space. Then the eigenvalues of a real symmetric ma trix A are all real. If all eigenvalues of A are positive (negative) then A is called po^-itive (negative)  definite. If all eigenvalues of A are nonnegative (nonpositive) then A is cailod positive  (negative)  semidefinite.  If A has positive and negative eigenvalues then A is said to be  indefinite. Next let A be the matrix of a linear transformation  d^  E  L(VV)  with respect to some basis. If A is symmetric then as indicated above all its eigenvalues are real. In this case d- is self-adjoint  and all its eigenvalues are also real; in fact the eigenvalues of si  and A are identical. Thus there exist unique real scalars Ki... \p  p  ^  n such that det  (si  -  X3)  =  det  (A  -  XI)  =  (Ai  -  A)^i(A2 "  A)'^^... (A; -  A)'^^ (2.32) We summarize the above observations  in the following.  Let si  E  L(V V).  If  si is self-adjoint  then si  has at least one eigenvalue. Furthermore all eigenvalues of  si 444 Linear Systems are real and there exist unique real numbers  Ai... A^ /? <  n such that Eq.  (2.32) holds. As  in  Eq.  (3.17)  of  Chapter  2  we  say  that  in  (2.32)  the  eigenvalues  A/ /  = 1...  p  <  n have algebraic  multiplicities  mi i  =  1... /? respectively. Next we examine some of the properties of the eigenvalues and eigenvectors of self-adjoint  linear transformations. The proofs of these assertions are  straightforward and follow  mostly from  definitions. Let si  G L(V V) be a self-adjoint  transformation  and let Xi...  Xp p  ^  n de note the distinct eigenvalues of ^4. If v^ is an eigenvector for A/ and if v^ is an eigen vector for  \j  then v'  J_ v^ for  all i  y^  j. Now  let ^  G L(V V)  and  let  A/ be  an eigenvalue  of ^.  Recall  that Xi  denotes the null space  of the linear transformation  d- -  A/^ i.e.. Xi  =  {vGV :(d-  A/J^)v  = 0}. (2.33) Recall  also  that Mi is  a linear  subspace  of  V. From  the last result  given  above  the following  result follows  immediately. Let d  G L(V V) be a self-adjoint  transformation  and let A/ and A^ be eigenval ues of 5i. If A/ 7^ Ay then Mi 1  Mj. The proof of the next result is somewhat lengthy and involved. The reader should consult the references  on linear algebra cited at the end of this chapter for details. Let si  G L(V V) be a self-adjoint  transformation  and let Ai... A^ p  <  n de note the distinct eigenvalues  of si.  Then dim V  =  n  =  dim^Ti  +  dim>r2  +  ...  +  dim^Tp. (2.34) The next two results are direct consequences  of the above result. 1.  Let ^  G L(V V).  If d  is self-adjoint  then (i)  there exists an orthonormal basis in V such that the matrix of si  with respect to this basis is diagonal; (ii)  for each eigenvalue  A/ of si  we have dim>f/  =  algebraic multiplicity  of A/. We note that in the above theorem  the matrix A  of the linear  transfor mation si  with respect to the chosen orthonormal basis in V is given by A  = A2 A2 (2.35) 2.  Let A be a real  nX  n symmetric  matrix. Then  there exists  an orthogonal  matrix P such that the matrix A  defined  by A  -  P-^AP  =  P^AP (2.36) 445 CHAPTER 6: Stability is  diagonal. For symmetric bilinear functionals  defined on Euclidean vector spaces we have the following result. Let f(v  w) be a symmetric bilinear functional  on V. Then there exists an orthonormal basis for  V such that the matrix of/  with respect to this basis is diagonal. For quadratic forms the following  useful  result is easily  proved. Let  f(x)  be  a quadratic  form  defined  on  V.  Then  there  exists  an  orthonormal basis  for  V  such that if ^^  =  (^i... ^„) is the coordinate representation  of x  with h a„^^ for some real scalars o^i...  a„. respect to this basis then f(x)  =  ai^f-\ The final result of this section which we state next is called the Spectral  Theo rem for  self-adjoint  linear transformations.  For the proof of this result the interested reader  should consult one of the references  on linear algebra cited at the end of this chapter. Before  stating this theorem  we recall that a transformation  2P G L(y V)  is a projection  on  a linear  subspace  of  V if  and  only  if  2^^  =  2?*  (refer  to  Subsection 2.2K). Also for  any projection  SP T  =  91(2^) 0  >r(S?>) where 2/l(2P) is the range of 2^ and M{^)  is the null space of 2^ (refer  to Subsection  2.2K). Furthermore we call 2^ an orthogonal  projection  if 2/1(9^) 1  >r(2^). The Spectral  Theorem for  self-adjoint  linear  transformations:  let ^  E  L{V V) be a self-adjoint  transformation  let Ai... A^ denote the distinct eigenvalues of  si and  let  Mi be  the  null  space  oi  d^ -  Xi3.  For  each  /  =  1...  p  let  ^t  denote  the projection  on Xi  along JV"-^. Then 1.  ^i  is an orthogonal projection  for each  /  =  1... /?. 2.  ?Pi?Pj  =  0  for  /  7^ J  ij  =  1...  p 3.  Xy^^i Sy" =  3  where 3"  ^  L(VV)  denotes the identity  transformation. 4.  ^  =  S ; . i A # ;. PARTI LYAPUNOV STABILITY 6.3 THE  CONCEPT  OF AN  EQUILIBRIUM In this section we concern ourselves with systems of first-order ordinary  differential equations  (£") i.e. X =  fit  x\ (E) where x  G R^. When discussing global results we shall assume that f  : R^  XR^  ^ R^  while when considering local results we may assume that f  : R^  X B(h)  ->  R^ for some h>  O.On  some occasions we may assume that t  G R rather than t  ^  R^. Unless otherwise stated we shall assume that for every (to XQ) to G R'^ the initial-value problem X =  fit  x) xito)  =  xo il) 446 Linear Systems possesses  a  unique  solution  (pit to XQ) that  exists  for  all  t  >  t^  and  that  depends continuously on the initial data (^o. -^o)- Refer to Chapter  1  for conditions that ensure that (/)  has these properties. DEFINITION 3.1.  A point Xe  E R^ is called an equilibrium point of (E) or simply an equilibrium of (E) (at time f G R^) if fit  Xe)  = 0 for all t>  t. m We note that if  Xe is an equilibrium  of (£") at f  then it is also an equilibrium  at all T ^  f. We also note that in the case of autonomous  systems and in the case of T-periodic  systems X =  fix) X =  fit  X) fit  X)  =  fit  +  T X) (A) iP) a point  Xe E  R^  is an equilibrium  at some time  f if  and  only if it is  an  equilibrium at all times.  [Refer  to Chapter  1 for  the definitions  of  symbols  in  (A) and  (P).]  We further  note that if Xe is an equilibrium at f of (£") then the transformation  s  =  t -  t yields dx r. ~  ^ and  Xe is  an  equilibrium  at 5" =  0 of this  system.  Accordingly  we  will  henceforth assume that  f  =  0 in Definition  3.1  and we will not mention  f again.  Furthermore we note that for  any to >  0 (f)it to Xe)  =  Xe for all t  >  to i.e.  the  equilibrium  Xe  is  a  unique  solution  of  iE)  with  initial  data  given  by (/)(^  to  Xe)  =  Xe. We will call an equilibrium point Xe of iE)  an isolated  equilibrium  point  if there is an r  >  0 such that Bixe  r)  C  R^ contains no equilibrium point of iE)  other than Xe itself. [Recall that Bixe  r)  =  {x  E. R^  : \\x -  Xe\\  <  r} where || • || denotes some norm defined on R^.] Unless stated otherwise we will assume throughout this chapter that a  given  equilibrium  point is  an isolated  equilibrium.  Also we will usually  assume that in a given discussion unless otherwise  stated the equilibrium  of interest is lo cated at the origin of R^. This assumption can be made without loss of generality by noting that if  Xg T^ 0 is an equilibrium  point of  (£*) i.e.  fit  Xe)  =  0 for  all t  ^  0 then by letting w  =  x  -  Xewe  obtain the transformed  system with Fit  0)  =  0 for  all t  >  0 where w  =  Fitw) Fit  w)  =  fit  w  +  Xe). (3.1) (3.2) Since  (3.2)  establishes  a  one-to-one  correspondence  between  the  solutions  of  iE) and  (3.1) we may  assume  henceforth  that the equilibrium  of interest  for  iE)  is lo cated at the origin. This equilibrium x  =  0 will be referred to as the trivial  solution of  iE). Before  concluding  this section it may be fruitful  to consider  some specific  cases. 447 EXAMPLES.1.  In Example 4.4 in Chapter  1 we considered the simple pendulum given in Fig.  1.7. Letting xi  =  x  and X2 =  i  in Eq. (4.12) of Chapter  1 we obtain the  system of equations CHAPTER  6: Stability Xi  =  X2 X2 =  -/csinxi (3.3) where ^  >  0 is a constant. Physically  the pendulum has two isolated equilibrium points: one where the mass M  is located vertically  at the bottom of the figure (i.e. at 6  o'clock) and  the  other  where  the  mass  is  located  vertically  at  the  top  of  the  figure  (i.e.  at  12 o'clock).  The  model  of  this  pendulum  however  described  by  Eq.  (3.3)  has  count-ably  infinitely  many  isolated  equilibrium  points  which  are  located  in  R^  at  the  points (7Tn0fn  =  0 ±1 ± 2  . . .. • EXAMPLE  3.2.  The linear homogeneous  system of ordinary  differential  equations X =  A(t)x (LH) has a unique equilibrium that is at the origin if A(to) is nonsingular for all to ^  0.  [Refer to Chapter  1 for the definitions  of symbols in (LH).] m EXAMPLE  3.3.  The  linear  autonomous  homogenous  system  of  ordinary  differential equations X =  Ax (L) has a unique equilibrium that is at the origin if and only if A is nonsingular. Otherwise (L) has nondenumerably  many equilibria.  [Refer to Chapter  1 for the definitions  of symbols in (L).] • EXAMPLE  3.4.  Assume that for the autonomous  system of ordinary  differential  equa tions X =  fix) {A) f  is continuously  differentiable  with respect to all of its arguments and let where dfldx  denotes the n X « Jacobian  matrix  defined  by J{Xe) (X) dx dx dXj If  f{Xe)  =  0 and J(Xe)  is nonsingular then Xe  is an isolated equilibrium of (A). EXAMPLE  3.5.  The system of ordinary  differential  equations given by xi  =  k  + sin(xi  +  X2) +  xi X2 =  k  + sin(xi  +  X2) — xi with  k>  I  has no equilibrium points at all. 6.4 Q U A L I T A T I VE  C H A R A C T E R I Z A T I O NS  OF  AN  E Q U I L I B R I UM In this section we consider several qualitative characterizations that are of  fundamen tal importance in systems theory. These characterizations  are concerned  with  various 448 Linear Systems types of stability properties of an equilibrium  and are referred  to in the literature as Lyapunov  stability. Throughout this section we consider systems of equations (£") X =  f(t  x) (E) and we assume that (E) possesses an isolated equilibrium at the origin. We thus have fit  0)  =  0 for  all t  >  0. DEFINITION 4.1.  The equilibrium ;c =  0 of (E) is said to be stable if for every e > 0 and any ^o ^  R^  there exists a 8(6 to)  > 0 such that ||(/)(^ ^0 -^0)11  <  ^ for all t > to whenever Ikoll <  5(6 to). (4.1) (4.2) In Definition  4.1 || • || denotes  any  one  of the equivalent  norms  on  R^  and  (as in Chapters  1 and 2) (/)(^ to XQ) denotes the solution of (E)  with initial condition  XQ at initial  time  to. The  notation  8(e  to) indicates  that  8  depends  on the  choice  of to and  e.  If  in particular  it is true that  8  is independent  of  ^o i-^-  S  =  8(e)  then  the equilibrium  x  =  0 of (E)  is said to be uniformly  stable. In words Definition 4.1 states that by choosing the initial points in a  sufficiently small  spherical neighborhood  when the equilibrium  x  =  0 of (E)  is stable we can force the graph of the solution for t  ^  toto  lie entirely inside a given cylinder. This is depicted in Fig. 6.1 for the case x  G  R^. We note that if the equilibrium x  =  0 of (E)  satisfies condition (4.1) for a single initial condition  ^o when (4.2) is true then it will also satisfy  this condition at every initial  time  t^  >  to where  a different  value  of  8  may  be  required.  To  see this  we note that  the  solution  (l)(t to xo)  determines  a mapping  g  of  B(8(€ to)) (at  t  =  to) onto  g(B{8(€ to)))  (at  t  ^  t'  >  to) that  contains  the  origin  by  assigning  for  every Xo E  B{8{e  to))  one  and  only  one  xo = (p(t\  to xo)  E  g(B(8(€  to)).  By  reversing time cj) determines a mapping of g(B(8(6  to)) onto B(8(e  to)) which is the inverse of ^ denoted by ^~^  Since cf) is continuous  with respect to t to and  xo then g and g~^  are  also  continuous.  Since  5(S(6 ^o)) is  a neighborhood  (an  open  set) then  so is g(B(8(e  to)) (refer  e.g. to  [16] pp. 320-321). This neighborhood  contains in its interior  a spherical  neighborhood  centered  at the origin  and with  a radius  §'.  If  we choose  X'Q  E  B(8') then (4.1) implies that \\(t)(t t'  X'Q)\\  <  e for all t  >  t^. This argu-• V" \ ^ .• FIGURE 6.1 Stability of an equilibrium ment  shows that in Definition  4.1 we  could have chosen  without  loss of generality the single value to  =  0 in (4.1) and (4.2). DEFINITION 4.2.  The equilibrium x  =  0 of (E) is said to be asymptotically stable if (i)  it is stable (ii)  for every ^  ^  0 there exists an i7(^) >  0 such that lim?_oo (l>(t to xo)  =  0 when 449 CHAPTER 6: Stability ever ||xo|| < rj. m The  set  of  all  XQ  G  J^" such  that  (pit to XQ) ^  0  as  ^ ->  oo for  some  ^o —  0  is called  the domain  of attraction  of the equilibrium  x  =  0 of (E)  (at to). Also if  for (E)  condition  (ii) is true then the equilibrium  x  =  0 is said to be attractive  (at to). DEFINITION4.3.  The equilibrium x  =  0 of (E) is said to be uniformly asymptotically stable if (i)  it is uniformly stable (ii)  there is a 5o >  0 such that for every e >  0 and for any to  G R'^ there exists a r(e)  > 0 independent of to such that \\(t)(t to xo)\\  < e for all ? >  ^ + T{e) whenever ||xo|| < 5o. • Condition  (ii) in Definition  4.3 can be paraphrased by saying that there exists a So >  0 such that lim (j){t +  to to xo)  =  0 uniformly  in (^o xo) for ^  ^  0 and for ||xoi| <  SQ. In words this condition states that by  choosing  the  initial  points  xo  in  a  sufficiently  small  spherical  neighborhood  at t  =  to WQ  can  force  the  graph  of the  solution  to lie inside  a given  cylinder  for  all t  >  to -^ Tie).  This is depicted in Fig. 6.2 for the case x  E  R^. In  linear  systems  theory  we  are  especially  interested  in  the  following  special case of uniform  asymptotic  stability. ^0 +  m) FIGURE 6.2 Attractivity of an equilibrium DEFINITION 4.4.  The equilibrium x  =  0 of (E) is exponentially stable if there exists an a  >  0 and for every e >  0 there exists a 6(e) >  0 such that U(t to xo)\\  <  e^-"^'-^o) for all t > ^o whenever \\xo\\  < 6(e) and ^  >  0. • Figure  6.3  shows the behavior  of  a solution  in the vicinity  of  an  exponentially stable equilibrium  x  =  0. 450 Linear Systems U(o K^ ^ ^ee-«(f-^o) / ^ / ^  # • o .^ r 1 1 1 y L'^"' •< ^  V \ \ \ -^ ^ ~"~""~~---^''"' \_ee-a(f-g FIGURE 6.3 An exponentially stable equilibrium DEFINITION 4.5.  The equilibrium ;t  =  0 of {E) is unstable if it is not stable. In this case there exists  a ^  >  0 an e  >  0 and a sequence  X;„ ^  0 of initial points and a sequence {r^} such that ||(/)(^o + ^m ^o. -^m)!!  —  ^ for all m ^^ —  0. • If  X =  0 is an unstable  equilibrium  of  (£") then  it  still can happen  that  all  the solutions tend to zero with increasing t. This indicates that instability and attractivity of an equilibrium are compatible concepts. We note that the equilibrium x  =  0 of {E) is necessarily unstable if every neighborhood of the origin contains initial conditions corresponding  to unbounded  solutions  (i.e. solutions  whose norm grows to  infinity on  a  sequence  tm-^  °°)- However  it  can  happen  that  a  system  (E)  with  unstable equilibrium  x  =  0 may have only bounded  solutions. The concepts that we have considered  thus far  pertain  to local properties  of  an equilibrium. In the following we consider global characterizations of an equilibrium. DEFINITION 4.6.  The equilibrium x  =  0 of {E) is asymptotically stable in the large if it is stable and if every solution of {E) tends to zero as r ->  oo. • When  the  equilibrium  x  =  0  of  (E)  is  asymptotically  stable  in  the  large  its domain of attraction is all of R^. Note that in this case x  =  0 is the only  equilibrium of(£). DEFINITION 4.7.  The equilibrium x  =  0 of iE) is uniformly asymptotically stable in the large if (i)  it is uniformly stable (ii)  for any a  >  0 and any 6 >  0 and to G R'^ there exists T(e a)  >  0 independent of to such that if ||xo|| <  a  then \\(f)(t to xo)|| <  e for all t ^  to  + T{e a). • DEFINITION 4.8.  The equilibrium x  =  0 of {E) is exponentially stable in the large if there exists a  >  0 and for any /3 >  0 there exists ^(j8) >  0 such that for all t > to U{t to xo)|| <  k(P)\\xo\\e-''^'-''^ whenever II xo 11  < (3. We conclude this section with a few  specific  cases. The scalar differential  equation • X  =  0 (4.3) has for any initial condition x(0)  =  XQ the solution <p(t 0 XQ)  =  XQ i.e. all solutions are  equilibria  of  (4.3). The  trivial  solution  is  stable;  in  fact  it  is  uniformly  stable. However it is not asymptotically  stable. 451 CHAPTER 6: Stability The scalar differential  equation X =  ax (4.4) has for every x(0)  == XQ the solution (/)(r 0 XQ)  =  xoe^^andx  =  0 is the only equi librium of (4.4). If (3  >  0 this equilibrium is unstable and when a  <  0 this equilib rium is exponentially  stable in the large. The scalar differential  equation i = {j=^} (4.5) has for every  x(to)  =  XQ  ^O ^  0 a unique solution of the  form 0(r to xo)  -  (1 +  ^o)^o I ^-^Tj j (4.6) and X =  0 is the only equilibrium of (4.5). This equilibrium is uniformly  stable and asymptotically  stable in the large but it is not uniformly  asymptotically  stable. As mentioned  earlier a system X =  fit  X) (E) can have all solutions approaching  an equilibrium  say x  =  0 without this equilib rium being  asymptotically  stable. An  example  of this type of behavior  is given  by the nonlinear  system of equations Xi ^2  = x\{X2  —  Xi)  +  x\ {x\ -h  xl)[\  + {x\ + X^)2] x]{x2  -  2xi) {x\  + xl)[\+{x\  + xl)^] For  a detailed  discussion  of  this  system  refer  to Hahn  [7] cited  at the  end  of  this chapter. Before proceeding any further  a few comments are in order concerning the rea sons for considering equilibria and their stability properties as well as other types of stability that we will encounter. To this end we consider linear time-varying  systems described by the  equations X -  A{t)x  +  B{t)u y  =  C(t)x  +  D(t)u and linear time-invariant  systems given by X =  Ax  + Bu y  =  Cx  + Du (4.7a) (4.7b) (4.8a) (4.8b) where  all symbols in (4.7)  and  (4.8) are defined  as in Eqs. (6.1) and  (6.8) of Chap ter 2 respectively. The usual qualitative analysis of such systems involves two con cepts internal  stability  and input-output  stability. 452 Linear Systems In the case  of  internal  stability  the output  equations  (4.7b)  and  (4.8b) play  no ^^^^ whatsoever  the system input u is assumed to be identically  zero and the  focus of the analysis  is concerned  with  the qualitative behavior  of the  solutions  of  linear time-varying  systems or linear time-invariant  systems X -  A{t)x X =  Ax (LH) (L) near the equilibrium x  =  0. This is accomplished by making use of the various types of  Lyapunov  stability  concepts  introduced  in  this  section.  In  other  words  internal stability of systems (4.7) and (4.8) concerns the Lyapunov stability of the equilibrium X =  0 of systems  (LH)  and  (L) respectively. In the case of input-output  stability  we view  systems  as operators  determined by  (4.7)  or  (4.8)  that  relate  outputs  y  to  inputs  u  and  the  focus  of  the  analysis  is concerned with qualitative relations between  system inputs and system outputs. We will address this type of stability in Section 9 of this  chapter. 6.5 LYAPUNOV  STABILITY  OF LINEAR  SYSTEMS In  this  section  we  first  study  the  stability  properties  of  the  equilibrium  x  =  0  of linear autonomous homogeneous  systems and linear homogeneous  systems X =  Ax t^  0 X =  A{t)x t> tQ>0 (L) {LH) where A{t)  is assumed to be continuous. Recall that x  =  0 is always an equilibrium of (L) and {LH)  and that x  =  0 is the only equilibrium of {LH)  if A{t) is nonsingular for  all t>Q.  Recall also that the solution of {LH)  for  x(^)  =  XQ is of the  form ^{t  to xo)  -  ^{t  to)xo t  >  to where  O  denotes  the  state  transition  matrix  of A{t)  and  that  the  solution  of  (L)  for x{to)  =  Xo is given by (l){t  to  Xo)  =  ^{t to)Xo  =  ^{t - to  0)X0 ^  0(r  -  to)xo  =  e^^'-'^^xo where in the preceding equation a slight abuse of notation has been  used. We first consider some of the basic properties of system  {LH). THEOREMS.1.  The equilibrium jc =  0 of {LH) is stable if and only if the solutions of {LH) are bounded i.e. if and only if sup||0(r^)||^  k{to)<^ where ||0(r ro)|| denotes the matrix norm induced by the vector norm used on /?"  and k{tQ) denotes a constant that may depend on the choice of to. Proof  Assume  that the equiUbrium  x  =  0 of  {LH)  is  stable. Then  for  any  ^o — 0  and for e  =  1 there is a 5  =  5(1 ^)  >  0 such that ||(^(r t^ XQ% <  1 for all t  >  t^ and all  XQ with  ||xo|| ^  6. In this case 453 CHAPTER  6 Stability U{t  to xo)\\  =  \\^(h  to)xo\\ = Ikoll < c^(t to){xo8) Ikoll w 8 for  all  ;co ^  0  and  all  t  ^  to. Using  the  definition  of  matrix  norm  [refer  to  (10.17)  in Chapter  1] it follows  that \\^(tJo)\\^S-\ t^to. We have  proved  that  if  the  equilibrium  x  =  0 of  {LH)  is  stable  then  the  solutions  of {LH)  are bounded. Conversely  suppose  that  all  solutions  (^{t to xo)  =  0(r ^o)-^o  are  bounded.  Let {ei...  en} denote the natural basis for ^-space  and let ||(/)(^ to ej)\\ <  (Sj for  all t  >  ^o-Then for  any vector xo  =  X'/=i  <^7^; we have that U{t.  to xo)\\ =  y^ aj(f){ttoej) 7 = 1 <  (max jS;)X 1^7-1 ^ ; = i 7 = 1 k\\xo\\ for some constant  ^  >  0 for ^ >  ^o- For given e  >  0 we choose 8  =  elk.  Thus if ||xo||  < 6  then  ||(^(^ to xo)|| <  ^||^o||  <  ^  for  all t  >  to- We have proved  that  if the  solutions of {LH)  are bounded then the equilibrium  x  =  0 of {LH)  is stable. • THEOREM  5.2.  The equilibrium  x  =  0 of {LH)  is uniformly  stable  if and only if sup ^(^o)  =  sup(sup||0(r ko <oo. The  proof  of  this  theorem  is  very  similar  to the proof  of Theorem  5.1  and  is  left as  an  exercise. EXAMPLES.1.  We consider the system given by with  x{0)  =  Xo. We transform  (5.1) by means of the relation  x  =  Py  where 0 e~' P  = 1  1 0  1 pi  = and obtain the equivalent  system Ji LJ2j 0 (5.1) (5.2) with y{0)  =  yo  =  P~^xo.  System  (5.2) has the solution  (//(r 0 jo)  =  "^{t 0)}^o where ^(r  0) ^1/2(1-^-^0 0 0 g(l-.-0 Thesolutionfor(5.1)isobtainedas(/>(r0  Xo)  =  0(r 0)xo where ^(r 0)  = From this we obtain for ^o ^  0 4){t to xo)  =  ^{t  to)xo where P'^{t0)P~\ 454 Linear  Systems Now ^(t  to) 0 Je~'0-e-^) ^l/2e"2^0 ^e~^0  __  ^l/le'^^O lim^itJo)  = 0 (5.3) y Xlj=i\ct>ij(tJo)\^  <  Xlj=i\cl>ij(^>to)\l  that  sup^^^(sup^J^(tJo)\\) We  conclude  that  limy^^oolimr^oo||4)(^ ^o)||  <  oo  and  therefore  (since  ||(|>(r ro)||  ^ <  ^.  There fore  the  equihbrium  x  =  0  of  system  (5.1)  is  stable  by  Theorem  5.1  and  uniformly stable  by Theorem 5.2. • THEOREM  5.3.  The following  statements  are  equivalent. (i)  The equilibrium  x  =  0 of (LH)  is asymptotically  stable (ii)  The equilibrium  x  =  0 of (LH)  is asymptotically  stable in the large (iii)  lim_oo||c|>(rro)||  =  0. Proof  Assume  that  statement  (i)  is  true.  Then  there  is  an  17(^0) >  0  such  that  when ||xo|| ^  vOo)^ then (^(^ to xo) ->  0 as ? ->  00. But then we have for any  xo ^  0 that (f){ttQXo) =  MttQy]{tQ)  • ^0 Ikoli 0 as r —>  CO. It follows  that statement  (ii) is true. Next assume that statement (ii) is true and fix ^0 ^  0. For any 6 >  0 there must exist a Tie)  >  0 such that for all t  ^  to + T(e)  we have that \\(l)(t to xo)\\  =  \\^(t to)xo\\  <  e. To  see  this  let  {e^  . . .  ^„} be  the  natural  basis  for  R^.  Thus  for  some  fixed  constant ^  >  0 if  xo  =  ( a i  . . . a«)^  and if  ||xo|| ^  1 then  xo  =  2 " =i  (^jej  and  2y==i  \o^j\  — k.  For  each J  there  is  a  Tj{e)  such  that  \\<^{t to)ej\\ <  elk  and  ^ >  ^0 +  Tj{e).  Define T{e)  =  max {Tj{e)  : j  =  I..  .n}.  For ||xo|| ^  1 and t  ^  to + T(e)  we have that ll^(^^)^o|| ^aj<^(tJo)ej 7 = 1  1'"^-By  the  definition  of  the  matrix  norm  [see  (10.17)  of  Chapter  1]  this  means  that ||0(r ^)|| <  6 for f >  ^0 +  T(e).  Therefore  statement  (iii) is true. Finally  assume  that  statement  (iii)  is  true.  Then  \\^(t to)\\ is  bounded  in  t  for  all t  >  to. By  Theorem 5.1 the equihbrium  x  =  0 is stable. To prove asymptotic  stability fix  ro >  0  and  e  >  0.  If  ||xo|| <  r](to)  =  1 then  Uit  to xo)|| <  ||^(^ ^)|| ||xo|| ^  0  as t -^  CO Therefore  statement  (i) is true. This completes the proof. • EXAMPLE  5.2.  The equilibrium  x  =  0 of system (5.1) given in Example 5.1 is  stable but it is not asymptotically  stable since lim^^oo ||^(^ ^)|| 7^ 0 [see Eq.  (5.3)]. • EXAMPLE5.3.  The solution of the  system -e^'x x(to)  =  Xo (5.4) is (/)(? to Xo) =  ^(t  to)xo where ^(t  to)  = e' (l/2)(e2^0-e20 Since lim^-^ 00 ^(^ ^0)  =  0 it follows that the equilibrium x  =  Oof system (5.4) is asymp totically  stable (in the large). • THEOREM  5.4.  The equilibrium  x  =  0 of  ilM)  is uniformly  asymptotically  stable if and only if it is exponentially  stable. Proof The exponential stability of the equilibrium x  = 0 implies the uniform asymp totic stability of the equilibrium x  =  0 of systems {E)  in general and hence for systems (L//) in particular. Conversely assume that the equilibrium x  =  0 of {LH)  is uniformly asymptotically stable. Then there is a 6 >  0 and a 7  >  0 such that if ||xo|| ^  5 then 455 CHAPTER  6: Stability for all t to >  0. This implies that \\^{t + to + T to)xo\\  <  ^ ||cD(/ + /o + rro)||<  i ifrro^O. (5.5) From Theorem 3.6 (iii) of Chapter 2 we have that (^(t  r)  =  <[>(r o-)0(a- r) for any t cr and T.  Therefore \\^(t +  to + 2T to)\\  = \\^(t +  to  + 2Tj  +  to + T)^{t  +  to + T to)\\ ^ | in view of (5.5). By induction we obtain for tJo  — O that \\^(t  +  to + nT to)\\  ^  T\ (5.6) Now let a  = (\n2)/T. Then (5.6) implies that for 0 <  ^ <  T we have that Uit  + to + nT to xo)\\ <  2||xo||2-(«+i>  =  2||xo|k-"^"^^^^ which proves the result. EXAMPLE 5.4.  For system (5.4) given in Example 5.3 we have <  2||;co|k-"^^^'^^> U(tJo.xo)\\  =  |c/>(Uoxo)|  =  \xoe^"^^^'''e-^"'^^''\ <  \xo\e^"^^'"''e-r >  ?o >  0 since e^^ > 2t. Therefore the equilibrium x  =  0 of system (5.4) is uniformly asymptot ically stable in the large and exponentially stable in the large. • Even though the preceding results require know^ledge of the state transition ma trix $(r ro) of {LH) they are quite useful in the qualitative analysis of linear systems. In view of the above results we can  state the following  equivalent  definitions.  The equilibrium x  =  0 of {LH)  is stable  if and only if for any ^  ^  0 there exists a finite positive  constant  y  =  y{to)  (which  in general  depends  on ^o) such that for  any  XQ the corresponding  solution  satisfies  the inequality U{tJo.xo)\\^ y{to)\\xol t^ to. Also the equilibrium  x  =  0 of {LH)  is uniformly  stable  if and only if there exists a finite positive constant y  (independent  of  to)  such that for  any to and  JCQ the corre sponding  solution satisfies  the  inequality U{ttoxo)\\^ rllxoll t^  to. Furthermore in view of the above results if the equilibrium x  =  0 of {LH) is asymp totically stable then in fact it must be globally asymptotically  stable and if it is uni formly  asymptotically stable in the large then in fact it must be exponentially  stable (in the large). In this case there exist finite constants y  >  1 and  A >  0 such that \mto>xo)\\^ye-^^'-'^^\\xo\\ for  r >  /o >  0 and xo  E  R"". The results in the next theorem which are important in their own right will be required  later. 456 Linear  Systems THEOREMS.5.  Let A be bounded on (-  oo  oo).  Then any one of the following  statements ^^ equivalent to the exponential  stability of the origin  x  =  0 of  {LH): cifox^wh^to. c2fox^\\h^  to. \;^\\^{hT)fdT^C^foX^\\h^to. (i) \;^mtto)\?dt^ (ii) \;^mtjo)\\dt^ (iii) (iv)  \;^mhT)\\dT^ (The constants ci are independent of ^  or h  and  ||  • || denotes a matrix norm induced Cfor2i\\h^to. by any one of the equivalent vector norms on /?".) Proof  If the  equilibrium  x  =  0 of  {LH)  is  exponentially  stable then  there  exist  con stants y  >  0  A  >  0 (independent  of to) such that  \\^{t ro)|| ^  y^-^(^-^o) t  >  t^. Substi tuting this estimate into (i) to (iv) and evaluating the integrals yields ci  =  C3 =  y^l{2K) andc2  =  C4 =  y/A. Therefore if the equilibrium  X  =  0 of (L//) is exponentially stable then the integrals  (i) to (iv) are bounded. We now prove the converse  statements by considering each case  individually. (a) Assume that the bound c\  in (i) exists. Since A is bounded there exists an a  >  0 such that ||ci>(Uo)||  =  ||A(Ocl>(Uo)||^  ||A(0||||^a^)|| <  a\\^{t  to)\\ t  >  ^0. Therefore ||cD(ri ^o)^^(^b ^0) -  /|| [6(r tof^it to)  + ^{t tof(^{t  to)]  dt\\ to '  \\^{t to)^^(t  to) +  (D(^ to)^i(t \\\[A{t)^(t to)n  mt  to)\\ + mt  to)^ IIA(O^(^ to)\\}dt to)\\dt <  2a rh J  to ||0(r to)fdt  <  2aci t  >  to. Using the triangle inequality  of norms  yields mti  to)^^(tu to)\\  = mtu  to)^^(tu to)-i+1\\ <  ||<D(^i to)^^(tu to) -  I\\ +  ||/|| ^  2aci  +  1 ti^ to. This shows that ||^(ri ^)|| is itself bounded  for ti  >  ^  by a constant  Li. To determine an exponential bound for ||0(f  ^o)||  we note that mtto)fdT= tQ J  to i'mtT)^(TJo)fdT mt  rt • mr toifdr  <  Lf  f'  ||<I.(T to)fdr to ^  LW Jto Noting that the integrand  on the left  side does not depend on r  we obtain {t -  to)\\^{t to)f  <  L\ci t  >  ^0. Let  ^  =  ^  -f-  7  and let T  =  AL\ci.  Then mto  +  Tjo)\\^ i Repeating the above procedure we obtain and in general we have 110^0 +2rro)N  ^5 ||<D(^ +  nT  to)\\  < 457 CHAPTER  6: Stability Proceeding now as in the proof of Theorem 5.4 it follows  that ||<l>(^ ^)||  is  exponentially bounded. (b) As in (a) but using ^{t\ to) instead of 4>(ri to)^(^(ti  to)  and using the inequality ||i>(r ^)|| <  a||0(f ^o)|| we obtain llcD^i ro) - / ||  = \\r^{tJo)dt\\ rh f'||(i>(Uo)||^^^c.f'||0(Uo)||^^ Jto JtQ using  the inequality property  of norms we have <  Q;C2 h  >  ho ||c|>(ri ^0)11  =  ||^(ri to)-I  + I\\ ^  mh to) -  I\\ +  ||/|| Using this estimate we now  obtain <  ac2  +  1. llcDfe to)fdt  <  (ac2  +  1) f''  ||Oa  to)\\dt JtQ JtQ <  (aC2  + l)C2. This  shows  that  (ii) implies  (i) and  therefore  it follows  that  ||0(/ /o)|| is  exponentially bounded. (c) The proof of this part follows the proof of (i)  with some modifications. Since A is bounded there exists a  >  0 such that ||A(0|| ^  «  for all t. From Exercise 2.35 in Chapter 2  we have ^-^(t  T)  =  -(D(?  T)A(T) dr ^^(tT) =  ||-OaT)A(T)||<  a\\^(tT)\ and  therefore. Then •^(tito)'^^(hJo)\\ Mtu  rf  ^{ti  T)  + ^(^1  rf ^{h  T)  dr As in (a) we now  obtain ||/  -  ^(^1  tof^ty to)\\ ^  2ac3 ti  >  to. Using the triangle inequality we obtain mt to)^^{ti to)\\ ^ mtx  to)^^{t to) -1\\  + ii/ii This  shows that  ||0(^ ^)||  is bounded  for  all ^ >  ro by  a constant  L3 that is  independent of rand  ^0-<  2Q:C3  +  1. 458 To obtain an exponential bound we proceed similarly as in (a) to obtain Linear Systems u  ^  .\\^u. ..M|2  ^  f'licT...  ..M|2 J to to J to < Ln 3 ||cD(riT)|pjT<L3C3 ^0 Therefore we have from which we obtain letting T  = 4L3C3 and more generally we obtain 11^^0 + 7^0)11^  h mto  +  nTto)\\^{kJ^ Again as in part (a) we conclude that O is exponentially bounded. (d) Assume that (iv) is satisfied. Then as in (b) we can show that (iv) implies (iii) which in turn implies that <I> is exponentially bounded. We obtain 110(^1 ^o) -  /|| <  ac4 t > to where a  is defined  as before  and C4  is given in (iv). From this we can conclude that \\^(ti to)\\ is bounded for all ^1  >  ^0 by a:c4 +  1. This in turn yields the inequality [''  \\^(t rtdr  <  (ac4 +  1) f'  mtu  r)\\dT Jto JtQ <  (ac4  + 1)C4. From this it follows that (iv) implies (iii). This concludes the proof of the theorem. • We  now  turn  our  attention  to  linear  autonomous  and  homogeneous  systems given by X =  Ax r >  0 (L) referring to the discussion in Subsection 2.4A  [refer to Eqs. (4.11) to (4.27) in Chap ter 2] concerning  the use of the Jordan  canonical  form  to compute exp  (At).  We let J  =  P~^AP  and defi[ne x  =  Py.  Then (L) yields y  =  p-^APy  =  Jy. (5.7) It  is  easily  verified  (the  reader  is  asked  to  do  so in  the  Exercises  section)  that  the equilibrium  jc  =  0 of  (L) is  stable  (resp. asymptotically  stable  or unstable)  if  and only if  y  =  0 of (5.7) is stable (resp. asymptotically  stable or unstable). In view of this we can  assume  without  loss of generality  that the matrix A  in  (L) is in  Jordan canonical form  given by where for the Jordan blocks Ji.. /Q =  diag [Ai... A^] .Js. A  =  diag [Jo> Ji' and - Js\y Jk  =  )^k+iU  +  M As in (4.21) (4.22) (4.26) and (4.27) of Chapter 2 we have oAt ^ (?Jof 0 where e-"" =  diagle""... 0 o^st „Atn e"'''] 1 t and ^•fit  = g^k+it 0  1 tm-i ••• ( «  - l )! {rii  -  2)! 2 t 0  0  0 1 459 CHAPTER  6: Stability (5.8) (5.9) for  /  =  1...  s. Now  suppose  that  Re kt  <  j8  for  all  /  =  1...  k.  Then  it  is  clear  that \imt-oo (ll^-^o^ll/^^O  <  °°^ where  ||^^°^||  is  the  matrix  norm  induced  by  one  of  the equivalent  vector norms defined  on R^.  We write this as \\e-^^%  =  €(e^^).  Similarly if jS  =  Re Xk+i then for  any e  >  0 we have that ||^"^^^|| =  ©(r^^-^^^O  = €(e^^^^^'). From the foregoing  it is now  clear  that  \\e^^\\  ^  K  for  some  ^  >  0 if  and  only if  all  eigenvalues  of A  have  nonpositive  real  parts  and  the  eigenvalues  with  zero real  part  occur  in  the  Jordan  form  only  in  JQ and  not  in  any  of  the  Jordan  blocks J I 1 <  /  <  5-. Hence  by  Theorems  5.1  and  5.2  the  equilibrium  x  =  0  of  (L)  is under these conditions  stable in fact uniformly  stable. Now  suppose that  all eigenvalues  of A have  negative  real parts. From the pre ceding  discussion  it is clear that there is a constant  ^  >  0 and  an a  >  0  such  that ll^^^ll <  Ke-""'  and  therefore  Uit  to xo)|| <  Ke-''^'-'^^xo\\  for  alW  >  ro >  0  and for  all  xo  ^  R'^. It follows  that the  equilibrium  x  =  0 is uniformly  asymptotically stable in the large in fact  exponentially  stable in the large. Conversely assume that there is  an eigenvalue  A/ with  nonnegative  real part.  Then  either  one term  in  (5.8) does not tend to zero or else a term in (5.9) is unbounded  at ^ ^  oo. In either case ^^^x(0) will not tend to zero when the initial condition x(0)  =  XQ is properly chosen. Hence the equilibrium  x  =  0 of (L) cannot be asymptotically  stable  (and hence it cannot be exponentially  stable). Summarizing the above we have proved the following  result. THEOREM 5.6.  The equihbrium x  = Oof (L) is stable in fact uniformly stable if and only if all eigenvalues of A have nonpositive real parts and every eigenvalue with zero real part has an associated Jordan block of order one. The equilibrium x  =  0 of (L) is uniformly asymptotically stable in the large in fact exponentially stable in the large if and only if all eigenvalues of A have negative real parts. • A direct consequence of the above result is that the equilibrium  x  =  0 of (L) is unstable  if  and  only  if  at least  one  of  the  eigenvalues  of A has  either  positive  real part or has zero real part that is associated with a Jordan block of order greater  than one. At this point it may be appropriate to take note of certain conventions  concern ing matrices that are used in the literature. It should be noted that some of these are 460 Linear Systems not entirely consistent with the terminology used in Theorem 5.6. Specifically  a real nXn  matrix A is called stable or a Hurwitz  matrix if all its eigenvalues have negative real parts. If  at least  one of the  eigenvalues  has  positive  real part  then A  is  called unstable.  A  matrix A  which is neither stable nor unstable is called critical  and the eigenvalues with zero real parts are called critical  eigenvalues. We  conclude  our  discussion  concerning  the  stability  of  (L)  by  noting  that  the results  given  above  can  also be  obtained  by  directly  using  the  facts  established  in Subsection  2.4C concerning modes and asymptotic behavior of time-invariant  sys tems. EXAMPLE 5.5.  We consider the system (L) with A 0  1 -1  0 The eigenvalues of A are Ai A2 =  ±j.  According to Theorem 5.6 the equilibrium x  = 0 of this system is stable. This can also be verified by computing the solution of this system for a given set of initial data x(0)^  = (xi(0) X2(0)) (j)i(t0xo)  = xi(0)cosr  + X2(0)sin^ 4>2(t 0 XQ) =  -xi(0)smt  + jC2(0)cosr r >  0 and then applying Definition 4.1. EXAMPLE 5.6.  We consider the system (L) with ro  n A  =  0  0 • The eigenvalues of A are Ai  =  0 A2 =  0. According to Theorem 5.6 the equilibrium X =  0 of this system is unstable. This can also be verified by computing the solution of this system for a given set of initial data x(0)^  = (xi(0) ^2(0)) (l)l(t 0 Xo)  = Xi(0) + X2(0)t (f>2it 0 Xo)  = X2(0) ^  >  0 and then applying Definition  4.5. (Note that in this example the entire xi-axis • consists of equilibria.) EXAMPLE 5.7.  We consider the system (L) with ^  [2.8 [9.6 9.6] -2.8j* The eigenvalues  of A are Ai A2 =  ±10. According  to Theorem  5.6 the  equilibrium X =  0 of this system is unstable. • EXAMPLE 5.8.  We consider the system (L) with The eigenvalues of A are Ai A2 =  - 1  - 2.  According to Theorem 5.6 the equilibrium X =  0 of this system is exponentially stable. • [-1 -2\-Next we consider linear periodic systems given by (P) where A(t)  is  a continuous  real  matrix  for  all  f G  (-00 00). We recall from  Section 2.5 of Chapter 2 that if 0(^ ^0) is the state transition matrix for (P) then there exists A(t)  =  A(t  +  Tl X =  A(t)x 461 CHAPTER  6: Stability a constant nX  n matrix R and a nonsingular  nX  n matrix "^(t to) such that ^(t  to)  =  nt to) exp [R(t  -  to)l (5.10) where ^(?  +  T to)  =  "i^it to) for  all  t  E  (-00 oo). Now  according  to  the  discussion  at  the  end  of  Section  2.5  of Chapter 2 the change of variables  x  =  '^(t  to)y transforms  (P)  to the  system y  = Ry^ (5.11) where R is given in  (5.10). Since ^(t  to) is nonsingular  it is clear that the equilib rium X =  0 of (P) is uniformly  stable (resp. uniformly  asymptotically  stable) if and only if the equilibrium  j  == 0 of system (5.11) is uniformly  stable (resp.  uniformly asymptotically  stable).  Now  by  applying  Theorem  5.6  to  system  (5.11)  we  obtain the following  result. THEOREM  5.7.  The cquiHbrium  x  =  0 of (P) is uniformly  stable if  and only if all eigenvalues  of the matrix R  [given  in  (5.10)]  have nonpositive  real parts  and every eigenvalue with zero real part has an associated Jordan block of order one. The equi librium X =  0 of (P) is uniformly asymptotically stable in the large  if and only if all eigenvalues of R have negative real parts. • 6.6 SOME GEOMETRIC AND ALGEBRAIC STABILITY  CRITERIA In  this  section  we  concern  ourselves  with  nth-order  linear  homogeneous  ordinary differential  equations  of the  form aox^""^  + axx^""-^^  + • -h an-ix^^"^  -h anX  =  0 ao 7^0 (6.1) where the coefficients  ao..  .an  are all real numbers. We recall from  Chapter  1 that (6.1) is equivalent to the system of first-order ordinary differential  equations where in (6.2) A  denotes the companion  matrix  given by X =  Ax A  = 0 0 1 0 0 1 On ao _ a „ -i ao _ ' ^ " -2 flo 0 0 fll flO (6.2) (6.3) To determine  whether  the equilibrium  x  =  0 of  (6.3) is  asymptotically  stable it suffices  to determine if  all the eigenvalues  of A have negative real parts or what amounts to the same thing if the roots of the polynomial /(A)  =  aoX""  +  (21A"""^  -h • • • +  (2^-1 A  -h an (6.4) all  have  negative  real  parts.  To  see  this  we  must  show  that  the  eigenvalues  of  A coincide  with  the  roots  of  the  polynomial  f(s).  This  is  most  easily  accomplished and  therefore by  induction.  For  the  first-order  case  k  =  1 we  have  A  =  -ajao det(\I\ 1.  Next assume that the assertion is true for k  =  n -  I.  Then -  A)  ^  A -h aJao  h  =  I  and  so  the  assertion  is  true  foYk= 462 Linear Systems det{XIn  -  A)  = 0 -1 A 0 flO 0 -1 0 ao 0 0 0 0 -1 A 02  A +  fli ao ao -1 A 0 "n ao =  Xdet(Xln-\ -  Ai)  +  (-1> n+l 0 -1 A 0 0 -1 0 0 0 where Ai  = 0 0 a„-i flO and In In-\  denote the n  X /i and {n- 1 0 0 1 0 0 0 0 _an-2 ao \)X  {n-  1) identity matrices.  Therefore an-3 ao flo ai det(\In  -  A)  =  Xdet(\In-i -  Ai)  +  — ao =  A" +  ^ A " -i  + ao 0 A+  — ao ao is equivalent to /(A)  =  0. Analogously  to matrices we now make the following  definitions.  An  nth-order polynomial  /(A)  with real coefficients  [such as (6.4)] is called stable  if  all zeros of /(A) have negative real parts it is called unstable  if at least one of the zeros of  /(A) has a positive real part and it is called critical  if /(A) is neither stable nor unstable. Also a stable polynomial is called a Hurwitz  polynomial. In view  of the  above the  stability  problem  for  nth-order  differential  equations with constant coefficients  has now been reduced to a purely algebraic problem of de termining whether the zeros of a polynomial  [such as (6.4)] all have negative (resp. nonpositive)  real  parts.  In  case  zeros  with  vanishing  real  parts  exist  it  is  further necessary  to determine their  multiplicity. In the following  we first present  some graphical  criteria  that  enable us to de termine the stability  of a polynomial  (6.4) without  determining  its roots  explicitly. These results which are important in their own right are then used to arrive at some algebraic  criteria  to determine the stability  of a polynomial  (6.4). A.  Some  Graphical  Criteria In establishing  our first result we assume that the polynomial  f(s)  [given by  (6.4)] has p  zeros  in the right  half  of the ^--plane and  (n  -  p)  zeros  in the left  half  of  the ^--plane and we assume that there shall be no zeros on the imaginary  axis. We let C denote the counterclockwise contour formed by a semicircle C with radius r and cen-tered at the origin together with its diameter  on the imaginary  axis and we choose r so large  that  the p  zeros  of  f(s)  in the right  half  of the ^-plane  lie in the interior of the circle. We now recall from  a well-known result from the elementary theory of functions  of a complex variable (called Cauchy's  integration formula)  that 463 CHAPTER 6: Stability P=  ^ 'l ^ T T ^^  =  ^ 8c  In f(sl (6.5) 277-J )c  f{s) 27TJ f'(s)  denotes  the  derivative  off  with where  5-  is  a  complex  variable  j  =  ^-l respect to s \(^[f'(s)/f(s)]  ds  denotes the integral of f'(s)/f(s) along the contour C and 8c In f(s)  represents the increment of In f(s)  along the contour C. Let Sq  In  f(s) denote the increment  of ln/(^)  along the  semicircular  arc  C  with  s  =  re^"^.  For s on C  we have f{s)  =  aor'^^^'"^(l  +  0(r"^)) ln/(^)  =  Inao  +  ^Inr  +  njcf) +  0(r"^). and Hence 8c>lnf(s)  = njl^^'^ =  niTJ +  ©(r"^). Letting  r ^  oo and using (6.5) we conclude that -n^^j  +  hs—: ]8i In  f(s) 277 j \27rj =  ^+  2 : ^ 5 / I n / ( ^ ) (6.6) where S/ In f(s)  denotes the increment of the logarithm of f(s)  along the imaginary axis /  from  -oo to  +oo. To determine this increment  we let s  =  jo)  and let f(s)  =  f(jay)  ^  Ri(o)eJ'(^^  ^  U{co)  +  jVico) (6.7) in  (6.7)  describe  the frequency  response  or frequency  plot  for and  we  consider  R 6  as polar  coordinates  and  U  V  as  coordinates  in  the  complex plane. As  the real parameter  co  (the real  frequency  w)  ranges  from  +00 to  -00 the points  f(jo)) f(s). Since/(s-) has real coefficents  we must have 7?(CL))  =  R(-a))  and 0((o)  = -6(-(o). It therefore  suffices  to consider the part of the response curve belonging to the posi tive values of the parameter 0;. It follows  from  (6.6) that 6(00) (6.8) n  _  ^(00)  _  1 2  ~V~  ~  2 where  ^(00) is the limit  to which  the polar  angle  6(a))  =  tan~^[V(o))/U((o)]  of  the frequency  response diagram tends as co  becomes unbounded. Since  U(co) and  V((o) are polynomials of different  degrees  \V(CO)/U((JO)\ will tend to either zero or infinity. In either case in view of (6.8) ^(00) must be an integral multiple of 77/2. Now when in particular p  =  0 then by necessity  we have that 6(00) =  n(7r/2). This yields the following  result. THEOREM 6.1. (LEONHARD-MIKHAILOV  STABILITY CRITERION)  The poly nomial f(s)  has only zeros with negative real parts if and only if its frequency response diagram f(jco)  0 <  co <  00 passes through exactly n quadrants in the positive sense.  • 464 Linear Systems (a)  Frequency response plot for f{s)  stable (n = 6) (b)  Frequency  response plot for f{s)  unstable (n = 6) FIGURE 6.4 Frequency response plot for f(s)  stable (n 6) In Fig.  6.4  we  depict  the frequency  response  plots  of  a  stable  and  an  unstable polynomial. Next since f(s)  has real coefficients  there are real polynomials  f\  and /2  such that fijco)  ^  Moj^)  4-  j(of2(co^\ (6.9) If  n  =  2k  then  deg fi(u)  =  k  and  deg f2(u)  =  k  -  I  and  if  n  =  2k  -{-  1 then deg f\{u)  =  k  and deg f2(u)  =  k. To the zeros  uik  i  =  1 2 and  k  =  1 2 3  . .. of the polynomials  fi(u)  /2(w) respectively  correspond  those  values  of  co^ at  which the frequency  response  diagram  intersects  the  axes.  In  the  case  of  stability  these values  ofcx)^ must  be  real  and  increasing  i.e.  the  zeros  ui^ must  be  positive  and alternate. 0  <  Wii  <  U2\  <  Ui2  <  U22 < '" (6.10) since otherwise the response curve f{jco)  will not make the proper number of turns at the appropriate locations. These considerations lead to the interlacing  of the roots u\k with the roots  U2k  k  =  1 2 3  . . .  which  is sometimes  called the gap  and  position criterion.  In Fig. 6.5 we depict typical situations for stability and instability (in terms of the variables  U and  V). We summarize the above in the following  result. u  V k u  V  k CO r (a) Stable case FIGURE 6.5 (a) Unstable case THEOREM  6.2.  (GAP  AND  POSITION  STABILITY  CRITERION)  The  polynomial f(s)  has only zeros with negative real parts if and only if the zeros of the polynomials defined in (6.9) are real and satisfy the inequalities (6.10). • The frequency  response plot  f(j(o) is unbounded  and hence can never be dis played  completely.  We  therefore  frequently  make  use  of  the  reciprocal  frequency response  diagram  i.e. the response  diagram  of  the function  l/f(jo)).  Such  a plot approaches  zero asymptotically  and in case of stability it rotates exactly through n quadrants in the negative  sense.  In applications of Theorem 6.1 the entire plot is not needed. It can be shown that it suffices  to consider the interval 0  <  w  <  CDQ^ where 465 CHAPTER 6* Stability (OQ  =  1 + 3  max\p\\. (6.11) We will not pursue the details concerning the proof of this  assertion. B.  Some Algebraic  Criteria In the next results we develop the Routh-Hurwitz criterion which yields  necessary and  sufficient  conditions  for  f(s)  to be  a Hurwitz  polynomial.  To accomplish  this we will make use of Theorem  6.2. We begin by establishing  a set of necessary  con ditions. THEOREM  6.3.  For f(s)  =  aos""  + ais'"''^  +  • • •  + an-is  + <2„ (6.12) to be a Hurwitz polynomial it is necessary that the inequalities ^ > 0  ^ > 0  . . .  ^ >0 Go UQ ao (6.13) hold. Proof Let si..  .SnhQ the zeros of (6.12) and in particular let s'j be the real roots and s'l the complex roots. Then f(s)  = aoYlis  -  s'j)Yl(s  -  s'l) j k =  aoYlis  -  s'j)Yl(s^  -  {2Res'l)s + |4f). j k If all the numbers s'j and Re s'l are negative then we can obtain only positive coefficients for the powers of s when we multiply the product out to obtain f{s). • Without loss of generality we assume in the following  that ao  >  0. In the next result we will require the Routh  array: c\Q  =  ao Cii  =  ai C20 =  <32 C21  =  as C30  =  a4 C31  =  as C40 = C41  = as... aj... C12 =  a2- r2fl3 C22 =  a^  -  r2as ^332  =  ^6  -  ^2<37... Ci3  =  C21  - r3C22 C23  =  C31  - r3C32 C33  =  C41  - r3C42  . . . ao ri  =  — ax Cll r^  =  cn <^1/-1 Cij  =  Ci+ij-2  -  rjCi+ij-i i=l2... J  =  2  3 . C\n  =  an 466 Linear Systems Note  that  if  n  =  2m  we  have ^ ^ ^ _^ ^ ^ _^ ^ ^ ^ _^ .—  n and  if  n  =  2m  -  1 we  have The above array terminates  after  {n—\)  steps in case all the numbers  ctj  are  different from  zero. The  last  line  defines  ci„. In  addition  to the  inequalities  (6.13) we  shall  require  the  inequalities  given  by C\\  >  0 Ci2  >  0 . . .  Cin  >  0. (6.14) THEOREM  6.4.  (ROUTH-HURWITZ  STABILITY  CRITERION)  The  polynomial f{s)  given  in  (6.12)  is  a Hurwitz  polynomial  if  and  only  if  the  inequalities  (6.13)  and (6.14) hold. Proof  First we assume that the degree of f{s)  is even by letting n  =  2m. We define the polynomials hi(s)  =  i[/(^)  +  f(-s)l h2(s)  =  l[/(^)  -  f(-s)l (6.15) Applying the Euclidean algorithm to determine the greatest common divisor of hi (s) and h2(s)  we obtain hi(s)  =  r2sh2(s) — hsis) h2(s)  =  r!^sh3(s)  -  h^is) (6.16) where the linear factors  arising in the division have no constant term and the remainders have been written with negative signs. It is readily verified that the constants r[ in (6.16) are related to the constants  r/  in the Routh array by the expression  r-  =  (-  1)V/. Next we define  a sequence of polynomials  given by h2i-i{s)  =  g2i-i(s^X h2i{s)  ^  sg2i{sh i  =  \.m. (6.17) From (6.16) and (6.17) we obtain the recursion formulas  given by gii+iiz)  =  r2iZg2i(z)  -  g2i-i(z) g2i+2(z) =  r2i+ig2i+i(z) -  g2i(z). The first two members of this sequence are given by gi(z)  =  aoz^  +  a2Z^-i  + giiz)  =  aiz""-'  + a^f"-^  +  •••  +  a2m-x. '"+a2m (6.18) (6.19) We can readily verify  that the above two polynomials agree with the polynomials /i  and /2 given in (6.9) except for  sign. In fact  we have fi{u)  =  gi{-u) i  =  12. (6.20) We are now in a position to construct the Routh array of a polynomial f{s)  by utiliz ing the coefficients  of the sequence of polynomials gi{z)- If in the process of doing so we encounter  a zero row  (i.e. an identically  vanishing  polynomial  say gi)  then  h\  and /z2 [and thus f{s)  and f{-s)\ have a common divisor. In this case f{s)  possesses a divisor of the form s^ + a. and is not a Hurwitz  polynomial. Next we  assume that the hypotheses  of this theorem  are satisfied  [i.e. (6.13)  and (6.14) hold]. Applying  definitions  we can readily verify  that the numbers  r[ have alter-nating signs that the signs of the leading coefficients  of the polynomials gi i = 12...  467 are siven bv -^ ^ /: CHAPTER 6: +  + - - + +  -  -  ••  • (6.21) Stability and that the degrees of these polynomials  are given by mm—  \m  — 1 m — 2 m —  2  . . .  1 1 0. (6.22) Next for fixed z -oo < z < oo we consider the sequence of numbers given by gl{z)g2{z\...g2m{z). (6.23) Note that the last term gimiz)  is a constant for all z. Let W{z) denote the number of sign changes in this sequence. When z > 0 and is very large then the signs of the gi in (6.23) correspond  to the signs of the leading  coefficients  of these polynomials. When  -z > 0 and  is very  large  then  the signs of gi in (6.23)  will  alternate. It now follows  that the difference  W{-^)  —  W{+^)  is always  equal  to m. Thus  as z varies  from  -oo to +°o m  sign  changes  in the sequence  (6.23)  will disappear.  Such a disappearance  can occur only at a zero of g\{z)  since if z passes through a zero say z' of gi{z) \ < i < 2m no disappearance in the number of sign changes  occurs because by (6.18) sgngi-\{z')  ^ sgn gi+i. We conclude that gi{z) has exactly m real zeros and since by hypothesis all its coefficients  are positive no positive zeros can occur. A  similar  argument  as  above  shows  that  g2 has exactly  (m -  I) negative  zeros. Now since in the sequence (6.23) the largest possible number of disappearances of sign changes  occurs  (as z is varied  from  -oo to  +oo) a disappearance in sign  change  must actually  occur  each  time z passes  through a zero of ^i. This however  is possible  only if  g2 in turn  changes  sign between  every  two zeros of ^i; otherwise  there  would be an additional change of signs in the sequence (6.23). It follows that the zeros of g2 separate the zeros of ^i  (the zeros of g2 are interlaced with the zeros of ^i). Now in view of (6.20) the  above  statement  concerning  the zeros of ^i  and g2 is equivalent  to the  inequahty (6.10);  thus. Theorem  6.2  applies.  Therefore  condition  (6.14) is sufficient  for f{s) to be a Hurwitz  polynomial. It is also a necessary  condition  since otherwise  the count of the sign changes in the sequence  (6.23) is too small and the hypotheses of Theorem 6.2 are not satisfied  i.e. either ^i  has too few zeros or the polynomial does not satisfy  con dition (6.10). To complete the proof we assume next that n = 2m + 1 i.e. n is odd. In this case we interchange the definitions  of hi(s)  and h2(s) given in (6.16) and we let h2i+l(s)  = Sg2i + \(S^\ h2i(s)  = g2i(s'^y The degrees of the polynomials gi formed in this manner are m m m - 1 m - 1  . . .  1 1 0. Following a similar procedure  as before  we show that the polynomials  gi(z)  and g2(z) each have m negative zeros and that Theorem 6.2 applies. This concludes the proof.  • EXAMPLE6.1.  We apply the Routh-Hurwitz criterion (Theorem 6.4) to the polynomial f(s)  = (s + 2)(s + 1 -  j)(s + 1  + j)(s + 1) =  /  + 5^^ + 10^2 + 10^ + 4. (6.24) For this  polynomial we form the Routh array and obtain / s^ s^ s' s' 1 5 i ( 5 - 1 0 - 1 - 1 0)  = 8 | ( 8 - 1 0 - 5 - 4)  = 7.5 7 ^ [ ( 7 . 5 ) - 4 - 8 - 0]  = 4 10 10 4 0 1 ( 5 - 4 - 0)  = 4  0 i ( 8 - 0 - 5 - 0)  = 0  0 0 0 468 Lh^i^Systems The conditions  of Theorem  6.4  are clearly  satisfied.  Hence  (6.24)  is  a Hurwitz polynomial. • EXAMPLE6.2.  We apply the Routh-Hurwitz criterion (Theorem 6.4) to the polynomial f(s)  = (s + 2)(s +  1  -  j)(s  +  1  + jXs  -  1) =  /  + 3^^ + 2s^ -2s-4. (6.25) We note that condition (6.13) is violated and therefore the polynomial (6.25) is not a Hurwitz polynomial. Condition (6.14) is also not satisfied. To see this we form the Routh array for (6.25) given as /I s^ s^ s' s' 3 2-4 -2 i(3-2  + 2 ) =f i [ 3 . ( - 4 ) - 0]  =  -4 i[f  •(-2)-(-4)-3]  =  f i[f  •(-4)-0]  =  -4 0 0 0. 0 • 6.7 THE MATRIX  LYAPUNOV  EQUATION In Section 6.6 we established a variety of stability results that require explicit knowl edge of the solutions of (L) or (LH).  We also derived  some geometric and  algebraic stability criteria for  (L) when the matrix A  is in companion form  that do not require explicit knowledge of solutions but instead  are deduced directly  from  the parame ters of A. In  this  section  we  will  develop  stability  criteria  for  (L)  with  arbitrary  matrix A.  In doing so we will employ Lyapunov's  Second  Method  (also called  Lyapunov's Direct  Method)  for  the  case  of  linear  systems  (L). This  method  utilizes  auxiliary real-valued functions  v(x)  called Lyapunov functions  that may be viewed as  "gen eralized  energy functions''  or "generalized  distance functions''  (from the equilibrium X =  0) and the stability properties  are then deduced directly from  the properties of v{x)  and its time derivative v(x) evaluated  along the solutions of (L). A logical choice of Lyapunov function  is v(x)  =  x^x  =  ||x|p which represents the square of the Euclidean distance of the state from  the equilibrium  x  =  0 of (L). The  stability  properties  of  the  equilibrium  are  then  determined  by  examining  the properties of v(x)  the time derivative of v(x)  along the solutions of (L) This derivative can be determined without explicitly  solving for the solutions of (L) by noting that X =  Ax. (L) v(x)  =  xF X +  x^ X  =  (Ax)^x  +  x^(Ax) =  Jc^(A^  +  A)x. If  the  matrix A  is  such  that  v(x)  is  negative  for  all  x  T^  0  then  it  is  reasonable  to expect that the distance of the state of (L) from  x  =  0 will decrease with increasing time  and  that  the  state  will  therefore  tend  to  the  equilibrium  x  =  0  of  (L)  with increasing time  t. It turns out that the Lyapunov function  used in the above discussion is not  suffi ciently flexible. In the following we will employ as a "generalized distance  function" the quadratic form  given by v(x)  =  X  Px P\ (7.1) where P is a real nXn  matrix. The time derivative of v{x)  along the solutions of (L) is determined  as v{x)  =  x^Px  +  x^Px  =  x^A^Px  +  x^PAx 469 CHAPTER  6: Stability I.e. where =  x^{A^P  +  PA)x V  =  x^Cx C  =  A^P  +  PA. (7.2) (7.3) Note that C is real and C^  =  C. The system of equations given in (7.3) is called the Lyapunov  Matrix  Equation. We  recall  from  Section  6.2  that  since  P  is  real  and  symmetric  all  its  eigen values  are real. Also we recall  that P is  said  to be positive  definite  (resp.  positive semidefinite)  if  all its eigenvalues  are positive  (resp. nonnegative)  and it is  called indefinite  if P has eigenvalues  of opposite  sign. The definitions  of negative  definite and negative  semidefinite  (for P)  are  similarly  defined.  Furthermore  we recall  that thQ function  v(x)  given in  (7.1) is said to be positive  definite positive  semidefinite indefinite  and  so forth  if  P has  the  corresponding  definiteness  properties  (refer  to Section  6.2). Instead of solving for the eigenvalues of a real symmetric matrix to determine its definiteness properties there are more efficient  and direct methods of accomplishing this. We now digress to discuss some of these. Let G  =  [gij] be a real nX  n matrix  (not necessarily  symmetric). Referring  to Subsection  2.2G we recall that the minors  of G are the matrix itself  and the matrix obtained  by  removing  successively  a row  and  a  column.  The principal  minors  of G  are  G itself  and  the matrices  obtained  by  successively  removing  an  ith row  and an  ith  column  and  the  leading  principal  minors  of  G  are  G itself  and  the  minors obtained by successively removing the last row and the last column. For example if G  =  [gij]  G  R^^^  then the principal minors  are gn 821 _g3l gn _g3l gn gii g32 gl3 g33_ gu] g23 ^33 J > gn g2l gl2 g22_ [gnl g22 g23 [g32  ^33. [g22l [g33l The  first  three  matrices  above  are the  leading  principal  minors  of  G.  On  the  other hand the matrix g21 _g3l g22 g32 is a minor but not a principal  minor. The following results due to Sylvester allow efficient  determination of the def initeness properties of a real symmetric  matrix. PROPOSITION 7.1.  (i) A real symmetricmatrix P  =  [pij] G R''^''is positive definite if and only if the determinants of its leading principal minors are positive i.e. if and only if 470 Linear  Systems Pn  > 0 det \Pn [Pl2 Pul Pll] >0...detP>0. (ii) A real symmetric matrix P is positive  semidefinite  if and only if the determinants of all  its principal  minors  are nonnegative. • Still  digressing  we  consider  next  the  quadratic  form v(w)  =  w^Gw G  =  G^ where  G  G  R^^^  Referring  to  Subsection  6.2C  [in  particular  Eqs.  (2.35)  and (2.36)]  there  exists  an  orthogonal  matrix  Q  such  that  the  matrix  P  defined  by is  diagonal.  Therefore  if  we  let  w  =  Qx  then P  =  Q-^GQ  =  Q^GQ v(2;c)  =  v{x)  =  x^Q^GQx = x^Px where  P  is  in  the  form  given  in  Eq.  (2.35)  i.e.. A2 0 From  this we  immediately  obtain  the  following  useful  result. PROPOSITION  7.2.  Let P  =  P^  G  R''^''  let \M(P)  and A^(P) denote the largest and smallest eigenvalues  of P respectively and let || •  || denote the Euclidean norm. Then A^(P)||x|p  <  v(x)  =  x^Px  < \M(P)\\xf (7.4) for  all X E  R\ Let  ci  =  \m(P)  and  C2  =  ^M(P)'  Clearly  v(x)  is  positive  definite  if  and  only is is  positive  semidefinite  if  and  only  if  C2  ^  ci  >  0  v(x) if  C2 ^  ci  >  0  v(x) indefinite  if  and  only  if  C2 >  0  ci  <  0  and  so  forth. We  are  now  in  a position  to prove  several  results. THEOREM  7.1.  The equilibrium  x  =  0 of (L) is uniformly  stable  if there exists a real symmetric and positive definite  nX  n matrix P such that the matrix  C given in (7.3) is negative  semidefinite. Proof.  Along any solution (/)(r to XQ)  =  (f)(t) of (L) with (/>(^ ^o XQ)  =  (^(^) have xo we ct>(tfPct>(t) xlPxo  +  I  ^(j>{y]fP(i>{r])dr]  =  x^Pxo  + d (fy{r]f C(j>{'r])dri for  all t  >  to >  0. Since P is positive  definite  and C is negative  semidefinite  we have 471 (l>{tYP(fy{t)  -  xlPxo  <  0 for  all t  >  to >  0 and there  exist  C2 ^  ci  >  0 such  that ciwmf  ^ m^pm  < xiPxo < c2iixoip for  all ^ >  ^.  It follows  that ||<^(ONgj  Nl CHAPTER 6: Stability for  allr  >  ^  >  0 and for any XQ  G R^.  Therefore  the equilibrium  x  =  0 of (L)  is  uni formly  stable  (refer  to Sections  6.4 and 6.5). • EXAMPLE  7.1.  For the system  given  in Example  5.5 we choose  P  =  I  and we  com pute C  =  A^P  + PA  =  A^ + A  =  0. According  to Theorem  7.1 the equilibrium  x  =  0 of this  system  is stable  (as  expected from  Example  5.5). • THEOREM  7.2.  The equilibrium  x  =  0  of  (L) is  exponentially stable  in  the  large  if there  exists  a real  symmetric  and positive  definite  nX  n matrix  P  such  that  the matrix C  given  in (7.3)  is negative  definite. Proof  We let (f){t to xo)  =  cf>(t) denote  an arbitrary  solution  of (L)  with  (/>(^)  =  ;co. In view  of the hypotheses  of the theorem  there  exist  constants  C2 ^  ci  >  0 and C3  >  C4 > 0  such  that and for  all /  >  ^  >  0 and for any ;co e  /?".  Then -C3\\m\? ^ v(0(O) = m ^ cm  ^ -C4\\m\? v(m) = j^im^pm] ^ (-^Am^pm - | ) v ( c ^ ( 0) for alU  >  ^  >  0. This implies after  multiplication by the appropriate  integrating  factor and  integrating  from  to to t  that V((/>(0)  =  (f)(tf  P(t>{t) <  xJPX0^~^'4/c2)a-%) or or ciWmf ^  m'^Pm \l/2 I ^ C2\\xo\fe-^'^''^^^'-'^^ | | c / > ( 0 | | <^ ||xo|k-^^/^^^^4/c2)a-^o) t^to^^. This  inequality  holds  for  all  xo  E  R^  and  for  any  to ^  0.  Therefore  the  equilibrium X =  0 of (L) is exponentially  stable in the large (refer  to Sections 6.4 and 6.5). • In Fig.  6.6  we provide  an interpretation  of Theorem  7.2  for  the  two-dimensional case  {n  ^  7).  The  curves  Q  called  level  curves  depict  loci  where  v{x)  is  constant i.e.  Ci  =  {x  G  R^  \ v(x)  =  x^Px  =  Ci} i  =  0  1 2 3 When  the  hypotheses of  Theorem  7.2  are  satisfied  trajectories  determined  by  (L)  penetrate  level  curves corresponding  to  decreasing  values  of  c/  as  t  increases  tending  to  the  origin  as  t becomes  arbitrarily  large. 472 Linear  Systems C2 = {xeR^:v{x) = C:i} Ci  =[xeR^: v{x)  = c-]} \ Co = {xe  R^  : v(x)  = Co = 0} |  C2 = {x€H^ : v(x)  = C2} 0  =  CQ<  C-\  <  C2  <  C3  •  •  " to<t^ <  f2  <  ^3  •  • • V(X)  =  C3 ' - fc v{x) = C2 -""^ \/(x) = ci -^ ^^N^^^Vl — ^ - ^2 ^^ >^1 FIGURE  6.6 Asymptotic  stability EXAMPLE  7.2.  For the system given in Example 5.8 we choose ^  n  0 0  0.5 and we compute the matrix C  =  A^P  +  PA  = -2 0 0 -2 According to Theorem 7.2 the equilibrium  x  =  0 of this system is exponentially  stable in the large (as expected from  Example 5.8). • THEOREM  7.3.  The  equilibrium  x  =  0 of  (L) is  unstable  if  there  exists  a real  sym metric nX  n matrix P that is either negative definite or indefinite  such that the matrix  C given in (7.3) is negative  definite. Proof  We first assume that P is indefinite. Then P possesses eigenvalues of either sign and every neighborhood  of the origin contains points where the  function v(x)  = x^Px is positive and negative. Consider the neighborhood B{e)  =  {x e  /?^ : \\x\\  < e} where || • || denotes the EucHdean norm and let G  =  {x e  B{e) : v(x) <  0}. 473 CHAPTER  6: Stability On the boundary of G we have either ||x||  =  e or v{x)  = 0. In particular note that the origin x  = 0 is on the boundary of G. Now since the matrix C is negative definite there exist constants C3  >  Q  >  0 such that -csll^lp  <  x^Cx  =  v(x)  <  -QII^IP for all X  E  /^". Let (t)(t to  XQ) = (i){t)  and let xo  =  (/>(^) G G. Then v(xo)  =  -(3 <  0. The  solution  (f){t)  starting  at  JCQ must  leave the  set  G. To see this note that  as long as  (f){t)  E  G  v((/)(0)  ^  -«  since  v(x)  <  0 in  G.  Let  -c  =  sup  {v(x)  : x  ^  G and v(x) <  -fl}. Then c >  0 and v{(f){t))  = v(xo) + v((t)(s))ds  <  -(2 - C(i5 =  —a  —  (t  —  to)c  t  >  tQ. This inequality  shows that 4>{t)  must escape the set G (in finite time) because v{x) is bounded from below on G. But (f){t) cannot leave G through the surface determined by v{x)  = 0 since v((/)(0) ^  - a. Hence it must leave G through the sphere determined by ||x||  =  6. Since the above argument holds for arbitrarily small 6 >  0 it follows that the origin x  =  0 of (L) is unstable. Next we assume that P is negative definite. Then G as defined is all of B(e) The • proof proceeds as above. The  proof  of  Theorem  7.3  shows  that  for  e  >  0  sufficiently  small  when  P  is negative definite all solutions (pit) of (L) v^ith initial conditions  XQ  E  B(e)  w^ill tend aw^ay from  the  origin. This  constitutes  a severe case  of instability  called  complete instability. EXAMPLE 7.3.  For the system given in Example 5.7 we choose P = 0.28 -0.96 -0.961 0.28 and we compute the matrix C  = A^P  + PA  = -20 0 0 -20 The eigenvalues of P are ±1. According to Theorem 7.3 the equilibrium x  =  0 of this system is unstable (as expected from Example 5.7). • In  applying  the  results  derived  thus  far  in  this  section  we  start  by  choosing (guessing)  a  matrix  P  having  certain  desired  properties.  Next  we  solve  for  the matrix  C using  Eq.  (7.3). If  C possesses  certain  desired  properties  (i.e. it is  neg ative  definite)  we  draw  appropriate  conclusions  by  applying  one  of  the  preceding theorems of this section; if not we need to choose another matrix P. This points to the 474 Linear Systems principal  shortcoming  of Lyapunov's  Direct  Method  when  appHed  to general  sys-terns. However in the special  case of linear systems described by  (L) it is possible to construct  Lyapunov  functions  of the form  v(x)  = x^Px  in a systematic  manner. In doing  so one first chooses the matrix  C in (7.3)  (having  desired properties)  and then  one  solves  (7.3) for  P.  Conclusions  are  then  drawn  by  applying  the  appropri ate results of this section. In applying this construction procedure we need to know conditions under which (7.3) possesses a (unique) solution P for a given  C. We will address this topic next. We consider the quadratic  form v(x)  =  x^Px P =  P^ and the time derivative of v(x)  along the solutions of (L) given by where C =  A^P  + PA v(x)  =  x^Cx C =  C^ (7.5) (7.6) (1.1) and where all symbols are as defined  in (7.1) to (7.3). Our objective is to determine the as yet unknown matrix P in such a way that v(x)  becomes a preassigned  negative definite  quadratic form  i.e. in such a way that  C is a preassigned negative  definite matrix. Equation  (7.7) constitutes  a system of  n(n + l)/2  linear  equations. We need to determine under what conditions we can solve for the n(n +  l)/2 elements pik given C and A. To this end we choose a similarity transformation  Q such that or equivalently QAQ-^  =  A A  = Q-^AQ (7.8) (7.9) where  A is  similar  to A and 2 is  a real nX n nonsingular  matrix.  From  (7.9)  and (7.7) we obtain (AfiQ-^PQ-^  + (Q'VPQ'^A = (Q-VCQ~^ or (A/Q ^QA = C P =  (Q~VPQ~\ C = {Q-^fCQ'K (7.10) (7.11) In (7.11) P and  C are subjected  to a congruence  transformation  and  P  and  C  have the  same  definiteness  properties  as P and  C  respectively.  Since  every  real nX n matrix  can be triangularized  (refer  to Subsection  2.2L) we can choose  Q in  such a fashion  that  A  == [dij] is  triangular  i.e.  dtj  = 0 for / > j. Note  that  in  this  case the eigenvalues  of A Ai... A„ appear  in the main diagonal  of A. To simplify  our notation we rewrite (7.11) in the form  (7.7) by dropping the bars i.e. A^P^-PA  = C C = C^ (7.12) 2indwe assume  that A  = [aij] has been triangularized  i.e. atj  =  0 f o r /> the eigenvalues  Ai... A„ appear in the diagonal of A we can rewrite (7.12) as j.  Since 2Xipn  =  cii ^2iPii  + (Ai  + X2)pn  =  ci2 (7.13) Since this system of equations is triangular and since its determinant is equal to 475 2^Ar  • -A^ n ( A/  +  A/) i^j (7.14) CHAPTER6: Stability the matrix P can be determined  (uniquely) if and only if its determinant is not zero. This is true when all eigenvalues of A are nonzero and no two of them are such that \i  +  Xj  =  0.  This  condition  is  not  affected  by  a  similarity  transformation  and  is therefore  also valid for the original system of equations (7.7). We summarize the above discussion in the following  lemma. LEMMA  7.1.  Let  A  E  /?"^" and  let  Ai... A„ denote  the  (not  necessarily  distinct) eigenvalues of A. Then (7.12) has a unique solution for P corresponding to each  C E /?«><« if and only if A 7^ a  A + Ay 7^ 0 for all / ;. (7.15) • To construct v(x) we must still check the definiteness  of P. This can be done in a purely  algebraic  way; however  in the present  case it is much  easier to apply  the results of this section and argue as  follows: 1.  If  all  eigenvalues  of A have  negative  real  parts  [or equivalently  if  the  equilib rium  jc  =  0 of  (L) is exponentially  stable in the large] and if  C in  (7.7) is neg ative  definite  then  P  =  P^  must  be  positive  definite.  To prove  this  assertion we choose for  (L) the function  v given in  (7.5) with v along the solutions of  (L) given by  (7.6)  and  (7.7). For purposes  of contradiction  we  assume  that P is not positive  definite.  Then  there  exists  XQ T^ 0  such  that  V(JCO)  =  :^o ^^o  —  0.  For the  solution  (pit) with  4>(to)  =  XQ  V((/)(0)  is monotone  decreasing  with  increas ing  t  since  v((/)(0)  — 0.  Also  since  v(0|r=^o  "^  ^QQ^O <  0  it  follows  that  for t  >  to v((f)(t)) <  v(x(to))  =  v(jco)  ^  0.  Since  by  assumption  all  the  eigenval ues  of A have  negative  real  parts  we  know  that  the  equilibrium  JC =  0  of  (L) is  uniformly  asymptotically  stable.  Thus  lim^-^oo v((/)(0)  =  0  which  leads  to  a contradiction. Thus P must be positive  definite. 2.  If  at least  one  of the  eigenvalues  of A has  positive  real  part  and  no real part  of any eigenvalue of A is zero and if (7.15) is satisfied  and if C in (7.7) is negative definite then P cannot be positive definite; otherwise we could apply Theorem 7.2 to come up with  a contradiction.  If in particular the real parts of all  eigenvalues of A  are  positive  then  P  must  be  negative  definite.  [Note  that  in  this  case  the equilibrium  JC =  0 of (L) is completely  unstable.] Now suppose that at least one of the eigenvalues of A has positive real part and suppose  that  any  one  of  the  two  conditions  or both  conditions  given  in  (7.15)  are not satisfied.  Then we cannot construct v(x)  given in (7.5) in the manner  described above (i.e. we cannot determine P in the manner described above). In this case we form  a  matrix  Ai  =  A  -  81  where  /  denotes  the  ^  X n  identity  matrix  and  8  is chosen  so that A\  has as many eigenvalues  with positive real part as A but none of the conditions in (7.15) are violated. Then the equation A\P  +  PAi  =  C with C negative definite can be solved for P and P is then clearly not positive defi nite. The derivative of the function  v(x)  =  x^Px  is a quadratic  form  whose  matrix 476 Linear Systems is of the  form (Ai  +  81/P  +  P(Ai  ^81)  =  C  +  28P which  is negative  definite  for  a sufficiently  small  8.  The function  v(x)  constructed in this way now satisfies  the hypotheses of Theorem 7.3 for  system x  =  Ax. Summarizing  the above discussion we have the following  result. THEOREM 7.4.  If all the eigenvalues of the matrix A have negative real parts or if at least one eigenvalue has a positive real part then there exists a Lyapunov function of the form v(x)  = x^Px  P  =  P^ whose derivative along the solutions of (L) is definite (i.e. it is either negative definite or positive definite). • EXAMPLE 7.4.  We consider the system (L) with 0  11 -1  oj' The eigenvalues of A are Ai A2 =  ±j  and therefore  condition  (7.15) is violated. Ac cording to Lemma 7.1 the Lyapunov matrix equation A^P  + PA  = C does not possess a unique solution for a given  C. We now verify  this for two  specific cases. (i) When C  =  0 we obtain "0 1 - 1] OJ [pn Pn P22. +  Pn .Pn P12I Pii]  [-1 0.  = -2p\2 Pn  -  P22 2/712 .Pn  -  P22 0  0 0  0. ' OT pi2  = 0 and pu  = ^22- Therefore for any a E:  R the matrix P  = al is d. solution of the Lyapunov matrix equation. In other words for C  =  0 the Lyapunov matrix equation has in this example denumerably many solutions (ii) When C  =  -21  we obtain -2/712 Pn  -  P22 Pn  -  P22 2/712 -2 0 0 -2 or Pn  = P22 and pu  =  1 and pu  =  - 1  which is impossible. Therefore for C the Lyapunov matrix equation has in this example no solutions at all. -2/ It turns out that if all the eigenvalues of matrix A have negative real parts then we can compute P in (7.7)  explicitly. THEOREM 7.5.  If all eigenvalues of a real n X n matrix A have negative real parts then for each matrix C E /?"><" the unique solution of (7.7) is given by /•CO Jo (7.16) Proof If all eigenvalues of A have negative real parts then (7.15) is satisfied and there fore (7.7) has a unique solution for every C E R^^f^ To verify that (7.16) is indeed this solution we first note that the right-hand side of (7.16) is well defined  since all eigen values of A have negative real parts. Substituting the right-hand side of (7.16) for P into (7.7) we obtain r°° T f^  T A^P  + PA=\ A^e"^  \-C)e'^'dt+\ e^'i-Qe'^'Adt Jo 0  at Jo All JJ. CHAPTER 6 =  e^X-Oe""'  =  C 10 which proves the theorem. 6.8 LINEARIZATION In this section we consider nonlinear  finite-dimensional  continuous-time  dynamical systems described by equations of the  form w  =  f(w\ (A) where  /  G  C^(R^ R^).  We assume  that  w  =  0 is  an  equilibrium  of  (A). In  accor dance with Subsection  1.11 A we linearize  system (A) about the origin to obtain X G R^  where F  G  C(R^  R^)  and where A denotes the Jacobian of f(w)  evaluated at w  =  0 given by x  =  Ax  + F(x) (8.1) A  =  ^ ( 0 ) dw and where Associated with (8.1) is the linearization  of (A) given by F(x)  =  o(\\x\\) ||;c|| ^  0. as (8.2) (8.3) (L) In the following we use the results of Section 6.7 to establish criteria that allow us to deduce the stability properties of the equilibrium w  =  0 of the nonlinear system (A) from  the stability properties of the equilibrium  y  =  Oof  the linear system (L). y  =  Ay THEOREM  8.1.  Let A  G R''^''  be a Hurwitz matrix let F  G C(/?" R"")  and assume that (8.3) holds. Then the equilibrium x  =  0 of (8.1) [and hence of (A)] is exponentially stable. Proof. Theorem  7.4  applies  to (L)  since all the eigenvalues  of A have negative real parts. In view of that theorem (and the comments following Lemma 7.1) there exists a symmetric real positive definite nX  n matrix P such that where C is negative definite. Consider the Lyapunov function PA + A^P  = C v{x)  = x^Px. The derivative of v with respect to t along the solutions of (8.1) is given by v(x)  =  x^Px  +  x^Px =  {Ax + F{x)fPx  + x^P{Ax  + F{x)) =  x^Cx  + 2x^PF(x). (8.4) (8.5) (8.6) 478 Linear Systems Now  choose  7 <  0  such  that x^Cx  <  3/  \\x\\^ for  all x  G  R^.  Since  it  is  assumed  that (^•^)  holds  there  is  a  5  >  0  such  that  if  ||x||  <  5  then  ||PF(x)||  <  —7  ||x||  for  all :\\x\\<  d}.  Therefore  for  all x  G B{d)  we obtain in view of (8.6) X e  B{d)  =  {xeR'^ the  estimate (8.7) Now  let  a  =  minn^^-n^^ v(x).  Then  a  >  0  (since P  is positive  definite).  Take  A  G v(x)<3r||x||2-27||x||2  =  7||x||2. (0 a )  and let C^={xe B{5)  =  {xeR'': \\x\\ <  5}  : v(x)  <  A}. (8.8) Then C^  C B{5).  [This can be  shown by  contradiction.  Suppose that C^  is not  entirely inside B{5).  Then there is a point xeC^ that lies on the boundary of 5(5). At this point v(x)  >  a  >  A. We have thus arrived at a contradiction.] The set C^  has the property that any solution of (8.1) starting inC^  ait  = to will stay in Q  for ^H ^  >  ^0 ^  0- To see this we  let  (l){ttoxo)  =  0(r)  and  we recall that  v(x)  <  7||x|p7<  0x  G B{d)  D Q-  Then v(0(O)  <  0 implies  that  v{^{t))  <  v(xo)  <  A for  all ^ >  ^o >  0. Therefore  ^{t)  G  Q for  all t>to>0. We now  proceed  in  a  similar  manner  as in the  proof  of  Theorem  7.1  to  complete this proof. In doing  so we first obtain the  estimate v((|)(0)<(^^)v((|)(0) where  7 is given in (8.7) and C2 is determined by the relation ci\\xf  < v{x)=x^Px<C2\\xf. (8.9) (8.10) Following now in an identical manner as was done in the proof of Theorem 7.1 we have m)\\<(-] ||xo|k^/'(^/^^)(^-^°\ t>to>0 (8.11) whenever  XQ  G  5 ( / )  where  /  has  been  chosen  sufficiently  small  so  that  B(r')  C  C^. • This proves that the equilibrium x =  0 of (8.1) is exponentially  stable. It is important to recognize that Theorem  8.1 is a local  result  that yields  sufficient conditions  for  the  exponential  stability  of  the  equilibrium  x  =  0  of  (8.1);  it  does  not yield  conditions  for  exponential  stability  in  the  large.  The  proof  of  Theorem  8.1 however  enables  us  to  determine  an  estimate  of  the  domain  of  attraction  of  the equilibrium x  =  0  of  (A) involving  the  following  steps: 1.  Determine  an  equilibrium  x^  of  (A)  and  transform  (A)  to  a  new  system  that translates Xe to the  origin x  =  0  (refer  to  Section  6.3). 2.  Linearize  (A)  about  the  origin  and  determine  F(x)A  and  the  eigenvalues  of A. If all eigenvalues  of A have negative  real parts choose  a negative  definite  matrix 3. C  and  solve the  Lyapunov  matrix  equation 4.  Determine  the  Lyapunov  function C=A^P^PA. v{x) =x^Px. 5.  Compute  the  derivative  of  V along  the  solutions  of  (8.1) given  by 6.  Determine  5  >  0  such  that  v(jc)  <  0 for  all x  G B{d)  -  {0}. v{x)=x^Cx^2x^PF{x). 479 CHAPTER  b Stability 7.  Determine  the  largest  A  =  AM  such  that  Cx^  ^  ^ ( S )  where CA  =  {x  G  B!" : v(x)  <  A}. 8.  Cxyi is  a  subset  of the  domain  of attraction  of the  equiUbrium  x  =  0 of  (8.1)  and hence  of  (A). The  above  procedure  may  be  repeated  for  different  choices  of  matrix  C  given in  step  (3)  resulting  in  different  matrices  P/  which  in  turn  may  result  in  different estimates  for  the  domain  of  attraction  C\   /  G  A  where  A  is  an  index  set.  The union  of  the  sets  C\^  =  Di  D  =  U/D/  is  also  a  subset  of  the  domain  of  attraction of  the  equilibrium  x  =  0  of  (A). THEOREM  8.2.  Assume  that A is a real  nX  n matrix  that has at least one  eigenvalue with positive real part. Let F  E  C(/?" /?'') and assume that  (8.3) holds. Then the equi librium  X  =  0 of (8.1)  [and hence of (A)] is  unstable. Proof  We  use Theorem  7.4  to choose  a real  symmetric  n  X  n matrix  P  such  that  the matrix PA  + A^P  =  C is negative definite. The matrix P is not positive definite or even positive semidefinite  (refer to the comments following  Lemma 7.1). Hence the  function v(x)  =  x^  Px  is  negative  at  some points  arbitrarily  close  to  the  origin.  The  derivative of  v(x)  with  respect  to  t  along  the  solutions  of  (8.1)  is  given  by  (8.6). As  in  the  proof of Theorem  8.1 we can  choose  a y  <  0 such that  x^Cx  <  3711x11^ for  all x  E  P"  and in  view  of  (8.3)  we  can  choose  a  5  >  0  such  that  ||PF(x)||  <  y\\x\\  for  all  x  E  B(8). Therefore  for all x  E  B(8)  we obtain  that Now let Hx) ^  3r||x|p  -  2r||x|p  = rll^lp. G  =  {xE  B(8)  : v(x)  <  0}. The boundary  of G is made up of points where either v(x)  =  0 or where  ||x||  =  8.  Note in particular that the equilibrium  x  =  0 of (8.1) is in the boundary of G. Now  following an identical procedure as in the proof of Theorem 7.3 we show that any solution (/)(0 of (8.1) with 4>(to)  =  XQ E: G must escape G in finite time through the surface  determined by  Ikll  =  8.  Since the  above argument  holds for  arbitrarily  small  5  >  0 it follows  that • the origin  x  =  0 of (8.1) is unstable. Before  concluding  this  section  we  consider  a few  specific  cases. EXAMPLES.1.  The Lienard  Equation  is given by w  +  g{w)w  +  w  =  0 where g  E  C^(R  R) with ^(0)  >  0. Letting  xi  =  w and  X2  =  w we obtain Xi  =  X2 X2  =  - Xi  -  g(Xi)X2. X2  =  - Xi  -  g{Xi)X2. (8.12) (8.13) Let x^ Then (xi  X2) f{xf  =  (/i(x)  /2(x)) and let "1^(0)  1^(0)" dxi 1^(0)  1^(0) _dX\ 7(0)  =  A  = dX2 dX2 = 0 -1 1 • - ^ ( 0 ). X =  A^ : +  [/(x)  -  Ax]  = Ax +  F(x\ 480 Linear  Systems where F(x) 0 l[^(0)  -  g(Xi)]X2\ The origin x  =  0 is clearly an equilibrium of (8.12) and hence of (8.13). The eigenvalues of A are given by Ai  A2  = JgW and therefore A is a Hurwitz  matrix. Also (8.3) holds. Therefore  all the conditions of Theorem 8.1 are satisfied. We conclude that the equilibrium x  =  0 of (8.13) is  exponen tially  stable. • EXAMPLE  8.2.  We consider the system given by X\  =  -Xi  +  X\{x\  +  x\) X2  =  -X2  +  X2(xj  +  x\). (8.14) The  origin  is  clearly  an  equilibrium  of  (8.14). Also the  system  is  already  in  the  form (8.1) with -1 0 0 -1 F{x)  = \x\{x\  +  X2)l [x2{x\ +  xl)y and  condition  (8.3)  is  clearly  satisfied.  The  eigenvalues  of A are  Ai  =  - 1  A2  =  - 1. Therefore  all conditions of Theorem  8.1  are satisfied  and we conclude that the equilib rium  x^  =  (xi  X2) =  0 is exponentially  stable;  however  we cannot conclude  that  this equilibrium  is exponentially  stable in  the large. Accordingly  we  seek to determine  an estimate for the domain of attraction  of this  equilibrium. We choose C  =  -I  (where /  G R^^^  denotes the identity matrix) and we solve the matrix equation A^P  +  PA  =  C to obtain  P  =  (1/2)/ and  therefore V(xi  X2) =  x^Px  =  \(x]  +  xl). Along the solutions of (8.14) we obtain v(xi  X2) =  x^Cx  +  2x^PF(x) =  -(x^  +  xj)  +  (xi  + xjf. Clearly  v(xi X2) <  0  when  (xi X2) ^  (0 0)  and  Xj  +  ^2  <  1. In  the  language  of  the proof of Theorem  8.1 we can therefore  choose 8  =  1. Now let Ci/2  =  {xeR^: v(xi X2)  =  \(x1  + x\)  <  i}. Then clearly. Cm  C  B(8X  8  =  1 in fact  Cm  =  B(8).  Therefore the set {x  1 X2 <  1} is  a subset  of the domain  of  attraction  of the equilibrium  (xi X2)^ tem(8.14). R^:xj  + •-  0  of  sys-• EXAMPLES.3.  The differential equation governing the motion of a pendulum is given by 6  -\-  asinO  =  0 (8.15) where  a  >  0 is  a constant  (refer  to Chapter  1). Letting 6 the system  description xi  and  6  =  X2 we  obtain Xi  =  X2 X2 = -asinxi. (8.16) The points  x^.^^  =  (0 0)^ and xf^  =  (77 0)^ are equilibria of (8.16). (i) Linearizing (8.16) about the equilibrium x^^\ we put (8.16) into the form (8.1) with A = 0  1 0 -a 481 CHAPTER  6 Stability The eigenvalues of A are Ai A2  =  ±7 V^. Therefore the results of this section (Theo rem 8.1 and 8.2) are not applicable in the present case. (ii) In (8.16) we let y\  ~  x\  -  TT and 3^2 =  ^2- Then (8.16) assumes the form yi -(2sin(yi  +  77). (8.17) The point  (3^1 3^2)^  =  (00)^  is clearly  an equilibrium  of  system  (8.17). Linearizing about this equilibrium we put (8.17) into the form (8.1) where A = 0  1 a  0 F(yh  yi)  = 0 -a(sin(3;i  +77)  +  yO. The eigenvalues of A are Ai A2 =  a -a.  All conditions of Theorem 8.2 are satisfied and we conclude that the equilibrium xf^  =  (TT 0)^ of system (8.16) is unstable. • PART 2 INPUT-OUTPUT STABILITY OF CONTINUOUS-TIME SYSTEMS 6.9 INPUT-OUTPUT STABILITY We now turn our attention to systems described by the state equations X =  A(t)x  +  B(t)u y  =  C(t)x  +  D(t)u (9.1) where  A  G  C(R  T^'^^'^) B  G  C(R  7^"^^)  C  G  C(K  R^'"'')  and  D  G  C(R  RP"""^) [resp.  A  G  C{R^  i?"^^'^)  B  G  C{R^  R'"'"'^)  C  G  C{R+ RP"""")  and  D  G  C(R^ RP^^)].  In the preceding sections of this chapter we investigated the internal  stability properties  of system  (9.1) by  studying the Lyapunov  stability  of the trivial  solution of the associated  system w  =  A(t)w. (LH) In  this  approach  system  inputs  and  system  outputs  played  no role. To account  for these  we  now  consider  the  external  stability  properties  of  system  (9.1)  called input-output  stability:  every  bounded  input  of  a system  should  produce  a  bounded output. More specifically in the present context we say that system (9.1) is  bounded-input/bounded-output  (BIBO)  stable  if for all to and zero initial conditions at f  =  to every bounded input defined  on  [to ^)  gives rise to a bounded response on  [to 0°). A  bounded  matrix  D(t)  does  not  affect  the  BIBO  stability  of  (9.1)  while  an unbounded  D(t)  will give rise to an unbounded response to an appropriate  constant input. Accordingly  we will consider  without  any loss of generality  the case  where D(t)  =  0 i.e.  throughout  this  section  we  will  concern  ourselves  with  systems  de scribed by equations of the  form 482 Linear Systems X  =  A{t)x  4- B{t)u y  =  C(t)x. (9 2) We will find it useful  to use a more restrictive concept of input-output  stability in establishing  various results: we will say that the system  (9.2) is uniformly  BIBO stable  if there  exists  a constant  k>  Q that  is independent  of  ^  such that  for  all  ^ the  conditions X{tQ)  =  0 ||w(0|| ^ 1 t^ to imply that ||j(Oli  — k for all t  >  to. (The symbol || •  || denotes the Euclidean  norm.) It turns out that for the class of problems considered herein BIBO stability  and uniform  BIBO  stability  amount to the same concepts. (We will not however  prove this  assertion  here.)  Accordingly  we will phrase  all  subsequent  results  in terms of uniform  BIBO  stability  rather  than  BIBO  stability.  These  results  will  involve  the impulse response matrix of (9.2) given by Har) = f^«*(''-)^(->'  ^;^' t <  T [  U and the controllability  and observability  Gramian  given respectively by W(to ^i)  - and M(% ^i)  = J/o ch 0 (%  t)B(t)B^(t)^^(to t)dt ^{t  tof  C{tf  C{t)^{t  to) dt (9.3) (9.4) (9.5) In these results we will establish  sufficient  conditions for uniform  BIBO stability of (9.2) and also necessary and sufficient  conditions for uniform BIBO stability of (9.2). Furthermore  we  will  present  results  that  make  a connection  between  the  uniform BIBO  stability  of  (9.2)  and  the  Lyapunov  exponential  stability  of  the  equilibrium w  =  0 of (LH).  In view of the latter results we will usually  assume that to ^  0. At the end of this section we will also present specialized stability results for the time-invariant  systems described by equations of the  form X =  Ax  +  Bu y  ^  Cx (9.6) where  A  G  Z^'^^^" B  E  Z^^^^'^ and  C  G  RP''''.  Associated  with  system  (9.6)  is  the free  system described by equations of the  form Recall that for  system (9.6) the impulse response matrix is given by P  =  Ap H(t)  =  Ce'^'B =  0 t  >  0 /  <  0 and the transfer  function  matrix is given by H(s)  =  C(sl  -A)-^B. (L) (9.7) (9.8) THEOREM 9.1.  The system (9.2) is uniformly BIBO stable if and only if there exists a finite  constant L >  0 such that for all t and to with t > to \\H{tT)\\dr£L. (9.9) 483 CHAPTER 6: Stability The first part of the proof of Theorem 9.1 (sufficiency)  is straightforward.  In deed if ||M(r)|| <  1 for all t^tQ  and if (9.9) is true then we have for all f ^  fo that lb(Oll = to Hit  T)U{T)  dr < \\H(tT)u(T)\\dr Jto rt \\H(tT)\\\\u(T)\\dT I <  \ Jto \\H(tT)\\dT^L. Therefore system (9.2) is uniformly BIBO stable. In proving the second part of Theorem 9.1 (necessity) we simplify  matters by first  considering in (9.2) the single-variable case (n = 1) with the input-output de scription given by y(t)  = h(tT)u(r)dT. (9.10) For purposes of contradiction  we assume that the system is BIBO stable but no finite  L exists such that (9.9) is satisfied. Another way of stating this is that for every finite L there exist to = to(L) and ti  = ^i(L) ti > to such that We now choose in particular the input given by \h(tuT)\dr>  L. to r +1 u(t) = \  0 [ -1 if/z(^T)>0 ifh(tr)  = 0 ifh(tT)<0 (9.11) to ^  t ^  ti. Clearly \u(t)\ <  1 for all t ^  to. The output of the system att  = ti due to the above input however is rti rti y(ti)  = h{ti  T)U{T)dr = \h{tiT)\dT  >  L Jto Jto which contradicts the assumption that the system is BIBO stable. The above can now be extended to the multivariable case. In doing so we apply the single-variable result to every possible pair of input and output vector compo nents we make use of the fact  that the sum of a finite number of bounded  sums will be bounded  and we recall that a vector is bounded if and only if each of its components is bounded. We leave the details to the reader. In the preceding argument we made the tacit assumption that u is continuous or piecewise continuous. However our particular choice of u may involve nonde-numerably  many  switchings  (discontinuities)  over a given finite-time interval.  In such cases u is no longer piecewise continuous; however it is measurable (in the 484 Linear Systems Lebesgue sense). This generalization can be handled though in a broader mathemat-i^al setting  that we do not wish to pursue  here. The interested  reader  may want to refer  e.g. to the books by Desoer and Vidyasagar  [5] Michel and Miller  [17]  and Vidyasagar  [25] and the papers by Sandberg  [21] to [23] and Zames  [26] [27]  for further  details. From Theorem 9.1 and from  (9.7) it follows  readily that a necessary  and suffi cient condition for the uniform  BIBO stability of system (9.6) is the condition \\H(t)\\dt<^. (9.12) Jo COROLLARY 9.1.  Assumc that the equilibrium w =  0 of (L^) is exponentially stable and suppose there exist constants ^  > 0 and y > 0 such that for all t \\B{t)\\  <  j8  and ||C(0|| ^  7- Then system (9.2) is uniformly BIBO stable. Proof Under the hypotheses of the corollary we have H{tT)dTl  <  [  \\H(tT)\\dT ^0 II -'^0 =  f'  \\C(t)^(t T)B(T)\\dT <  7i8 f'  ||cD(^ T)|| Jr. Since the equilibrium w =  0 of (LH) is exponentially  stable there exist 6 > 0 A > 0 such that ||0(r T)|| <  8e~^^^~^\ t >  r. Therefore \\H{tT)\\dT^  [  ypde-^^'-'-Ur to JtQ ySp  A A for all T t with r >  r. It now follows from Theorem 9.1 that system (9.2) is uniformly BIBO stable. • As  indicated  earlier  we seek  to  establish  a  connection  between  the  uniform BIBO  stability  of (9.2) and the exponential  stability  of the trivial  solution of (LH). We will  accomplish  this by means  of an intermediate  result for systems  described by equations of the  form X = A(t)x  + B(t)u ^'' y  = X. ^' (9.13) Before stating and proving the next result we recall that if 5  G R^^^^ J  E R^^^ are  symmetric  then the notation  S  > 0 signifies  that S is positive  definite  and the notation S  > T indicates  that the matrix S -  T is positive definite  i.e. S -  T  > 0. Also  if  Q(t)  =  Q(tf  G C(R /^"><") the condition  that  there  is a constant  r] > 0 such that Q(t)  >  17/ for all ^ E  i? is equivalent to the statement that z^Q(t)z  ^  r]\\z\\^ for all f E  /? and all z E R"". Next  suppose that there is a constant a  > 0 such that ||A(0|| ^  oc for all t ^  R and  let $(^ r)  denote  the state  transition  matrix  of (LH). In the proof  of the next result we will require the estimate ||(I)(rT)||<  ^"^ k - T| <  8. (9.14) To  obtain  this  estimate  we let (f)(t r ^)  =  0 (0  denote  the solution  of  (LH)  with (/)(T)  == ^ and we compute ^Mfm at = ^U(tt  = ktfm  + <i>itfm =  <f){tY Aitf ^{t)  +  (i){tf  Ait)<f){t) at ——^ CHAPTER 6: Stability for  all  t  >  T. Letting  ||(^(0|F  =  v{t)  we  have — < lav V(T)  = ll^lp which  yields or mt ^  e-^'-^m  ^ e-'\U which  in  turn  yields  (9.14). The  case  when  r  >  ns  treated  similarly. THEOREM  9.2.  Suppose that there exist positive constants a  /3 e and 8 such that for all t \\A{t)\\  <  a  \\B{t)\\  <  j8 and ^(^0 ^o +5)  ^  e/  where /  denotes the n X w identity matrix  and  W(-)  denotes  the  controllability  Gramian  given  in  (9.4). Then  the  system (9.13) is uniformly BIBO stable if and only if the trivial solution of {LH) is exponentially stable. Proof  Under  the  above hypotheses it follows  from  Corollary  9.1 that if the trivial so lution of {LH)  is exponentially  stable then  system (9.13) is uniformly  BIBO  stable. Conversely  assume  that  the  system  (9.13)  is  uniformly  BIBO  stable  and  assume that the hypotheses of the theorem are satisfied  with the given constants a  /3 6 and  e. The assumption  W(^ t^ + 8)>  el  ensures that  W~^{tQ to +  8)  exists is bounded  and is independent  of ^o- We now  consider /  =  [ Jr-8 [(I>(T r])B(r])Bir]Y^(T  r]f  d7]W\T -  8 r). (9.15) Since  B(r])  is  bounded  and  since  ^(r  r]) is  bounded  over  |T -  r/l  <  5  there  exists  a constant  c  >  0 such that ||5(r/)^cI)(T r])'^W~\T  -  8 T)||  <  c. (9.16) Premultiplying  (9.15) by ^(t  r)  and using the bound (9.16) and the properties of norms we obtain | | c D a T ) | | < c f'  mrj)B(rj)\\dri. JT-8 (9.17) Since system (9.13) is uniformly  BIBO stable there exists a /: >  0 such that mtr])B(v)\\dri<k Jt-n8 (9.18) for all positive integers n where k is independent  of n and t. From (9.18) it follows  that m.V)B(v)\\dv =  f  mtv)B(ri)\\d7]+[ \\^(t7])B(rj)\\drj t-nd Jt-S +  •••+ rt-n8+8 Jt-28 \\^(tr))B{'n)\\dy)<k. (9.19) From (9.17) to (9.19) it follows  that c-^W^it Oil +  c-^W^iU t  -  8)\\ +  •"  + c-^W^it t-n8  + 5)11 Jt-n8 \\^{ty))B{y])\\d7)<k. t-n8 486 Linear Systems Since the above is true for  any positive integer n we have c'Wl^it t)\\  + \\^(t t  -  d)\\ +  • • • +  \\^{t t -  nd  + 8)\\ +  •••)<  ^ or \\^{t Oil +  ll^a  t -  5)11  +  • • • +  \\^{t t-nd  +  5)11  +  "•  <ck. To complete the proof we must show that \[^  \\^{t y]% drj  is finite. We will accom plish this by  showing that |_^g ||^(^  17)11 drj  is finite for  any positive integer. To this end we observe that for  any n t given there exists a positive integer m such that ||a>(/ T/)|Mr7  <  f \m7j)\\drj. -n8 Jt—m8 If we apply the Mean Value Theorem for Integrals to ||cDar;)||Jr^= f Jt-m8 f  mtv)\\d7i+C Jt~8 Jt-28 110^77)11^7 +  •••+ rt-(m-l)8 Jt-m8 mtv)\\dv> we obtain for  fjt  ^  [t -  id t -  (i  -  1)5] /  =  1...  m that ||(Da 77)11^7 <  [ 110^77)11^7 -nS Jt-m8 <  5[||oa 77011 + l|oa 772)11 + • • • + iioa 77^)111 Now invoking Theorem 5.5(iv)  of this chapter we conclude that the equilibrium w  =  0 • of (LH)  is exponentially  stable. THEOREM  9.3.  Suppose  that  there  exist  positive  constants  a  jS  and  y  such  that ||A(0|| ^  « II^Wll  —  P^ and ||C(0|| ^  y for all t and assume that there exist positive con stants e 1 62 5i and 52  such that for all to  W(to ^o + ^i)  —  ^ i^ and M(to  to + 82)  ^  62/ where M(')  denotes  the observability  Gramian  given  in (9.5). Then  the system  (9.2)  is uniformly  BIBO  stable  if  and  only  if  the  equilibrium  w  =  0 of  (LH)  is  exponentially stable. Proof  Uniform  exponential  stability  of the trivial  solution of (LH)  and the  hypotheses of this  theorem  imply  the uniform  BIBO  stabiHty  of  system  (9.2)  by  Corollary  9.1. To complete the proof we show that the hypotheses  of the theorem and the BIBO  stability of system (9.2) imply the exponential  stability of the equilibrium w  =  0 of  (LH). To set up a contradiction  assume that uniform  BIBO stability of system (9.2) does not imply  exponential  stability  of the trivial  solution  of  (LH).  Then by Theorem  9.2 it must not imply uniform  BIBO stability  of system (9.13). For if there is no bound on the state then there can be no bound on the output. To see this let  w  =  0 on  ^ <  r  <  r +  5 and  obtain t+8 |b(T)|pdr  =  xitf[\ rt+8 Jt <Dft rfdryCirmt r)dr]x{t) =  x(tfM(t t  +  8)x(t)  <  max  d\\y(T)f t<T<t  + 8 and  therefore max ||>;(T)f  >  d-'\\x(t)f\^AM(t t<T<t + 8 t + 5)] where Amin[^(^ t +  5)1 denotes the smallest eigenvalue of M(t  t +  5). Thus if the state X is not bounded for all bounded  w then y will also not be bounded. This shows that the uniform  BIBO  stability  of (9.2) implies the uniform  BIBO stability  of (9.13). Applying Theorem 9.2 we conclude that the equilibrium  w  =  Oof  (LH)  is exponentially  stable. • 487 CHAPTER 6* Stability EXAMPLE  9.1.  We consider the system described by the scalar  equations . X  +  U 1 ^^^' y  =  X. (9.20) This  system is clearly  controllable  and observable. The zero-input response of this sys tem is determined by the differential  equation •^ Jw w(to)  =  xo to >  0. (9.21) It is easily verified  that the solution of (9.21) is given by ct>(ttoxo)=  \ ^ ^o (9.22) [refer to Eqs. (4.5) and (4.6)]. The origin w  =  0 is the only equilibrium of (9.21) and as seen  from  (9.22) this  equilibrium  is uniformly  stable  and  asymptotically  stable;  how ever it is not uniformly  asymptotically  stable and hence it is not exponentially  stable. The state transition matrix of system (9.21) is given by 1'(Uo)  =  y ^. With ^  =  0 and bounded input u(t)  =  1 /  >  0 the zero-state response of system (9.20) is y(tto>xo)= r  t ^(tT)u(T)dT  = Jo r  ^  1  _(_ /T-T-r-dr Jo  ^ + ^ =  ^. (9.23) Summarizing  even  though  the  zero  input  response  of  system  (9.20)  tends  to  zero as  ^ -^  00 the  hypotheses  of  Theorem  9.3  are  not  satisfied  since  this  decay  is  not  ex ponential  and  uniform  [i.e. the  origin  w  =  0 of  (9.21)  is  not  exponentially  stable]. In accordance with Theorem 9.3 we cannot expect the zero-state response of system (9.18) • to be bounded for  arbitrary bounded inputs. This is evident from  expression  (9.23). Next  we  consider  in particular  the  time-invariant  system  (9.6). For  this  system it  is  easily  verified  that  Theorem  9.3  reduces  to  the  following  appealing  result  that connects  the uniform  BIBO  stability  of  system  (9.6)  and  the  exponential  stability  of the  trivial  solution  of  (L). THEOREM  9.4.  Assume  that  the  time-invariant  system  (9.6)  is  controllable  and  ob servable. Then  system  (9.6) is uniformly  BIBO  stable if and only if the trivial  solution • of (L) is exponentially  stable. EXAMPLE9.2.  Consider the  system where "^  " 0  1 i  or .1  0. X =  Ax  +  Bu y  =  Cx 0 ir .1.  ' B  = ^" ^  =  ^^ - H-The eigenvalues  of A are A i  =  1 A2  =  - 1 and therefore  the equilibrium w of the system w  =  Aw  is unstable. The state transition matrix of this system is 488 Linear Systems 0(^ 0)  = and the impulse response of the system is H{t)  =  COaO)5  = -e-\ Thus even though the equihbrium  w  =  0 of w  =  Aw is unstable the system is uni formly BIBO stable. The reason for this is that the system is not observable as is verified by noting that C CA 1 -1 -1 1 which is singular. Thus the hypotheses of Theorem 9.4 are not valid with the conse quence that the unstable mode of the system is not observable at the system output. • EXAMPLE9.3.  We no w consider the system X  =  Ax  ^- Bu y =  Cx where A and B are as in Example 9.2 and C  =  [\  2]. The  eigenvalues  and  the  state  transition  matrix  of  this  system  are  identical  to those of the system given in Example 9.2. This system is both controllable and observable and the impulse response is H{t)  =\e'- \e-\ Since the system is controllable and observable and since the equilibrium w =  0 of the system w  = Aw is unstable it follows from Theorem 9.4 that the system cannot be • uniformly BIBO stable. This can be verified directly by inspecting H(t) above. Next  we  recall  that  a  complex  number  Sp is  a pole  of  H(s)  =  [hij(s)]  if  for some pair (/ j)  we have \hij(sp)\  =  oo (refer to the definition  of pole in Section 3.5). If  each  entry  of  H(s)  has  only  poles  with  negative  real  values  then  as  shown  in Chapter  2 each  entry  of H(t)  =  [hij(t)]  has  a sum of exponentials  with  exponents with real part negative. It follows  that the integral \\H(t)\\dt 0 is finite and  any realization  of H(s)  will result in a system that is uniformly  BIBO stable. Now conversely if r  00 0 \\H(t)\\dt is  finite  then  the  exponential  terms  in  any  entry  of  H(t)  must  have  negative  real parts. But then every entry of H(s)  has poles whose real parts are negative. We have proved the following  result. THEOREM 9.5.  The time invariant system (9.6) is uniformly BIBO stable if and only if all poles of the transfer function H(s) given in (9.8) have only poles with negative real • parts. PART  3 STABILITY  OF  DISCRETE-TIME  SYSTEMS ^ • H ^ ^ H ^ ^H 6.10 DISCRETE-TIME  SYSTEMS 489 ^^^^6: Stability In this section we address the Lyapunov  stability of an equilibrium of  discrete-time systems  (internal  stability)  and  the  input-output  stability  of  discrete-time  systems (external stability). We could establish results for discrete-time systems that are anal ogous  to  practically  all  the  stability  results  that  we  presented  for  continuous-time systems.  Rather  than  follow  such  a  plan  we  will  instead  first  develop  Lyapunov stability  results  for  nonlinear  discrete-time  systems  and  then  apply  these  to  obtain results  for  Hnear  systems.  This  will  broaden  the  reader's  horizon  by  providing  a glimpse  into  the  qualitative  analysis  of  dynamical  systems  described  by  nonlinear ordinary  difference  equations. In the Exercise  section  we  ask the reader  to  imitate these  results  in  establishing  Lyapunov  stability  results  for  dynamical  systems  de scribed by nonlinear ordinary differential  equations. To keep our presentation  simple and manageable we will confine ourselves throughout this section to time-invariant systems. Among other issues this approach will allow us to avoid most of the issues involving  uniformity. This section is organized into seven  subsections. In the first subsection  we pro vide essential preliminary material. In the second we establish results for the stabil ity instability asymptotic stability and global asymptotic stability of an equilibrium and boundedness  of  solutions  of systems  described  by  autonomous  ordinary  differ ence equations. These results are utilized in the third and fifth subsections to arrive at stability  results  of an equilibrium  for  linear time-invariant  systems  described  by ordinary difference  equations. In the fourth  subsection we briefly  address a result for discrete-time  systems called  the  Schur-Cohn  criterion  which  is in the  same  spirit as  the  Routh-Hurwitz  criterion  is  for  continuous-time  systems.  The  results  of  the second  and  fifth  subsections  are used  to develop Lyapunov  stability  results  for Hn-earizations  of  nonlinear  systems  described  by  ordinary  difference  equations  in  the sixth subsection. In the last subsection we present results for the input-output  stabil ity of time-invariant  discrete-time  systems. A.  Preliminaries We concern ourselves here with  finite-dimensional  discrete-time  systems  described by difference  equations of the  form x(k  -\-l)  =  Ax(k)  +  Bu(k) y(k)  =  Cx{k) (10.1) where  A  G  /^'^X" B  G W''''^ C  G  /?^><^ k  ^  k^  and  k ko^Z+. time-invariant  we  will  assume  without  loss  of  generahty  that  ko  =  0  and  thus X  : Z+  ->  7?^  J  : Z+  ->  RP  and w  : Z+  ^  R"^. Since  (10.1)  is The internal  dynamics  of  (10.1) under  conditions  of no input  are described  by equations of the  form 490 ^ x(k  +  1)  =  Axik). (10.2) Such equations may arise in the modehng process or they may be the  consequence of the hnearization  of nonlinear  systems described by equations of the  form x(k  +  1)  =  g(x(k)) (10.3) where  g  : R^  ->  R^.  For  example  if  ^  E  C^{R^ R^)  then  in  linearizing  (10.3) about e.g.  x  =  0 we obtain x(k  +  1)  =  Ax(k)  +  f(x(k)\ (10.4) where A  =  (df/dx)(x)\^^Q  and  where  f  : R^  ^  R^  is  o(\\x\\)  as a norm of x (e.g. the Euclidean norm) approaches zero. Recall that this means that given e  >  0 there is a S >  0 such that \\f(x)\\  <  e||x|| for  all ||x|| <  8. As  in  Section  6.9  we  will  study  the  external  qualitative  properties  of  system (10.1) by  means  of  the BIBO  stability  of  such  systems.  Since  we  are dealing  with time-invariant  systems we will not have to address  any issues  of uniformity.  Con sistent  with  the  definition  of  input-output  stability  of  continuous-time  systems  we will  say that the  system  (10.1) is BIBO  stable  if there exists  a constant  L >  0  such that the  conditions jc(0)  -  0 \\u{k)\\  <  1 yt >  0 imply that \\y{k)\\  <  L for  all fc >  0. We will  study  the  internal  qualitative  properties  of  system  (10.1) by  studying the Lyapunov  stability  properties  of  an  equilibrium  of  (10.2). We  will  accomplish this in a more general context by  studying the stability properties of an  equilibrium of system (10.3). Since system (10.3) is time-invariant we will assume without loss of generality that  ko  =  0. As  in  Chapters  1 and  2 we  will  denote  for  a given  set  of  initial  data x(0)  =  XQ  the  solution  of  (10.3)  by  (p^k  XQ).  When  XQ  is understood  or  of  no  im portance we will frequently  write (/)(fc) in place of (/)(A: XQ).  Recall  that for  system (10.3)  [as well as systems (10.1) (10.2) and (10.4)] there are no particular  difficul ties concerning the existence  and uniqueness  of solutions and furthermore  as long as g in  (10.3) is continuous the  solutions  will be  continuous  with respect  to initial data. Recall also that in contrast to systems described by ordinary differential  equa tions the  solutions  of  systems  described  by  ordinary  difference  equations  [such  as (10.3)] exist in general only in the forward  direction of time (k  >  0). We say that Xe E  R^  is an equilibrium  of system  (10.3) if cl)(k Xe) =  Xe for  all /: >  0 or equivalently g{Xe)  =  Xe. (10.5) As in  the continuous-time  case we  will  assume  without  loss  of  generality  that  the equilibrium of interest will be the origin i.e. Xe =  0. If this is not the case then we can  always  transform  (similarly  as in the continuous-time  case)  system  (10.3)  into a  system of equations  that have an equilibrium  at the origin. Also as in the case of continuous-time  systems  we  will  generally  assume  that  the  equilibrium  of  (10.3) under  study is an isolated  equilibrium. EXAMPLE  10.1.  The system  described  by the equation x{k  +  1)  =  x{k)[x{k) -  1] has  two equihbria  one dX Xe\  =0  and another  at Xg2  =  2. EXAMPLE10.2.  The system  described by the equations xi{k  +  1)  =  ^lik) X2(k +  1) =  -^i(^) has an equiUbrium at xf  =  (0 0). 491 CHAPTER 6: Stabihty • • Throughout this section we will assume that the function  g in (10.3) is continu ous  or if required continuously differentiable.  The various definitions  of Lyapunov stability  of  the  equilibrium  x  == 0  of  system  (10.3)  are  essentially  identical  to  the corresponding definitions of Lyapunov stability of an equilibrium of continuous-time systems described by ordinary  differential  equations replacing  ^ G /?+  by  ^  G  Z+. Since  system  (10.3) is time-invariant  we will not have to explicitly  address the is sue of uniformity  in these  definitions.  We will concern  ourselves  with  stability  in stability asymptotic stability and asymptotic stability in the large of the equilibrium X =  Oof  (10.3). We  say that the  equilibrium  x  == 0 of  (10.3)  is stable  if  for  every  e  >  0  there exists  3.  8  =  S(e)  >  0  such  that  \\4>(k  xo)\\  <  e  for  all  /: ^  0  whenever  ||xo|| <  8. If  the  equilibrium  x  =  0  of  (10.3)  is  not  stable  it  is  said  to  be  unstable.  We  say that  the  equilibrium  x  =  0 of  (10.3)  is asymptotically  stable  if  (i) it is  stable  and (ii) there  exists  an  17 >  0 such that  if  ||xo|| <  17 then  lim^^^oo ||</)(^ xo)||  =  0. If  the equilibrium  x  =  0 satisfies  property  (ii) it is  said  to be attractive  and  we call  the set  of  all  XQ G  R^  for  which  x  =  0  is  attractive  the  domain  of  attraction  of  this equilibrium. If x  =  0 is asymptotically  stable and if its domain of attraction is all of R^  then it is said to be asymptotically  stable  in the large or globally  asymptotically stable.  Finally we say that a solution of (10.3) through  XQ is bounded  provided there is a constant M  such that ||(/)(/: xo)|| ^  M  for all ^  >  0. In establishing  results for  the various  stability  concepts  enumerated  above  we will make use of  auxiliary  functions  v G  C{R^ R)  called Lyapunov functions.  We define  the first forward  difference  ofv  along  the solutions  (9/(10.3) as Dv{x)  =  v(g(x))  -  v(x). (10.6) To see that this  definition  makes  sense note that  along any  solution  (/)(^) of  (10.3) we have v[ct(k  +  1)] -  v[cl>(k)]  =  v[g{cl>(k))]  -  vmk)] ^^Q ^^ =  Dvmk)] for  all fc >  0.  Note  that  in  evaluating  the  first  forward  difference  of  v  along  the solutions of (10.3) we need not know explicitly the solution (/>(/:) of (10.3). We will require several characterizations of the Lyapunov functions. We say that a function  v G  C(R^  R)  is positive  definite  if  v(0)  =  0 and if v(x)  >  0 for  all  x G B{j])  -  {0} for  some ry >  0.  [Recall that B{j])  =  {x G /?" : ||x|| <  ry}.] The  function V is negative  definite if  -v  is positive definite. A function  v G  C{R^ R) is said to be positive  semidefinite  if v(0)  =  0 and if v(x)  >  0 for all x  G ^(17) and it is said to be negative  semidefinite  if  -v  is positive semidefinite. A positive definite  function  v is said to be radially  unbounded  if v(x)  >  0 for all x  G  7?" -  {0} and if limiij^iuoo v(x)  = 492 Linear Systems 00. Finally a function  v E  C(R^  R) is said to be indefinite if v(0)  =  0 and if in every neighborhood  of the origin v assumes positive and negative values. B.  Lyapunov  Stability  of an  Equilibrium In establishing various Lyapunov stability results of the equilibrium x  =  0 of system (10.3)  we will find it useful  to employ  a preliminary  result  that  is important  in  its own right. To present  this result  we require  some  additional  concepts  given  in  the following. We  say  that  a  subset  A  C  7^" is positively  invariant  [with  respect  to  system (10.3)]  if  g{A)  C  A where  g{A)  =  {y  E. R""  \ y  =  g{x)  for  some x  G A}. Thus if X EL  A  then g{x)  G A. EXAMPLE 10.3.  It is clear that any set consisting of an equilibrium solution of (10.3) is positively invariant. Also the set {^{k XQ) k E Z^}  [where 4> is the solution of (10.3) with (/)(/co)  =  ^o] which is called i\\Qpositive orbit of XQ for (10.3) is positively invari ant. In particular for arbitrary initial conditions x(0)^  == (^i(O) ^2(0)) the system given in Example 10.2 has iho periodic solution with period 4 and positive orbit given by the set A =  {(xi(0) X2(0))^ (X2(0) -xmV (-xm -^2(0))^ (-X2(0) xmYl and in general we have (f)(k XQ) = (t)(k -\-  3 xo\  k  = 0 1 2 3 positively invariant. The set A is clearly • Next consider a specific  solution (/)(/: XQ) for  system  (10.3). A point y  E  R"^  is called Si positive  limit point  of 4>{k  XQ) if there is a subsequence {hi} of the sequence {k} /: >  0 such that (f){ki  XQ) -^  y. The set of all positive limit points of (f){k  XQ) is called the positive  limit set (o(xo) of (/)(/: XQ)  or simply the co-limit set. Before  stating  and  proving  our  first  result  which  is  a  preliminary  result  we recall  that  a  sequence  {x^}  C  R^  is  said  to  approach  a  set  A  C  R^  if  d(x^  A)  = inf {||x^  -  y\\ : y  E  A} approaches zero as fc ^  oo  [d(x y) denotes the distance  func tion defined  in Subsection  I.IOC]. THEOREM  10.1.  If  the  solution  (t)(k XQ) of  (10.3)  is bounded  for  all  k  E  Z+ then (o(xo) is a nonempty compact positively invariant set. Furthermore (t)(k XQ) -^  w(jco) as  /: ^  00. Proof The complement of (O(XQ) is open and therefore the set (w(xo) is closed. Since 4>(k XQ) is bounded  ||(^(/: xo)|| ^  M  for  some fixed M. Therefore  (o(xo)  is bounded and IIJII <  M for all y  E  (O(XQ). Thus (JO(XO) is compact. Since (f>(k  XQ) is bounded the Bolzano-Weierstrass Theorem guarantees the existence of at least one limit point and therefore (x)(xo) is not empty (refer to Subsection 1.5A). Next let y E co(xo) so that (/)(A:/ XQ) -^  y as / ^  OO.  Since the function  g in (10.3) is continuous it follows that (l)(ki  + I XQ) = g{4^{ki  XQ)) -^  g{y) or g{y) E  O){XQ). The set  O){XQ)  is positively  invariant  [with respect  to  system  (10.3)]. Also d{(j){k  XQ) CL>) is  bounded  since  both  (j){k XQ) and  w(xo)  are  bounded.  To  set  up  a  contradiction assume  that  d{(f){k  XQ) O){XQ)) does  not  converge  to  zero.  Then  there  is  a  subse quence  [ki] such  that  (/)(/:/ XQ) ->  y  and d((p(ki XQ) (i^(xo)) ->  a  >  0.  But  then  y E ct)(xo) d{(j){ku  XQ) (I>{XQ))  <  d{(f)(ku  XQ) y) -^  0 and therefore  d(<p(ki  XQ) CO(XO)) -^ 0 which is a contradiction. • We are now  in  a position  to  state  and prove  several  Lyapunov  stability  results for  system (10.3). 493 CHAPTER 6-Stability THEOREM  10.2.  The  equilibrium  x  =  0  of  system  (10.3)  is  stable  if  there  exists  a positive definite  function  v such that Dv  is negative  semidefinite. Proof  We take r/  >  0 so small that v(x)  >  0 for all x  E  5(7]) -  {0} and Dv(x)  <  0 for all X E  5(77) [recall that Birf)  =  {x  ^  R^  \ ||x|| <  r/}]. Let e  >  0 be given. There is no loss of generality  in choosing  0  <  e  <  17. Let m  =  min {v(x)  : \\x\\  =  e}. Then m is positive since we are taking the minimum  of a positive continuous  function  over a compact  set. Let  G  =  {x  B  R^  : v(x)  <  mil}  which  may  consist  of  several  disjoint  connected  sets called  connected  components  of  G. Let  Go denote  the  connected  component  of  G  that contains  the  origin  x  =  0.  Then  both  G  and  Go  are  open  sets.  Now  if  XQ E  GO  then DV{XQ)  <  0  and  so  v(^(xo))  ^  V{XQ)  <  m/2  and  therefore  g^xo)  E  G.  Since  XQ  and the origin  x  =  0 are both in the same component  of G then  so are ^(0)  =  0 and g(xo). Therefore  Go is an open positively invariant set containing x  =  0 and contained in  B{e). Since  v is  continuous  there  is  a  5  >  0  such  that  B{b)  C  Go. Therefore  if  xo  E  B{d) then  Xo  E  Go and ^(xo)  E  Go C  B{e).  This shows that the equilibrium x  =  0 of system (10.3) is stable. • EXAMPLE  10.4.  For  the  system  given  in  Example  10.2  we  choose  the  function v(xi X2)  =  x\  + x\.  Then Dv(xi X2)  =  v(g(xi X2)) -  v(xi X2) =  (xif  +  (-xif -  xj- xl  =  0. Therefore  by Theorem  10.2 the equiUbrium  x  =  0 of the system is stable. • By  using  a  similar  argument  as  in  the  proof  of  Theorem  10.2  we  can  prove  the boundedness  result  given  below.  We  will  not present  the  details  of  the  proof. THEOREMIO.3.  Ifvis  radially  unbounded  and Dv(x)  <  0 on the set where ||x|| >  M (M is some constant) then all solutions of system (10.3) are bounded. • EXAMPLE  10.5.  For  the  system  given  in  Example  10.2  we  choose  the  radially  un bounded  function  v(xi X2)  =  x^  +  X2. As  shown  in Example  10.4 Dv{x\  X2) ^  0  for all X E  /?^.  It follows from Theorem  10.3 that all the solutions of the system are bounded. • By  definition  the  equilibrium  x  =  0  of  system  (10.3)  is  asymptotically  stable if  it  is  stable  and  attractive.  Theorem  10.2  provides  a  set  of  sufficient  conditions for  the  stability  of  the  trivial  solution  of  system  (10.3).  In  the  next  result  known  as 's  Theorem  or the Invariance  Principle  we  present  a method  that  enables  us LaSalle to determine the attractivity  of a set. If this set consists only of the equilibrium  x:  =  0 this result yields a method  of determining  the attractivity  of the trivial  solution  x  =  Q of  system  (10.3). Let  V E  C{R''  R)  and  let  v(x)  -  c.  In the  following  we  let v~^{c)  =  {x  B  R""  : v(x)  =  c}. THEOREM  10.4.  Let V  E  C(/?" R)  and let G  C  R"". Assume that (i) Dv(x)  <  0 for  all X G  G  and  (ii) the  solution  4>(k XQ) of  (10.3)  is in  G for  all  A: >  0 and is bounded  for all  ^  >  0. Then  there  is a number  c  such  that  (/)(^ xo) ^  M  Pi v~^(c) where M  is  the largest positively invariant  set contained in the set E  =  {x  G R'^ : Dv(x)  =  0} D G. Proof.  Since  (f)(k  XQ)  is  bounded  and  in  G  it  follows  that  co(xo) 7^ 0  CL)(XO)  C  G and  (/)(/: xo)  tends  to  a>(xo). Now  v{(j){k  XQ))  is  nonincreasing  with  increasing  k  and bounded  from  below.  Therefore  v(4>(k  XQ))  ->  c.  If  y  G  CL)(XO) there  is  subsequence {ki}  of  the  sequence  {k}  such  that  (/>(/:/ xo) -^  y  and  therefore  v{(f)(ki xo)) ->  v{y)  or v(y)  =  c. Therefore  V{(X){XQ)) =  c [i.e. v(y)  =  c for all y  E  a;(xo)] or ^(xo)  C  v~^(c). Since v(6o(xo))  =  c and a;(xo) is positively invariant it follows that Dv(o;(xo))  =  0 [i.e.. 494 Linear Systems Dv(y)  =  0 for all y  G co(xo)]. Therefore  0(^ XQ) -^  (x)(xo)  C {x E  /?" : Dv(x)  =  0} Pi G fl v^^c). Since co(xo) is positively invariant it now follows  that a)(xo)  CM. • The  next  results  which  are  direct  consequences  of  Theorems  10.2 10.3 and 10.4  were  originally  established  by  Lyapunov. COROLLARY  10.1.  The equilibrium  x  =  0 of (10.3) is asymptotically  stable  if there exists a positive definite  function  v such that Dv is negative  definite. Proof.  Since Dv is negative definite  and v is positive definite it follows  from  Theorem 10.2  that  the equilibrium  x  =  0 is  stable.  From  the proof  of that  theorem  there  is an arbitrarily  small neighborhood  Go of the origin that is positively invariant. We can make Go so small that v(x) >  0 and v(x)  < 0 for all x G Go -  {0}. So given  any xo G Go it follows  from  the invariance  principle  (Theorem  10.4) that  (pik XQ) tends to the largest invariant  set in Go H {Dv(x)  =  0} =  {x  =  0} since Dv is negative  definite.  Therefore the equilibrium  x  =  0 is asymptotically  stable. • COROLLARY  10.2.  The equilibrium  x  == 0 of (10.3)  is asymptotically  stable  in the large (or globally  asymptotically  stable)  if there exists a positive definite function  v that is radially unbounded and if Dv is negative definite on R"" i.e. Z)v(O)  =  0 and Z)v(x)  <  0 for all X  7^ 0. • Proof  From Theorem  10.3 all solutions  of system  (10.3)  are bounded. The proof  now • follows by modifying  the proof of Corollary  10.1 in the obvious way. EXAMPLE  10.6.  Consider the system described by the equations xi(k+  1)  = X2(k  +  1)  = ax2(k) 1  +  xi(y^)2 bxi(k) 1 + X2(k)^' where it is assumed that a^ <  I and b^ <  I. We choose a Lyapunov function  v{x\ X2)  = x\  +  x\  that  is  positive  definite  and radially  unbounded.  Along  the  solutions  of  this system we compute the first forward  difference  as ^ v t e  . . ) = ^^  + ^ ^ - ( x?  + 4) (1 + x\y 2\2 1  Xo  + b^ 1 2\2 (1 + 4> <  {a^ -\)xl  + (b^  -  l)x?. Since  by  assumption  a^ <  I  and Z?^ <  1 it follows  that  Dv(xi  X2) <  0 for all x^  = (xi X2) 7^ 0 and Dv(xi  X2)  =  0 when x  =  0. It follows  from  Theorem  10.3 that all so lutions of the system are bounded and from Corollary  10.2 it follows that the equilibrium • X =  0 of the system is globally  asymptotically  stable. EXAMPLE  10.7.  We reconsider the system given in Example  10.6 under the assump tion that a^ ^  I and Z?^ <  1 but a^ + b'^ 7^ 2. Without loss of generality we consider the case a^  <  1 and b^  =  1. As in Example  10.6 we again choose v(xi X2)  =  x^ + x^ and from the computations in that example we see that Dv(xu  X2) <  (a^ -  \)x\  +  Q? - \)x\ =  {a^ -  \)x\  <  0 (xi X2)^ G R^. It still follows  from  Theorem  10.3  that all solutions of the system are bounded. 495 CHAPTER 6* stabilitv Since Dv(xi X2) is not negative definite but negative semidefinite we cannot apply Corollary 10.2 to establish the asymptotic stability of the equilibrium x  =  0. So let us try to use Theorems 10.2 and 10.4 to accomplish this. From the former we conclude that the equilibrium is stable. Using the notation of Theorem  10.4 we note that E  = {(xi 0)^} which is the xi-axis. Now g((xi OY)  =  (0 bxiY  =  (0 xi)^  [where g(') is defined in (10.3)] and therefore the only invariant subset of (E) is the set consisting of the origin. All conditions of Theorem  10.4 are satisfied  (with G  = R^) and we conclude that the • equilibrium x  =  0 of the system is globally asymptotically stable. THEOREM10.5.  Let Dv be positive definite and assume that in every neighborhood of the origin there is x such that v(x) >  0. Then the equilibrium x  =  0 of system (10.3) is unstable.  [Alternatively let Dv be negative definite and assume that in every neighbor hood of the origin there is x such that v(x) <  0. Then the equilibrium x  =  0 of system (10.3) is unstable.] Proof To set up a contradiction assume that x  =  0 is stable. Choose e >  0 so small so that Dv{x) > 0 for all x G B{e) -  {0} and choose 6 >  0 so small so that if  XQ G B(8) then 4>{k  XQ) G B{e) for all ^ >  0. By hypothesis there is a point XQ G B{3) such that v(xo) > 0- Since  (j){k XQ) is bounded  and remains  in 5(e)  the  solution  (f){k XQ) will tend to its Hmit set {x G /?" : Dv{x)  =  0} Pi B{e)  = {0}. Since (/)(/: XQ) -^  0 we have v((l)(k XQ)) -»  v(0)  =  0. But Dv((l)(k xo)) >  0 and therefore v((/)(^ XQ)) >  0 and thus v(4>(k Xo)) ^  v(cl)(k-l  Xo)) >  ••• >  v(xo) >  0. We have thus arrived at a contradiction that proves the theorem. • EXAMPLE 10.8.  The system described by the equation x(k  +1)  =  2x(k) has an equilibrium  x  =  0. The function  v(x)  =  x^ is positive definite  and along the solutions of this system we have Z)v(x)  =  4x^-x^  = 3x^ which is also positive definite. The conditions of Theorem 10.5 are satisfied and the equilibrium x  =  0 of the system is unstable. • C.  Linear  Systems In proving some of the results of this section we require a result for system (10.2) that is analogous to Theorem  3.1  of Chapter  2. As in the proof  of that theorem  we note that the linear combination of solutions of system (10.2) is also a solution of  system (10.2) and hence the set of solutions {4>:  Z^  XR^  -^  R^} constitutes a vector space (over F  =  Ror  F  =  C).  The dimension  of this vector  space is n. To show this we choose a set of linearly independent vectors  XQ ...  XQ  in the n-dimensional  x-space (R^  or  C^)  and  we  show in  an identical  manner  as in the proof  of Theorem  3.1 of Chapter 2 that the set of solutions (/)(fc XQ) i  =  1...  n is linearly independent and spans the set of solutions of system (10.2). (We ask the reader in the Exercise section to provide the details of the proof of the above assertions.) This yields the  following result. THEOREM 10.6.  The set of solutions of system (10.2) over the time interval Z+ forms • an ^-dimensional vector space. Incidentally  if  in  particular  we  choose  (/)(^ e^) i  =  I..  .n  where  e\  i  = 1...  n  denotes  the  natural  basis  for  R^  and  if  we  let  ^{k  yto  =  0)  =  0(A;)  = [ct){k e^)... (pik e")] then it is easily verified  that the n  X n matrix 0(A:)  satisfies 496 Linear Systems the matrix  equation ci>(k  +  1)  -  A^(k\ c|)(0)  = / and that (5(fc)  =  A^k^ 0 [i.e. 3>(/:) is the state transition matrix for system (10.2)]. THEOREM  10.7.  The  equiHbrium  x  =  0 of  system  (10.2)  is  stable  if  and  only  if  the solutions of (10.2) are bounded. Proof.  Assume that the equilibrium x  =  0 of (10.2) is stable. Then for 6  =  1 there is a 8  >0  such that \\cl)(k xo)\\ <  1 for all  ^  >  0 and all ||jco|| <  6. In this  case U(k  xo)\\  =  WA'xoW  = A^XQS m\\ M 8 <  8 for  all  xo  7^ 0  and  all  k  >  0.  Using  the  definition  of  matrix  norm  [refer  to  (10.17)  of Chapter  1] it  follows  that  ||A^|| <  8~\  A:  >  0.  We have  proved  that  if  the  equilibrium X =  0 of (10.2) is stable then the solutions of (10.2) are bounded. Conversely suppose that all solutions (j6(/: XQ)  =  A'^XQ are bounded. Let{^\  ...  ^"} denote the natural  basis  for  ^-space  and let  \\(p(k e^)\\ <  jBj for  all  k  >  0. Then  for  any vector  XQ =  S"=i  cty^^ we have that ll^(^xo)|| ^^aj(f)(k 7 =  1 e^) Xk-|i8;^(max^.)Xl^il 7 =1 ^ 7 =1 c\\m 0 for  some  constant  c. For given  6  >  0  we choose  8  =  elc.  Then  if  ||xo|| <  8  we  have \^{k  xo)|| <  c||xo|| <  6 for  all  /: >  0. We have proved  that if the  solutions  of (10.2)  are bounded then the equilibrium  x  =  0 of (10.2) is stable. • THEOREM  10.8.  The following  statements  are  equivalent: (i)  The equilibrium  x  =  0 of (10.2) is asymptotically  stable. (ii)  The equilibrium  x  =  0 of (10.2) is asymptotically  stable in the large (iii)  lim^^oo ||Ai  =  0. Froof  Assume that statement (i) is true. Then there is anry  >  0 such that when ||xo||  ^ 17 then ^{k  XQ) ->  0 as  /: -^  00. But then we have for any  XQ ^  0 that (^{k xo)  =  A^xo Ikoll 17x0 Ikoll 0  as  /: ^  00. It follows  that statement  (ii) is true. Next  assume  that  statement  (ii)  is  true.  Then  for  any  e  >  0  there  must  exist  a K  =  K{e)  such  that  for  ?^k>  K  we have  that  \^{k  xo)||  =  ||A^xo|| <  6.  To see this let  {^^  ...  ^"} be  the  natural  basis  for  R^.  Thus  for  a  fixed  constant  c  >  0  if  XQ  = (a\... ji  -^  c. For eachy there is a Kj  -  Kj{e)  such that  \A^e^ K(€)  =  max {Kj(e)  : j  =  1...  n}. For ||xo| 2:;.i«y^^'and2:;=iK <6/cfory^>  TTy. Define TT <  1 and  k  >  K  we  have that anf  andif ||xo|| ^ lthenxo ll^'^oll  = 7 =  1 7 =  1 By the definition  of matrix norm [see (10.17) of Chapter  1] this means that ||A^|| <  e  for k>  K.  Therefore  statement  (iii) is true. Finally assume that  statement  (iii) is true. Then  ||A^|| is bounded  for  all  /c >  0. By Theorem  10.7 the equilibrium  x  =  0 is stable. To prove asymptotic  stability fix e  >  0. If  llxoll <  77 =  1 then  \\(t)(k  xo)\\  <  \\A^\\  \\xo\\ ^  0 as  y^ ^ true. This completes the proof. 00.  Therefore  statement  (i) is 497 • CHAPTER  6: Stability To arrive  at the next result  we  make  reference  to the results  of  Subsection  2.7E. Specifically  by  inspecting  the  expressions  for  the  modes  of  system  (10.2)  given  in (7.50)  and  (7.51)  of  Chapter  2  or by  utilizing  the  Jordan  canonical  form  of A  [refer to  (7.54)  and  (7.55)  of  Chapter  2] the  following  result  is  evident. THEOREM  10.9.  (i) The equilibrium  x  =  0 of system  (10.2) is asymptotically  stable if and only if all eigenvalues  of A are within the unit circle of the complex plane (i.e. if Ai...  A„ denote the eigenvalues  of A then  \Xj\ <  I j  =  1...  ^). In this case we say that the matrix  A  is Schur  stable  or simply the matrix  A  is  stable. (ii) The  equilibrium  x  =  0 of  system  (10.2)  is stable  if  and  only  if  \Xj\ <  1 j  = .n  and for each eigenvalue with  |Aj|  =  1 having multiplicity  rij  >  1 it is true that 1.. }^^S^^^'-'^^"'^''-^^"^\='^ h...nj- 1. (iii) The equilibrium x  =  0 of system (10.2) is unstable  if and only if the conditions • in (ii) above are not true. Alternatively  it is  evident  that  the  equilibrium  x  =  0  of  system  (10.2)  is  stable if  and  only  if  all  eigenvalues  of  A  are  within  or  on  the  unit  circle  of  the  complex plane  and  every  eigenvalue  that  is  on  the  unit  circle  has  an  associated  Jordan  block of  order  1. EXAMPLE  10.9.  (i) For the system in Example  10.2 we have A  -0  11 -1 0 The eigenvalues of A are Ai A2  =  ±  v  - 1-  According to Theorem  10.9 the equilibrium x  =  0 of the system is stable and according to Theorem  10.7 the matrix A^ is bounded for all k^O. (ii) For system (10.2) let 0 -1 The  eigenvalues  of A  are  Ai A2  =  ±1/V2.  According  to  Theorem  10.9  the  equilib rium  X =  0  of  the  system  is  asymptotically  stable  and  according  to  Theorem  10.8 lim^^^ooA^  =  0. (iii) For system (10.2) let A The eigenvalues of A are Ai A2  =  ±  V3/2. According to Theorem  10.9 the equilibrium X =  0 of the  system  is  unstable  and  according  to Theorem  10.7 the  matrix  A^ is  not bounded  with increasing  k. (iv) For system  (10.2) let A  = 1  1 0  1 The  matrix A is  a Jordan  block  of  order  2 for  the  eigenvalue  A =  1. Accordingly  the equilibrium  x  =  0  of  the  system  is  unstable  (refer  to  the  remark  following  Theorem • 10.9) and the matrix A^ is unbounded  with increasing k. 498 Linear Systems D.  The Schur-Cohn  Criterion In this section we present a method called the Schur-Cohn  criterion  that enables us to determine whether or not the roots of a polynomial with real coefficients  given by +  • • • + ^0 an > 0 (10.8) lie inside of the unit circle in the complex plane by examining the polynomial  coef ficients  rather than  solving for the roots. This method provides us with an efficient means of studying the stability of the equilibrium x  =  0 of system (10.2) by apply ing it to the characteristic  polynomial of the matrix A. The Schur-Cohn  criterion is in the same spirit  [for discrete-time  systems  (10.2)] as the Routh-Hurwitz  criterion [for  continuous-time  systems  (L)]. In presenting  this  criterion  we will  require the concept of inners of a square matrix A defined  as the matrix itself and all the ma trices obtained by omitting  successively the first and last rows and the first and last columns. In the following  we depict the inners for a matrix A E R^^^ and a matrix A  G i?6x^: an an cin <2i4 «21 1  Cl22 «23 <324 ^25 A  = ^ 31 1  ^32 1 (233 (234 (241 1 ^ 42 (243 CI44 Cl5l «52 «53 ^54 <335 (245 ass. A  = (211 <321 (231 (241 asi (261 an ail <332 ^ 42 asi (262 L ai3 ^23 «33 <^43 <353 ^ 63 ai4 (224 ^ 34 (244 asA (264 (215 ^25 ^35 (245 ciss aes (216 ^26 <^36 (246 (256 ^66 A  matrix is said to be positive  innerwise  if the determinants  of all of its inners are positive. THEOREM 10.10. (SCHUR-COHN CRITERION)  A neccssary and sufficient  condi tion that the polynomial (10.8) with real coefficients has all its roots inside the unit circle in the complex plane is that (i)  p(l) > 0 and (- 1)V(-1)  > 0 (ii)  the following (n —  I) X (n —  1) matrices are both positive innerwise: 0 Cln-l as (32 as 0 0 Cln~\ Cln. 0 (3o (31 0 (3o (31 (10.9) 0 (30 ai (3^n-l ^ n -1 We  will  not present  a proof  of Theorem  10.10. The interested  reader  should consult Jury  [91 cited in the reference  section for a proof of this result. EXAMPLE 10.10.  For the 2 X 2 matrix A = (311 CI2I an ail. the characteristic polynomial is given by p(\)  =  A^  +  (3iA  +  (3o. We have p(l)  =  I + ai  + ao p(-l)  =  1  -  ai  + ao and Af  =  1  ± (^o  >  0. Therefore the roots of p(X) he within the unit circle of the complex plane if and only if  |ao| <  1 and |ai| <  1  + ao. • EXAMPLElO.il.  For the 3 X 3 matrix 499 CHAPTER  6: Stability an dii .asi the characteristic polynomial is given by an «22 ^32 a i3 <223 <^33 We have p(l)  =  1 + ^2 + ai  + <3o  -p(-l) =  I -  a2  + ai  ao and p(X)  =  A^  + (22 A^  + ai\  + ao. det{^^)  = 1 a2  ± ao  1 -ao ±ai > 0. Therefore the roots of p(X) lie within the unit circle of the complex plane if and only if Wo  + a2\  < I + ai and \ai -  aoa2\  <  I -  al. • E.  The Matrix  Lyapunov  Equation In this subsection we apply the Lyapunov theorems of Subsection B to obtain another characterization of stable matrices. This gives rise to the Lyapunov matrix equation. Returning to system (10.2) we choose as a Lyapunov  function v(x)  =  x^Bx B  =  B^ (10.10) and we evaluate the first forward  difference  of v along the solutions of (10.2) as v{x{k  +  1)) -  v{x{k))  =  x(k  +  lfBx(k +  1) - x(kfBx(k) =  x{kfA^BAx{k) - x{kfBx{k) =  x{kf{A^BA -  B)x(k\ and  therefore Dv(x)  =  x^{A^BA -  B)x  =  x^Cx where A^BA  -B  =  Q C^  =  C. (10.11) Invocation of Theorem  10.2 Corollary  10.2 and Theorem  10.5 readily leads to the following  results. THEOREM  10.11.  (i) The equilibrium x  =  0 of system (10.2) is stable if there exists a real symmetric and positive definite matrix B such that the matrix C given in (10.11) is negative semidefinite. (ii) The equilibrium x  =  0 of system (10.2) is asymptotically stable in the large if there exists a real symmetric and positive definite matrix B such that the matrix C given in (10.11) is negative definite. (iii) The equilibrium ;\f  =  0 of system (10.2) is unstable if there exists a real sym metric matrix B that is either negative definite or indefinite such that the matrix C given in (10.11) is negative definite. • In applying Theorem  10.11 we start by choosing  (guessing)  a matrix B  having certain desired properties and we then solve for the matrix C using equation (10.11). 500 Linear Systems If C possesses certain desired properties (i.e. it is negative definite) we can draw ap-propriate conclusions by applying one of the results given in Theorem  10.11; if not we need to choose another matrix B. This approach is not very satisfactory  and in the following  we will derive results that will allow us (as in the case of  continuous-time systems)  to  construct  Lyapunov  functions  of  the  form  v{x)  =  x^Bx in  a  system atic  manner.  In  doing  so we  first  choose  a matrix  C in  (10.11)  that  is either  nega tive  definite  or positive  definite  and  then  we  solve  (10.11)  for  5.  Conclusions  are then made by applying Theorem  10.11. In applying this construction procedure we need  to  know  conditions  under  which  (10.11)  possesses  a  (unique)  solution  B  for any  definite  (i.e. positive or negative definite)  matrix  C. We will address this issue next. We first show  that  if A is  stable i.e.  if  all eigenvalues  of  matrix A  [in  system (10.2)]  are  inside  the  unit  circle  of  the  complex  plane  then  we  can  compute  B  in (10.11) explicitly. To show this we assume that in  (10.11)  C is a given matrix  and that A  is stable. Then (A^)^+i5A^+i  -  {A^fBA^  = (A^fCA^ and summing from  ^  =  0 to / yields A^BA  -B  + {A^fBA^ -  A^BA  + B  = ^{A^fCA^ / or (A^y^^BA^^^ -B== Letting  / ^  oo^ we obtain k = 0 I k = 0 ^(A^)^CA^. B  =  -^(A^)^CA\ k = 0 (10.12) It is easy to verify  that (10.12) is a solution of (10.11). We have - A^  ^(A^)^CA^ A  +  ^(A^)^CA^ -  C k = 0 k = 0 -A^CA  +  C  -  (A^fCA^  +  A^CA  -  (A^fCA^  +  (A^fCA^ or Therefore  (10.12) is a solution of (10.11). Furthermore if C is negative definite then B is positive  definite. =  C. Combining the above with Theorem  lO.ll(ii)  we have the following  result: THEOREM  10.12.  If there is a positive definite  and symmetric matrix B and a neg ative definite  and symmetric matrix  C satisfying  (10.11) then the matrix A is stable. Conversely if A is stable then given any symmetric matrix C Eq. (10.11) has a unique solution and if C is negative definite then B is positive definite. • Next we determine conditions under which the system of equations (10.11) has a (unique) solution B  =  B^  E  R^^^  for a given matrix C  =  C^  E:  R^^^  To accom plish this we consider the more general  equation A1XA2-X  =  C (10.13) where A\  E  R"^^^^  A^  G  R""^"" and X  and  C are m X n matrices. LEMMA 10.1.  Let Ai  G /^'^^^^ and A2 G /?"><^ ThenEq. (10.13) has a unique solution X E R^^^  for a given C E  /^'^x" if and only if no eigenvalue of A\  is a reciprocal of an eigenvalue of A2. Proof We need to show that the condition on Ai and A2 is equivalent to the condition that A1XA2 =  X implies X  ^  0. Once we have proved that A1XA2 =  X has the unique solution X  =  0 then it can be shown that (10.13) has a unique solution for every C since (10.13) is a linear equation. Assume first that the condition on Ai and A2 is satisfied. Now A1XA2 =  X implies that A\~^XA\~^  = X and 501 CHAPTER 6* Stability' A{X  =  A\XA\~^ for  y^ >  7  >  0. Now for a polynomial of degree k k 7 =0 we define the polynomial of degree k from which it follows that /(A) = X^i^'"'' = ^'/^fT\ 7 =0 p{A)X  =  A\Xp\A2l Now let (/)/(A) be the characteristic polynomial of A/ /  =  1 2. Since (f)\ (A) and (/>2(A) are relatively prime there are polynomials p{\)  and ^(A) such that p{\)cj>{K) + q{k)cf>l{K)  = 1. Now  define  (j>{X)  =  q(X)(f)l(X)  and  note  that  (/)*(A)  =  ^*(A)(/)2(A).  It  follows  that (/)*(A2) =  0 and (p(Ai) = I. From this it follows that A1XA2 =  X impUes X  = 0. To prove the converse  we assume  that  A  is  an eigenvalue  of  Ai  and  A~^ is an eigenvalue  of  A2  (and  hence  is  also  an  eigenvalue  of  A^).  Let  Aix^  =  Xx^ and A^x^  =  X'^x^  x^ ¥^  0 and  x^ 7^  0. Define X  =  {x\x\  xlx\  ...  JC^X^). Then X 7^  0 and A1XA2 =  X. • To construct v{x)  by using Lemma  10.1 we must still check the definiteness  of B.  To accomplish this we utilize Theorem  10.11. 1.  If all eigenvalues of A [for system (10.2)] are inside the unit circle of the complex plane then no reciprocal of an eigenvalue of A is an eigenvalue and Lemma  10.1 gives  another  way  of  showing  that Eq.  (10.11)  has  a unique  solution B  for  each C if A is stable. If  C is negative  definite  then B is positive definite.  This can be shown as was done for the case of linear ordinary differential  equations. 2.  If  at  least  one  of  the  eigenvalues  of A is  outside  the  unit  circle  of  the  complex plane  and  if  no  reciprocal  of  an  eigenvalue  of A  is  an  eigenvalue  and  if  C in (10.11) is negative definite then B cannot be positive definite; otherwise we could apply  Theorem  lO.ll(iii)  to  come  up  with  a  contradiction.  If  in  particular  all eigenvalues of A are outside the unit circle of the complex plane then B must be negative definite.  [In this case the equilibrium  of (10.2) is completely  unstable.} Now  suppose  that  at  least  one  of  the  eigenvalues  of A  is  outside  of  the  unit circle in the complex plane and suppose that the conditions of Lemma  10.1 are vio lated (i.e. an eigenvalue of A is the reciprocal of an eigenvalue of A). Then we cannot 502 Linear Systems construct  v(x)  given  in  (10.10)  in the manner  described  above  (i.e. we  cannot  de termine B in the manner  described  above). To overcome this  difficulty  we form  in this case the matrix Ao =  (1 + lir"'A (10.14) and  we  choose  |j8| arbitrarily  small  and  in  such  a manner  so that  AQ has  no  eigen values on the unit circle and has the same number of eigenvalues outside of the unit circle of the complex plane as matrix A.  Then  AQ satisfies  the conditions of Lemma 10.1. For every given  C  =  C^  ^  R^^^  we can now  solve the equation AlBAo B  =  C (10.15) to  obtain  a  unique  matrix  B.  We  use  this  matrix  to  form  the  Lyapunov  function (10.10). Now since A  =  (1 + /3)^^^Ao the first forward  difference  Dv(x)  of the Lya punov function  v(x)  =  x^Bx  along the solutions of (10.2)  yields Dv{x)  =  X'^IAIBAQ -  B  + [5AIBAO]X =  x^{C  +  I3AIBAO)X = x^Cx (10.16) where  C is  given  in  (10.15). It now  follows  that  if  C is negative  definite  (positive definite)  then  we  can  choose  |jS| sufficiently  small  so that  C  will  also be  negative definite  (positive  definite). Summarizing  the above discussion we have proved the following  result. THEOREM  10.13.  If all the eigenvalues of the matrix A are within the unit circle of the complex plane or if at least one eigenvalue is outside the unit circle of the complex plane then there exists a Lyapunov function  of the form v{x)  = x^Bx B  = B^ whose first  forward  difference  along the solutions of system (10.2) is definite  (i.e. it is either negative definite or positive definite). • We conclude this subsection with some specific  examples. EXAMPLE 10.12.  (i) For system (10.2) let A = 0  n •1  0 Let B  = I which is positive definite. From (10.11) we obtain C  = A^A  -  I ro 1 -11 o| r  0  1 [-1  0 1  0 0  1 0  0 0  0 It follows  from  Theorem  lO.ll(i)  that the equilibrium  x  =  0 of this system is stable. This is the same conclusion that was made in Example 10.9. (ii) For system (10.2) let A = Choose which is positive definite. From (10.11) we obtain r  0 [-1 r^  01 3 ^ 0  ^ 3  J L^ A^BA  -  B = -11 oj 0 1 2 L n 2 oJ r^  0" 3 ^ 0  ^ 3-L^ -1 [  0 0" - ij which is negative definite. It follows from Theorem 10.1 l(ii) that the equilibrium x  = 0 of this system is asymptotically stable in the large. This is the same conclusion that was made in Example 10.9(ii). (iii) For system (10.2) let 503 CHAPTER  6: Stability Choose A = 0 -3 -i 0 C  = -1 0 0 -1 which is negative definite. From (10.11) we obtain pll [b\2 C  =  A^BA  -B  = bn'] Z722J [  0 L-3 n 2 0. bn bu bn b22. 0 1 L  2 -1 0 -3^ 0^ 0] ' - ij m22-bn) [ \bn \bn {\bn  - b22). which yields B = -8 0 0 -1 which is also negative definite. It follows from Theorem 10.11 (iii) that the equilibrium X =  0 of this system is unstable. This conclusion is consistent with the conclusion made in Example 10.9(iii). (iv) For system (10.2) let I  1 0  3 The eigenvalues of A are Ai  =  |  and A2 =  3. According to Lemma 10.1 for a given C Eq. (10.13) does not have a unique solution in this case since Ai  =  I/A2. For purposes of illustration we choose C  =  - /.  Then -I  =  A'^BA -  B [^11 [bl2 .1  3j bn] ^22] H  1^ [0  3.  =  bn bn bn b22. 1^11 ^bn 1^11 bn  + €>bi2  + 8^22. -1 0 0  -- 1. which shows that for C  =  - /  Eq. (10.13) does not have any solution (for B) at all. R  Linearization In  this  subsection  we  determine  conditions  under  which  the  stability  properties  of the equilibrium  w  =  0 of the linear  system w(k+  1)  =  Aw(k) (10.17) determine the stability properties of the equilibrium  x  =  0 of the nonlinear  system under the assumption  that f(x)  =  o(\\x\\)  as ||x||  -^  0 (i.e. given  e  >  0 there  exists x(k  +  1)  =  Ax(k)  +  f(x(k)) (10.18) 504 Linear Systems S  >  0 such  that  \\f(x(k))\\  <  e\\x(k)\\  for a lU  >  0 and all \\x(k)\\  <  8).  [Refer  to the discussion  concerning  Eqs.  (10.2)  to (10.4)  in Subsection  A of this  section.] THEOREM  10.14.  Assumethat/  G C(R''  R"") and thai f(x)  is o(\\x\\)sis\\x\\  -»  0. (i)If A is stable (i.e. all the eigenvalues of A are within the unit circle of the complex plane) then  the equilibrium  x  =  0 of  system  (10.18)  is asymptotically  stable  (ii) If at least one eigenvalue of A is outside the unit circle of the complex plane then the equilibrium X =  0 of system (10.18) is  unstable. Proof  (i) Assume that A is stable. Then for any negative definite matrix C the equation has a unique positive definite  solution B. Let A^BA -B  = C v(x)  —  x^Bx. Along the solutions of (10.18) we compute the first forward  difference  Dv(x) as Dv(x)  =  x^Cx  + 2x^A^Bf(x) +  v(/(x)). This allows us to estimate  [since f(x)  is 6>(||x||) as ||x|| -^  0] Dv(x)  <  AM(C)|WP  + 2||x|| ||A|| \\B\\ \\f(x)\\  +  v(/(x)) <  AM(C)||X|P  + 2||A|| \\B\\e\\xf  + AM(5)6^||X|P (10.19) =  [AM(C)  +  2||A|| \\B\\e  +  XM(B)e^\xf  =  y||x|p for  all X E  B(8) and for some 6 >  0 where  AM(C)  <  0 XM(B) > 0 denote the largest eigenvalues  of C and B respectively.  We can make e  as small  as desired  by choosing 8  sufficiently  small  resulting  in y  <  0. Therefore  Dv(x)  is negative  definite  and the equilibrium  x  =  0 of system (10.18) is asymptotically  stable. (ii) Assume that A has at least one eigenvalue outside the unit circle of the complex plane. Following the procedure given in proving Theorem  10.13 [refer to Eqs. (10.14) to (10.16)] we construct a Lyapunov function  v(x)  =  x^Bx  whose first forward  difference along the solutions of (10.17) is given by Dv(x)  =  x^Cx  [refer to (10.16)]. We choose C  to be negative  definite.  Then B is indefinite  and in every  neighborhood  of the  origin there are x  G R^ such that v(x)  =  x^Bx  < 0. Next we evaluate the first forward  difference  of v along the solutions of (10.18) to obtain  [identically  as in (10.19)] Dv(x)  <  [AM(C)  +  2||A|| \\B\\€  +  AM(B)6^]||X|P  =  y\\xf for  all X G B(8)  for  some  6  >  0 where  C is defined  by (10.14)  to (10.16). As in the proof  of part  (i) we can force  e  to be as  small  as desired  by choosing  8  sufficiently small  resulting  again  in y  <  0. Therefore  Dv(x)  is negative  definite.  It follows  from Theorem  10.5 that the equilibrium  x  =  0 of (10.18) is unstable. • Before  concluding  this  subsection  we consider  some  specific  examples. EXAMPLE  10.13.  (i) Consider the system Xi(k+l)  =  -{X2(k)  + Xi(kf  +  X2(kf X2(k +  1)  =  -xi(k)  +  xi(kf  +  X2{kf. Using the notation of (10.18) we have A  = 0 -1 -1 0 f{M  X2) The linearization of (10.20) is given by w(k +  1) -  Aw(k). (10.21) From Example 10.9(ii) [and Example 10.12(ii)] it follows that the equilibrium w = 0 of (10.21) is asymptotically stable. Furthermore in the present case f(x)  =  o(\\x\\) as \\x\\ ->  0 Therefore in view of Theorem 10.14 the equilibrium x  =  0 of system (10.20) is asymptotically stable. (ii) Consider the system 505 CHAPTER 6: Stability Xi(/:+  1)  =  -^X2(k)  + xi(kf  + X2(kf X2(k +  1)  =  -3x1 (y^) + xt(k)  -  X2{k)\ Using the notation of (10.17) and (10.18) we have in the present case A  = 0 3 i 2 0. / ( • ^b - ^ 2) X-\  T"  X9 Since A is unstable [refer to Example 10.12(iii) and Example 10.9(iii)] and since f{x)  = o(\\x\\) as ||x|| ->  0 it follows from Theorem 10.14 that the equilibrium x  =  0 of system (10.22) is unstable. • G.  Input-Output  Stability We conclude  this chapter  by considering  the input-output  stability  of  discrete-time systems described by equations of the  form x(k  +  1)  -  Ax(k)  +  Bu(k) y(k)  -  Cx(kl (10.23) where  all matrices  and vectors  are defined  as in  (10.1). Throughout  this  subsection we will assume that  ^0  =  0.  -^(0)  =  0 and  ^ >  0. As  in  the  continuous-time  case  we  say  that  system  (10.23)  is BIBO  stable  if there exists a constant  c  >  0 such that the conditions x(0)  =  0 \\u(k)\\ <  1 k^O imply that ||3;(^)||  <  c for all fc >  0. The results that we will present involve the impulse response matrix of  (10.23) given by H(k)  = fCA^-^B [ 0 k>0 yk  <  0 and the transfer  function  matrix given by Recall that H(z)  =  C{zl-  AY^B. n y(n)  =  ^H(n- k)u{k). (10.24) (10.25) (10.26) 506 Linear Systems Associated with system (10.23) is the free dynamical system described by the equa-^^^^ (10.27) THEOREM 10.15.  The system (10.23) is BIBO stable if and only if there exists a con stant L >  0 such that for all n >  0 p{k  +  1)  -  Ap{k) X  11^(^)11  ^L. (10.28) As in the continous-time case the first part of the proof of Theorem  10.15 (suf ficiency)  is  straightforward.  Specifically  if  ||w(fe)|| <  1 for  all  /: >  0 and if  (10.28) is true then we have for all n  >  0 \\y{n)\\ =  \\^H(n -  k)u(k)\\  ^J^\\H(n -  kMk)\\ Therefore  system  (10.23) is BIBO  stable. In proving  the  second  part  of  Theorem  10.15  (necessity)  we  simplify  matters by first considering  in (10.23) the single-variable  case (n  =  1) with the system de scription given by t y(t)  =  ^h(t- k = 0 k)u{k) t  >  0. (10.29) For purposes of contradiction we assume that the system is BIBO stable but no finite L exists such that (10.28) is satisfied. Another way of expressing the last assumption is that for any finite L there exists t  =  ki(L)  =  ki  such that ^1 ^ k = 0 \h(ki  ~  k)\ >  L. We now choose in particular the input u given by + 1 if  h(t- k)>0 u(k)  =  ^ 0 ifh(t-k) =  0 [-1 ifh(t- k)<0 0  <  k  ^  ki.  Clearly  \u(k)\  <  1 for  all  k  ^  0. The output  of the  system  ait  =^  ki due to the above input however  is y(ki)  =  ^  h(ki  -  k)u(k)  =  ^ \h(ki  -  k)\ >  L which contradicts the assumption that the system is BIBO  stable. The above can now be extended to the multivariable case. In doing  so we apply the  single-variable  result  to  every  possible  pair  of  input  and  output  vector  compo nents we make use of the fact that the sum of a finite number of bounded  sums will be bounded and we note that a vector is bounded if and only if each of its components is bounded. We leave the details to the reader. Next we establish  a connection between the asymptotic  stability  of the equilib rium p =  0 of system (10.27) and the BIBO stability of system (10.23). First we note that the  asymptotic  stability  of the  equilibrium x  =  0 of  system  (10.27)  implies  the BIBO stability of system (10.23) since the sum 507 CHAPTER  6: Stability <l\\c\ k=l \k-l\ B is finite. The  main  task  in  proving  the  converse  to  the  above  statement  is  to  show  that controllability  and  observability  of  system  (10.23)  and  the  finiteness  of  the  sum Er=i  IIC'A^"^^!! imply the finiteness of the  sum S^Li  ||^^~^ ||- ^^  ^^^ ^^ assume that the system (10.23) is BIBO stable. Then  since \\yik)\ 7=0 we must have that the power  series k-l <  y  \\CA''-^J+^^B\\ I —  .^ II y X-u  II \\CA^-^B\\ II k=\ is finite (i.e. absolutely convergent) and this implies that limCA^"^5 =  0. (10.30) From (10.30) we can conclude that lim CAA^-^B  =  lim CA^'^AB  =  lim CA^B = 0 and repeating we arrive at lim{CA^)A^-\A'B) = 0 ^r  =  0  1  . ..   n-  1. We can write this as lim C CA CA n-\ A^-^[5A5--A^-^5]=0. (10.31) If we now assume that (10.23) is controllable  (from-the-origin)  and observable then we can select n linearly independent columns of the controllability matrix to form an invertible n x n matrix W and n linearly independent rows of the observability  matrix to form  an invertible n x n matrix M.  Using (10.31) we conclude that l i m M A ^ -V  =  0 (10.32) 508 Linear Systems which  yields M-'[limMA^-^w]w-^ =  limA^-^  =  0. (10.33) From  (10.33)  we can conclude that the equilibrium  j:?  =  0 of (10.27)  is  asymptoti cally stable. To prove this assertion we assume to the contrary that j:?  =  0 of (10.27) is not asymptotically stable. This implies that A has an eigenvalue A with |A| >  1. We assume the case when A is real and leave the case when A is complex as an exercise for the reader. Let ry be an eigenvector associated with A (which must be real). Then A^T]  =  X^rjk>  0. If  17 =  XQ denotes  an initial condition then the  corresponding solution  of  (10.27)  is  given  by  x{k)  =  A^T]  =  X^TJ fc >  0  which  does  not  go  to zero as ^ —>  00 i.e. limy^_oo A^~^  7^  0 which contradicts (10.33). {Note:  The above proof  solves Exercise 6.31 for the case where the eigenvalues  are real.) We have thus arrived at the following  result. THEOREM  10.16.  Assume that system (10.23) is controllable  and observable. Then system (10.23) is BIBO stable if and only if the equilibrium /? =  0 of system (10.27) is asymptotically stable. • Next  we  recall  that  a  complex  number  Zp is  dipole  of  H(z)  =  [hjiz)] if  for some (/ j)  we have  \hij(zp)\  =  ^  (refer  to Section  3.5 for  the definition  of a pole). If each entry of H(z)  has only poles with modulus  (magnitude)  less than  1 then as shown in Chapter  2 each entry of H(k)  =  [hij(k)]  consists of a sum of  converging terms. It follows  that under these conditions the  sum 00 k = 0 is finite and any realization of H(z)  will result in a system that is BIBO  stable. Conversely if 00 k = 0 is finite then  the terms  in every  entry  of H(k)  must be convergent.  But then  every entry of ^(z) has poles whose modulus is within the unit circle of the complex plane. We have proved the final result of this  section. THEOREM 10.17.  The time-invariant system (10.23) is BIBO stable if and only if the poles of the transfer  function are within the unit circle of the complex plane. • H(z)  =  C(zI-A)-^B 6.11 SUMMARY In this chapter we first addressed the stability  of an equilibrium  of  continuous-time finite-dimensional  systems  (Part  1). In  doing  so we first introduced  the concept of equilibrium and defined  several types of stability in the sense of Lyapunov  (Sections 6.3  and 6.4). Next we established  several  stability  conditions of an equilibrium  for 509 CHAPTER 6: Stability linear time-varying  systems  {LH)  in terms of the state transition matrix and for lin- ear  time-invariant  systems  L  in  terms  of  eigenvalues  (Section  6.5).  In  Section  6.6 we  established  several  geometric  and  algebraic  stability  criteria  for  nth-order  lin- ear  time-invariant  systems  [including  the  Leonhard-Mikhailov  stability  criterion (Theorem  6.1)  the  gap  and  position  stability  criterion  (also  called  the  interlacing stability criterion)  (Theorem  6.2)  and the Routh-Hurwitz  criterion  (Theorem  6.4)]. Next  we  established  various  stability  conditions  for  linear  time-invariant  systems that are phrased  in terms  of the Lyapunov  Matrix Equation  for  system  (L)  (Section 6.7). In Section  6.8  we established  conditions  under  which  the asymptotic  stability and  the instability  of  an equilibrium  for  a nonlinear  time-invariant  system  (A)  can be deduced from  the linearization  of (A). Then in Part 2 we addressed the input-output stability of time-varying and time-invariant linear continuous-time  and  finite-dimensional  systems  (Section  6.9). For such  systems  we  established  several  conditions  for  bounded  input  bounded  output stability  (BIBO  stability)  and we related  some of these to the stabiHty properties of an  equilbrium. The  chapter  concluded  with  Part  3  (Section  6.10)  where  we  addressed  the Lyapunov  stability  and  the  input-output  stability  of  (time-invariant)  systems.  For such  systems  we  established  results  that  are  analogous  to  the  stability  results  of continuous-time  systems  considered  in Parts  1 and 2.  [Among other topics we dis cussed  in  Subsection  6.10D  the  Schur-Cohn  criterion  (which  is  analogous  to  the Routh-Hurwitz  criterion  for  continuous-time  systems)  and  in  Subsection  6.10E we  established  stability  criteria  involving  the  Lyapunov  Matrix  Equation  for  the discrete-time  case.] 6.12 NOTES The initial contributions to stability theory that took place toward the end of the last century  are primarily  due to physicists  and mathematicians  (Lyapunov  [14]) while input-output stability is the brainchild of electrical engineers (Sandberg  [21] to [23] Zames  [26]  [27]).  Sources  with  extensive  coverage  of  Lyapunov  stability  theory include e.g. Hahn [7] Khalil [11] LaSalle [12] LaSalle andLefschetz  [13] Michel and Miller [17] Michel and Wang [18] Miller and Michel [19] and Vidyasagar [25]. Input-output  stability  is addressed  in great detail in Desoer  and Vidyasagar  [5] and Vidyasagar  [25]. For  a  survey  that  traces  many  of  the  important  developments  of stability in feedback  control refer  to Michel [15]. In  the  context  of  linear  systems  nice  sources  on  both  Lyapunov  stability  and input-output  stability  can be found  in numerous texts including Brockett  [2] Chen [3] DeCarlo  [4] Kailath  [10] and  Rugh  [20]. In  developing  our presentation  we found  the  texts  by  Brockett  [2] Hahn  [7] LaSalle  [12] Miller  and  Michel  [19] and Rugh  [20] especially helpful. For a proof of the Schur-Cohn criterion and other related results refer  to the elegant book by Jury [9]. The background material summarized in the second section is developed in most standard linear algebra texts including the classic books by Birkhoff  and  MacLane [1]  Gantmacher  [6] and  Halmos  [8]. For  more  recent  references  on  this  subject refer  e.g. to the books by Michel and Herget  [16] and Strang [24]. 510 Linear  Systems 6.13 R E F E R E N C ES 1.  G.  Birkhoff  and  S.  MacLane  A  Survey  of  Modern  Algebra  Macmillan  New  York 1965. 2.  R. W. Brockett Finite  Dimensional  Linear  Systems  Wiley New York  1970. 3.  C. T. Chen Linear  System  Theory  and Design  Holt Rinehart  and Winston New  York 1984. 4.  R. A. DeCarlo Linear  Systems  Prentice-Hall Englewood  Cliffs  NJ  1989. 5.  C. A. Desoer and M. Vidyasagar Feedback  Systems:  Input-Output  Properties  Academic Press New York  1975. 6.  F. R. Gantmacher  Theory  of Matrices  Vols. I II Chelsea New York  1959. 7.  W  Hahn Stability  of Motion  Springer-Verlag New York  1967. 8.  P. R. Halmos Finite  Dimensional  Vector Spaces  Van Nostrand Princeton NJ  1958. 9.  E. I. Jury Inners  and Stability  of Dynamical  Systems  Robert E. Krieger Publisher Mal abar PL  1982. 10.  T. Kailath Linear  Systems  Prentice-Hall Englewood  Cliffs  NJ  1980. 11.  H. K. KJialil Nonlinear  Systems  Macmillan New  York  1992. 12.  J.  P. LaSalle  The  Stability  and  Control  of  Discrete  Processes  Springer-Verlag  New York  1986. 13.  J. P. LaSalle  and  S. Lefschetz  Stability  by Liapunov's  Direct  Method  Academic  Press New York 1961. 14.  M.  A.  Liapounoff  "Probleme  generate  de  la  stabilite  de  mouvement"  Ann.  Fac.  Sci. Toulouse  Vol.  9  1907  pp.  203-474.  (Translation  of  a  paper  published  in  Comm. Soc.  Math.  Kharkow  1893  reprinted  in  Ann.  Math.  Studies  Vol.  17  1949  Prince ton NJ.) 15.  A. N. Michel "Stability: the common thread in the evolution of feedback  control"  IEEE Control  Systems  Vol.  16 1996 pp. 50-60. 16.  A. N. Michel  and C. J. Herget Applied  Algebra  and  Functional  Analysis  Dover  New York 1993. 17.  A. N. Michel and R. K. Miller  Qualitative  Analysis  of Large  Scale  Dynamical  Systems Academic Press New York  1977. 18.  A. N. Michel  and K. Wang  Qualitative  Theory  of Dynamical  Systems  Marcel  Dekker New York  1995. 19.  R. K. Miller and A. N. Michel  Ordinary  Differential  Equations  Academic Press New York  1982. 20.  W. J. Rugh Linear  System  Theory Second Edition Prentice-Hall Englewood CHffs NJ 1996. 21.  I. W. Sandberg "On the L2-boundedness of solutions of nonlinear functional  equations" BellSyst.  Tech. J. Vol. 43 1964 pp.  1581-1599. 22.  I. W.  Sandberg  "A  frequency-domain  condition  for  stability  of feedback  systems  con taining  a  single  time-varying  nonlinear  element"  Bell  Syst.  Tech.  J.  Vol.  44  1974 pp.1601-1608. 23.  I. W. Sandberg  "Some results on the theory of physical  systems  governed  by  nonlinear functional  equations" Bell  Syst.  Tech. J. Vol. 44  1965 pp. 821-898. 24.  G. Strang Linear Algebra  and its Applications  Harcourt Brace Jovanovich San Diego 1988. 25.  M. Vidyasagar Nonlinear  Systems Analysis  2d edition. Prentice Hall Englewood  Cliffs NJ  1993. 26.  G.  Zames  "On  the  input-output  stability  of  time-varying  nonlinear  feedback  systems. Part I" IEEE  Transactions  on Automatic  Control  Vol. 11 1966 pp. 228-238. 27.  G.  Zames  "On  the  input-output  stability  of  time-varying  nonlinear  feedback  systems. Part II" IEEE  Transactions  on Automatic  Control  Vol.  11 1966 pp. 465-476. 6.14 E X E R C I S ES 6.1.  Determine the set of equilibrium points  of a system  described by the differential  equa tions 511 CHAPTER 6: StabiHty Xi  =  Xi  -  X2  +  X3 X2  =  2xi  +  3X2  +  -^3 ^3  =  3JCI  +  2X2  +  2X3. 6.2.  Determine the set of equilibria of a system described by the differential  equations Xi  =  X2 ^2  =  S xi  sin  —   when  xi  T^ 0 0 when  xi  =  0. 6.3.  Determine the equilibrium points and their stability properties of a system described by the ordinary  differential  equation x  =  x(x-l) (14.1) by solving (14.1) and then applying the definitions  of stability uniform  stability asymp totic stability etc. 6.4.  Determine the set of equilibria and their stability properties of a system described by the ordinary differential  equation X =  (cost)x (14.2) by solving (14.2) and then applying the definitions  of stability uniform  stability asymp totic stability etc. 6.5.  Determine the set of equilibria and their stability properties of a system described by the ordinary  differential  equation X =  (4tsmt-2t)x (14.3) by solving (14.3) and then applying the definitions  of stability uniform  stability asymp totic stability etc. 6.6.  Determine the state transition matrix ^(t  to) of the  system U\ [x2 0  1 -2t\ _{2t  -  t) X2_ Xi' -t Use  Theorems  5.1  to  5.4  to determine  the  stability  properties  of  the  trivial  solution  of this  system. 6.7.  Show that the second-degree  polynomial f{s)  =  s^ + 2as  + b is  a  Hurwitz  polynomial  if  and  only  if  a>  0  and  Z?  >  0  by  (i)  solving  the  equation f{s)  =  0 and (ii) using the Routh-Hurwitz  criterion  (Theorem  6.4). 6.8.  Determine whether the third-degree  polynomial f(s)  =  s^ + 3s^ + 3s  + 2 512 Linear Systems is  a  Hurwitz  polynomial  by  (i)  solving  the  equation  f(s)  =  0  (ii)  applying  Theorem ^•^'  ^^^^^ ^PP^yi^g Theorem  6.2 and (iv) applying Theorem  6.4. 6.9.  Let A  G  C[/?+ /?"><"] and x  G 7?" and  consider X =  A(t)x. (LH) Show  that  the  equilibrium  x^  =  0  of  (LH)  is  uniformly  stable  if  there  exists  a  2  G C^ [/?+ Z?^"""] such that 2 (0  =  [Q(t)f  for all r and if there exist constants C2 >  ci  >  0 such that cil  <  2 (0  <  C2/ r G R (14.4) and such that [A(0]^2(0  +  Q(t)A(t)  + m (14.5) where /  is the  fz X ^  identity  matrix. Hint:  The proof  of this  assertion  is  similar  to  the proof of Theorem 7.1. <  0 t^R 6.10.  Show that the equilibrium  Xg =  0 of {LH) is exponentially  stable  if there exists a 2  ^ C^/?^^''''''] such that 2 (0  =  [2(01^  for alU and if there exist constants C2 >  ci  >  0 and C3 >  0 such that (14.4) holds and such that [A(0]^2(0  +  Qit)A{t)  +  2 (0  ^  -C3I t  G R. (14.6) Hint:  The proof  of this assertion  is similar to the proof  of Theorem  7.2. 6.11.  Assume that the equilibrium Xg =  0 of {LH) is exponentially  stable and that there exists a constant a>  0 such that ||A(0||  ^  a for  all t  G R  Show that the matrix given by 2 (0  -^  j [a)(T0]'^O(T0^T (14.7) satisfies  the hypotheses  of the result  given  in Exercise  10. Hint:  The proof  of this as sertion is similar to the proof of Theorem 7.5. 6.12.  For  {LH)  let  A^(0  and  AM(0  denote  the  smallest  and  largest  eigenvalues  of  A{t)  + [A{t)]^  at r G /? respectively. Let (/)(^ ^o XQ) denote the unique solution of {LH) for the initial  data  x{to)  =  XQ =  (^{t^ to x).  Show  that  for  any  XQ  G  R^  and  any  ^o G  R  the unique solution of {LH) satisfies  the estimate ||^(^^ ^^^ ^^^11 ^ 11^^11^(1/2)1; A„(.).x  ^ ||^^||^o/2)(; A(.)..^ ^ ^  ^^^ ^j4 g^ //mL- Let v(r ^0 XQ)  =  [4>{t to xo)V[(l){t to xo)]  =  \\(l){t to xo)p evaluate v{t to xo) and then estabHsh (14.8). 6.13.  Use Exercise  6.12  to show  that the equilibrium  Xg =  0 of  {LH) is  uniformly  stable  if there exists a constant c such that ft XM{T)dT^c (14.9) for  all  t a  such  that  t  >  a  where  AM(0  denotes  the  largest  eigenvalue  of  A{t)  + [A{t)Y  t  G R. Hint:  Use (14.8) and the definition  of uniform  stabihty. 6.14.  Use Exercise 6.12 to show that the equilibrium  Xg =  0 of (LH) is exponentially  stable if there exist constants e  >  0 a;  >  0 such that XM{T)dT  <  -a{t -  (7)  +  6 (14.10) for all t a  such that t  >  a.  Hint.  Use (14.8) and the definition  of exponential  stabihty. 6.15.  Let V be a quadratic function  of the form 513 V(XJ)  =  x'^Q(t)x (14.11) CHAPTER6: where xGR^'.Qe Evaluate the derivative of v with respect to t along the solutions of {LH) to obtain C^[R /?"><«] Q(t)  =  [2(01^  and  Q(t)  ^  klk> 0 for  all t  e  R. Stability ViLH){x t)  =  x^[[A(0]^G(0  +  Q(t)A(t)  +  Q(t)]x. (14.12) Assume  that  there is  a quadratic  form  w(x)  =  x^Wx  <  0 where  W'^  =  W  G 7?«x« such that V(LH)(xt)^  W(X) for  all (x t)  G  G  X  R where G is a closed and bounded  subset of /?". Let E  =  { x E G:  w(jc)  =  0} (14.13) (14.14) and assume that for  (LH)  \\A(t)\\ is bounded  on R. Prove that any solution of (LH)  that remains in G for all ^ >  fo —  0 approaches £" as ^ ^  oo. 6.16.  Consider the  system which by letting  xi  =  x  and X2 -  x  can be written as X  +  a(t)x  + X  =  0 Xi  =  X2 X2  = -a(t)X2 -  Xi. Assume  that  a  G  C[R R^]  and  that  there  are  constants  ci C2  such  that  0  <  ci  < a(t)  ^  C2 for all t  G R. Let v(x)  =  x\-\r  x\.  First show that all solutions of this system are bounded.  Next  use  the  results  of  Exercise  6.15  to  show  that  ^2it  to -^o) ->  0  as 6.17.  Assume  that  for  system  {LH) there  exists  a  quadratic  function  of  the  form  v(jc 0  = x^Q(t)x  where  Q(t)  =  [Q(t)f  G  C^[R 7?"''"]  and  Q(t)  >  cl  for  some  c>  0  such that  V(LH)(x t)  <  x^Wx  where  W  =  W'^  G  R""^""  is negative  definite.  Show that if v is  negative  for  some  {x t)  then  the  equilibrium  Xg  =  0  of  system  {LH)  is  unstable. Hint.  The  proof  of  this  assertion  follows  along  similar  lines  as the proof  of  Theorem 7.3. 6.18.  It is shown that if the equilibrium  Xe =  0 of system {LH) is exponentially  stable then there exists  a function  v that  satisfies  the requirements  of the result given  in  Exercise 6.10 i.e. the present result is a converse  theorem  to the result given in Exercise  6.10. In  system  {LH)  let A  be  bounded  for  all  ^ G  /? let  L  =  L^  G  C^[R /?"><"] and assume that L is bounded for all t  G R.  Show that the integral Q{t)= f mat)VL{a)^{(Tt)da exists for all t  G R.  Show that the derivative of the  function v{xj)  =  x^Q{t)x (14.15) with respect to t along the solutions of {LH) is given by y(LH){xj)  = -X^L{t)x. Next  show  that  if  L{t)  >  c^I  C3  >  0  for  all  t  G  R  then  there  exist  constants  C2 ^ Cl  >  0 such that for all t  G  R c i /<  Q{t)^  C2I (14.16) where /  G /?"^"  denotes the identity  matrix. 514 Linear  Systems Note  that  the  above  result  constitutes  a  generalization  to  Theorem  7.5  for  time-invariant  systems (L). 6.19.  Apply  Proposition  7.1  to  determine  the  definiteness  properties  of  the  matrix A  given by A  = 1 2 1 2 5 -1 1 -1 10 6.20.  Use Theorem 7.3 to prove that the trivial solution of the  system is unstable. 3  4 2  1 6.21.  Determine the equilibrium points of a system described by the differential  equation X =  -X  + x^ and determine the stability properties of the equilibrium points if applicable by using Theorem  8.1 or  8.2. 6.22.  The system described by the differential  equations Xi  =  X2 + Xi(xj  +  xl) Xi  +  X2{x\  +  x\) (14.17) has  an equilibrium  at the origin  x^  =  {x\  X2) =  (0 0). Show that the trivial  solution of  the  linearization  of  system  (14.17)  is  stable.  Prove  that  the  equilibrium  x  =  0 of system (14.17) is unstable. (This example shows that the assumptions on the matrix A in Theorems  8.1  and 8.2 are absolutely  essential.) 6.23.  Prove that the system given by 'xx' M. 'yi J2J {It -1) \xi 0] -2t\ + cost sint sin^l cos^J U\ [X2_ -uit) is BIBO  stable. 6.24.  Use Theorem 9.3 to analyze the stability properties of the system given by i:  =  Ax  +  Bu y  =  Cx A 1 1 0 -1 B  = 1 -1 C  =  [0  1]. 6.25.  Determine  all equilibrium points for the discrete-time  systems given by (a) xi(k+  1)  -  X2(k) +  \xi(k)\ X2(k+  1)  =  -Xi(k)  + \x2(k)\ (b) Xi(k+ 1)  =  Xi(k)X2(k) -  1 X2(k  +  1)  =  2xi(^)jC2(^)  +  1. 6.26.  Consider the discrete-time  system given by 515 CHAPTER  6: Stability where for 0  =  (Oi...  OnV  e  /?" sat6  =  [satOi... satdnV.  and x(k-{-  1)  -  sat[Ax(k)] (14.18) 5<2^^/  = r  1 ^/  >  1 - 1 Oi  <  1. (a)  For A G /?"^" arbitrary use Theorem  10.1 to analyze system  (14.18). (b)  Imposing  various restrictions  on the locations  of the eigenvalues  of A in the com plex plane use  as many  results  of this chapter  as you can  to analyze the  stability properties of the trivial solution of system  (14.18). 6.27.  Determine  the  stability  properties  of  the  trivial  solution  of  the  discrete-time  system given by the  equations 'xx{k+  1)" Mk  +1). cos 0 - s i n^ sin ^ 1 cosOj \xx{k) [x2{k) with 6  fixed. 6.28.  Analyze  the  stability  of the equilibrium  x  =  0 of the  system  described  by the  scalar-valued difference  equation x{k  +1)  =  sin[x(^)]. 6.29  Analyze the stability of the equilibrium x  =  0 of the system described by the  difference equations Xx{k  +  1)  =  Xx{k)  +  X2{k){Xi{kf X2(k  +  1)  =  X2(k)  -  xi(k)[xi(kf -h  X2{kf^ + X2(kfl 6.30.  Determine  a basis of the solution space of the  system xi(k+  1) X2(k +  1)J 0  1 -6  5 xi(k) [X2(k) Use your answer in analyzing the stability of the trivial  solution of this  system. 6.31.  Let A E  /?"^". Prove that part (iii) of Theorem  10.8 is equivalent to the statement  that all eigenvalues  of A have modulus less than  1 i.e. lim ||Ai  =  0 if and only if for any eigenvalue  A of A it is true that  |A| <  1. 6.32.  Use Theorem  10.7 to show that the equilibrium  x  =  0 of the  system x(k  +  1)  = 1 0 0 1 1 1 1 . . . . 0 0 . . 1 1 1 x(k) is unstable. 516 6.33.  (a) Use Theorem  10.9 to determine the stability of the equilibrium x  =  0 of the system Linear  Systems "1  1 x(k  -\-l)  =  0  1 - 21 3 9 -1 x(k). 0 (b) Use Theorem  10.9 to determine the stabiHty of the equilibrium x  =  0 of the system x(k+l)  =  \o ri  0 1 [o  9 -21 slxik). - ij 6.34.  Apply the Schur-Cohn criterion (Theorem  10.10) in analyzing the stability of the trivial solution of the system given by the  equations xi(y^+l)] X2(k+  1)  = U3(/^+l)J r - 0 .5 0.5 I.  0 0.5irxi(^) 0 0 0 \\x2(k) -0.5  0JU3W 6.35.  Apply  Theorems  7.2  and  10.11  to  show  that  if  the  equilibrium  x  =  0  (x  E. /?")  of the  system x(k  +1)  =  e^x(k) is asymptotically  stable then the equilibrium  x  =  0 of the  system is also asymptotically  stable. X =  Ax 6.36.  Apply Theorem  10.11 to show that the trivial solution of the system given by xi(k+  1) [X2(k +  1)J 0  2  xi(k) [2  Oj [X2(k) is unstable. 6.37.  Determine the stability of the equilibrium  x  =  0 of the scalar-valued  system given by 6.38.  Analyze the stability properties of the discrete-time  system given by x(k+ I)  =  ^x(k)  +  f  sin jc(^). x(k+ I)  =  x(k)  +  ^u(k) y(k)  = '^x(k) where x y and u are scalar-valued variables. Is this system BIBO stable? Can Theorem 10.16 be applied in the analysis of this  system? CHAPTER? Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems In this chapter representations of linear time-invariant systems based on polynomial matrices called Polynomial  Matrix  Description  (PMD)  or Differential  (Difference) Operator  Representation  (DOR)  are  introduced.  Such  representations  arise  natu rally  when  differential  (or difference)  equations  of  order  higher  than  one  are  used to describe  the behavior  of  systems  and  the  differential  (or difference)  operator  is introduced  to  represent  the  operation  of  differentiation  (or  of  time-shift).  Polyno mial  matrices  in place  of polynomials  are involved  since this  approach  is  typically used to describe MIMO  systems. Note that  state-space  system descriptions  involve only first-order differential  (or difference)  equations and as such PMDs include the state-space descriptions  as special cases. A rational function  matrix can be written as a ratio or fraction  of two polynomial matrices or of two rational matrices. If the transfer function  matrix of a system is ex pressed  as  a fraction  of two polynomial  or rational  matrices this  leads  to a  Matrix Fraction(al)  Description  (MFD)  of the system. The MFDs that involve  polynomial matrices called polynomial MFDs can be viewed as representations of internal real izations of the transfer  function  matrix i.e. as system PMDs of special form.  These polynomial  fractional  descriptions  (PMFD)  help establish  the relationship  between internal and external system representations in a clear and transparent manner. This can be used  to advantage  for  example  in the  study  of feedback  control  problems leading to clearer understanding  of the phenomena  that occur when  systems  are in terconnected  in  feedback  configurations.  The  MFDs  that  involve  ratios  of  rational matrices in particular ratios of proper and stable rational matrices offer  convenient characterizations  of transfer  functions  in feedback  control problems. 517 518 Linear Systems MFDs that involve ratios of polynomial matrices and ratios of proper and stable rational matrices are essential in parameterizing  all stabilizing feedback  controllers. Appropriate  selection  of the parameters  guarantees  that a closed-loop  system is not only  stable but will also  satisfy  additional  control  criteria. This is precisely  the ap proach  taken  in  optimal  control  methods  such  as  //°°-optimal  control.  Parameter-izations  of  all  stabilizing  feedback  controllers  are  studied  extensively  in  Part  2 of this chapter. We note that extensions of MFDs are also useful  in linear  time-varying systems and in nonlinear  systems. These extensions  are not addressed here. In  addition  to  the  importance  of  MFDs  in  characterizing  all  stabilizing  con trollers and in //°°-optimal control PMFDs and PMDs have been used in other con trol design  methodologies  as well  (e.g.  self-tuning  control). The use  of PMFDs  in feedback  control leads in a natural way to the polynomial Diophantine matrix equa tion that is central in control design when PMDs are used and that directly leads to the characterization  of  all  stabilizing  controllers. The  Diophantine  Equation  is  studied at length in Part  1 of this chapter. Finally PMDs  are generalizations  of  state-space descriptions  and  the  use  of  PMDs  to  characterize  the  behavior  of  systems  offers additional  insight  and  flexibility.  These  issues  are  also  explored  in  Part  1 of  this chapter. 7.1 INTRODUCTION In this chapter. Polynomial Matrix Descriptions  (PMDs) and Matrix Fractional De scriptions (MFDs) are used to study properties such as controllability  observability and stability primarily  of interconnected  systems and to conveniently  characterize all stabilizing feedback  controllers. These system descriptions are important in feed back control system analysis and design and are the key to developing control design theories such as H'^-opiimal  control. The  development  of  the  material  in  this  chapter  is  concerned  only  with continuous-time  systems;  however  completely  analogous  results  are  valid  for discrete-time  systems and can easily be obtained by obvious  modifications. In the following  PMDs and MFDs  are first introduced by an illustrative  exam ple. Next the contents of the chapter  are briefly  described  and  some guidelines  for the reader are provided. An important comment on notation In this chapter we will be dealing with matrices with entries that are polynomials in s or q denoted by e.g. D(s)  or D(q).  For simplicity of notation we frequently  omit the argument 5* or ^ and we write D to denote the polynomial matrix on hand. When ambiguity  may  arise  or  when  it  is  important  to  stress  the  fact  that  the  matrix  in question is a polynomial matrix the argument will be  included. A.  A Brief  Introduction  to Polynomial  and Fractional  Descriptions The PMD and the MFD of a linear time-invariant system are introduced via a simple motivating  example. CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems EXAMPLE  1.1. by In the  ordinary  differential  equation  representation  of  a system  given 519 yi(t)  +  yi(t)  +  yiit)  =  uiit)  +  ui(t) yi(t)  + Ht)  + 2y2(t)  = U2(t) (1.1) yi(t\  yiit)  and ui(t)  U2(t) denote respectively outputs and inputs of interest. We assume that  appropriate  initial  conditions  for  the  w/(r) yi(t)  and  their  derivatives  at  r  =  0  are given. By  changing  variables  one  can  express  (1.1)  by  an  equivalent  set  of  first-order ordinary  differential  equations  in  the  sense  that  this  set  of  equations  will  generate  all solutions  of (1.1) using  appropriate  initial conditions  and the  same inputs. To this  end let xi  =  yi  -  U2 X2  =  yi X3 yi  +  y i-  U2. Then  (1.1) can be written  as X =  Ax  +  Bu y  =  Cx  +  Du u{t)  = Ui(t) U2(t) y(t)  = yi(t) yiit). and B  = 1 0 0 -1 1 -2 C  = 1 -1 D  = lxi(t) X2(t) X3(t)\ -1 0 -2 where x{t)  = A  = 0  0 1  0 0  2 with initial conditions  x(0)  calculated by using (1.2). More directly however  system (1.1) can be represented  by where P(q) P(q)z(t)  =  Q{q)u{t) Z{t) = zx{t) Z2(t)\  U(t)  = y{t)  =  R{q)z{t)  +  W{q)u{t) Ui(t) uiit) y{t)  = and 2 +  1 1 q q  + 2 Q(q)-q q R(q) W(q) yi(t) yiit). I 0 (1.2) (1.3) (1.4) with q  =  didt  the differential  operator. The variables zi{t)  ziif)  are called/7(2r^/(2/ state variables  z(t)  denotes iht  partial  state  of the system description  (1.4) and u{t) and  y{t) denote the input and output vectors respectively. • Polynomial  matrix  descriptions  (PMD) Representation  (1.4) also  denoted  as {P{q)  Q{q)  R{q)  W{q)}  is  an  example  of a  PMD  of  a  system.  Note  that  the  state-space  description  (1.3)  is  a  special  case  of (1.4).  To  see  this v^rite  (1.3)  as {ql  -  A)x{t)  =  Bu{t\ y(t)  =  Cx(t)  +  Du(t) (1.5) Clearly  description  {ql  -  A  B  C D]  given  in  (1.5)  is  a  special  case  of  the  general P MD  {P{q)  Q(q\  R(q\  W(q)}  with P(q)  =  ql-A Q(q)  =  B R{q)  =  C ^{q)  =  D. (1.6) The  above  example  points  to  the  fact  that  a  PMD  of  a  system  can  be  derived in  a  natural  way  from  differential  (or  difference)  equations  that  involve  variables that  are  directly  connected  to  physical  quantities.  By  this  approach  it  is  frequently 520 Linear Systems possible  to  study  the  behavior  of  physical  variables  directly  without  having  to transform  the  system  to  a  state-space  description.  The  latter  may  involve  (state) variables  that are quite removed  from  the physical phenomena  they represent  thus losing  physical  insight  when  studying  a  given  problem.  The  price  to  pay  for  this additional insight is that one has to deal with differential  (or difference)  equations of order greater than  1. This typically adds computational burdens. We note that certain special  forms  of PMDs namely the polynomial  MFD  are easier  to deal with  than general  forms.  However  a  change  of  variables  may  again  be  necessary  to  obtain such  forms. Consider a general PMD of a system given by P{q)z{t)  =  Q(q)u(t\ y(t)  =  R(q)z(t)  +  W(qMt) (1.7) withP(^)  G Rlgy^'K  Q(q)  G Riq]^'''^ and R(q)  G RlqV^'K  W(q)  G /?[^]^X'^ where Rlq^^^  denotes the set of /  X / matrices  with entries that are real polynomials  in  q. The transfer  function  matrix H(s)  of (1.7) can be determined  by taking the Laplace transform  of  both  sides  of  the  equation  assuming  zero  initial  conditions  (z(0)  = ^(0)  =  ...  =  0 u(0)  =  u(0)  =  '"  =  0). Then H(s)  =  R(s)p-\s)Q(s) +  W(s). (1.8) For the special case of state-space representations [see (1.6)]; H(s)  in (1.8) assumes the well-known expression H(s)  =  C(sl  — Ay^B  + D. For the study of the relation ship between external and internal descriptions (1.8) is not particularly  convenient. Indeed  it appears  that it is as difficult  to investigate the relationship  between  H{s) and PMDs as it was to study the relationship between H{s)  and state-space descrip tions. There are however special cases of (1.8) that are very convenient to use in this regard.  In particular  as will be  shown  in  Section  7.3 if the  system  is  controllable then there exists a representation  equivalent to (1.7) that is of the  form DM)Zcit)  =  u{t) yit)  =  NM)Zc{t\ (1.9) where Ddq)  G R[qY''''^  and Nc{q)  G Riq^"^.  Representation  (1.9) is obtained by letting  Q{q)  =  I^  and  W(q)  =  0 in (1.7). It is common to use D and N  instead of  P and R in view of H(s)  =  Nc(s)Dc(sr\ (1.10) where Nds)  and  Dds)  represent  the matrix  numerator  and  matrix  demonimator  of the transfer function respectively. Similarly if the system is observable there exists a representation  equivalent to (1.7) that is of the  form Do(q)Zo(t)  =  No(q)u(t) y(t)  =  ZoU) (1.11) where  Do{q)  G  R[q\P^P  and  A^^(^)  G  RiqY^"^.  Representation  (1.11)  is  obtained by letting R{q)  -  Ip and  W{q)  =  0 in (1.7) with P{q)  -  Do{q) and Q{q)  = No{q)-Here H{s)  =  D-\s)No{s\ (1.12) Note that  (1.10)  and  (1.12)  are generalizations  to the MIMO  case of the SISO sys tem  expression  H{s)  =  n(s)/d(s).  In the  same  manner  as H(s)  =  n(s)/d(s)  can  be derived  from  the  differential  equation  d(q)y(t)  =  n(q)u(t)  (1.12)  can  be  derived from  (1.11) usually written as i)o(^)};(0  =  No(q)u(t). Returning now to (1.3) in the example notice that the system is observable (state observable from the output y). Therefore the system in this case can be represented by a description of the form {Do No h  0}. In fact  (1.4) is such a description  where Do and No  are equal to P and  Q respectively  i.e. Do(q)  = q^+  1 1 q  + 2   and No(q)  = H(s) 1  q [O  q C{sl-  Ay^B  + D . The transfer function  matrix is given by 521 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems "0 0 1  Ol ij -1 '  s 0 - 1 ^0 0 -2 D-\s)No{s)  = 1 ^3 +  2^2 +  2 52 +  1 5 \s + 2 [  -s "0  0" 0  1 + -1  r 1  " s + 1 -1 1  " 5 +  2 "1 0 0 "1 0 -1 1 -2_ s s -1  1 s^ +  ij ri  s [0  s = 1 S'  + 2^ 2 +  2 -s s{s +  1) Matrix fractional  descriptions  (MFDs) of system transfer  matrices A given pXm  proper rational transfer  function  matrix H{s)  of a system can be represented  as H{s)  =  NR(S)D^\S) = DI\S)NL(S) (1.13) E  R[sr'''^  mdNiis)  E  RlsV'^.DUs) whevQ NR(S)  E  Rlsy'^.DRis) RISVP. The pairs {NR(S)  DR(S)}  and {DL(S)  NL(S)}  are called Polynomial  Matrix  Fractional Descriptions  (PMFDs)  of the  system  transfer  matrix  with  {NR(S\  DR(S)} termed  a right Fractional  Description  and {DL(S)  NL(S)}  a left Fractional  Description.  Notice that in view of (1.10) the right Polynomial Matrix Fractional Description  (rPMFD) corresponds to the controllable PMD given in (1.9). That is {DR  Im NR 0} or E DR(q)zR(t)  =  u(t) y(t)  =  NR(q)zR(t) (1.14) is a controllable PMD of the system with transfer function  H(s).  The subscript c was used in (1.9) and (1.10) to emphasize the fact that Nc Dc originated from an internal description  that  was  controllable.  In  (1.13)  and  (1.14)  the  subscript  R  is  used  to emphasize that {NR  DR} is a right fraction  representation of the external  description H(s). Similarly  in view  of  (1.12) the left  Polynomial  Matrix Fractional  Description (IPMFD) corresponds to the observable PMD given in (1.11). That is {DL  NL Ip 0} or DL(q)ZL(t)  =  NdqMt) y(t)  = ZLO) (1.15) is an observable PMD of the system with transfer  function  H(s).  Comments  analo gous to the ones made above concerning controllable and right fractional  descriptions (subscripts  c and R) can also be made here concerning the subscripts o and L. An MFD  of a transfer  function  may not consist necessarily  of ratios of polyno mial matrices. In particular given a /? X m proper transfer  function  matrix H(s)  one 522 Linear Systems can  wnte H(s)  =  NR(S)D^\S) = DI\S)NL(S\ (1.16) where  NR DR DU NL  are proper  and  stable  rational  matrices.  To illustrate  in  the example considered  above H{s)  can be written  as H{s) 1 5  +  2 s{s +  1) s^ +  2^2 +  2 -s S{S^  - 5 + 1) '\{S  +  1)2 [  0 0 s +  2 -1 ?2 +  1 1 s + 2 \s  +  1)2 0 0 5  +  2 52  +  1 (5 +  1)2 5" L  ^ + 2 1 1 {S +  1)2 (^  +  1)2 {S +  1)2 1 0 ^ + 2  J = DI\S)NL{S). Note that Di{s)  and  A^L(^) are proper and stable rational matrices. Such representations of proper transfer  functions  offer  certain advantages  when designing  feedback  control  systems.  They  are  discussed  further  in  this  chapter  in Part 2 Subsection  7.4D. B.  Chapter  Description This chapter consists of two principal parts. In Part 1 the emphasis is on properties of systems described by PMDs. First background  on polynomial matrices is provided in Section 7.2 and the Diophantine Equation is studied at length in Subsection 7.2E. Equivalence  of representations  and  system properties  are addressed  in Section 7.3. Properties  of  systems  consisting  of  subsystems  interconnected  in parallel in  series (cascade)  and  in  feedback  configurations  are  investigated  in  Subsection  7.3C.  In Part  2  Section  7.4  feedback  control  systems  are  studied  with  emphasis  placed  on parameterizing  all stabilizing feedback  controllers. Further details  follow. In Section 7.2 polynomial matrices and their properties are studied and  special forms  for  polynomial  matrices  which  are  useful  in  subsequent  developments  are introduced.  In particular  polynomial  matrices  in  column  reduced  triangular  Her-mite and  Smith form  are defined  and  algorithms  to obtain  such forms  by pre- and postmultiplication  by unimodular matrices are given in Subsections 7.2B  and 7.2C. Coprimeness of polynomial matrices is related to controllability and observability of PMDs  and is studied in Subsection 7.2D. The Diophantine Equation which plays a central role in feedback  control is studied at length in Subsection 7.2E and methods for deriving particular  solutions are given. PMDs of systems are addressed throughout Section 7.3. Controllability observ ability  and  stability  are  revisited  in  Subsection  7.3B.  Also  PMD  realizations  of transfer  function  matrices are studied and realization algorithms are developed. The relationships among different  PMDs and state-space descriptions of a system are ex plored  in  Subsection  7.3A  using  equivalence  of representations.  The properties  of systems consisting of interconnected subsystems are best explored using PMDs and this is accomplished  in Subsection  7.3C. Feedback control systems are studied in Section 7.4 using PMDs and MFDs with emphasis on stabilizing controllers. All stabilizing controllers are parameterized us ing  PMDs  in  Subsection  7.4A  and  proper  and  stable  MFDs  in  Subsection  7.4C. State feedback  controllers and state observers important in the development involv ing MFDs are discussed in Subsection 7.4B. The relationships  among all  feedback controller  parameterizations  discussed  herein  are  derived  and  fully  explained.  The complete  theory  of parameterizing  all stabilizing  feedback  controllers  is  developed in this  section. Two degrees  of freedom  controllers their  stability properties and their  param eterizations  are explored in Subsection 7.4D. Several implementations  of such con trollers  are  introduced  and  their  limitations  are  addressed.  Finally  several  control problems  such  as the  model  matching  problem  the  diagonal  decoupling  problem and the static decoupling problem are formulated  and briefly  discussed. 523 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems C.  Guidelines  for the  Reader As with every chapter of this book this chapter can be approached at different  levels. If the characterization of all proper stabilizing feedback  controllers using proper and stable MFDs which  arises in optimal control problems is of primary  interest  then the reader should focus on Subsection 7.4C. For better understanding of such MFDs of systems and of polynomial MFDs and their use in the study of systems the reader at first reading  should  study  selected topics from  all sections  of this chapter.  In  the following  the material that should be covered at first reading is described. The reader should first study coprimeness of polynomial matrices in Subsection 7.2D  with  emphasis  on  the  tests  for  coprimeness  (Theorem  2.4).  To  determine  a greatest common divisor of polynomial matrices one needs the algorithms given in Subsection  7.2D.  All  solutions  of  the  Diophantine  matrix  equation  are  derived  in Theorem 2.15 of Subsection 7.2E with particular solutions obtained in Lemma 2.14. The  study  of  equivalence  representations  in  Subsection  7.3A  leads  to  insight concerning  the relationships  between  different  PMDs  and  state-space  descriptions of a system. Tests for  controllability  observability  and  stability  are given in Theo rems 3.43.5 and 3.6 of Subsection 7.3B. Feedback configurations  of interconnected systems  are  studied  in  Subsection  7.3C.  Here the closed-loop  descriptions  are  also derived which are then used in Section 7.4 to study the class of stabilizing  feedback controllers. All  stabilizing  controllers  are  expressed  in  terms  of  PMDs  in  Theorem  4.1  of Subsection 7.4A. Different  parameters  are introduced in Theorems 4.2 and 4.3 and Corollaries  4.5 4.6  and  4.9. To fully  understand  proper  and  stable  MFDs  of  sys tems  one needs  to  study  state feedback  controllers  and  state  observers  in terms of PMDs. This is accomplished  in  Subsection  7.4B. All proper  stabilizing  controllers are parameterized  (using proper  and  stable MFDs)  in Theorem  4.13  given  in  Sub section 7.4C  and also in Theorem 4.16. The exact relationship between  such MFDs and internal descriptions is provided by Theorem 4.20. Two degrees of freedom  controllers that offer  advantages concerning  attainable system responses are studied in Subsection 7.4D. Theorem 4.21 is the principal  sta bility theorem with all stabilizing controllers being parameterized in Theorem 4.22. 524 Linear Systems Theorem 4.23 characterizes the response maps attainable via two degrees of freedom controllers under internal stability. Several control configurations are then examined. Finally several control problems are formulated. PARTI ANALYSIS OF  SYSTEMS 7.2 BACKGROUND MATERIAL ON POLYNOMIAL MATRICES Let R[s]P^^ denote the set of p  X m matrices with entries that are polynomials in s with real coefficients.  If P(s) G R[sy^^  then P(s) will be called a. p  X m polyno mial matrix. Frequently it will be necessary to determine the rank of P(s) which is defined as the maximum number of linearly independent rows (or columns) of P(s) over the field of rational functions. The rank of a polynomial matrix is discussed in Subsection A. In Subsection B unimodular matrices are introduced and transforma tions of polynomial matrices to column and row reduced form are discussed. Hermite and Smith canonical forms are addressed in Subsection C and in Subsection D the important concept of coprimeness of polynomial matrices is studied. In Subsection E the linear Diophantine Equation is examined. A.  Rank and Linear Independence The linear independence of a set of vectors in a vector space defined in Chapter 2 is recalled here for convenience. Let (V F) denote a vector space V over the field F and let v/ G K  /  == 1...  /c. The set of vectors {vi... Vyt} is F-linearly dependent i. e. it is linearly dependent over the field F if there exists a set {ai... a^^jof scalars in F with at T^ 0 for at least one / such that a\v\  + fl2V2 +  • • • +  akVk  =  Oy. (2.1) The set of vectors {vi... v^^} is linearly independent over the field F if (2.1) implies that ai  = 0 for each /  =  1...  k. Linear dependence of p  X  1  polynomial vectors pi(s)  G R[sy  is defined  sim ilarly. This warrants  some explanation.  Let  R[s] be the ring  of polynomials  with coefficients  in R (see Subsection 7.2E) and let R(s) be the field of rational  fractions over R[s]  (called the field of rational functions) i.e. R(s)  = {t(s)\t(s) =  ^  with ndG  R[sl d ^  0}. d(s) Note that if p(s) G R[s] it can always be considered as being divided by 1 in which case p(s)/l  is a scalar in the field of rational fractions  over R[s].  Thus a polyno mial vector pi(s) G R[S]P may be viewed as a special case of a rational vector i.e. Pi(s) G R(sy  whose elements are in the field of rational fractions. Now consider the polynomial vectors pi(s) G R[sy  i  =  I...  k. The set of vectors {pi(s)...  Pk(s)} is  said  to  be  R(s)-lmQ2ir\y  dependent  (i.e.  linearly  dependent  over  the field of CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems rational  functions)  if  there  exists  a  set  {ai(s)... ai{s)  G  R{s)  i  =  I.. .k)  with  ai{s)  ¥'  0  for  at least  one  /  such  that a^is)}  of  rational  functions  (i.e. 525 ax{s)px{s)  +  •••  +  ak{s)p^{s) -  0  G  R{sy. (2.2) This  set  of  vectors  is  linearly  independent  over  R(s)  if  (2.2)  implies  that  aiis)  =  0 for  each  /  =  1... k. EXAMPLE  2.1.  LQipiis)  =  r '^^\P2(s) 's + 3 0 s+  1 0 . Note that a 1(5') (s +  l)/(s  +  3) a2(s)  =  -I  satisfy  (2.2)  since ai(s)pi(s)  +  a2(s)p2(s) s+  1 s + 3 s + 3 0 +  (-1) 's+  1 0 Therefore  the set {p\(s)  P2(s)} is linearly dependent over the field of rational  functions. It is of interest to notice that {p\(s)  p2(s)} is linearly  independent  over the field of reals. In particular  if ai  a2 are restricted  to be reals then  (2.2) implies  that ai  =  a2  = 0  (verify  this).  This  stresses  the  importance  of  the  particular  field  over  which  linear independence is considered  (refer  to Section 2.2 of Chapter 2). • It is not difficult  to see that if the set {pi  (s)...  Pk(s)}  is linearly  dependent  then (2.2)  is  also  satisfied  for  some  polynomials  ai(s).  To  see  this  simply  multiply  both sides  of  (2.2) by  the least  common  multiple  of the denominators  of the rational  func tions  ^1(5")... ak(s).  This  implies  that  linear  dependence  over  R(s)  can  be  tested merely  by  searching  for  polynomials  ai(s)  E  R[s]  not  all  zero  satisfying  (2.2).  To illustrate  consider: EXAMPLE  2.2.  In  Example  2.1 {pi(s)  p2(s)}  is  linearly  dependent  over  R(s)  since ( ^ + 1) s +  3' 0 +  (-(s  +  3)) 's+  1 0 DEFINITION  2.1.  The normal  rank of a polynomial  matrix  P(s)  G  /?[5]^^'" is the max imum number of linearly independent rows (or columns) over the field of rational  func tions R(s). • EXAMPLE  2.3.  (i)  rankPi(s)  =  rank rank s 1 s + 1 s+  1 2. r^ +1  ^ + 31 0 0 =  1  and  (ii)  rankP2{s)  = It  can  be  shown  that  the  (normal)  rank  of  P{s)  is  also  equal  to  the  order  of  the largest  order  nonzero  minor  of  P{s). EXAMPLE  2.4.  (i) P\{s)  in Example 2.3 does not have a second-order  nonzero  minor since det  P\{s)  =  0 and therefore  its rank is less than  2. The entries  are the  first-order minors and since nonzero entries exist  rankPi(s)  =  1. (ii) P2(s) in Example 2.3 has a second-order nonzero minor since det  P2(s) ^ c 2. 1 ^ 0.  Therefore  rank P(s)  =  2. Notice  that  if  ^  =  1 or  - 1 then  det  P2(l)  =  det  P2(-1) =  0.  This  is  true  be cause in this case P2(l)  [or P2(-1)]  has only one linearly independent  column  (over the  field  of reals R)  and  rank  P ( l)  =  1. Since this loss of rank  (from  2 to  1)  occurred for  only  special  values  of s  rank  P(s)  defined  above is referred  to as the normal  rank of  P(s)  instead  of just  the  rank  of  P(s)  when  there  is  ambiguity.  Note  that  unless otherwise  stated  the  linear  dependence  of  rows  or  columns  of  a  matrix  considered 526 Linear Systems for  rank  evaluation  is taken  over the smallest  field  that  contains  the entries  of the matrix. B.  Unimodular  and Column  (Row) Reduced  Matrices is called  unimodular  (or /?[5-]-unimodular) if A polynomial  matrix  U(s) G R[sy^^ there exists a U(s) G  R[SY^P such that  U(s)U(s)  =  I p.  This is the same as saying that U~^(s)  =  U(s) exists and is a polynomial matrix. Equivalently U(s) is unimod ular if det  U(s)  =  a  G  Ra7^0. It  can be  shown  that  every  unimodular  matrix  is a matrix  representation  of a finite number of successive elementary row and column operations. The  elementary row and column  operations  on any polynomial matrix P{s) E  R[sY^^  consist of the 1.  interchange of any two rows (or columns) of P{s) 2.  multiplication of any row (or column) of P{s) by a nonzero real a  G 7? a: 7^ 0 (a unit in R[sY) 3.  addition  to any row (or column)  of P{s) of a multiple by a nonzero  polynomial p{s)  of another row (or column). These elementary row (and column)  operations can be performed  by multiply ing  P{s) on the left  (right)  [i.e. by pre-(post-)  multiplying  P{s)\  by  elementary unimodular  matrices.  These  elementary  unimodular  matrices  are obtained  by per forming  the elementary  operations  (1) to (3) on the identity matrix /. As mentioned above it can be shown that every unimodular matrix may be represented as the prod uct of a finite number of elementary  unimodular  matrices. EXAMPLE  2.5.  The interchanging  of rows  1 and 3 in  a  specific  example  is  accom plished e.g. as shown: UL{S)P{S)  = "0 0 _1 0 1 0 11 0 oj 1 s +  1 0 1 0 ^ + 2 0 ^ + 2 s+  1 1 1 0 Also addition to the second column of the third column multiplied by 5" in a specific example is accomplished e.g. as shown: s~\ 0 ij ri  0  0" 0  1  0 LO  s  1. P(S)UR(S)  = 2s + 2 s+  1 5+ 1 0 1 = 1 1 1 s + 2 0 0 Let  the degree  of a polynomial  (row or  column)  vector  be the degree  of the highest  degree  entry  and let deg^. (P)  [deg^. (P)] denote  the degree  of the /th row (jth  column)  of  P(s).  Also  let  Cr(P)  [Cc(P)] be the highest  row degree  (column degree)  coefficient  matrix  of P(s) defined  as the real matrix with entries that are the coefficients  of the highest degree s terms in each row (column) of P(s). We note that P(s) G RlsY^"^  can be written as P(s) diag  (s'^'i  s'^'-2 ^ S--p)Cr(P)  +  Pr(s) Cc(P)diag(s'^ '^S^'2 .. .S^' ) +  Pc(sl (2.3) wheredn  =  deg^. (P) i  =  1...  p  anddcj Pc(s)  appropriate polynomial  matrices. deg^.  (P) j  =  1... m with  Pr(s) EXAMPLE  2.6.  Let  P(s)  = 's+l s 3^2  +  21 1 .  The  row  degrees  are  deg^^ (P) [5^  +  3 s^ + 5  2deg^2(P)  ^  1'  ^^^  ^^^r3 (^)  ^  ^'  while  the  column  degrees  are  deg^i(P)  =  2 3] 0 1_ and J^^c2 (P)  ^  3- The highest row degree coefficient  matrix of P(s) is  Cr(P) ro 1 .0 and the highest column  degree coefficient  matrix is CdP)  = We have 0  0 0  0 1  1 527 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems P(s) 0 0 0 0 Sdr3 0 0 Cr  +  Pr{s) 0 0 s 0 0  ^3 Cc  0 +  Pc(s) 0  0 0  0 1  1 0 0  3 1  0 0  1 s +1 s 3 s+l 0 2 1 s^  + 3  5 + 3^2 +  1 1 5 P(s)  is row  (column)  proper  also  called  row  {column)  reduced  if  Cr{P) [CdP)] has  full  rank. EXAMPLE  2.7.  P{s)  in Example  2.6 is row proper  since  rankCr  =  2 but not  column proper since rank Cp  =  I  <2. • Consider  now  the  first  two  rows  of  ^(5") in  Example  2.6  and  note  that det s  +1 s 3s^  +  2 1 = ( - 3 )/ s+  1 =  det 0 1 3 0 ^(dri +dr2)  _^  lower  degree  terms  . This  illustrates  the following  result  that can be  derived  from  (2.3): any  highest  order minor  of  P{s)  E  7^ [5']^^'"  is  a polynomial  of  degree  equal  to  the  sum  of  the  degrees of  the  rows  {p  >  m)  or  of  the  columns  (;?  <  m)  with  leading  coefficient  equal  to the  corresponding  minor  of  Cr{P)  or  of  CdP) respectively.  For  the  case  when  P(s) is  square  this  immediately  implies  that detP(s)  =  detCdP)s^^'^ +  lower  degree  terms =  detCdP)s^^'^ +  lower  degree  terms. (2.4) Clearly  then  P{s)  G  R[sY^^^  is row  (column) proper if and only if ^i^^ {det  P{s))  ^ X  dr{deg{det{P{s)) is  not  of full  rank  it  can  be  neither  a column  nor  a row  proper  matrix. =  Z  dcj).  (Show  this.)  Note  that  if  P{s)  G  R[S]P'''^ Equation  (2.3)  leads  to useful  polynomial  matrix  representations  given  by P{s)  =  diag  [s'^'i^Cr  +  block  diag [ls •.s'^^i-^]Cr =  block  diag  [1 ^  . . .  s^'i]Pr and P{s)  =  Cc diag  [/^^  ]  +  Cc block  diag  [[1 s s'^'j  ^ ] = Pcblockdiag{[\s...s'^'jfl (2.5a) (2.5b) 528 Linear Systems EXAMPLE 2.8.  In view of (2.5a) we have the representation r  1 1 2 1 0 s + 1 3s^ + 2" P(s) s 1 = s^ + 3 s^ +5  j s' 0 0 0  01 s  0 0  ^^J ro 3" 1  0 LO  ij + 1  5  0  0  0 0  0  10 0 0  0  0  1  5 r 1 1 0 21 0 3 01 0 ^'J 0 1 3 0 [ 1 5 0 0_ 1 0 0 2 s s^ 0  0 0  0 :  0  0 :  1  s :  0  0 0  0  0 0  0  0 1  s s' 0 0 ^'J 0 1 1 0 3 0 1 N 5 0 0 1 _ Similarly a representation in terms of column degrees can be obtained using (2.5b). • P{s) can also be expressed  as a matrix polynomial  of the  form P{s)  =  Pks^  +  Pk-is"-'  +  "'  + Pis  + Po (2.6) where  Pi  G  RP^^  A  square  polynomial  matrix  is  called  regular  if  rankP^  =  m. Note that if P(s)  is regular then it is both row and column  proper. EXAMPLE 2.9.  P(5) of Example 2.8 can be written as P(5)  =  P3S^ + P2S^-\-Pis + Po = s+  1 3^2 + 2 1 s^ + 3 = ro 01 0  0 0  1 ^3  + ro 3" 0  1 1  0 s^  + ri 0" 1  0 0  0 s + ri 21 0  1 3  5 Reduction to a row (column) proper polynomial  matrix Given P(s)  G Rlsy^'^  of full rank there exists a unimodular matrix  UL(S)  such that  UL(S)P(S) is row proper. This is shown here by using a constructive proof. At each step of the  algorithm below the degree of a row (the highest degree row) is reduced by at least one using elementary  row operations. Since the matrix  is of full  rank the algorithm will  stop after  a finite number of steps. Algorithm Let drf  =  degj.. (P) i  =  \... p. (i)  Ohi2im diag[s^^i]Cr{P). (ii)  Determine/? monomials pi{s)  such that {p..Pp)diag[s'^qCr{P) = Q. 529 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Take  pk  =  I.  This  is  accomplished  by  dividing  all  monomials  by  the  lowest degree (nonzero) monomial assumed here to be the  /cth one. (iii)  Premultiply  P(s)  by "1 0 •••  0 ••• 0 Ux(s)  =  Pi Pi Pp kih row. 0 0 0 1 (iv)  Stop if  U\P  is row proper. Otherwise set P  =  U\P\  repeat the steps. To determine the appropriate unimodular matrix  UL{S)  directly so that  UL(S)P(S) is  row  proper  one  may  decide  on  the  necessary  row  operations  based  on  P(s) (as  in  the  algorithm  above)  but  apply  these  to  [P(s\  Ip].  Then  UL(S)[P(S) Ip]  = [P(s)  UL(S)]  where P(s)  is the row proper matrix and  UL(S)  can be read off  directly from  the resulting  matrix. EXAMPLE  2.10.  Fov  P(s)  = 's+ 1 s s s^ + 2 s + 2  the row degrees are dr^ =  I dr^ = 2 dr  -  1  and  rankCr(P)  = rank =  1 <  2 and thus P(s) is not row proper. The algorithm is now applied. We have 0 01 's 0 s 2 0 ) ^J .0  C (i) diag  [s'^i][Cr(P)]  = ri  1" 1  1 Li  1. = '  s s^ _s s ' s^ s _ (nl(ni)Ui(s) = 's  +  1 (iw)UiP  = -s s 0" 1  0 " 10 -s . 0 0 1. ' ' s 2 s  +  2.  ai idCriUiP)  = '  1 -1 1 11 0   ij which is of full rank. Thus ULP.  where UL  U\ is row proper. • Note that a unimodular matrix  UL such that  ULP is row proper is not unique. In -1 [I  0 fact  in Example  2.10  another  choice  for  UL could  have  been  Ui  = since  U\P  = -2 s^  + 2 s + 2 with  CriUiP)  = "1 1 1 - 2" 1 1 therefore  Ui P is row proper.   which  is  of  full  rank  and It  is  possible  to  reduce  a  polynomial  matrix  to  a row  proper  polynomial  ma trix  using  elementary  column  operations  (in  place  of  elementary  row  operations). In  particular  given  P(s)  G  R[sy^^  of  full  rank  there  exists  a unimodular  matrix UR(S)  such that  P(S)UR(S) is row proper.  Such  a  UR(S)  is determined  for  example by  the  algorithm  described  in  Subsection  7.2C  which  reduces  a matrix  to a  lower left  Hermite form.  Other algorithms to accomplish this can also be  derived. 530 Linear Systems EXAMPLE 2.11.  Consider P(s) of Example 2.10 and apply the algorithm of Subsection 7.2C to reduce P(s) to lower left Hermite form. Take P(sY  and determine U^(s)  such that U^P^ is reduced to upper right Hermite form. Then -si s+l\ '  1 -2 . -2 1.  = P(s) 1  01 Ij s^ + 2s + 2 =  P(S)UR(S)  = n  —s P{s) .-1 -I 1 0 3s+  2 which is row proper. Similar  results  for  reducing  a polynomial  matrix  to a  column  proper  polyno mial matrix  can easily  be derived.  Given  P(s)  E  R[sy^^  of full  rank  there  exists a unimodular  matrix  UR(S)  such that  P(S)UR(S) is column  proper.  [Take P(sy  and apply  steps  (i) to (iii) of the above algorithm.]  Also there exists  a unimodular ma trix  UL(S)  such that  UL(S)P(S) is column proper.  [Use the algorithm to reduce  P(s) to upper right Hermite  form  described  in Subsection  7.2C.]  Finally we note that if P(s)  E  R[sy^^  has full  rank there exist unimodular matrices  UL and UR such that ULPUR is both row and column proper. One such example is when  UL  and  UR  are chosen so that  ULPUR  is in Smith form  (see Subsection  7.2C). Proper rational  matrices Recall that a rational matrix H(s)  E  R(sy^^ is called proper  if limH(s)  =  D 5—»oo DGRP'''^ and if D  =  0 then H(s)  is called strictly proper  Frequently H(s)  is expressed as H(s)  = N(s)D-\sl where A^(^) and Z)(^) are polynomial matrices (A^(^) E  RlsV"^  Sind D(s)  E  Rls]"^"""^) The pair {N(s) D(s)} can be viewed as an rPMFD of a system described by a transfer function  matrix H{s). It is of interest to relate the propemess  of the rational  matrix H(s)  to the (column)  degrees  of N{s)  and D(s). Note that  when  N(s)  and D(s) are polynomials  it is easily  seen  that  H(s)  is  strictly  proper  (is proper)  if  and only if degN(s)  < degD(s)  [if and only if degN(s)  <  degD(s)].  In the matrix case sim ilar necessary  and sufficient  conditions given in Lemma 2.2 exist only when  D(s) is  column  reduced.  Necessary  conditions  for propemess  are given  in Lemma 2.1. Completely  analogous  results to Lemmas  2.1 and 2.2 hold for left  factorizations  of H(s)  =  D-\s)N(s) as  well LEMMA 2.1.  Let H(s) be a proper (or strictly proper) rational matrix and let H(s)  = N(s)D(s)-\  Then  deg^.N(s)  <  deg^.D(s)  [or  deg^.N(s)  <  deg^.D(s)]  for  j  = 1.... m. Proof. N(s) =  //(5)D(^) and for the yth column of A^(^) m ^O'W  =  ^Hik(s)dkj(s\ ^=1 i  =  I. • A where nij(s) denotes the ith element in the jih  column of N(s).  Since every element Hik(s) of H(s) is strictly proper (or proper) all entries nij(s) must have degrees less than (or less than or equal to) the degree of the highest degree polynomial in the jih column • of D(s). The converse to the above result is not always true. For example let N{s)  =  [ls] and D(s)  = s^  +  \ s+l IJ where deg^^ Nis)  =  0 <  deg^^ D{s)  =  2 and deg^^ N(s)  =  1 =  deg^^ D(s). Here Nis)D-\s)  =  {\s] 1 ?  -  1  ^2 + 1 -5 1 1 -s 2 -s^  — s 1  -^ 1 ^  1  -^ which is not proper. Notice that D{s) is not column  reduced.  When D{s) is column reduced  (column  proper)  the conditions  of the above  lemma  are sufficient  as well as the following  result shows. 531 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems LEMMA 2.2.  Let H{s)  = N(s)D~\s)  with D(s) column reduced. Then H(s) is proper if and only if deg  N(s) <  deg^  D(s) j= I . m. H(s) is strictly proper if and only if deg^. N(s) < deg^. D(s) j  =  1... m. Proof Necessity  was shown in the previous lemma. To show sufficiency  notice that by applying Cramer's rule for the inverse to solve H(s)D(s)  = N(s) we have hij(s) = [det D^J(sydet D(s)] where D^J(s) is the matrix obtained by replacing the jth row of D(^) by the /th row of N(s). In view of (2.3) D(s) can be written as D(s) =  Cc(D) diag [s ""J ] + Dc(s) j  =  1... m where dcj = deg^. D{s\ CdD) is the highest column degree coef ficient  matrix of D(sX and deg^. D^s)  < dc.. Similarly D'^{s) = Cc(D'J) diag [A ] + &-^(s) where Cc(D^^) is the same as CdD) except for the 7th row which may or may not be zero since each entry of the jth row of N(s) is of lower than or equal degree of the corresponding entry of the jih row of D(s). Since D(s) is column proper det CdD) ¥" 0 and in view of (2.4) degdetD(s)  = X%idj  while degdetD'ds)  <  YJ]=idj.  There fore  hij{s) is  proper.  It  is  strictly  proper  when  deg^.  N(s) < deg^.  D(s) for  j  = 1... m. • EXAMPLE  2.12.  H(s)  = N(s)D~\s)whQrQ \s + l -1 D(s) = and A ^ ( ^ )= ais  +  aQ CiS  +  CQ d\S  -\r  do is proper for any values of the parameters since D(s) is column reduced and deg^. N(s) < deg^. D(s) j  =  1 2. H(s) is strictly proper only when ai  = bi  = ci  = di  = 0. • Let  H{s)  =  D-\s)N(s)  where  D{s) G  R[syp and N(s)  G  Rlsy"^.  Com pletely  analogous  results  to  Lemmas  2.1  and  2.2 hold  also  for  the  left  factoriza tion  matrices.  In particular  if  H(s)  is  proper  (strictly  proper)  then  deg^. N(s)  < deg^. D(s)  [deg^.  N(s)  < deg^..  D(s)] for /  =  1... /? (see Lemma 2.1). When  D(s) is row reduced then the conditions are necessary  and sufficient  (see Lemma 2.2). C.  Hermite  and Smith  Forms By elementary row and column operations a polynomial matrix P(s) can be reduced to the Hermite form or the Smith form.  These special forms  are studied in this sub section together with algorithms to reduce P(s) to such  forms. 532 Linear Systems Hermite  form Given P(s)  G R[sY^^  with p  >  m there exists a unimodular matrix  UL(S)  such that  UL(S)P(S) is an upper (right) triangular matrix of the  form ••  X ••  X X X "  X 0 UL(S)P(S) Pm(s) 0 0 0 0 0 0 0 X ••  0 •• 0 (2.7) r  =  rankP(s) where  Pm(s)  G  R[s]^^^.  When  p  >  m^ the  last  p  -  r rows  are identically zero. In column J 1 <  j  <  r the diagonal element is monic and of higher degree than any (nonzero) element  above it. If the diagonal element is one then all elements  above it are zero. No particular  form  is assumed  by  the remaining  m  -  r columns in the top r rows. This is the (upper  triangular)  column  Hermite form.  Note that if  P(s)  is of full  rank  then  UL(S)P(S) is column  proper.  By  postmultiplication by a unimodular matrix  UR(S)  it is possible to obtain the row Hermite form  of  P(s) when  p  <  m.  To  accomplish  this  simply  determine  UL(S) in  such  a manner  that UL(S)P^(S) is  in  column  Hermite  form  and  take  (UL(s)P^(s)y  =  P(s)Ul(s)  = P(S)UR(S)  which is in row Hermite  form. The following  algorithm reduces  P(s)  G R[sy^^  p  >  m to (upper  triangular) column  Hermite  form  by  elementary  row  operations  [premultiplication  by  UL(S)]. This algorithm can also be used to constructively prove our desired result that there exists a unimodular matrix  UL(S)  such that  UL(S)P(S) (p  >  m) is in column Hermite form. The  algorithm  is  based  on  polynomial  division.  Given  any  two  polynomials a(s)  b(s)  b(s)  #  0  there  exist  unique  polynomials  q(s) r(s)  such  that  a(s)  = q(s)b(s)  +  r(s)  [q(s) is the quotient and r(s) is the remainder] where either r(s)  =  0 or deg r(s)  <  deg b{s). By row interchange transfer  to the (1 1) position the lowest degree element  in the  first  column  and  call  this  element  pn.  Every  other  element  pn  in  this  column can be expressed by polynomial division as a multiple of pn  plus a remainder term of lower degree than pu  i.e.. qnPn  +  ni where deg rn  <  degp\\. (2.8) By elementary row operations the appropriate multiple of j^n  can be subtracted  from each entry of column  1  leaving only remainders rn  of lower degree than pi\.  Repeat the above steps until all entries in the first column below  (1  1) are zero and note that the (1 1) entry can always be taken to be a monic polynomial. Consider next the second column and position (2 2) while temporarily  ignoring the first row. Repeat the above procedure to make all the entries below the (2 2) entry equal to zero. If the  (1 2) entry does not have lower degree than the (2 2) entry use polynomial  division  and row  operations  to replace the  (1 2) entry  by  a polynomial of lower degree than the (2 2) entry. If the (2 2) entry is a nonzero constant use row operations to make the (1 2) entry equal to zero. Continuing this procedure with the third fourth  and higher columns results in the desired Hermite  form. 533 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems EXAMPLE  2.13. s(s  + 2) 0 s(s  + 2) 0 5 + 2 5 +1 P(s)  = 0 (s +  l)(s  + 2) 0 Ui (S+ 1)2 5 + 1 s(s + 1) 0 5 +2 0 (s+  1)2 5 +1 5(5 + 1) U2 0 (5 + 1)2 5(5 + 2) 0 0 S(S  + 1) 5 + 2 5 +1 5 + 2 0 0 0 ^4 (5 + 1)2 -s(s  + 1) s(s + 1) 0 0 0 5+  1 (s + 1)2 5+  1 5+  1 ^5 5 + 2 0 0 0 5 +1 5 +1 (5 + 1)2 5+ 1 U3 t/6 "5 + 2 5 + 2 0 0 0 5 + r 5+  1 Uj 0 0 0 0 0 0 0 1 0 0 f  1 5 -0 0 ri 0] 0 0 0 0 ij [0 5+  1 -5 = UL(S)P(S) 0 1 0 0 - ( 5 + 1)  1 0 -1 0" 0 0 1 ' 1 0 00 0  10 - 10 0 10 0  0  0  1 S(S  + 1) 5 - (5  + 2) 5 +1 - (5  +  1)2 - ( 5 + I) -1 1 -5 0 where L(5)  =  UU6'"Ui 1 0 0 0 -1 1 0 0 Notice that P{s) has full  rank and  UL{S)P(S)  here is column proper. • Note  that  to  determine  UL(S) directly  one may  decide  on  the  necessary  op erations  based  on  P(s)  but  apply  these  elementary  row  operations  to [P(s)Ip]. Then  UL(S)[P(S) Ip]  =  [Hp(s)  UL(S)]  where  Hp(s)  is the column  Hermite  form of P(s)  (p  >  m). Also  note  that  if the algorithm  is applied  to ^(5") G  R[S]P^^  where p  ^  m  then UL(S)P(S) =  [Pi(s\P2(s)] = where  Pi (5)  G Ris^P. X  X 0 X 0  0 X X X X X X (2.9) Smith  form Given  P(s)  G  R[sy^^  with  rankP(s)  =  r  there  exist  unimodular  matrices UL(S)  and UR(S) such  that UL(S)P(S)UR(S) =  Sp(s) (2.10) where Sp(s)  = K{s) 0 0 0 A(^)  = diag{ei{s)..er{s)). 534 Linear Systems Each ei(s)  i  =  1...  r is a unique monic polynomial satisfying  ei(s)  \ ei+i(s)  i  = 1...  r  -  1 where P2(s)\pi(s)  means that there exists a polynomial P3(s) such that Pi(s)  =  P2(s)p3(s)  that is €i(s)  divides ei+i(s).  Sp(s)  is the Smith form  ofP{s)  and the ei{s)  i  =  1...  r are the invariant polynomials  of  P{s).  It can be shown that etis)  = Di(s) Di-i(sy 1  ...r (2.11) where  Di(s)  is  the monic  greatest  common  divisor  of  all  the nonzero  zth order mi nors of  P(s).  Note that Do(s)  =  1 and Di(s)  are the determinantal  divisors  of  P(s). The  Smith  form  Sp(s)  of  a matrix  P{s)  is unique  however  UL(S)  UR(S)  such  that UL(S)P(S)UR(S) =  5'p(5') are not unique. EXAMPLE  2.14.  P(s)  = s(s + 2) 0 0 (s+  1)2 I I (s +  1)(^ + 2) ^ + 1 0 s(s + 1)J with  rankP(s)  =  r  =  2. Here Do  =  hDi  =  hD2  =  (s  +  l)(s  +  2)  and (s +  l)(s  + 2). Therefore the Smith form of P(s)  is ei  =  DI/DQ  =  1 €2  =  D2/D1  = 1 0 0 (s+  l)(s  + 2) Sp(s) AW 0 The  invariant  factors  6/  of  a  matrix  are  not  affected  by  row  and  column  ele mentary operations. This follows from the fact that the determinantal divisors Dt are not  affected  by  elementary  operations  (refer  to the Binet-Cauchy  formula  in  Exer cise 7.3). In view of this the following  result  can now be easily  established:  given Pi(s)> Piis)  ^  R[s]P^^  there exist unimodular matrices  Ui(s)  U2(s) such that U(S)PI(S)U2(S) =  P2{S) if and only if Pi(s)  P2(s) have the same Smith  form. The following  algorithm reduces P(s)  E  R[sy^"^  to its unique Smith form  Sp(s) =  Sp(s).  The algorithm  can and determines  UL(S)  UR(S)  such that  UL(S)P(S)UR(S) be used to constructively  prove that the Smith form of a matrix exists. Using row  and column elementary  operations transfer  the element  of least de gree in the matrix P(s) to the (1  1) position. By elementary row operations make all entries  in the first column  below  (1 1) equal  to zero  (refer  to the  algorithm  for  the column Hermite form). Next by column operations make all entries in the first row zero except  (1  1). If nonzero entries have reappeared  in the first column repeat the above steps until all entries in the first column and row are zero except for the  (1  1) entry.  [Show  that  at each  iteration  the  degree  of the  (1 1) element  is reduced  and thus the algorithm is  finite.] If the  (1  1) element does not divide every other entry in the matrix use polyno mial  division  and row  and column  interchanges  to bring  a lower degree element to the (1  1) position. Repeat the above steps until all other elements in the first column and row are zero and the  (1  1) entry divides every other entry in the matrix that is. 535 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems 0 eiis) 0 Ei(s) 0 where 61(5') divides all entries ofEi(s).  Repeat the above steps on Ei(s)  and on other such terms if necessary  to obtain the Smith form  of  P(s). Two  polynomial  matrices  Pi(s)  P2(s)  E  R[sy^^ there exist unimodular matrices  U\{s)  U2(s) such that are  said  to  be  equivalent  if Ui(s)Pi(s)U2(s)  =  P2(S\ (2.12) Recall that as was mentioned earlier (2.12) is satisfied if and only if Pi (s) and P2(s) have the same Smith  form. The Smith form of P{s) is a canonical form for the relation (2.12) on R[sy^^.  To see this we first recall the definition of relation and equivalence relation: a relation  p in  a set X  is any  subset  of X  X X  and  p  is  an equivalence  relation  if  and only  if  it satisfies  the following  axioms: 1.  xpx-reflexivity  (every  x  G X is equivalent to  itself). 2.  {xpy)  =^ (3;px)-symmetry  {x is equivalent to y implies that y is equivalent to x). 3.  {xpy)  and {ypz)  =^ (xpz)-transitivity  {x is equivalent to y and y is equivalent to z imply that x is equivalent to z). The  relation  p  described  by  U1P1U2  =  P2  in  (2.12)  (i.e.  P1PP2)  satisfies the  above  axioms  (verify  this)  and  therefore  it  is  an  equivalence  relation.  The  p-equivalence  class  or  the  "orbit"  of  a  fixed  P(s)  G  RlsY^'^  is  denoted  by  [P(s)]p. The Smith form Sp(s)  of P(s)  is a canonical form  for  p on  R[sY^^. D.  Coprimeness  and  Common  Divisors Coprimeness  of  polynomial  matrices  is  one  of  the  most  important  concepts  in  the polynomial matrix representation  of systems  since it is directly related to controlla bility and observability  (see Subsection  7.3B). A polynomial  g(s)  is a common  divisor  (cd) of polynomials  pi(sX  P2(s) if  and only if there exist polynomials  pi(s)  p2(s)  such that Pi(s)  =  pi(s)g(s) p2(s)  =  p2(s)g(s) (2.13) The highest degree cd of pi(^) P2(s) g'^is) is a greatest  common  divisor  (gcd) of  pi(s)  P2{s). It is unique  within  multiplication  by  a nonzero  real  number.  Alter natively  g*(5') is  a gcd  of  pi{s)  P2(s) if  and  only  if any  cd  ^(^5") of pi(s)  P2(s) is a divisor of g'^is) as well that is. g^'is)  =  m(s)g(s) (2.14) with  m(s)  a polynomial.  The polynomials  pi(s)  P2{s) are coprime  (cp) if  and  only if a gcd g^'is) is a nonzero real. The above can be extended to matrices. In this case both right divisors and  left divisors must be defined  since in general two polynomial matrices do not commute. Note that one may talk about right or left divisors of polynomial matrices only when the matrices have the same number of columns or rows respectively. 536 Linear  Systems An  mx  m  matrix  GR{S) is a  common  right  divisor  (crd) of the pi  x  m  poly nomial  matrix  Pi{s) and the p2Xm  matrix  P2{s) if there  exist  polynomial  matrices PiR{s)P2R{s)sothsit PI{S)=PIR{S)GR{S) P2{S)=P2R{S)GR{S). (2.15) Similarly  a.  p  x p  polynomial  matrix  GL{S) is a  common pxmi matrices  PIL{S)^P2L{S) SO that polynomial  matrix Pi {s) and the pxm2  matrix P2{s)  if there exist  polynomial left  divisor  (eld) of the Px{s)  =  GL{S)PIL{S) P2{S)  =  GL{s)P2ds). (2.16) Also  G^{s) is a greatest  common  right  divisor  (gcrd)  of Pi{s)  and P2{s) if and  only if  any crd GR{S) is an rd of G^(^).  Similarly  G£(^) is a greatest  common left  divisor (geld)  of A [s) and P2{s) if and only if any eld GL{S) is an Id of G£(^). That is. GI{S)=M{S)GR{S) Gl{s)  = GL{S)N{S) (2.17) with  M{s)  and N{s)  polynomial  matrices  and GR{S) and Gi{s)  any crd and eld  of Pi {s) P2 ('^)  respectively. Alternatively  it can  be shown  that  any crd G\{s)  of Pi{s)  and P2{s)  [or a  eld G£(^)  of A('^)  and P2{s)]  with  determinant  of the highest  degree  possible  is a  gcrd (geld)  of the matrices. It is unique  within  a premultiplication  (postmultiplication) by a  unimodular  matrix.  Here it is assumed  that  GR{S) is nonsingular.  Note  that if  rank : m [a (pi + P 2)  X mmatrix]  which  is a typical  case  in polynomial  matrix Piis) system  descriptions  then  rank  GR{S) =  m that  is GR{S) is nonsingular.  Notice  also that if rank  GR{S) < m then in view of Sylvester's  Rank Inequality  rank \Piis)\ Piis) < m as  well. The  polynomial  matrices  Pi (s)  and P2 (s)  are right  coprime  (re) if and only  if a gcrd  G^{s) is a unimodular  matrix.  Similarly Pi{s) and P2{s) are left  coprime  (Ic) if and  only if a geld  G2{s) is a unimodular  matrix. E X A M P LE  2.15.  Let Pi Two distinct crds are GR^  = 5(5 +  2) 0 0 0 5 +1 ( 5 + 1 )2 and  GR^ and  P2  = • ( 5 + l ) (5 +  2)  5 +1  • 5 ( 5 + 1) 0 5 +  2 0 0 ( 5 + 1 )2 GR. G/?2 5 +2 0 5 +1 5 ( 5 + 1) 5 +  2  0 1 0 GR^ 1 0 0  5 +1 GR^.  N O W - » * —1 0 5 +1 1 5 5 ( 5 + 2) 0 (5+l)(5 + 2) 0 5 +  2 0 5 +1 0 0 5 +1 where  Pf^  and P2j^  are re. Note  that  a geld  of Pi  and P2 is A  gcrd is  G^ s 0 5 +1 0 1 5 G; = 0 s+  1 Both  G^  and  Gl  were  determined  using  an  algorithm  to  derive the Hermite form of as will be described later in this  section. It  can  be  shown  that  two  square  p  X  p  nonsingular  polynomial  matrices  with determinants that are prime polynomials  are both re and Ic. The converse of this is not true  that  is two re polynomial  matrices  do not necessarily  have  prime  determinant polynomials.  A  case  in point  is Example  2.15 where  P j^ and PJ^  are re;  however detP\j^  =  detP2R  =  s(s  +  1). Left  and right  coprimeness  of  two polynomial  matrices  (provided  that  the ma trices  are  compatible)  are  quite  distinct  properties.  For  example  two  matrices  can be  Ic but not re and vice  versa  (refer  to the following  example). 537 CHAPTER?: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems EXAMPLE  2.16.  Pi  =  |  ^ 0 .  J  and P2  =  I s+  1.  and P2 = (s + 1)(^ + 2)  1 s ^ 0 I are Ic but not re since a gcrd is G^ with detGl  = (s + 2). \s(s + 2) L  0 's + 2  0 1. 0 Finally  we note  that  all the above  definitions  apply  also  to more  than  two  poly nomial  matrices. To see this replace  in all definitions  P i P2 by P i P2 • . .  P^. This is  not  surprising  in  view  of  the  fact  that  the  p\X  m  matrix  P\{s)  and  the  p2  X  m matrix  P2(s)  consist  of  pi  and p2  rows  respectively  each  of  which  can be  viewed as a  1 X m polynomial  matrix;  that is instead  of  e.g. the coprimeness  of Pi  and P2 one  could  speak  of the coprimeness  of the (pi  +  P2) rows  of Pi  and P2. How  to determine  a greatest  common  right  divisor  (gcrd) LEMMA  2.3.  Let Pi(s)  e  /?M^i><^ and P2(s) e  R[S]P^'''^  with pi  + P2 ^  m. Let the unimodular matrix  U(s) be such that U(s) Piis) 0 Then  G^(^) is a gcrd of Pi(^)  P2(s). Proof.  Let U  = X -Pi Y Pi (2.18) (2.19) withX  G RisT""^^ y  e  R[sT''P\  P2 G RWP\2indPi G PM^><^2 where^  =  (pi + P2)  -  m. Note  that  X Y  and P2 Pi  are Ic pairs. If  they  were  not then  det U  7^ a 3. nonzero real number. Similarly X P2 and Y Pi  are re pairs. Let [/-' \Pi -Y] vh  ^\' (2.20) where  Pi  e  RW^'^"'  P^  G RisY^^""  are re  and X  G  R[s]P2'"i Y  G RisV^'^''  are re. Equation  (2.18) implies  that Pi 1P2  Gl i.e. G^ is a crd of Pi Pi- Equation  (2.18) implies  also that f/-' 0 XPi  +  YP2  =  G^. (2.21) (2.22) 538 Linear  Systems This  relationship  shows  that  any  crd  GR of  Pi P2 will  also  be  an  rd  of  G\.  This  can be  seen directly by  expressing  (2.22)  as MGR  =  G*j^  where Af is a polynomial  matrix. Thus  G^  is  a crd of  Pi P2 with the property  that  any  crd  GR of  Pi P2 is an rd of  G^. This impHes that G^ is a gcrd of Pi P2. • EXAMPLE  2.17.  Let  Pi  = Then 's(s +  2) . 0 0 (s + 1)^ P2 (s +  l)(s  +  2) 0 ^ +  1  ' s(s  + 1) U  Pi X -Pi Y Pi} Pi IP2I -(s  +  2) s+  1 -(s+lf -(s  +  1) -1 1 —s 0 s+  1 -s 0 0 s(s  +  1) 0 s -1 Pi P2} s + 2 0 0 s+  1 G-0 0 In view of Lemma 2.3 G^  = 5 +  2 0 0 s-hl is a gcrd (see also Example 2.15). Note that  to derive  (2.18)  and  thus  determine  a gcrd  G^  of  Pi  and  P2 one  could use  the  algorithm  that  was  developed  above  in  Subsection  7.2C  (or  a  variation  of this  algorithm)  to  obtain  the  Hermite  form.  Finally  note  also  that  if  the  Smith  form of  Pi Pi is known  i.e.  UL  PI PI  then  (diag  [e/] 0)t/^  ^ is UR  =  SP  = diaglEi] 0 a gcrd  of  Pi  and  P2  in view  of Lemma  2.3. When  rank =  m which  is the  case 0 0 Pi Pi of  interest  in  systems  then  a  gcrd  of  Pi  and  P2  is  diag  [6/]L^^ ^. Criteria  for  coprimeness There  are  several  ways  of  testing  the  coprimeness  of  two  polynomial  matrices as  shown  in  the  following  theorem. THEOREM  2.4.  Let  Pi  E  R[sV^'''^  and  P2  G  P[5]^2xm  with  pi  + p2  ^  m.  The  fol lowing  statements are  equivalent: (i)  Pi  and P2 are re. (ii)  A gcrd of Pi  and P2 is  unimodular. (iii)  There exist polynomial matrices X  G  R[S]'^^P^  and  Y  G  R[S]'^^P^  such that XPl  +  FP2  =  Im. (2.23) (iv)  The Smith form of (v)  rank  PliSi) PliSi). m for any complex number Si. constitutes m columns of a unimodular  matrix. 539 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems Proof  Statements (i) and (ii) are equivalent by definition.  Assume now that (iii) is true. Then  (2.23)  impUes that  any  crd  of  Pi  and  P2  must  be  an rd  of Im which  is of  course a crd of Pi  and  P2. Therefore  in view  of the definition  of a gcrd Im is a gcrd  (see also proof  of Lemma  2.3)  and  so (ii) is true. To show that  (ii) also implies  (iii) determine  a gcrd  GJ  as in the Lenrnia 2.3 and note that  GJ  is unimodular.  Premultiplying  (2.22) by we obtain  (2.23). To show (iv) recall that a gcrd of Pi  and P2 can be determined from the Smith  form Px' P2 as GJ  =  (diag [£/] 0)t/^ ^  (refer to the discussion following Example 2.17). It is of now clear that a gcrd will be unimodular if and only if the Smith form of IS (i.e.. r/1 LOJ (iv) and (ii) are equivalent). To show  (v) consider  (2.18) and note that  rank \Pl(Si)]  ^ [Piisi)! rank GJ(5'/). The only Si that can reduce the rank are the zeros of the determinant of  GJ. Such  Si do not exist  if  and  only  if  detG]^  =  a  a nonzero  real number  i.e. if  and  only if G^ is unimodular. Therefore  (v) implies and is implied by (ii). Part (vi) was shown in • the proof of Lemma 2.3. EXAMPLE  2.18.  (i)  The  polynomial  matrices  Pi  = 0 ^+  1 P2  = s+  1 0 (see also Example  2.15)  are re in view  of the following  relations. To use  condition  (ii) of Theorem 2.4 let -(s  +  2) -1 s+  1 U  Pi Pil s+  1 1 -(s  +  If —s -(s  +  1) 0 s(s  +  1) \PI [Pl. = '  1 0 0 0 0" 1 0 0 = .  0  . Then  G^  =  h  which is unimodular. Applying condition  (iii) XPi  +  FP2  = "  (^ + 2)  -11 s+1 l|  ^ p +1  0" I  -^ 0 / 2. To use (iv) note that the invariant polynomials of areei  =  e2  =  1; the Smith Pi . To use  condition  (v) note that  presently  the  only  complex  values st form  is  then h 0 that  may  reduce  the  rank  of ~Pl(Si)] P2(Si)\ are  those  for  which  detP\{Si)  or  detP2(si)  =  0 i.e. ^1  =  0 and S2 =  - 1.  For these values we have  rank and  rank  'Pl(S2) Piisi). =  rank ' -1 0 0 0 0" 0 1 -1 =  2 i.e. both are of full  rank. 'PliSl) Piisi). =  rank 0  0 0  1 1  1 0  0 =  2 Note  that  if  the  criteria  for  coprimeness  given  in  Theorem  2.4  are  used  for  a pair  P i  P2  that  is  not  re  then  (ii)  will  provide  a  gcrd  G^  of  P i  ^2- The  other  tests provide  only  partial  information  about  G^.  In  particular  applying  (iv)  one  obtains the  Smith  form  of  G^  (show  this) while  (v)  will  give  information  about  the  zeros  of 540 Linear Systems THEOREM 2.5.  Let Pi  G RlsV""^ and P2 ^  RisV"^^ with mi + m2 >  p. The fol-lowing statements are equivalent: (i)  Pi and P2 are Ic. (ii)  A geld of Pi and P2 is unimodular. (iii)  There exist polynomial matrices X G R[S]'^I^P  and Y G R[S]'^^^P  such that PiX + P2Y = Ip. (2.24) (iv)  The Smith form of [Pi P2] is [/ 0]. (v)  rank [Pife) P2(si)]  = p for any complex number si. (vi)  [Pi P2] are p rows of a unimodular matrix. Proof. The proof is completely analogous to the proof of Theorem 2.4 and is omitted. E.  The Diophantine  Equation The linear Diophantine Equation of interest to us is of the  form X(s)D(s)  +  Y(s)N(s)  =  Q(sl (DIO) where D(s) N(s)  and Q(s) are given  polynomial  matrices  and X(s) Y(s) are to be determined.  This  equation  will  be  studied  in  detail  in  this  subsection.  First  par ticular  solutions  of the Diophantine  Equation  are derived.  Next  all  solutions  are conveniently  parameterized.  These parameterizations  are used  later in this  chapter (in Section 7.4) to characterize  all stabilizing  linear feedback  controllers of a linear time-invariant  system. This subsection is concluded  with some historical remarks. In  greater  detail  solutions  of the polynomial  Diophantine  Equation  of low de gree are derived in Lemma 2.6 using the Division Theorem for polynomials and in Lemma  2.8 using the Sylvester  Matrix  of two polynomials. Corresponding  results for the polynomial  matrix Diophantine Equation  are derived in Lemma  2.12 using the Division Theorem and in Lemma 2.14 using the Eliminant Matrix. All solutions of the Diophantine Equation are parameterized  in Theorem 2.15. The polynomial  case Given  coprime  polynomials  d{s)  and n(s) it was shown  in Theorem  2.4 that there exist polynomials  x{s) and y{s) such that x(s)d(s)  +  y(s)n(s)  =  1. (2.25) It can be shown that x(s) and y(s) are unique when certain restrictions  are imposed on their degrees. In particular the following  result is true. LEMMA 2.6.  Let d(s) and n(s) be coprime polynomials. Then there exist unique poly nomials x(s) and 5^(^) that satisfy with deg y(s) < deg d(s) [or deg x(s) < deg n(s)]. x(s)d(s) + yisMs)  = 1 (2.26) (2.27) Proof. In view of Theorem 2.4 d(s) and n(s) are coprime if and only if there exist poly nomials x(s) and y(s) such that x(s)d(s) + y(s)n(s)  =  1. The basic Division Theorem for polynomials can now be used to obtain polynomial solutions of lower degree. In par ticular there exists a unique quotient polynomial q(s) and a unique remainder polynomial 541 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems r{s) such that Now  define y(s)  =  q(s)d(s)  +  r(s) with deg r(s)  <  deg  d(s). y(s)  =  r(s)  =  y(s)  -  q(s)d(s) x(s)  =  x(s)  +  q(s)n(s) (2.28) + where  degy(s)  <  degd{s).  Then  x{s)d{s)  +  y{s)n{s)  =  x(s)d(s)  +  y(s)n(s) [q(s)n(s)d(s)-q(s)d(s)n(s)] =  x(s)d(s)  + y(s)n(s)  =  I. Also  note that deg [x(s)d(s)]  = deg [I  -  y(s)n(s)]  <  deg[d(s)n(s)]  so  that  deg x(s)  <  degn(s).  To prove  uniqueness suppose there exists another pair x(sX y(s)  that satisfies  the lemma.  Then [x{s)  -  x{s)]dis)  +  [>^(^) -  y(s)]n(s)  =  0 which imphes  that [ x W - x W]  = -[y(s) d{s) Since deg [y{s) -  y{s)\  <  deg d(s)  and the left-hand  side is a polynomial this rela tion implies that d(s)  and n(s) must have a common factor that contradicts the assumption that d(s)  and n(s) are coprime. Therefore  x(s)  and y(s)  are unique. Note that the proof of the lemma when in (2.27) deg x(s)  <  deg n(s) (for the part in parentheses) is completely analogous. In this case let x(s)d(s)  +  y(s)n(s)  =  1 and divide x(s)  by  n(s). • Given  coprime polynomials  d(s)  and  n(s)  one way  of determining  x(s)  and  y(s) that  satisfy  the  above  lemma  is  to  follow  the  procedure  outlined  in  its proof.  In  par ticular  x(s)  and  y(s)  that  satisfy  (2.25)  are  determined  first  and  then  polynomial division  is  used  to reduce  their  degrees  if  necessary. The  following  example  illustrates  this  procedure. EXAMPLE  2.19.  Let  d(s)  =  s(s  -  1)  and  n(s)  =  s x(s)d(s)  + y(s)n(s)  =  \s[s(s-l)]-l(s^ + s + 2)[s-2]  = Us^  + s  + 2)  =  q(s)d(s)  +  r(s)  =  (-\)[s(s Then y{s)  • ^ which we obtain k(s'-2  which  are  coprime.  Let =  1. -  1)] +  [-\{s  +  1)] from ^)-Us^-s^-4) yi^s) =  r{s)  =  -\{s+ 1)  and x{s)  =  x(s)  +  q(s)n(s)  =  ^s  + (-\)(s -  2)  =  ^ Note that  x(s)d(s)  +  y(s)n(s)  =  ^[s(s  -  1)] +  (-\(s  +  l))ls  -  2]  =  1 and degy(s)  = 1  <  degd{s)  =  2 [degx(s)  =  0<  degn(s)  =  1]. The polynomials  x(s)  and  y(s)  can  also be  determined  using  the  Sylvester  Matrix of the polynomials d(s)  and n(s)  which in fact provides an alternative way of testing the coprimeness of d(s)  and n(s).  The Sylvester Matrix of d(s)  and n(s)  is now  introduced. Consider the polynomials d(s)  =  dnS""  +  dn-lS''^^  +  '"  +  diS  +  do n(s)  — riynS^  +  nyn-\s^~^  +  • • • +  n\s  -\-  n^ dn^O m <  n. (2.29) The Sylvester  Matrix  of the polynomials d{s) n(s)  is defined  as 0 0 " dl ••  d2 dn 0 do dx 0 do dn-2 dn-l dn-l dn ' ••  0 ••  0 S(d  n)  = 0 ^m 0 0 rim-i ^m 0 nm-2 nm-\ " 0 ••  no •' nx ' ' dn 0 no dn-l 0 0 dn-2 0 0 ' ••  do ••  0 ••  0 (2.30) 0 0 0 0 nm-i no 542 Linear  Systems where the block that contains the coefficients  of d(s)  has m rows and the one that contains the coefficients  of n(s)  has n rows. The coefficients  are arranged  so that there art  n + m c o l u m n s  i.e.  S(d  n)  E  Rin+m)X{n+m)^ The determinant  of S{d n) is known  as the Sylvester  Resultant  or Resultant  of  the polynomials  d{s)  and  n{s).  Note that the matrix  S{dy n)  is  sometimes^eferre4ta^s  the Eliminant  Matrix  of the polynomials d{s) n(s). • THEOREM2.7.  The polynomials d(s)  and n(s)  are coprime if and only if det S{d n) 7^ 0 that is if and only if their resultant is nonzero. Proof  (Necessity)  Suppose  that  d{s)  and  n{s)  are  coprime  but  their  resultant  is  zero. This implies that there exists a nonzero vector a  a^  G /?""+" such that aS{d  n)  =  OOY such that ^n+m-l ^n+m-2 aS{d  n) =  a S 1  J 's'^-^disY sd(s) d(s) s^'-^nis) sn(s) .  n(s)  J = [ai(s)a2(s)] d(s) n(s)^ =  0 where 0:1(5)  =  ai anda2W  =  «2 with [010:2]  =  a. Note that both oiC^y) and  0:2(5)  are  nonzero  since  otherwise  either  n(s)  or d(s)  would  be  zero  and  therefore n(s) and d(s)  would not be coprime. Write d(s)/n(s)  =  -a2(s)/ai(s).  Since deg02(5)  < degd(s)  and  degai(s)  <  degn(s)  it  follows  that  d(s)  and  n(s)  must  have  a  common factor.  Therefore  d(s)  and  n(s)  are not coprime which is a contradiction. Thus S(d  n) has full  rank or detS(d  n)  #  0. (Sufficiency)  Assume  that  detS{d  n)  ^  0.  Take  a  =  [0...  0 l]S(d  n)'^  and let 0:1(5)  =  o:i[5'"~^ ...  5 1]^ 02(5)  =  02[5'^~^ .. .5 1]^ where  [ai 02]  =  a.  Then 0:1(5)^(5)  +  0:2(5)^^)  =  o:5(J^)[5"+^-i5"+'^-2  . . .  5  1]^  =  1  which  implies  in view of Theorem 2.4 that d(s)  and n(s)  are coprime. • Remarks (i)  Theorem  2.7  is  attributed  to  Sylvester  (1840). (ii)  A  useful  relation  used  in  the proof  of  Theorem  2.7  is s'^-^dis)] ^n+m-\ ^n+m—2 S(d  n) sd{s) d(s) sn(s) n(s) (2.31) where  d(s)  and  n(s)  are  given  in  (2.29)  and  S(d  n)  is  defined  in  (2.30). (iii)  It is possible to arrange the coefficients  in S{d  n) in several different  ways recalling  that  elementary  row  and  column  operations  leave  the  rank  of S{d  n) unchanged. For example forn  =  3 m  =  2 (a) ^3 0 «2 0 0 di d3 n\ «2 0 d\ d2 no n\ ni do  0" d\ do 0  0 no 0 n\  no J  (b) d^  ^2 0 d3 0  0 n2 0 _n2 ni dx d2 n2 ni no do d\ ni no 0 0 do no 0 0 543 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems (c) di do 0 do no  ni 0 no 0  0 d3  0" d2 d2  d3 di 0 0 n2 ni n2  0 no  n 1  n2 are all nonsingular  if and only if d(s)  and  n(s)  are coprime. The matrix in (a) is the Sylvester Matrix as defined  in (2.30). The matrices in (b) and (c) are variations of the Sylvester Matrix found  in the literature (iv)  By adding zero coefficients  in n(s) it can always be assumed that the poly nomials d(s)  and n(s) have equal degrees (m  =  n) when using the test pro vided by Theorem 2.7. This leads yet to another variation of the  Sylvester Matrix of two polynomials  of dimension  2n  X  2n. Using  the  procedure  followed  in  the  sufficiency  proof  of  Theorem  2.7  it  is straightforward  to  derive  the  unique  polynomials  x(s)  and  y(s)  that  satisfy  (2.26) and (2.27) of Lemma 2.6. This is illustrated in the next example. EXAMPLE2.20.  Lti d(s)  = s(s-l)  midn(s)  = ^-2 which are coprime as in Example 2.19. In this  case the  Sylvester  Matrix  or the eliminant  of d(s)  and  n(s) in  (2.30) is -1 -2 01 0  and the resultant  is detS{d n)  =  2 T^ 0 as expected  since n S(d n)  =  \l LO 1  - 2J d{s) and n{s) are coprime polynomials. In view of the sufficiency  proof of Theorem 2.7 let a  =  [aia2\  =  [00l]S(d  n)'^  =  [001] [i i-ij.Then [4 -2 0 x(s)  = ai(s)  =  ^y(s)  = a2(s) determined in Example 2.19. - ^(5 +1) which also is the answer Using the Sylvester Matrix of coprime polynomials enables one in fact to deter mine solutions to the more general Diophantine  Equation x(s)d(s)  +  y(s)n(s) q(sl (2.32) as the following  lemma  shows. LEMMA  2.8.  Let  d(s)  and  n(s)  be  coprime  polynomials  with  degd{s)  =  n  and degn{s)  =  m ^  n  given  by  (2.29). Let  q(s) be  a polynomial  of  degree  n + m -  1. Then there exist unique polynomials x{s) and y{s) that satisfy with f x(s) < m and deg y(s) < n. x{s)d{s) + y{s)n{s) = q(s) (2.33) (2.34) 544 Linear  Systems Proof  The  proof  is  constructive.  Let  x{s)  =  [x^-i.  • • xi  xo][s'^  ^..  .sl]^ y(s)  =  [yn-h-"yhyo][s''~K...slV2indwntQx(s)d(s) yn-h  . •. yo][s'^~^d(s)...  sd(s)  d{s) s^'-^nis)... yn-u  ...  yo]S(d n) [s-^^-\ where relation  (2.31) was used. This equality  is now satisfied  if and only if ...  ^ 1]^ =  q(s)  =  [qn^^-i  •..  ^i q^Ms^^^'K  •. • ^ 1 ]^ let + y(s)n(s)  =  [ x ^ - i  . . . XQ sn{s) n{s)Y [Xm-h • • • -^o = [Xm-\ ...XQyn-\... yQ\S{d u)  -  [qn+m-\ - "  q\  <?o] (2.35) is satisfied.  Since S{d n) is nonsingular Eq. (2.35) has a unique solution that determines unique x{s)  and y{s) that satisfy  relations  (2.33) and (2.34). • EXAMPLE  2.21.  Consider  d{s)  =  s{s  -  1)  and  n{s)  =  s  -  2  which  are  coprime as  in  Examples  2.19  and  2.20.  Let  q(s)  =  q2S^ +  qis  +  qo be  an  arbitrary  second-degree  polynomial  (n  +  m  -  I  = 2 + 1 -1  =2).  Relation  (2.35)  yields  in  this 0" 0 - 2. =  [q2 qi  qo]^ from  which  we  obtain  [XQ yiyo]  = case  [xoyiyo] "1 1 .0 -2 '4 2 -2 .1 -1 -2 1 0' 0 - 1 \[q2q\qo\ =  [2q2 +  ^1 +  ^0  -qi q\  -  hqo -\qo\  and -1 x{s)  =  XQ  =  2q2 +  q\ \qo y(s)  =  [yi yo] =  (-\qo)  + {-qi -  q\- \qQ)s. For  q{s) +  2 ^ + 1 x{s)  =  |  and y(s)  =  - \- Is. • COROLLARY  2.9.  Let  d(s)  and  n(s)  be  coprime  polynomials  with  degd(s)  =  n  and deg n(s)  <  n. Let q(s) be a polynomial with deg q(s)  <  2n- I. Then there exist unique polynomials  x(s)  and y(s)  that satisfy  the relation with deg x{s)  <  n and deg y{s)  <  n. x(s)d(s)  +  y(s)n(s)  =  q{s) (2.36) (2.37) Proof  The  proof  is  similar  to  the  proof  of  Lemma  2.8  where  in  place  of  S{d  n)  E ^(n+m)x(n+m)^  the  2n  X 2n  Sylvester  Matrix  S{d  n)  of  d{s)  =  dnS""  +  • • • +  Ji^  +  JQ (i„  7^ 0 and  n{s)  =  rinS^  +  ---  + nis  +  HQ  is used. Zero coefficients  are added in n(s)  if necessary.  Note  that  S(d  n)  is  exactly  S(d  n)  of  (2.30)  with  m  =  n.  This  leads  to  the relation {Xn-\  ...XQ  yn-\... y()\S{d n)  =  [qm-i  • • • ^b ^oL (2.38) which is the equation  corresponding  to (2.35). Since d{s)  and  n{s) are coprime S{d  n) • is nonsingular  and the x{s)  and y{s) that satisfy  (2.38) are unique. EXAMPLE  2.22.  As in Example  2.21 consider d{s)  =  s{s  — \)  =  s^  -  s and  n{s)  = 2  =  O'S^  -\-s- 2 which are coprime. Then  (2.38) implies  that [xi  xo yi yo] Ti 0 p 0 0 -1 -1 1 1 -2 0 1 ol 0 0 -2 =  [qs qi.  qi  qol where ^(5-)  =  ^3^^ + ^2-y^ + ^i^ + ^o is a third-degree polynomial ( 2 ^ -1  =  2 . 2 -1  =  3). It  can  easily  be  seen  that  one  could  equivalently  solve  xi  =  qs  and  [XQ yi  yo] X "1 -1 =  [qi + q-iy qu  qo] (compare with Example 2.21) from  which we obtain 01 0 -2 545 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems x(s)  =  [xi xo] =  qss +  [2(q2 +  ^3) +  ^1 + ^qo] y(s)  =  [yu yo] =  [-(qi  +  qs) -  qi- \qQ\s +  (-5^0). Note that for q^  =  0 this solution is precisely the solution of Example 2.21. • The  polynomial  matrix  case Similar  results  as in the preceding  can be  shown  for polynomial  matrices.  First the  Division  Theorem  for polynomial  matrices  is  established. THEOREM  2.10.  Let D{s) E  R[sT^'^  be nonsingular.  Then  for  any A^(^) G RisY^"^ there exist unique polynomial matrices  Q{s) R(s) such that N(s)  =  Q(s)D(s)  +  R(s) (2.39) with  R(s)D(s)  ^ strictly  proper  or with  deg^. R(s)  <  deg^. D(s) j  =  1... m  when D(s)  is column  reduced. Proof.  Let H(s)  =  N(s)D~^(s)  be a rational matrix. By polynomial division in each en try decompose this matrix uniquely into H(s)  =  Hsp(s) + P(s) where Hsp(s) is a strictly proper  rational  matrix  and  P(s)  is  a  polynomial  matrix.  Then  N(s)  =  H(s)D(s)  = P(s)D(s)  +  R(s) where R(s)  =  Hsp(s)D(s).  Then  R(s) is a polynomial  matrix  (since it is equal to N(s)  -  P(s)D(s))  and by definition  R(s)D~^(s)  =  Hsp(s) is strictly  proper. Note  that  when  D(s)  is  column  reduced  then  in  view  of  Lemma  2.2  R(s)D~^(s)  is strictly  proper  if  and  only  if  deg^. R(s)  < deg^. D(s)  j  =  1...  m.  We  shall  now verify  uniqueness  of  Q(s)  and  R(s)  directly;  it  also  follows  from  the  construction. Suppose  there  are  two  pairs  such  that  Q(s)D(s)  +  R(s)  =  Q(s)D(s)  +  R(s).  Then Q(s)  ~  Q(s)  =  [R(s) —  R(s)]D~^(s).  Since  the left-hand  side  is  a  polynomial  matrix and the right-hand  side is a strictly proper rational or zero matrix this equality can hold only when both sides are zero i.e. Q(s)  =  Q(s) and R(s)  =  R(s). • As  expected  the following  result  also  holds. THEOREM  2.11.  Let D(s)  G R[sY^P  be nonsingular.  Then  for  any N(s)  G RlsY^"" there exist unique polynomial matrices  Q(s) R(s) such that N(s)  =  D(s)Q{s)  +  R(s) (2.40) with D~^(s)R(s)  strictly proper or with deg^. R(s)  < deg^. D(s) i  =  1...  p when D(s) is row reduced. Proof.  The proof of this result is completely  analogous to the proof of Theorem 2.10.  • Given  re  polynomial  matrices  D(s)  G  R[s]^^^ it  was shown  in  Theorem  2.4  that  there  exist  polynomial  matrices  X(s)  and  Y(s)  such that and  A^(^) G  R[sy^"^ Let X(s)D(s)  +  Y(s)N(s)  =  Im. H(s)  =  N(s)D-\s) = D-\s)N(s) (2.41) (2.42) be proper  where D(s)  G  R[sy^P  and N(s)  G  R[sy^^ Let  V be the highest  degree  of the polynomial  entries  in D(s)  and denote  this  as are Ic with D(s)  row reduced. p  = degD(s). (2.43) 546 Linear Systems We point  out that  it can be  shown  that  v  is the observabihty  index  of the  system His)  = N{s)D-\s)  =  D-\s)N{s). LEMMA 2.12.  Given D{s) G RisT^"^ and N{s) E RisY^"^ that are coprime there exist polynomial matrices X{s) and Y{s) that satisfy X{s)D{s) +  Y{s)N{s) (2.44) such that deg Y(s)  < V. Proof This result can also be established by using the eliminant of N{s) and D{s) to be introduced in Lemma 2.14. In the present proof use is made of the Division Theorem for polynomial matrices (Theorem 2.10). Let X{s) E Risf'''^  and Y{s) E  R[SY'''P  satisfy (2.41). There  exist unique polynomial  matrices  Q{s)  E  R[ST^P  and R{s) E RisT'^P such that  Y{s) =  Q{s)D{s)  + R{s) with R{s)D'^{s)  strictly proper where b{s) is row reduced and satisfies  (2.42). Now define  Y{s) = R(s) = Y(s) -  Q(s)D(s) and X(s) = X(s) + Q(s)N(s). Then X(s)D(s) + Y(s)N(s)  = X{s)D(s) + Y(s)N(s) + [Q(s)N(s)D(s) -Q(s)D(s)N(s)] =  X(s)D(s) +  Y(s)N(s)  =  /^. Note also that in view of Lemma 2.1 if  R(s)D'^(s)  is strictly proper then the column degrees of R(s) are less than  v i.e. deg Y(s) <  V. In  addition  note  that  from  X{s)D{s) = Im -  Y(s)N(s)  it  follows  that • deg. (X(s)D(s)) = deg. (/^ -  Y(s)N(s)) <v  + deg. N(s\  j  =  l...m. Remarks (i)  When  D(s)  is  row  proper  then  n  =  deg\D(s)\  =  sum  of  the  row de grees  of  D(s)  <  pp  that  is  v  ^  n/p.  If  D(s)  is  not  row  proper  then n  =  deg \D(s)\  <  sum of the rows  degrees  of D(s)  and degD(s)  can be large. (ii)  In the polynomial  case  we have  that  v  =  n and Lemma  2.12 reduces to Lemma 2.6. (iii)  Using the Eliminant  Matrix of the re polynomial  matrices A^(^) and D(s) where D(s) is column proper it will be shown (in Lemma 2.14) that (2.44) has a solution with both deg Y(s)  <  v and degX{s)  <  v. The Eliminant  Matrix Consider  the polynomial  matrices  N{s)  E  R[sy^'^  and D{s) E  RisY"^"^ with D{s)  column  proper. Let deg^.N(s)  <  deg^.D(s)  =  dj j  =  1...  m and assume that dj  >  I j  =  1... m. The Eliminant  Matrix  Me of N(s)  and D(s) is a general ization of the Sylvester Matrix or of the Eliminant of two polynomials introduced in (2.30) and is defined in an analogous manner (see also Remarks following  Theorem 2.7). In particular we write A^(^) sN(s) D(s) sD(s) e ^ -l  D(s) =  MekSek(s)  =  Mek  block  diag (2.45) ^dj +  k-\ for  some integer k where M^k G RKp+rn)x{j^dj+mk)^  ^^^^  ^^^^ f^^ ^ > will  have  as many  as or more  rows  than  columns.  Let ^ =  V be the least  integer k that minimizes  (^dj  + mk) — rank M^k or equivalently  (as can be shown) let ^ = V denote the observability index of the system {DJ^N}  or of its equivalent  state-space description  (refer  to the discussion  on equivalence  of representations  in  Subsection 7.3A). {ldj)/pMek The Eliminant  Matrix  of N{s)  and D{s) is defined  as the v{p + m)  x{n  + mv) matrix Me=Mev 547 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems where n = ^dj  = deg  det D{s) since D{s) is column proper. Note that the observ ability index v satisfies  v > n/p  and therefore Me has as many as or more rows than columns.  The following  result  is a generalization  of Theorem  2.7 (which  involved the Sylvester Matrix of two polynomials) to polynomial matrices. THEOREM 2.13.  The polynomial matrices N{s)  and D{s) given above are re if and only if their eliminant matrix M^ has full column rank i.e. if and only if Proof The proof is omitted. For a proof of this theorem refer to Wolovich [36]. rank Me = n + mv. (2.46) • Remarks (i)  The Eliminant  Matrix  Me becomes  a  Sylvester  Matrix  when  applied  to -\-dis-\-do  and n{s) this is true where n{s) sn{s) polynomials.  In particular  for d{s) n^s^ -\-n2S^ -\-nis-\-no^p  = m=  l/i = n\ no 0 di do 0 "no 0 0 do 0 .0 =  ^3^^  + d2S^ di=v  =  3 m ni n\ d3 di di n2 ni no di di do d{s) sd{s) s^d{s)\ •MeSeis) 1 0 m ni 0 d3 di 0" 0 "3 0 0 d3. See also Remarks following  Theorem 2.7. It can be shown that (ii) rankMek =  rankMey ^>  V i.e.  the  rank  of  Mek is  the  maximum  possible  when  k  is  equal  to the observability  index of the  system.  This is a consequence  of the  alternative definition  for v previously  given. The  Eliminant  Matrix  can be used  to  determine  solutions  of the  Diophantine Equation The Diophantine Equation is further  discussed later in this  subsection. X{s)D{s)^Y{s)N{s)  = Q{s). (2.47) LEMMA 2.14.  Consider the re polynomial matrices N(s)  e  R[S]P'''^D(S)  e R[s]'^'''^. Let  D{s)  be  column  proper  and assume  deg^.N{s) < deg^.D{s) = djdj  > lj  = 1...m [note that H{s) = N{s)D-^ (s) is proper]. Let Q{s) G Rls]^"""^ with deg.Q{s)<dj  + v-l (2.48) 548 Linear  Systems where  v  is  the  observabiHty  index  of  the  system  {D I N O}. Then  there  exist  X(s) R[s]qxm and  Y(s)  G R[s]^''P that satisfy  the equation X(s)D(s)  +  Y(s)N(s)  =  Q(s) (2.49) with degX(s)  <  v  ~  I degY(s)  <  v  -  I where degX(s)  denotes the highest polynomial  degree in the entries of  X(s). Proof.  The proof is by construction.  Let X(s)  =  Xo+Xis  +  '-'+X^-is""^ =  [XoXu ...X-i] and Y(s)  =  Fo +  Fi5 +  • • • +  F.-i^""^  =  [YQ FI  ...  F-i] In. Sim 3 Iffi sin S^'-^In (2.50) (2.51) Then  Y(s)N(s)  +  X(s)D(s)  =  [YQ  F^ Yp-iXoXi.. .Xj-\] N(s) sN(s) D(s) sD(s) [Fo...  F^-i Zo... X^-i]MeSe(s)  =  Q(s)  =  QSe(s)  in  view  of the  definition  of  the eliminant  matrix  Me  and  the  assumptions  on  the  column  degrees  of  Q(s)  in  (2.48). Therefore  (2.49) is satisfied  with X(s)  and  Y(s)  given in (2.50) and (2.51) if and only if [Fo...F_iXo...X_i]M  =  Q (2.52) is  satisfied  where  M  G  R^(p+m)x(n+mv)  ^^^  Q  ^  j^qx(n+mv)  ^^^^  ^  ^  ^"J^i^j  = degdetD(s).  Since N(s)  and D(s)  are re it follows  from  Theorem  2.13 that rank Me  = n  + mv  i.e.  Me  has  full  column  rank.  This  implies  that  a  solution  to  (2.52)  exists  for arbitrary  Q.  Therefore  solutions  X(s)  and  Y{s)  of  (2.49)  with  degX(s)  ^  v  -  I  and • deg Y(s)  <  z^  -  1 always exist. Remark When  N(s)  and  D(s)  are  polynomials  Lemma  2.14  reduces  to  Corollary  2.9 (with  n(^)/(i(5*) proper).  In  this  case  v  =  di  =  n  = deg(detD(s)). EXAMPLE  2.23.  The polynomial  matrices N(s)  = s+l 1 0 1 D(s) 0 -s  + 1 are  re  since  rank rA^(A) LD(A). 2 for  A =  0 and A =  1 the  roots  of  detD(s).  D(s)  is  col-umn proper  with deg^  D(s)  =  d\  =  2 deg  D(s)  =  di  =  \  n  =  d\  -\-  d2  =  3 while deg^^ N(s)  =  1 <  Ji  =  2 and deg^^ N(s)  =  0  <  d2  =  I p  =  m  =  2. To construct  the Eliminant  Matrix  of N(s)  and D(s)  we note that  n/p  =  |  and therefore  we let  A: =  2 549 CHAPTER?: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems be an initial value. Then N(s) sN(s) D(s) sD{s) MelSe: 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 -1 0 0 0 0 0 1 0 -1 ri s \s^ r' 0 0 [o 01 0 0 0 1 s s^\ Note thai rank Me2  =  1  =  ^z + m^ which is full column rank. Therefore  z^ =  2 and the EHminant Matrix of N(s)  and D(s)  is Me  =  Mei- The fact that rank Me  =  1 also  verifies that N{s)  and D{s)  are re. We consider the Diophantine Equation  (2.49)  next (i)  First  we let  Q{s)  =  [s^ +  1 s^] which  satisfies  (2.48) and we  write  Q(s)  = QSe(s)  =  [1 0  0  1 0  0  1] I s 0  0 s^ 0 s^  0  0  0 0 1  5  ^2 .  Then  Eq.  (2.52) assumes the  form [Yo YiXoXi]Me  =  Q  =  [l  0  0  1 0  0  1] which is an algebraic system of linear equations with 8 unknowns and 7 linearly independent  equations  that have more than  one solution.  One  such  solution  is given by [701^1^0^1]  =  [I  0  - 1  1  1 0  1  - 1 ] which in view of (2.50) and (2.51) implies  that X(s)  =  Xo + Xis  =  [1 0] +  [1  -l]s  =  [s+l -s] Y(s)  -  Fo +  Yis  =  [1 0]  +  [ - 1  1]^  =  [1 -  ss] is  a  solution  to  the  Diophantine  Equation  with  degX(s)  =  I  =  v degY(s)  =  \  =  v - \. 1  and (ii)  Next we let Q(s)  =  ^ and we write Q{s)  =  QSeis)  = 10 0 0 : 0 00 0  0  0 assumes the  form 0 : 1 00 s 1 0  0  0  0 0  0 1 s Then  Eq.  (2.52) [YoYuXoXi]Me  =  Q  = 10 0  0  0  0  10 0  0  0  0  0 0 A solution of this equation is [FQ  FI  XQ Xi ]  = This implies  that 1 0 - 10 1 0 00 1 0 - 1 0 00 - 11 X(s)  =  Xo+  Xis Y(s)  =  Yo +  Yis ro  0 1 0] -1  oJ"^Lo  0 r -1  0' 1  0 1  01 1  + -1 1  0 0 -1 1  -s -1  +s 0 1 is a solution of the Diophantine Equation with deg X(s)  =  0 < z ^ -l  =  1 and • degY(s)  =  \  =  v-\. 550 Linear  Systems In  the preceding  development  it was shown  how to derive  particular  solutions of  the Diophantine  Equation  given  coprime  polynomials  and polynomial  matrices. In  the following  conditions  for existence  of solutions  are derived  and all  solutions of  the Diophantine  Equation  are conveniently  parameterized. THEOREM  2.15.  Consider nonzero polynomial matrices D(s) G RlsY^^"^ and N(s) G R[s]P2Xm  with pi+  p2^  m let GR(S) G /?[^]^^^  be a gcrd of D(s)  and A^(^) and let Q(s)  G R[sY'''^.  The Diophantine  Equation X(s)D(s)  +  Y(s)N(s)  = Q(s) (2.53) if and only if has  polynomial  matrix  solutions  X(s)  G R[SY^P^  and Y(s) G R[S]^^P^ GR(S)  is an rd of Q(s). If GR(S) is an rd of Q(s) then (2.53) has infinitely many polynomial matrix solutions. If X(^)  =  Xo(s) and Y(s)  =  FQ(5) is one such solution then all solutions of (2.53) are given by X(s)  = Xo(s) -  K(s)Nds) Y(s)  =  Yo(s) +  K(s)Ddsl (2.54a) (2.54b) where K(s) G Ris]^""' and where NL(S) G RIS^^P^  DL G RISY^'P^  r  = (pi  + P2) -  m are Ic and  satisfy  NL(S)D(S)  +  DL(S)N(S)  =  0 i.e.  [-NL(S)  DL(S)] is a minimal basis of the left  kernel of N(s) Proof  If there  exist X  and Y that  satisfy  (2.53)  then  (XDR +  YNR)GR  =  Q which implies that QG^ ^ must be a polynomial matrix. Thus GR is an rd of Q that is a necessary condition for solutions  of (2.53) to exist. To show  that  this  also  constitutes  a  sufficient condition  assume  that  GR is an rd of Q i.e. Q =  QRGR  for some  polynomial  matrix QR  and recall  that  there  exist  polynomial  matrices X  G RisY'^P^  Y G RISY'^P^ such that XD  +  YN (2.55) The proof of this result is constructive and is based on the algorithm to determine a gcrd of D and N using the Euclidean algorithm. (Refer to Subsection 7.2D where it is shown that U Y X -NL GR 0   where  [/ is a unimodular  matrix.)  Premultiplying (2.55) by  QR we now obtain  (QRX)D  +  (QRY)N  =  QRGR a  or XQD  + YoN = Q (2.56) i.e.  GR  being  an rd of Q is also a sufficient  condition  for the existence  of solutions of (2.53). If (2.56) is satisfied  then (Xo  -  KNL)D  +  (Fo +  KbL)N  =  XoD +  YoN  =  Q and therefore (2.53) has infinitely many solutions among them the infinitely many given by  (2.54). It remains to be shown  that every  solution of (2.53) can be put into the form (2.54).  Suppose  that X and Y satisfy  (2.53).  Subtracting  (2.56)  from  (2.53) we obtain (X  -  Xo)D + (Y-  Yo)N  = 0 or [X -  Xo F -  Fo] D 0. LQiNi^RisY''^' and^L RlsY^'P^ r  = (pi m be relatively Ic and such that =  0. 551 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems . (A discussion of polynomial Then [-NL  DL] is a minimum basis of the left kernel of bases of vector spaces will be given shortly.) This implies that there exists some K E Rls]^""' so that which is (2.54). Thus every solution of (2.53) can be written in the form of (2.54). • [X-Xo.Y-Yo] = K[-NLDLI Remarks (i)  If Z)-i  exists thenin the proof of Theorem2.15(X-Xo)D + (F-7o)A^  = 0 implies that Z - Zo  =  -(Y-Yo)ND-^ = -(F-Fo)^Z^^z.  where A^L and DL  are Ic. Since X -  XQISSL  polynomial matrix this implies that Y -  YQ = i'^DL for some iT where X-Xo  =  -^A^L-Thus when i)~^  exists the above results can be derived without directly using the concept of minimum basis \D] of the left kernel of  A^ (ii)  In the theory of systems  and  control the Diophantine  Equation  is of great use  particularly  for  the case when D~^  exists  and ND~^  is a proper ratio nal matrix. In this case  solutions of the Diophantine Equation  with  special properties are of interest particularly  solutions where X~^ exists  a n d Z ' ^F is a proper  rational  matrix  (see  Subsections  7.4A  7.4B  and  7.4C).  Such solutions satisfying  particular properties may also be determined via poly nomial matrix interpolation  (see the Appendix for theory and examples). It will be shown later in this chapter that the Diophantine Equation is central in the study of systems wherever feedback  is used. Rings modules and polynomial  bases of rational vector  spaces Now some useful concepts from algebra are briefly reviewed. In particular rings of polynomials of proper rational functions  and of proper and  stable rational  func tions  are briefly  discussed  together with modules  and polynomial bases of  rational vector spaces. Consider  the  set of polynomials  R[s] in s with  real  coefficients  together  with the  usual  operations  of addition  + and  multiplication  • of polynomials.  Then (R[s] + •) is a ring  since  all the  axioms of a ring  are  satisfied.  These  axioms  are the same as the axioms of a field (see Section  1.10  of Chapter  1) except for  axiom (vii)  which  assumes  existence  of the  multiplicative  inverse  in a field. Indeed if p(s)  G R[s] then  l/p(s)  ^  R[s] since in general  l/p(s)  is a rational function  but not a polynomial. Another  example of a ring is  the  set of integers taken  together  with the usual operations of addition and multiplication  on integers. We note  that  R[s] is a Euclidean  ring  i.e.  (i) for  a{s) b(s)  G R[s]  such  that a(s)b(s)  7^ 0deg[a(s)b(s)]  >  dega{s)\  and  (ii) for every  a{s)b{s)  G R[s^  with b{s)  ¥- 0 there exist ^(5-) r{s)  G R\s\  such that a(s)  = b(s)q(s)  + r(s)  where either r(s)  = 0 or deg r(s)  < deg b(s)  (polynomial division). If R{s) is the field of rational fractions  over R[s] called the field of rational functions  (see Subsection 1.2A)  then R[s] C R{s)  and  R{s) is also a Euclidean  ring. Note that the units of R{s] are those polynomials  p{s)  for  which  there exist a p\s)  G R[s]  such that  p(s)p'(s)  =  1 the unity element of R[s] i.e. the units of R[s] are the nonzero elements of  R. Another Euclidean ring of interest is the set of proper  rational functions  taken together with the operations of addition and multiplication. A proper rational  function 552 Linear Systems t(s)  =  n(s)/d(s)  (d(s)  #  0) is  a unit  of this ring  if deg n(s)  =  deg d(s)  i.e.  if  it is ^ biproper  rational function.  The set  of proper  and  stable  rational functions  (with poles in the stable region of the ^--plane) is also a Euclidean ring. In this case if  t{s) is a unit then t{s) and t~^{s) are proper and stable rational  functions. Polynomial  bases  of  rational  vector  spaces  are  discussed  next.  Recall  that  if Pi{s)  G  R[s]  then  it  may  always  be  viewed  as being  divided  by  1 in  which  case Pi{s)  =  Pi(s)/1  E  R(s)  the  field  of  rational  functions.  Thus  a polynomial  vector p(s)  E  R[s]P may be viewed  as a special case of a rational vector p(s)  E  R(sy. is  also  a  basis  for Now let H(s)  E  R(sy^^  assume that rankH(s)  =  m and consider the rational vector  space spanned by the columns  hj(s)  E  R(sy  j  =  1...  m of H(s)  and de note this space by  Y(s).  Thus the vectors in  Y(s)  are generated by H(s)a{s)  where a(s)  =  [ai(s)... ap(s)]^  cxj(s)  E  R(s).  The  matrix  H(s)  is  a basis for  Y(s)  and if  T(s)  E  Ris)"^"""^  and  rankT(s)  =  m  then  H(s)  =  H(s)T(s) Y(s).  Consider a right polynomial MFD of H(sl  H(s)  =  B(s)A~\s)  where B(s)  E R[sy^^  and A(s)  E  R[s]^^^  are not necessarily coprime. Note that rankB(s)  =  m and  B(s)  is  a polynomial  basis  of the rational  vector  space  Y(s).  Consider  now  the set M  of all polynomial  vectors  h(s)  E  R[sy  that can be written  as linear  combina tions  over  the  ring  R[s]  of  the  columns  bj(s)  E  R[S]P  j  =  1...  m  of  B(s).  The set M  is  3. free  R[s]-module  and  5(^)  is  a basis  of M.  The  dimension  of M  is <i/m M  =  rankB(s)  =  m.  If  the p  rows  of  B(s)  are  re  then  the  free  7?[5']-module M* generated  by  the  columns  of  B(s)  is  called  the  maximal  module  contained  in  Y{s) and  coincides  with  the  set  of  all polynomial  vectors  contained  in  the rational  vec tor space  Y{s)  [i.e. any polynomial vector in  Y{s) can be expressed  as B(s)p(s)  for some  p(s)  E  R[s]^].  Note  that  if  U(s)  E  R[s]^^^ is  any  unimodular  matrix  then B{s)  =  B(s)U(s)  is  another  basis  for  M  and  U(s)  can be  chosen  to reduce  B{s)  to a column proper  (reduced)  form  if  so desired. A minimal  basis  of a rational  vector space  Y(s)  is  defined  as  a polynomial  basis  B(s)  of  Y(s)  with  all  its  rows  re  i.e. if  B(s)  E  R[s]P^^  is  a minimal  basis  of  7(5*) then  any  polynomial  vector  in  Y(s) can  be  expressed  as  B(s)p(s)  for  some polynomial  vector  p(s)  E  R[s]"^  Note  that a minimal  basis  5(5*) is  sometimes  defined  in the literature  to be column  proper  as well. Summarizing  given  H(s)  E  R(sy^^ and  rankH(s)  =  m  ^  p  a  minimal polynomial  basis  for  the  rational  vector  space  spanned  by  the  columns  of  H(s) can  be  found  as  follows:  write  H(s)  =  B(s)A~^(s)  where  B(s)  E  R[S]P^^ and A(s)  E  R[s]^^^  are not necessarily  coprime. Let  GR(S)  E  R[S]^^"^  be a gcrd of the p rows of 5(5') and write B(s)  =  B(S)GR(S).  Then B(s)  is a minimal basis. To reduce B(s)  to  column  proper  form  consider  a  unimodular  matrix  U(s)  E  /^[^J'^^'^  such that 5(5')  =  5(5)6^(^) is column proper (see Subsection  7.2B.) Historical remarks on the Diophantine  Equation If  n d  are two nonzero  integers  and  the integer  g is their  gcd  then  there  exist integers  x y  so that  xd  -\-  yn  =  g  (see also Lemma  2.3). This result was known to the fourth  century  B.C. Greek mathematician  Euclid. The Diophantine Equation xd-^yn  =  q given above where d n and q are spec ified  and  solutions  x y  are  to be  determined  is  one  of  the  equations  in  more  than one indeterminants  introduced  by Diophantus  of Alexandria  who lived  around  A.D. 250. It is in fact  called  a linear Diophantine Equation  or a Diophantine Equation of first-order. Diophantus who introduced letter symbols for quantities in mathematical 553 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems problems is considered to be one of the greatest mathematicians. His treatise on alge bra considered to be the first ever is called "API0MHTIKA"  and consists of seven books six of which have survived. Diophantus worked with equations involving in tegers. An example of a specific case of the Diophantine Equation is 3x-\-2y  =  4 the =  -1  +  3^ with k equal to any general solution of which is given byx  =  2-2ky integer  (see Theorem  2.15).  In this  case  the particular  solution  XQ =  2  yo  = -I was used.  The parameterization  of  all  solutions  in this  convenient  form  appears  to be a later development  the work of Hindu  mathematicians  in the fifth century  A.D. apparently  of  Aryabhata.  It  is  worth  mentioning  at  this  point  that  another  class  of famous  Diophantine  Equations  is  x^  + y^  =  z^  where  x y z  and  n  are  integers. When  n  =  2 then x  =  3y  =  4 and z  == 5 is a solution since 3^ +  4^  =  5^. Solu tions for integer n greater than 2 do not appear to exist. In fact the famous  Fermafs last  theorem  (c.  1637)  states that no solution  exists for  ^  >  3. Fermat  wrote on the margin  of  his  copy  of  the  works  of  Diophantus  referring  to this  theorem  "I  have discovered  a truly  remarkable  proof  which  this  margin  is  too  small  to contain."  In spite  of  attempts  over  the  next  centuries  by  mathematicians  like  Euler  Legendre and many others this result has not yet been proved in its generality although a re cent proof  (1994)  shows  great promise  of being  completely  correct. It is however known to be true for n up to tens of thousands using computer methods. Diophantus worked with integers. However integers and polynomials obey sim ilar rules (they are both rings) and therefore  all results of interest on the linear Dio phantine  Equation  xd  -\-  yn  =  q  that  were  developed  for  integers  can  readily  be written for polynomials and this is what was done in Subsection 7.2E. These results for  the  polynomial  Diophantine  Equation  were  pointed  out  in  the  books  by  Gant-macher  and McDuffee  (refer  to the bibliography  at the end of the chapter). In view of this it is not  surprising  that  solutions  of linear Diophantine  Equations  involving elements other than integers or polynomials but still members of rings can be stud ied and expressed in a completely analogous manner. In fact Diophantine Equations with elements that are polynomials in z~^ and (s -f- a)~^ were studied in the systems and  control literature  in the  1970s by  Kucera  and Pernebo among  others. Later  in 1980 Desoer et al. clearly pointed this fact out in the systems and control literature namely  that  the  solutions  of  the  linear  Diophantine  Equation  when  the  equation involves  elements  of rings  other than  integers  or polynomials  are analogous  to the integer and polynomial cases. They also addressed conditions on system descriptions under which  such Diophantine Equations may  appear. It will be  shown in Subsection  7.3C  how the linear Diophantine Equation  is of fundamental  importance  in the  study  of  feedback  systems. It  should be noted  here that when  q  =  1 the Diophantine  Equation  is  sometimes referred  to as the  Bezout identity  (see e.g. Kailath  [21]). For the role played by the Diophantine Equation in control the reader  should also refer  to the survey paper by Kucera  [24]. For  further details refer  to Sections 7.6 and 7.7 Notes and References  respectively. 7.3 SYSTEMS  REPRESENTED  BY  POLYNOMIAL MATRIX  DESCRIPTIONS We consider system representations  of the  form P(q)z(t)  =  Q(q)u(tl yit)  =  R(q)z(t)  +  W(qMt) (3.1) 554 Linear Systems with  P{q)  G R[q]^''\  Q{q)  G R[qY'''^ R{q)  G R[q]P''K  and  W{q)  G R[q]P'''^ wliere R[qy^^  denotes tlie set of / x / matrices whose entries are polynomials in q with real coefficients  {q =  (d/dt) the differential  operator)  and we assume that  det P{q)  ^  0 and  that  u{t)  is  sufficiently  differentiable.  We  also  assume  that  the  set  of  homo geneous  differential  equations  P{q)z{t)  =  0 is  "well  formed"  that  is  for  all  initial values  of  z(-)  and  its  derivatives  at  f =  0  the  solution  does  not  contain  impulsive behavior  at  f =  0.  (This  is  true  if  and  only  if  P~^{q)  is  a  proper  rational  matrix which  is true  for  example  when  P{q)  is  column  or row reduced  (for  further  details refer  e.g.  to  Section  3.3  of  [9])).  Such  representations  called  system  Polynomial Matrix  Descriptions  (PMD) were introduced in Section 7.1. In this  section  equivalence  of  system  representations  is discussed first in  Sub section  A.  This  notion  of  equivalence  establishes  relations  not  only  between  poly nomial  representations  but  also  with  state-space  representations  which  are in  fact a  special  case  of  PMDs  (see  Section  7.1).  Equivalence  of  PMDs  preserves  certain system  properties  including  controllability  observability  and  stability  that  are the topics of discussion in Subsection B together with polynomial matrix realizations of transfer  function  matrices.  In  Subsection  C Polynomial  Matrix Fractional  Descrip tions (PMFDs) are employed to study properties of interconnected  systems. A.  Equivalence of System Representations Consider the PMDs Pi{q)zi{t)  =  Qi{q)u{t) y{t)=Ri{q)zi{t)+Wi{q)u{t) 12 (3.2) w\\QXQPi{q)eR[q]^'''^'Qi{q)eR[qy''''^Ri{q)eR[q]P''^\ and W-(^) G/^[^]^>^ DEFINITION  3.1.  The representations  {Pi gi/?i\^i} and {P222^2^2} in (3.2) are called equivalent if there exist polynomial matrices M ^ R[q\^^^^^ N ^ R[q\^^^^^ X  ^ RW^K  and Y G P[^]^2xm ^^^^ ^^^^ 'M X 0" h. Pi - ^1 Gi Wi Pi -R2 Qi W2 'N 0 -Y' ^m with (M P2) Ic and (Pi A^) re. (3.3) • The  equivalence  of  the  representations  {P/2//^/W^}/  =  12  or  of  the  corre sponding system  matrices  ^1^2 where Pi  Qi -Ri  Wi €R[s]  {li+p)^{li+m) (3.4) is sometimes referred to as Fuhrmann's  System Equivalence  (FSE). This equivalence relation  will  be  denoted  by  pf  or  simply  p.  It  can  be  shown  that  (3.3)  with  its associated conditions given in Definition  3.1 defines  an equivalence relation p  on the set of matrices S with p m fixed \P\  ^  0 and / any positive integer (see the discussion of  equivalence  relations  at  the  end  of  Subsection  7.2C).  The  polynomial  matrix  S defined  in (3.4) for  system  {P g P W}  is called Rosenbrock's  System  Matrix  of the system Pz =  Qu^y =  Pz +  Wu. 555 CHAPTER?: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems EXAMPLE  3.1.  Consider  the  representations  (1.3)  and  (1.4)  in  Section  7.1  and  let Pi  =  qI-AQi =  BRi  =  CWx  =  D given in (1.3) and let P2  =  P Q2  =  Q> R2  = R^W2  =  W  given  in  (1.4).  If  A^  =  C  = [0 0 1 -1 M  =  1  q  0" 0" ' 1 0  0  q_ D andX  = 0  0 0 1. 1  ^  0  ^^  +  1 are  Ic  while  (Pi N)  are  re  as  can  be  verified  by  apply 0  0^ ing  for  example  the  rank  test  for  coprimeness  of  Theorem  2.4(v)  to  (M P2)  above then  (3.3)  is  satisfied  (verify  this). Note  that  (M P2)  = 0  0 0  0 1 Q +  2 q and  to 0 q -1 0 0 0 1 -1 1 0 ' + 2  .  These  representations  satisfy  all  the  conditions  of 0 1 Definition  3.1 and are therefore  equivalent  (or Fuhrmann  system equivalent). • There is an alternative definition  of system equivalence  sometimes referred  to as (RSE).  Subsequently strict  system  equivalence  or Rosenbrock's  System  Equivalence PR will  denote  this  equivalence  relation.  It is  defined  as  follows. The  representations  given  in  (3.2)  are  called  strict  system  equivalent exist  unimodular  polynomial  matrices  MN X  G  Riq^^ Y  G  Riqf"^  with  m  =  deg  |P/| k  >  max(ni  ^2) such  that ^  R[q\^^^ if  there and  polynomial  matrices M 0 Pi 0 0 Gi Wi h-h 0 Pi 0 Q2 N 0 -R2 :  W2 h k>  h Relation  (3.5) is in general easier to use than (3.3) since the matrices M A^ \N -Y and are  all  unimodular. (3.5) 0 M X The  following  theorem  establishes  that  the  preceding  two  definitions  of  system equivalence  [i.e. Fuhrmann's  (FSE)  (3.3) and Rosenbrock's  (RSE)  (3.5)]  are in  fact equivalent.  Note  that it was  for  this reason  that  in Definition  3.1 which  is in fact  the definition  of  FSE  only  the  term  "equivalent"  was  used. THEOREM  3.1.  Consider  the  representations  given  in  (3.2). If  they  are  Fuhrmann's System  Equivalent  (FSE)  [satisfying  (3.3)]  i.e.  {Pi Qi Rx  WI}PF{P2  Qi Ri W2} then i.e. {Ph  Qh  Rh  WI}PR{P2  Q2  Ri  W2\  and vice versa. they  are  Rosenbrock's  System  Equivalent  (RSE) [satisfying (3.5)] 556 Linear Systems  where [-XY]  and Proof If the representations  are FSE then MPi  =  P2N and  -X Pi I  0 were chosen so that the block matrices are unimodular. 0 / This can always be done since (P2 M) are Ic and (Pi N) are re. [See also the discussion on doubly coprime factorizations  of a transfer function in (4.18) of Subsection 7.4A.] It can now be seen that -N Pi Y M X Y -X P2 Y M -R2 X 0 Gi 0 0 Pi ~Ri 0 -X h. YPi N YQi Y 0 P2 0 -R2 W2 (3.6) Observe that  \ .0 matrix in the left-hand side is unimodular. Therefore FSE  ^  RSE of the representations. To show the converse suppose that (3.5) is satisfied. Partitioning we can write which implies that the second block xl ^J \-x U/ YPi N _ "0 A W N. Mil Mu M21  M22 0 Pi 0 0 Xi X2 -Ri Wi (3.7) h-i 0 0 P2 0 Qi Nil N12 N21  N22 -Yi -Y2 0 - ^2 W2  L  0 where M22 N22  ^  ^[^l^^x^i. Then M22 0 Pi -Ri Qi Wi Pi -Ri Qi W2\ 'N22 0 -Y2 Im (3.8) It turns out that  (M22 P2) are Ic and  (Pi N22)  are re. This can be shown  as follows. Consider the unimodular matrices M  = Mil 1P2N21 M12 M22. andA^  = Nil N21 MnPi N22  J where the relations M12P1 =  A^i2 M21 =  P2N21  from (3.7) were used. Now if (P2 M22) were not Ic then  they  would have  a nonunimodular  common  left  divisor  that  would have caused M not to be unimodular. Therefore they are Ic. Similarly A^ is unimodular implies that (Pi N22)  is re. This shows that (3.8) is indeed the defining  relation for FSE and • therefore RSE => FSE of the representations. In the following  we enumerate  some of the important  properties  of FSE and RSE. THEOREM 3.2.  Assume that the representations (3.2) are equivalent i.e. they are FSE or RSE. Then the following  are invariants of the equivalence relation  p  =  pF  = PR-(i)  deg\Pi\  =  nj  =  12. (ii)  The nonunity  invariant polynomials  (in the Smith form)  of P/ [Pi Qi\  Pi -Ri Si  i  =  1 2. (iii)  The transfer  function  matrix//(^)  =  Ri(s)P;^(s)Qi(s)  + Wi(s)i  =  12. and Proof  Recall  from  Section  7.2  that  two polynomial  matrices  Mi  M2  E  R[qy^^  have the  same  Smith  form  if  and  only  if  there  exist  unimodular  matrices  Ui  U2 such  that UiMi  =  M2U2. In this case Mi  M2 are called equivalent polynomial matrices. To show 557 CHAPTER  7: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems (i)  note  from  (3.5)  that M 0 0 Pi 0 Pi 0 N  where  M N  are  unimodular. Therefore  Pi  and  P2  must  have  the  same  nonunity  invariant  polynomials  (note  that Pi P2  may  have  different  dimensions)  which  implies  that  |Pi|  =  a\P2l  where  a  is some nonzero real number (show this). Clearly then deg  \Pi \ =  deg  IP2I. To verify  (ii) note that in (3.5) fore  the extended  system  matrices Se  = h-ii 0 M X 0 ^1 0 and are unimodular  and there-^€2 ~ lk-l2 0 0 ^2 with Si i  =  1 2 defined  in  (3.4) have the  same Smith  form.  Thus ^1 and ^2 have  the same nonunity invariant polynomials. In a similar manner  (3.5) implies  that M 0 0 Pi 0 Qi 0 Pi 0 0 Qi which  shows that  [Pi Qi]  and  [P2 Q2]  have the same nonunity  invariant  polynomials. Similarly and have the same nonunity  invariant  polynomials. Pi -Ri Pi -Ri Finally to show (iii) we write in view of  (3.5) Hi RiPi'Qi  + Wi  =  [0  Pi] +  Wi = [0 R2W  +  X 0 Pi I 0 0  Pi +  Wi = [0  P2 0 7 0  P2 -1 M  + X +  Wi =  [0  P2] M =  [0  P2] 7 0 0  P2 / 0 =  [0 P2] i 0  P2 +  Wi + 0 GiJ 7 0 0  P2 +  [W2 +  [0P2]?] +  W2  =  RiP2^Qi  +  W2  =  H2. 558 Linear Systems In the above derivations the following relations which can directly be seen  from (3.5) were used: M 7 0 0  ^ 1. 0 Qi. M 7 0" 0  P2_ ^ N I —  — 0  P2_  f  + 0 "0  ' .Q2_ I 0 0  Pi +  [0 -/?i]  =  [0  -R2\N +  Wi  =  -{Id-R2\Y +W2. Now consider the representations  (3.2) and notice that they can be written as SM)  Ziit) -u(t) Piiq)  Qiiq) -Riiq)  Wiiq) Zi(t) -uit) 0 -y(0. (3.9) (3.10) (3.11) (3.12) (3.13) i  =  1 2 where Si(q)  is Rosenbrock's  System Matrix  given in (3.4). If these repre sentations are equivalent then the relation between the states zi  and Z2 can be found in the following  manner. we ob-\  Px  Qx [~Rx  Wi 'Nzi  +  Yu\' — u  J Pi Qi -R2  W2_ r 0' [-y. Postmultiplying both sides of (3.3) by  1 r  "^1 Qi\ Ri  W2J [ —M M X \ ^A \_—u  \ -u'Y 0 lp\ 0' -y. There-'  Pi \—u tain M or = 0 T \  Pi Q2] [-R 2  W2J fore Z2(t) =  N(q)zi(t)  + Y(q)u(t) (3.14) is  a possible  relation  between  the  partial  states. To obtain  the  inverse  relation  we could express  z\  as a function  of Z2 by considering  (3.3) and  selecting X  Y X  Y so that -x .Pi Y^ M\ \-N  X [Px  Y. I  0" / 0 "-A^  X] Pi Y\ \-X [Pl f M (3.15) are unimodular polynomial matrices. Note that this is always possible since MP\  = P2N  with (M P2) Ic and {Pi N)  re (see also the proof of Theorem 3.1). Now in view of (3.15) Eq. (3.3) implies  that Px -Rx Qx] WiJ \X [0 -E ^m x  0] F  h\ Pl Qi -R2  W2 (3.16) where £•  =  YQx-XYandF =  i?2^-XF. This relation was of course expected due to the symmetry property of the equivalue relation pp.  Postmultiplying both sides of (3.16) by  [z\y -u^]^  and proceeding  similarly  as before  we can now show that Zi(t)  =  X{q)z2(t)  ^  EiqMt). (3.17) Equations  (3.14)  and  (3.17)  determine  an invertible  mapping  that relates the  states of  the  equivalent  representations  (3.13).  It  can  be  shown  that  if  (3.14)  and  (3.17) hold for  some polynomial matrices A^  Y X E  then the representations  in (3.13) are equivalent. 559 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Polynomial Matrix Fractional Descriptions  (PMFDs) Next  we  consider  the  right  Polynomial  Matrix  Fractional  Descriptions (rPMFDs)  given by Di{q)zi{t)  =  u{t) (3.18) THEOREM 3.3.  The rPMFDs given in (3.18) are equivalent if and only if there exists a unimodular matrix U such that y{t)  -  Ni{q)Zi{t\ i  =  12. Proof Suppose that (3.19) is true. Rewrite it as U. D2 -N2 0 0 (3.19) (3.20) and note that this is a relation of the form (3.3). Therefore the representations {Di I Ni} {D2 / N2} are equivalent. Conversely suppose that (3.3) is true i.e. M X 01 \ ^' l-Ni Imi Im 0_ D2 -N2 Im] \^ [0 oj -Y' Im. with (M 7)2) Ic and (Di TV) rc. Then M  =  Im-D2Y  and MDi  = Z)2iV from which it fol lows that Di  = D2(N + YDi)  = D2U.A\soXDi  -Ni  =  -N2NmdX  = A^2i'which imply that A^i  =  A^2(A^ +  YDi)  = N2U. From Di  = D2U and the fact that degDi  = degD2 it now follows that U is unimodular • The reader should verify  that in this case the relations between the states (3.14) and (3.17) are given by Z2 =  Uzi  andzi  =  t/-iz2 i.e. A^ =  uY  =  0X  =  U-\ mdE  = 0. A similar result is true for IPMFDs given by Di(q)zi(t)  =  NiiqMt) y(t)  =  Zi{t\ 12. (3.21) Specifically  the IPMFDs  given in (3.21) are equivalent  if and only if there exists a unimodular matrix  U such that [DiNi]  =  U[D2N2l (3.22) The proof  of this result is completely  analogous to the proof  of Theorem  3.3  and is omitted. State-space  descriptions We now consider the state-space representations  given by Xiit)  =  AiXiit)  +  Biu{t) y(t)  =  CiXi +  Di{q)u{t) i  =  1 2  (3.23) and  we  assume  that these  are related  by  a similarity  transformation  i.e. there is a g ^ ^ n x« |g|  ^ 0  such that Q 0 0 \qln  -  Ay In\ [ -Ci By Di(q)_ qln  -A2 - C2 B2 \ D2{q)\ \Q  0 [0 /„ (3.24) Clearly  this  is  a  relation  of  the  form  (3.3)  with  M  =  N  =  Q  2ind  X  =  Y  ^  0. Therefore  {Ai Bi  Ci Di(q)}  {A2 B2 C2 D2(q)}  are  equivalent  in  the  Fuhrmann or  Rosenbrock  sense.  The  converse  is  also  true.  In  particular  if  two  state-space representations  are  equivalent  in  the  sense  of  Definition  3.1 then  they  are  related 560 Linear Systems via  a  similarity  transformation.  The  proof  of  this  result  is  not  presented  here.  We ^^^  the  reader  to  verify  that  in  this  case  the  relations  between  the  states  (3.14) (3.17)  are  given  by  X2 =  Qxi  and  xi  =  Q~^X2 i.e.  N  =  QY  =  0X  =  Q-\ and E  =  0. Note that the state-space representations  considered  in (3.23)  are more general  than the state representations  of previous  chapters  since the term  D{q)u{t) may  contain  derivatives  of  the  input.  In  fact  it  can  be  shown  that  every  PMD {P{q) Q(q) R(q)  W(q)}  in  (3.1)  is  equivalent  to  a  state-space  description  of  the form  {A B C D(q)}  (see e.g..  Section 2.2 of  [30]). Note that a state-space  descrip tion of the form {A B C D) that is equivalent to a given PMD as in (3.1) exists only when that PMD has a proper transfer  function  H{s). Now  recall  the  relation  between  a  state-space  realization  {Ac Be Cc Dc]  in controller  form  of the transfer  function  matrix  H{s)  and the corresponding  rPMFD {DR  I NR}. Specifically  (see Subsection  3.4D  of Chapter  3 and the Structure  The orem) we have H{s)  = NR(S)DR\S) with NR(S)  =  CcS(s)  +  DCDR(SI (SI  -  Ac)S(s)  =  BCDR(SI (3.25) s^^~^)^\  with dt i  =  1...  m where the n X m matrix S(s)  =  blockdiag  [(1 s... the controllability  indices of (Ac Be) or the dimensions of the subblocks in the con troller form  (Ac Be). The representations  DR(q)z(t)  =  u(t) y(t)  =  NR(q)z(t)  and  xdt)  =  Acx(t)  + Bcu{t)  y(t)  =  Ccx(t)  +  Dcu(t)  are equivalent  since Be Dc 0 Ip\ DRiq) y-NRiq) Im 0 ql  -  Ac -Cc Bc\ Dc\ \Siq) [  0 0 ^m (3.26) with  (Be ql  -  Ac)  Ic  and  (DR(q) S(q))  re  (show  that  this  is  true).  Verify  that  in this case the relation between the states given by Eq. (3.14) is ^^(0  =  S(q)z(t)  and determine the relation between the states given by Eq. (3.17) for this case. We note that (3.25) can be written  as NR(S)  =  CS(s)  +  DDR(S\ (SI  -  A)S(s)  =  BDR(SI (3.27) where  A  =  Q~^AcQB  =  Q~^BcC  =  CcQD  =  Dc  with  \Q\ ¥^ 0  and  S(s)  = Q~^S(s).  Equation  (3.27)  relates  NR(S)  and  DR(S) to  a  controllable  realization  of H(s)  that is not necessarily  in controller form. The matrix  2  is a similarity  transfor mation  matrix. Completely  analogous results exist for a state-space realization {Ao  Bo Co Do} in observer form  and the corresponding  IPMFD {DL NL Ip}-Finally  note  that  the  above  results  involving  the  Structure  Theorem  are  also valid after the obvious modifications when the state-space description is of the more general form {A B C D(q)}. B.  Controllability  Observability  Stability and  Realizations Consider now the PMD P(q)z(t)  =  Q(q)u(t) y(t)  =  R(q)z(t)  +  W(q)u(t\ (3.28) where P((^)  E  Riql^^'K Q(q)  G R[q]^'''^ R(q)  E  R[qy\ 2indW(q)  G  /?[g]^x^.The 561 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems important  system properties  of controllability  observability  and  asymptotic  stabil ity  can  be  developed  directly  in  a  manner  similar  to  that  in  earlier  chapters  us ing state-space descriptions. However instead of reintroducing these properties we shall  concentrate  on  establishing  criteria  for  (3.28)  to be  controllable  observable and asymptotically  stable. Assume  that the PMD  given  in  (3.28)  is equivalent  in the  sense  of  Definition 3.1 and (3.3) to some state-space  representation x(t)  =  Ax(t)  +  Bu(t\ y(t)  =  Cx{t)  +  Du{t\ (3.29) where A  G /^'^x^ B  G  R'''''^  C  G 7^^x^  and D  G T^^^^. That is for the  discussion that follows we restrict the PMD in (3.28) at least initially to the class of PMD that is  equivalent  to  state-space  descriptions  {A B C D] that  have been  studied  exten sively throughout this book. For the more general case of (3.28) being equivalent to state-space  descriptions  of the  form  {A B C D(q)}  refer  to the remarks  following Theorem  3.6. Controllability Recall  from  Section  3.2  of  Chapter  3  that  {A B C D}  is  controllable  if  and only if 1.  rank [sil  -  AB]  =  n for  any st complex  number. 2.  The Smith form  of  [si  -  A B] is  [/ 0]. Now  if {A B C D} in  (3.29) is controllable then  in the equivalent  description [P Q R W} given in (3.28) the Smith form  of  [P Q] will be  [/  0] and  rank  [P(si\ Qisi)]  =  I for any complex number Sj in view of Theorem 3.2(ii). Notice that these conditions  are  precisely  the  conditions  for  P Q  to  be  Ic polynomial  matrices  (see Theorem  2.5 in Section 7.2). These observations  give rise to the following  concept and result. DEFINITION  3.2.  The representation  {P Q R W} given in (3.28) is said to be con trollable if its equivalent state-space representation {A B C D} given in (3.29) is state controllable. THEOREM 3.4.  The following statements are equivalent: (i)  {P e R W} is controllable. (ii)  The Smith form of [P Q] is [/ 0]. (iii)  rank [P(si) Q(si)] = I for any complex number Sf. (iv)  P garelc. Proof Parts (ii) and (iii) follow  from the fact that (A B) is controllable if and only if the Smith form of  [ql -  A B] is  [/ 0] or equivalendy  rank [siI -  A B]  =  n for any complex number (see Section 3.2 of Chapter 3) and from Theorem 3.2(ii) in this section. In particular the Smith form  of [P Q]  in (ii) is [/ 0] if  and only if the Smith form of [ql — A B] is [/ 0] in view of Theorem 3.2(ii). This in turn is true if and only if (A B) is controllable which is equivalent to (i) by definition. So (ii) is true if and only if (i) is tme. Similarly one can show that (iii) is true if and only if (i) is true. Part (iv) follows • easily from (ii) or (iii) in view of Theorem 2.4 in Section 7.2. Remarks 1.  From the above results it follows that (A B) is controllable if and only if ^/  -  A and B are Ic polynomial  matrices. 562 Linear Systems 2.  If P Q are not Ic then the Smith form of [P Q] is [A 0] where A  =  diag  [6/]  ¥= I.  It  can  easily  be  shown  that  if  GL is  a geld  of  [P Q]  =  GL[P Q]  then  the Smith form of  GL  is A. The roots of the invariant polynomials €( of [P Q] or of GL  are the uncontrollable eigenvalues of the  system. 3.  Similar tests as the eigenvalue/eigenvector  tests of Subsection 3.4B of Chapter 3 can also be developed for the representation {P Q R  W}. 4.  The  rPMFD  {DR  Im NR} is  clearly  controllable  since  DR  and  /  are  Ic. This fact justifies  the  alternative  notation  {Dc / Nc} for  an rPMFD  when  the  con trollability  of the representation  is emphasized;  here Dc  =  DR and A^^  =  NR are  viewed  as  matrices  of  an  internal  system  representation.  Note  that  the term rPMFD stresses the relation to the transfer function  H(s)  = NR(S)D^^(S) where NR and DR  are intrepreted as the numerator and denominator of a transfer function  respectively. Observability Observability can be introduced in a manner completely analogous to controlla bility. This leads to the following  concept and result. DEFINITION 3.3.  The representation {P Q P W} given in (3.28) is said to be observ able if its equivalent state-space representation {A B C D} given in (3.29) is state ob servable. • THEOREM3.5.  The following statements are equivalent: (i)  {P 2 P W} is observable. (ii)  the Smith form of =  / for any complex number st. \P(Si) (iii)  rank [R(Si)\ (iv)  P R are re. Proof The proofs of these results are completely analogous to the proof of Theorem 3.4 and are therefore omitted. • Remarks .  From the above results it follows  that (A C) is observable if and only if ql  -  A and  C are re polynomial matrices. 2.  If P R are not re then the Smith form of IS  where A  =  diag  [e^]  T^ /. It  can  easily  be  shown  that  if  GR is  a gcrd  of GR  then  the  Smith form  of GL is A. The roots of the invariant polynomials  6/ of or of  GR  are the unobservable eigenvalues of the  system. 3.  Similar tests as the eigenvalue/eigenvector  tests of Subsection 3.4B of Chapter 3 can also be developed for the representation {P Q R  W}. 4.  The  IPMFD  {DL  NL Ip}  is  clearly  observable  since  DL  and  Ip  are  re.  This fact justifies  the  alternative  notation  {Do No Ip} for  an  IPMFD  when  the ob servability  of the representation  is emphasized;  here Do  =  DR and A^^  =  NR are  viewed  as  matrices  of  an  internal  system  representation.  Note  that  the term IPMFD stresses the relation to the transfer  function  H(s)  = DI^(S)NL(S) where NL and DL are interpreted as the numerator and denominator of a transfer function  respectively. Stability DEFINITION 3.4.  The representation {PQRW}  given in (3.28) is said to be asymp totically stable if  for  its  equivalent  state-space  representation  {ABCD}  given  in (3.29) the equihbrium x = 0 of the free system i  = Ax is asymptotically stable. • THEOREM 3.6.  The representation  {PQRW}  is asymptotically  stable if and only if Re  ?ii  <0i  = 1... n  where Xii=  1... n  are the roots of det P{s)\  the Xi are the eigenvalues or poles of the system. Proof In view of Theorem 3.2(ii) det {si — A) = a det P{s)  for some nonzero a  ^ R. 563 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems At this point it is of interest to briefly  discuss  controllability  observability  and stability  for  the more general  case when the PMD in (3.28) is equivalent to a state-space representation of the fonnx{t)  =Ax{t)  +Bu{t)^y{t)  =  Cx{t) +D{q)u{t)  instead of  (3.29).  First  note  that  the  concepts  of  state  controllability  and  observability  in state-space descriptions  of the form  {A5CD(^)}  are completely  analogous to the {A5CD}  case.  Furthermore  the  criteria  for  controllability  and  observability  are exactly  the  same  for  the  above  cases  and  they  depend  only  on  (A5)  and  {A^C) respectively.  In yiow  of this it can be  shovv^n that Definitions  3.2  and  3.3 and Theo rems  3.4  and  3.5  for  controllability  and  observability  respectively  are valid for  the more general PMD. For  similar reasons. Definition  3.4 and Theorem  3.6 on asymp totic  stability  are  valid  for  the  general  PMD  (3.28).  Hovv^ever  care  should  be  exer cised  vv^hen discussing  input-output  stability  of  a  system  {A5CD(^)}  or  of  their equivalent  descriptions  of the form  (3.28) but this topic vv^ill not be addressed here. For an extended  discussion  of controllability  observability  and  stability  of  systems described by a general PMD (3.28) refer to Chapter 3 of  [9] and Chapter 6 of [33]. It is of interest to note that equivalent representations  not only have exactly  the same  eigenvalues  but  also  the  same  invariant  zeros  decoupling  zeros  and  trans mission  zeros  (see Section  3.5  of Chapter  3 for  the definitions  of  zeros  and  also the discussion belovv^). These assertions follovv^ directly from  Theorem  3.2. Poles and zeros In  the  follovv^ing  wt  vv^ill  find  it  useful  to  first  recall  the  definitions  of  poles and  zeros  introduced  in  Section  3.5  of  Chapter  3  and  to  shovv^ how  these  apply  to PMDs. To this end consider the representation in (3.28) vv^ith transfer function  matrix H{s)=R{s)p-\s)Q{s)+W{s). Let the Smith-McMillan form  of H{s)  be given by SMH{S)  = 0 0 (3.30) v^here  A(^)  =  diag  — —  • • •   —— \l/r{s) 1  and  \i/i^i{s)  divides  \l/i{s)J • \\l/l{s) 1... r — 1 and  1/A/+1 (s) divides  i/A/(^) / =  1... r —  1 [see (5.3) in Chapter  3]. Then the poles  ofH(s)  are the roots of the characteristic  or pole polynomial  PH{S)  ofH(s) defined  as r  =  rank  H{s)ei{s)  divides  e/+i(^)/ (3.31) Note  that  PH{S)  is  the  monic  least  common  denominator  of  all  nonzero  minors  of H{s). =  XI/i{s)---XI/r{s). PH{s) 564 Linear Systems It is straightforward  to show that {poles oiH(s)}  C  {roots of  detP(s)}. (3.32) The  roots  of  det P  are the  eigenvalues  or  the poles  of  the  system  {P Q R W}  and are equal to the eigenvalues  of A in any equivalent  state-space representation {A B C D]. Relation  (3.32) becomes an equality when the system is controllable and ob servable since in this case the poles of//are  exactly those eigenvalues of the system that are both controllable and  observable. Comidtr  iht  system matrix or RosenbrockMatrix  of \htvepr:QS&rA&iion{P  Q R  W} Sis)  = -Ris) Q(s) Wis) (3.33) and an equivalent state-space representation {A B C D} given in (3.29). In view of (3.5) we have M X 0 I p^ h-i 0 0 P 0 -R 0 Q w h-n 0 0 si -A 0 -C :  o] \N :  B [0 :  D\ -Y ^m (3.34) where MN  E. R[s^^^^  are unimodular  and  k  >  max {deg \P\ n) (see (3.5)). The zeros of {P Q R W} can now be defined  in a manner completely  analogous to the way they were defined for state-space representations. Following the develop ment in Section 3.5 let fc =  nandr  =  rank \sl  -  A  B~ D -C  where n r  <  min(p + nm-\-  n). Consider all those rth-order nonzero minors of Se{s)  = \h-i [  0 0 S{s) that are formed  by taking  the first n rows  and n columns  of Se{s). The zero  polynomial of the system  {P Q R W} Zs(s) is defined  as the monic gcd of all those minors. The roots  of  Zs(s) are  the zeros  of  the  system  {P Q R W}.  The  reader  is  encouraged  to show that this definition  is consistent with the definitions  given in Section 3.5. The invariant  zeros  of the system  are the roots of the invariant zero  polynomial that is the product  of all the invariant factors  of S{s).  The input-decoupling/output-decoupling  and  the  input-output  decoupling  zeros  of  {P Q R W}  can  be  defined in  a manner  completely  analogous  to  the  state-space  case.  For  example  the  roots of the product  of all invariant  factors  of  [P(s) Q(s)] are the input-decoupling  zeros of the system; they  are also the uncontrollable  eigenvalues  of the system. Note that the  input-decoupling  zeros  are  the  roots  of  detGiis)  where  GL{S) is  a  geld  of  all the columns of {P{s) Q{s)\  =  GL(S)[P(S)  Q(S)].  Similar results hold for the output-decoupling zeros. The zeros  ofH(s)  also called the transmission  zeros  of the system  are  defined as the roots of the zero polynomial  ofH(s) ZH(S)  = ei{s)...6r(s\ (3.35) where  the  6/  are  the  numerator  polynomials  in  the  Smith-McMillan  form  of  H(s) given  in  (3.30). When  {P Q R W}  is  controllable  and  observable  the  zeros  of  the system the invariant zeros and the transmission  zeros coincide. 565 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Consider  the  representation  DRZR  =  uy  =  NRZR  with  DR  G  R[S]^^^ and and  notice  that  in  this  case  the  Rosenbrock  Matrix  (3.33)  can  be NR  E  R[sy^^ reduced via elementary  column operations to the  form 0  /] r^R  oj \0 [l  0 r  / [-DR '  DR _-NR 0] I\ I] 0\ I ro  /" [/ 0 7 0 0 -NR In  view  of  the  fact  that  the  invariant  factors  of  S  do  not  change  under  elementary matrix operations it is clear that the nonunity invariant factors  of S are the nonunity invariant factors of NR.  Therefore the invariant zero polynomial  of the system  equals the  product  of  all  invariant  factors  of  NR  and  its  roots  are  the  invariant  zeros  of the  system.  Note  that  when  rankNR  =  p  ^  m  the  invariant  zeros  of  the  system are the roots  of  detGi  where  GL is  the  geld  of  all  the  columns  of NR  i.e.  NR  = GLNR.  When A^^^  DR  are re the system is controllable and observable. In this case it can be shown that the zeros  ofH  {=  NRD^^) also called the transmission  zeros of the system  are equal to the  invariant  zeros  (and to the system  zeros  of  {DR  I NR}) and  can  be  determined  from  NR.  In  fact  the  zero  polynomial  of  the  system  Zs(s) equals  ZH(S)  the zero polynomial  of H  which  equals  ei(s)... er(s)  the product of the invariant factor  of NR i.e.. The pole polynomial of H(s)  is Zs(s)  =  ZH(S) = ei(s)...er(s). PH(S)  = kdetDR(s) where  k  G  R. (3.36) (3.37) Realization theory and  algorithms A  representation  {P  Q R W}  is  a realization  of  a rational  function  H{s)  if  the transfer  function  of {P Q R W)  is H{s)  i.e. if R{s)p-\s)Q{s) +  Vi^(^)  =  H(s). (3.38) The realization  theory  for  PMDs  is analogous  to the theory  for  state-space  descrip tions developed  in Chapter  5. Results  concerning  existence  and minimality  can  be developed  in  the  obvious  way  using  the  results  on  equivalence  of  representations developed  in  Subsection  7.3A  and  the  above results  on controllability  and  observ ability.  The  following  theorems  provide  results  that  correspond  to  Theorems  3.9 3.10  and  3.11  of  Section  5.3  of  Chapter  5.  The  reader  is  encouraged  to  give  full proofs  of these results. THEOREM  3.7.  A realization  {P Q R W} of H(s)  of order  n  = deg |P| is minimal (irreducible of least order) if and only if it is both controllable and observable. Proof  The proof is left as an exercise. THEOREM 3.8.  Let{P Q R W}md{P_ Q R W} be realizations of//(^). If {P Q R W} is a minimal realization then {P g R W} is also a minimal realization if and only if the two realizations are equivalent. Proof. The proof is left as an exercise. • THEOREM 3.9.  Let {P Q R W} be a minimal realization of H(s). Then the character istic polynomial of H(s)  PH{S)  is equal to detP(s) within a multiplication by a nonzero real number i.Q. PH(S)  =  kdet P(s). Therefore the McMillan degree of//(^)  equals the order of any minimal realization. • 566 Linear Svstems Proof To prove this result refer to the proof of Theorem 3.11 in Chapter 5 and use the results on equivalence given in Subsection 7.3A. • Realization  algorithms It is rather straightforward  to derive a realization of H in PMD form. In fact real izations in right (left)  PMFD form were derived in Chapter 5 as a step toward deter mining a state-space realization in controller (observer) form  (see Subsections 5.4B and  5.4D).  However  these  realizations  of the  form  {DR  Im NR} and {DL  Ni Ip] are typically not of minimal order i.e. they are not controllable and observable. This implies that the controllable realization {DR  Im NR} for example is not observable i.e. DR NR are not re. Similarly the observable realization  {DL  NL Ip} is not con trollable i.e. DL NL are not Ic. To obtain a minimal realization a gcrd must be ex tracted from  DR  NR and similarly a geld must be extracted from  DL  NL. This leads to the following  realization algorithm that results in a minimal realization {D Im N} ofH.  A minimal realization of the form {D N Ip} is obtained in an analogous  (dual) manner. Consider H(s)  =  [nij(s)/dij(s)]  i  =  I...  p j  =  1...  m and let lj(s) be the (monic)  least  common  denominator  of all entries  in the jth column  of H(s).  Note that lj(s) is the (monic) least degree polynomial divisible by all dij(s)  i  =  1... p. Then His) can be written as H{s)  =  NR(S)DR\S) (3.39) where  NR(S)  =  [nij(s)]  and DR(S)  =  diag(li(s)... nij{s)ldij{s)  for /  =  I...  p and all j  =  I...  m. Now lm(s)). Note  that  nij/lj(s)  = DR(q)ZR(t)  =  u(t) y(t)  = NR(q)zR(t) (3.40) is a controllable  realization  of H(s). If DR NR are re it is observable  as well and therefore  minimal. If DR and NR are not re let GR be a gcrd and let D  =  DRG^^ andA^  =  NRG^\  Thm D(q)z(t)  -  u(t) y(t)  = N(q)z(t) (3.41) is  a controllable  and observable  and therefore  minimal  realization  of H(s)  since D I and D N are Ic and re polynomial matrix pairs respectively. Note that ND~ ^ = (NRG^'XDRG^')-' =  NRD-^'  =  H. =  (NRG^'XGRD-^') There is a dual algorithm which extracts an IPMFD resulting in H{s)  =  DI\S)NL{S) (3.42) which corresponds to an observable realization of H{s)  given by DL(q)ZL(t)  = NL(qMt) (3.43) The details of this procedure  are completely  analogous to the procedure  that led to (3.39). If DL  NL  are not Ic let GL be a geld and let D  -  G^^DL  and N  =  G^^NL. Then  a controllable  and observable  and therefore  minimal  realization  of H(s) is given by y(t)  = ZL(t\ D{q)z(t)  = N(qMt) (3.44) In  Subsection  5.4B  a controllable  state-space  realization  that is equivalent to (3.40)  was obtained  first  using  the controllable  version  of the Structure  Theorem. Next the unobservable part of this state-space realization  was separated and a con-y(t)  = z(t\ trollable  observable  and  minimal  realization  was  extracted  (see  Example  4.3  in Subsection  5.4B).  This  minimal  state-space  realization  is of course  equivalent  to  the realization  (3.41)  which  is  in  PMFD  form.  It  is  possible  to  extract  an  equivalent minimal  state-space realization  directly  from  (3.41). However  to use the  convenient Structure  Theorem  typically  D  has  to be reduced  first  to  a column  proper  form  i.e. (3.41)  must  be  reduced  to D{q)z{t)  =  u{t) y{t)  =  N(q)z(t\ (3.45) where  D  =  DU  is  column  proper  N  =  NU  and  f/  is  a unimodular  matrix.  Analo gous  results  are  valid  for  PMFD  and  realizations  (3.42)  through  (3.44). The  following  example  illustrates  the realization  algorithms. 567 CHAPTER 7: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems EXAMPLE  3.2.  We wish to derive  a minimal realization  for  H(s) s^  +  1  ^ +  1 ' s^ c3 s Note that this is the same H(s)  as in Example 4.3 in Section 5.4B where minimal  state space realizations were derived. The reader is encouraged to compare those results with the  realizations  derived  below.  We  shall  begin  with  a controllable  realization.  In  view of  (3.39) s^Ji and  H  =  NRD-J [s^  +  1 ^  +  1] s^  0"  ^ n c3 .  Therefore J^RZR  =  u  and  y  =  NRZR  constitute  a  controllable  realization.  This  realization  is  not observable  since  rank DR(S) NR(S) =  rank not re. ro 0 _i 01 0 1. 1  <  m  =  2 i.e.  DR  and  NR  are Another way of determining that DR and NR are not re would have been to observe that deg det D(s)  =  5  =  order of the realization {DR  / NR}. NOW the McMillan  degree of i/ which is easily derived in the present case is 3. Therefore the order of any minimal realization  for  this  example  is  3. Since  {DR  I NR} is  of  order  5  and  is  controllable  it cannot be observable i.e. DR and NR cannot be re. We  shall  now  extract  a  gcrd  from  DR  and  NR  using  the  procedure  described  in Subsection  7.2D. We have DR NR 0 c3 0 S^  +  1 5 +1 S+  1 c3 S 0 Li 1 0 0 0 c3 5-5 +  1. s+V s^ s^  J —> 5 +1 "1 5^ .0 0 5^ "1  5 +  r 0 .0 s^ 0  J Therefore  GR  = 1 0 5 +  1 52 NRG^^  using  DR  = 0 _0 5^ 1  5+  1]  =  [5^ +  1  - ( 5+  1)] is  a  gcrd.  We  now  determine  D  =  DRGJ^^  and  A^  = 52 0 - (5  +  1) 5 1 0 5 +1 s' =  DGR  and  NR  =  [s^ + 1 0 S+\ s' NGR  and we verify  that they are re. Then {DRINR} = ^ - ( ^ + 1) 1  0 0  1 [q^  + h-(q+\)] is a minimal realization  of  H(s). To  determine  a  minimal  state-space  realization  from  Dz  =  u  and  y  =  Nz  using equivalence  of  representations  if  so  desired  we  notice  that  D  is  already  in  column proper  form.  Using  the  Structure  Theorem  and  the  notation  used  in  Examples  4.3 568 Linear  Systems and 4.5  in  Subsection  5.4B  we  obtain  D(s) -(s  +  1) s =  DhA(s)  +  DiS(s)  = 1 0 -1 1 s^  0 0 s + 0 I u  u  — J  11 0  0 0 -1 0 .  Therefore  B^  =  D^  = 1  1 0  1 and Am  = -Du'Di  = 0  0  1 0  0  0 Also  Dc  =  lim^oc//W  =  [10]  and  A^(^)  =  [s^ +  1 -{s  + 1)]  =  CcS(s)  +  DcD(s)  =  [100] +  [10] -(s+l) s Therefore  a  mini-1  0 s  0 0  1 "1  0^ s  0 .0 IJ mal state-space realization  of H(s)  is given by {Ac Be Cc Dc}  = "0  1  0"  0  0  1 .0  0  Oj "0  0" 1  1 .0  1_  [ 1  0  0 ]  [ 1  0 ]^ Note that this realization  is precisely  the minimal realization  derived in Example 4.3 in Subsection  5.4B. This will not occur in general as can be seen if for example a slightly different  GR is  used.  The  realization  {Ac Be Cc Dc} determined  here  is  in  controller form  since D^ was an upper triangular matrix with ones on the diagonal. In general Dh will just  be nonsingular  since  D(s)  will be column  proper  and  therefore  the  resulting controllable realization  will not be in controller form  since Be will not be of the appro priate form. This point was also discussed in Subsection 5.4B and illustrated in Example 4.5 in that  subsection. Alternatively  given  //  we  shall  first  derive  an  observable  realization.  In  view  of (3.42) H  =  DI^NL  =  (s^y^[s(s^  +  1)^  +  1]. Here  Di^(q) and  Niiq)  are  Ic  and  therefore  D(q)z(t)  =  N(q)u(t)  and  y(t)  =  z(t)  with D(q)  =  Di{q)  and A^(^)  =  Niiq)  is controllable and observable and is therefore a min imal realization. Note that the order of this realization is deg det DL{S)  =  3 which equals the McMillan degree ofH{s).  In Example 4.3 in Subsection  5.4B  a minimal  state-space realization  in observer form  was derived from  D{q)  and A^(^). • C.  Interconnected  Systems Interconnected  systems  connected  in  parallel  in  series  and  in  feedback  configura tions  are  studied  in  this  subsection.  Polynomial  matrix  and  transfer  function  matrix descriptions  of  interconnected  systems  are  derived  and  controllability  observabil ity and  stability  questions  are addressed.  It is shown  that particular  interconnections may  introduce  uncontrollable  unobservable  or unstable  modes  into  a  system.  Feed back  configurations  as  well  as  series  interconnections  are  of  particular  importance in the  control  of  systems.  Feedback  control  systems  are  studied  at length  in  the  next section. Systems  connected  in  parallel Consider  systems  S\  and  ^2  connected  in  parallel  as  shown  in  Fig.  7.1  and  let Pi{q)ziit)  =  Qi(q)um yi(t)  =  Ri(q)zi(t) +  Wi(q)ui(t) (3.46) and P2(q)z2(t)  =  Q2(q)u2(t\ y2(t)  =  R2(q)z2it)  +  ^ 2 ( ^ ) ^ 2 (0 (3.47) be  representations  (PMDs)  for  S]  and  ^2 respectively.  Since  u(t)  =  u\{t)  =  U2(t) 569 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems FIGURE 7.1. Systems connected in parallel and y(t)  =  yi(t)  +  y2(t)  the overall system description is given by Pi(q) 0 0 Piiq) Zi(q) Z2(q)\ Qi(q) Qiiq). u(t) y(t)  = [R(q)R2(q)] zi(t) ziit) +  [WM)  + ^2mu{t\ (3.48) If the systems ^i  and S2 are described by the state-space representations i/  =  Atxi  + BiUu yi  =  CiXi +  DiUu i  =  1 2 then  the overall  system  state-space  description  is given by i i]  _ X2\ 01  r^i \AI [ 0  A2J [^2. Bi Bi y  =  [Ci  C2] +  [Di  -h  D2]u. (3.49) If Hi(s)  H2(s) are the transfer  function  matrices of 5i  and ^2 respectively then the overall  transfer  function  can  be  found  from  y(s)  =  yi(s)  +  y2(s)  =  Hi(s)ui(s)  -h H2(s)u2(s)  =  [Hi(s)  +  H2(s)]u(s)toht H(s)  =  Hi(s)  +  H2(s\ (3.50) Now if both Hi(s)  and H2(s)  are proper then H(s)  is also proper. The  stability  controllability  and  observability  of  the  overall  system  S  repre sented by Pz  =  Qu y  =  Rz+  Wu  in (3.48) will now be examined briefly.  In view of  |P|  =  |Pi| IP2I it is clear that the overall  system is internally  stable if and only if each of the systems 5*1  and ^2 is internally  stable. Also if both Si  and S2 are BIBO stable i.e. all poles in Hi  and in H2 have strictly negative real parts then the overall system is also BIBO stable. The converse is also true i.e. if the overall system S is BIBO  stable then  so are Si  and S2. This can be seen from  Fig. 7.1  since if  say ^i were  not BIBO  stable  then  neither  would  S.  Next  this  result  is  also  shown  using alternative  arguments. All uncontrollable  or unobservable  eigenvalues  of Si  and ^2 will be uncontrol lable or unobservable eigenvalues of the overall system S. To see this consider Pi 0 0 Qi P2  Q2 and  let  Gi  G2 be  gelds  of  [Pi  Qi\  and  [P2 Q2]  respectively.  Then (3.51) 0 Gi 0  G2  IS an 570 Linear Systems Id of the matrix (rows) in (3.51) i.e. the uncontrollable eigenvalues of 5i  and ^2 will be uncontrollable eigenvalues  of the overall system. Similarly it can be shown  that the unobservable  eigenvalues  of Si  and 52 will be unobservable eigenvalues  of the overall  system.  Note  that  the  overall  system  S  may  have  additional  uncontrollable and  unobservable  eigenvalues  as  is  now  shown.  It is  easier  to  show  this  when  5i and 52 are controllable and observable. Assume then that DMziit) =  uiit) yiit)  =  Ni(q)zi{t) and D2iq)Z2it)  = U2(t) yiit)  =  N2(q)Z2(t) (3.52) (3.53) are descriptions for 5i  and 52  with//i (5)  =  Ni(s)D^^^(s)andH2(s)  =  N2(s)D2^(s). mq]""""  and  Ni(q)N2{q)  e  R[qV'"".  The  overall  system  is Let  Diiq)D2iq) then described by Di(q) 0 0 D2iq) zi(0 Z2(t) uitx y{t) =  miqiN2m zi(t) Z2(t) (3.54) Note  that  0 0 £»2 / 10 [0 0 / -D2 0] 0 l\ Di 0 -D2 0 Now  if  for  some  com-plex  value  A rank[Di(X)  -D2(A)]  <  m  i.e.  there  are  fewer  than  m  linearly  in <  2m dependent  columns  in  [A (A)-D2(A)]  then  ran^t  P^^^^^ which  impHes  that  A is  an  uncontrollable  eigenvalue  of  the  overall  system.  If  G is  a  geld  of  [Di  -D2]  then  A is  a  root  of  detG  and  it  is  an  eigenvalue  that  is common  in  both  systems  Si  and  S2.  Note  that  the  uncontrollable  eigenvalues  in NiD^^  +  A^2^2 ^  =  (^1  +  A^2^2 ^^O^F^  =  (A^i  + G  cancel mH  =  Hi+H2= Completely  analogous A^2^2 ^ i ) ^r results  can  be  shown  concerning  the  unobservable  eigenvalues  by  considering  the representations '  where  D^^Di  =  (GD2)~^GDi  =  D^'D -^2(A) /„ Di(q)zi(t)  =  Ni(q)ui(tl yi(t)  =  Zi(t) and D2(q)z2(t)  =  N2(q)u2(tl y2(t)  -  Z2(t) (3.55) (3.56) with Hi(s)  == D^^(s)Ni(s)  and H2(s)  =  D2^(s)N2(s)  The unobservable  eigenval ues cancel in Di(s)D2^(s)  (show this). In view  of the above it can be  seen that if all the poles of H  have negative real parts then all the poles of both Hi  and H2  also have negative real parts since possible additional  poles of Hi  and H2 are in the  same locations  as  some of the poles of  H. Therefore  if S is BIBO stable so are both 5i  and  ^^2. Systems connected in series (or in cascade or in tandem) Consider  systems ^i  and  S2  connected in series as shown in Fig. 7.2 and let Pi(q)zi(t)  =  Qi(q)ui(t) yi(t)  =  Ri(q)zi(t)  +  Wi(q)ui(t) (3.57) be a polynomial matrix representation for Si  and P2(q)z2(t)  -  22(^)^2(0. (3.58) be a representation for  ^2.  Here  U2(t)  =  yi(t).  To derive the overall system descrip tion  consider  ^2^2  =  Q2U2 =  G2J1  =  62(^1^1  +  Wiui)  and  Pizi  =  Qiui  and also  y2  =  R2Z2 +  W2U2 =  R2Z2 +  W2yi  =  R2Z2 +  ^2(^1^1  +  WiwOand  ji  = yiit)  =  R2(q)z2(t)  +  ^2(^)^2(0 571 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems ^1 Vi Si t/2 ^2 1  ^^ FIGURE 7.2. Systems connected in series RiZ\  +  WiMl. Then Pi -QiRi 0 Pi yi  =  WtPx  /?2] (3.59) If an external input r2 is introduced as in Fig. 7.3 that is M2  =  Ji  + ''2» then P2^2  = 22^2  =  Qi{y\  + ^2)  =  QiiRiZi  + Wi^O + 22^2 and y2  =  R2Z2 +  ^2^2  =  ^^2^2 + W2(yi  +  r2)  =  /^2Z2 +  ^2(^1^1  +  WiUi)  +  ^2^2. In this case a more complete description of the two systems in series with inputs wi r2 and outputs y2 yi  is given by Pi 0 -22^1  P2 \zi [Z2_ \  «2i [22^1 \yi [y2. = Ri W2R1 0] 22 J 0] Ril lui U2. \zi] U2J + Wi 0 \W2WI  W2 (3.60) If  the  systems  Si S2 are described  by  the  state-space  representations  xt  =  Atxt  4-CiUi yi  =  CiXi + DiUi i  =  1 2 then it can be shown similarly that the overall sys tem state-space description is given by Xi hi J 2. = = B2C1 Ci D2C1 0] A2J 0] C2J Ixi [X2_ Ixi [X2_ + + Bi B2D1 '  D P2D1 0] Bil 0" JO2J «il 72 J \ui U2. (3.61) If Hi {s) H2(s) are the transfer function matrices of 5i and ^2 then the overall transfer function  };2(^)  =  H(s)ui(s)is His)  =  H2(s)Hi(s). (3.62) It can be  shown that if both Hi  and H2 are proper  then H  is also proper. Note  that poles of Hi  and H2 may  cancel  in the product H2H1 and  any cancellation  implies FIGURE 7.3. Systems connected in series 572 Linear Systems that there are uncontrollable/unobservable  eigenvalues in the overall system internal description. This is discussed further  next. The stability controllability and observability of the overall system S described by  Pz  =  Qu  y  =  Rz+  Wu  and given in (3.60) will now be examined. In view of I P|  =  I Pi 11P21 it is clear that the overall system is internally stable if and only if each of the  subsystems  Si  and ^2 is stable. If both 5*1  and S2 are BIBO  stable i.e. if  all poles in Hi  and H2 have  strictly negative real parts then the overall  system is also BIBO stable. The converse is not necessarily true since cancellations may take place in the product H2H1 i.e. the unstable poles of Hi  and H2 may cancel in H2H1  =  H thus leading to a BIBO stable system where Si  and/or ^2 might not be BIBO stable. Concerning  controllability  and observabiHty  of  (3.60) we make  several  obser vations: 1.  All eigenvalues of Pi  are uncontrollable from  r2 and all eigenvalues  of P2 are unobservable from  yi.  This is of course as expected  since Fig. 7.3 reveals that the input  r2 does  not  affect  system  5*1  at  all  and  observation  of  the  output  yi will not reveal any information  about system 5*2. 2.  All  uncontrollable  eigenvalues  of  ^i  and  5*2 are  uncontrollable  from  ui.  This is  true  because  any  geld  of  [Pi Qi]  and  any  geld  of  [P2 Q2] are  elds  of ~  Pi -Q2R1 Finally  all  unobservable  eigenvalues  of  Si  and  ^2  are  unobservable  from and  any  gcrd  of  P2  are  crds  of y2.  This  is  true  because  any  gcrd  of 0 Qi  ~ P2  Q2W1 (show this). Pi ^1 Pl -Q2RI W2RI 0 P2 R2 (show this). It  should  be  noted  that  other  uncontrollable  and/or  unobservable  eigenvalues may exist. These correspond to poles of Hi  and Hj  cancelling in the product H2H1 and can be found from representation (3.60) using say the eigenvalue tests for con trollability  and  observability.  It  is  of  interest  to determine  these  additional  uncon trollable  and  unobservable  eigenvalues  directly  from  the  PMFD  of  Si  and  52. To simplify  the  analysis  we first assume  that  both Si  and  52 are controllable  and ob servable. Let Di(q)zi(t)  =  uiit) yiit)  =  Ni(q)zi(t) (3.63) be a description for 5i with Hi{s)  =  Ni{s)D]^^{s)  and let D2(q)Z2it)  =  U2{t) y2(t)  =  N2iq)Z2(t) (3.64) be a description  for 52 with H2(s)  =  N2is)D2\s).  Let Di(q)  £  Riqr"""  Ni(q)  G Rlqy'""  and Diiq)  E  R[q]P''P N2(q)  G RlqY^'P  and recall that yi  and  U2  have the same dimensions  (yi  =  U2).  In this case description  (3.60)  becomes 0 Di -A^i  D2 I  0 / 0 0 N2 0 (3.65) The uncontrollable  eigenvalues  from  ui  are the roots  of  a geld of 0 / D2  0 Using  elementary  column  operations  corresponding  to postmultiplication  by  a uni-Di [-Ni modular  matrix this matrix is reduced  to 0 -Ni 3stmultip] 0 /" £>2  0  a geld of which is given by / 0 0 GL\   where  GL is  a geld  of  [-A/^i Di]-  Thus  all  the  uncontrollable  eigen-values of the overall system are the roots of the determinant of a geld of [-iVi  Di]. Note that these are poles of//2  cancelling in the product H2H\  =  N2D2^N\D^^ in D2^N\  or equivalently in  H2N\. The unobservable eigenvalues may be obtained  similarly using the  representa tions and DM)zi{t)  =  Ni{q)ui(t) yxit)  =  zi(t) D2(q)Z2(t)  =  N2{q)u2{t) yiit)  =  ziit) (3.66) (3.67) 573 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems with  Hi(s)  =  Dl\s)Ni(s) in this  case and  H2(s) D2 ^(s)N2(s)- Description  (3.60)  becomes Di -N2 0 Dil \zi U2. Ni N2 Ol A^2j \ui V2_  ' y\ J2. /  Ol 0 l\ \z\ L^2 (3.68) The  unobservable  eigenvalues  from  y2 are the roots  of  a gcrd of Dx -N2 0 0 62 / . Us-ing elementary row operations corresponding  to premultiplication  by a unimodular matrix  this  matrix  is reduced  to Dx  0" -N2  0 0 /  a gcrd  of  which  is given  by GR 0 0 / where  GR  is a gcrd of  Dx -N2 . Thus the unobservable eigenvalues of the overall sys-tem are the roots of the determinant of a gcrd of Dx -N2 . Note that these are poles of Hx cancelling  in the product //2//1  =  D2^N2D\^Nx-> in N2D^^  or equivalently  in NiHx. Systems connected in feedback  configuration Consider  systems ^i  and ^2 connected  in a feedback  configuration  as shown in Fig. 7.4A  or equivalently  as in Fig. 7.4B. Let Pi(q)zi(t)  =  Qx(q)ux(t) yx(t)  =  Rx(q)zx(t)  +  Wx(q)ux(t) (3.69) and P2(q)z2(t)  =  62(^)^2(0 J2(0  =  R2(q)z2(t)  +  ^2(^)^2(0 (3.70) be polynomial matrix representations  of 5*1  and ^2 respectively.  Since ui(t)  -  y2(t)  + rx(t) U2(t) =  yi(t)  + r2(tl (3.71) (3.72) where rx and r2 are external inputs the dimensions of the vector inputs and outputs ux and  j2  and  also  U2  and  j i  must be the  same i.e.  ux y2  ^  R^  and  U2 yx  ^  ^^• To derive the overall system description we consider yx  =  i^iZi  +  WxUx =  ^iZi  + ^1(3^2 +  n)  =  Rxzx  +  Wx(R2Z2  +  W2U2) +  Wxrx  =  RxZx +  WXR2Z2  +  WxW2(yx  + ^2) +  Wxrx from  which we obtain (/  -  WxW2)yx  =  R\zi^  WxR2Z2  +  Win  +  WxW2r2. (3.73) 574 Linear  Systems ''^ • >! -^ ^1 Si yi S2 .  J L ( JH '^  FIGURE  7.4A Feedback  configuration ^1 'L.. -^0^ + S2 + -^H(>- Sl yi FIGURE  7.4B Feedback  configuration Similarly  we have  yi  =  R2Z2 +  W2U2  =  R2Z2 +  ^2(^1  +  ^2)  =  R2Z2 +  ^2(^1^1  + Wiui)  +  W2r2  =  R2Z2 +  W2RiZ\  +  ^2^1(3^2  +  n)  +  ^ 2 ^ 2.  from  which  we  obtain (/  -  W2Wi)y2  =  R2Z2  +  W2R1Z1  +  W2Wiri  +  W2r2. (3.74) LEMMA  3.10.  det(I  -  W1W2)  =  det(I  -  W2W1). =  det  1 Proof  det(I  -  W1W2)  =  det \ I .0 =  det  1 I -Wx 0 I-WxW2_ ) r / 01 l\ r / =  det  { =  det -W2I I /  J 0 I  --W2WI 0 -W2 \i W2I / J W2] IJ r  / [-Wi 0' /. 0" /. =  det(I -W2W1) Now  assume  that  det(I -  W1W2)  =  det(I -  W2W1)  ¥^  0.  If  Mi  =  (/ -^ 1 ^ 2 ) '^  and  M2  =  {I  -  ^ 2 ^ 1 ) -^  then  (3.73)  and  (3.74)  imply  that  yi  = [MiRiMiWiR2] [M2W2WiM2W2\ Z\ Z2 r\ r2 + [MiWiMiWiW2] a n d j2  =  [M21^2^!.  ^ 2 ^ 2] + .  Now  Piz\  =  Q\ux  =  Qi(y2  +  n)  and  P2Z2  =  Q2U2  = 62(^1  +  ^2) where  y\  and  j2  are  as  above.  Then  the  closed  loop  is  described  by Pi  -  Q1M2W2RX -Q1M2R2 -Q2M1R1 P2  -  Q2M1W1R2 Q1M2 Q2M1W1 Q1M2W2 Q2M1 MiRi M2W2R1 M1W1R2 M2R2 +  MxWi M2W2W1 M1W1W2 M2W2 (3.75) where  the  identity  /  +  (/  -  W i ^ 2 ) '^ V^i^2  =  (/  -  ^ 1 ^ 2 ) "^  was  used At  this  point  it  is  of  interest  to  point  out  that  when  Wi  =  0W2  =  0  then  the 575 closed-loop  description  (3.75)  is  simplified  to  the  PMD ^1 - 6 2 ^1 -QxRi] \ Pi Ui 1.^2. Qi [o 0 Qil n ['•2.  ^ y\ J 2. Ri [o 0 R2\ Ui [zi (3.76) Note  that  given  system  descriptions  (3.69)  and  (3.70) it is always  possible  to  obtain equivalent  representations  with  Wi  ^  0  and  W2  =  0.  Assuming  this  is  the  case (3.76)  is  a  general  description  for  the  closed-loop  system  and  the  condition  for  the closed-loop  system  to be  well  defined  is  that CHAPTER  7: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems del ^1 -Q2R1 -Q1R2] Pi 7^0. (3.77) If  this  condition  is  not  satisfied  then  the  closed-loop  system  cannot  be  described  by the  polynomial  matrix  representations  discussed  in  this  chapter. If  the  systems  ^i  and  S2  are  described  by  the  state-space  representations  ki  = AiXi  +  BiUi  yt  =  CfXi  +  DiUi  i  =  1 2 then  in  view  of  (3.75) the  closed-loop  sys tem  state-space  description  is Xi U2J Jl [3^2] ^  Ai  +  B1M2D2C1 B1M2C2 B2M1C1 A2  +  B2M1D1C2 B1M2 B2M1D1 B1M2D2 B2M1 M i Ci M2D2C1 M1D1C2 M2C2 +  M2D2D1 M1D1D2 M2D2 (3.78) where  Mi  =  (I  -  DiD2)~^  and  M2  =  (I  -  D2DiyK D1D2)  =  det(I D2Di)y^0. - It  is  assumed  that  det(I -It  is  not  difficult  to  see  that  in  the  case  of  state-space  representations  the  con ditions  for  the  closed-loop  system  state-space  representation  to  be  well  defined  is det(I -  D1D2)  ^  0. When  Di  =  0  and  D2  =  0  then  (3.78)  simplifies  to Xi = y\ J 2. Ai B2C1 Ci 0] 0  C2J B1C2 A2  _ Ui\ U2J +  Bi 0 [X2_ 0] B2\ (3.79) EXAMPLE  3.3.  Let 5*1  and ^2 be described by zi  =  u\y\  =  qz\  Sindqzi  ~  ^2y2  = Z2 i.e. {Pi Gi Ru  Wi}  -  {1 1 q 0} and {P2 Q2 Ri  W2} =  {q 1 1 0}. These  systems have transfer  functions  H\{s)  =  s and H2{s)  =  l/s  i.e.  an ideal  inductance  Si  is con nected via feedback  to an ideal capacitance S2. Then  (3.76) assumes the  form '  1 -q -1] q  \ \zi U2. > i" yi. 01 \ 0 ij q  01 Ij P r^i U'2_ \z\ U2. 0 this does not constitute a well-defined  closed-loop descrip Since det 1 -q -1 q tion. It is  of interest  to note that Si  cannot  be  described  by  a state-space  description  of • the form {Ai  Bi  Ci Di]  since its transfer  function  Hi{s)  =  sh  not proper. EXAMPLE  3.4.  Consider  systems  ^i  and  S2  connected  in  a  feedback  configuration with//i(^)  =  —-^-^  and//2(5')  = and consider the  realizations s  +  \ s + 2 s  + 2 s+  1 576 Linear  Systems [Pi  Qh  R\  Wx]  =  {q + Zq+11 (3.76)  becomes 0} and {P2 Qi Ri  W2} =  {q+\q+ 21  0}. Then q  + 2 -(q  +  2) (q + 1)1 \zi U2. \ q+  I > i" J2. 0  1 q + 2\ r^i L^2. 'q+  1 _  0 "1  01 ij 0 \zi~ U2. Since det q  + 2 -(q  + 2) - ( ^ + 1 )1 q+l \ =  0 the present example is not a well-defined  poly-nomial matrix system description. It is of interest to note that here state-space realizations ofH\  and H2 exist. Let H\  = +  I H2  = 7 +  1 and consider the  state-space s + 2 s  -\-  I realizations  {Ai Bi  Ci Di}  =  {-2  - 1 1 1} and {A2 B2 C2 D2}  =  {-1  1 1 1}. Here I  -  D1D2  =  0  and  therefore  a  state-space  reahzation  of  the  closed-loop  system  does not exist as expected. • EXAMPLE  3.5.  Consider  systems 5*1  and S2 in a feedback  configuration  with Hi(s)  = and//2(^)  =  1 and consider the realizations {Pi Qi Ri  Wi}  =  {q+l  q 1 0} and [Pi  Qi Ri  W2} =  {1 1 1 0}. Then  (3.76)  becomes q+  1 -1 -q\ 1 J ki U2. > i' J2. q  01 [0 Ij "1  01 ij [0 In [ri \zi [z2 Since det +  1 -1 -q 1 =  1 7^ 0 this is a well-defined  polynomial matrix  description for the closed-loop system. Note that the transfer  function  matrix of the closed-loop sys-tem is  H(s) 1  01 p  +  1 0 ijL  -1 -s^ 1J \s [0 0]\s ij  ~  [^  ^ + 1 s Hi  and H2 were both proper.  which is not proper whereas If  the  realizations  to  be  considered  are  {Pi gi. Pi  W\}  =  {q  +  1  - 1  1 1} and {P2 Qi Ri  W2]  =  {1 0 0 1} then  1 -  WiTy2  =  1 -  1 • 1  =  0  and  no  representation of  the  form  (3.75)  can  be  derived.  This  stresses  the  point  that  the  conditions  for  the PMD of the closed-loop  system to be well defined  are given by  (3.77) since the condi tion det (I  -  W\W2)  y^ 0 may  lead  to erroneous  results. Note that det (I  -  W1W2)  =  0 does not necessarily  imply  that a well-defined  polynomial  matrix representation  for  the closed-loop  system does not exist. Nowif  state-space realizations of ^i(>s)  = -  + lmidH2(s)  =  1  are considered s + I namely  {Ai Pi Ci D^}  =  {-1  1 - 1 1} and  {A2 P2 C2 D2}  =  {0 0 0 1} then  1  -D1D2  =  I  -  I  '  I  =  0 i.e. a state-space  description  of the closed-loop  does not  exist. This  is to be  expected  since the  closed-loop  transfer  function  is nonproper  and  as  such cannot be represented by a state-space realization {A B C D}. • Next  let  Hi(s)  and  H2(s)  be  the  transfer  function  matrices  of  S\  and  ^2  i.e. =  7^2W^2W-In  view  of  wi  =  })2 +  n a n d w2  =  j^i  + yi{s)  =  Hi(s)iii(s)sindy2(s) r2  we  have  yi  =  H\U\  =  H\{y2  +  n)  =  H1H2U2  +  / ^ in  =  H\H2y\  +  HiH2f2  + H\  f\  or (I-HiH2)yi =  HH2r2  +  Hn. (3.80) Also  y2  =  H2U2  =  HziSi  +  h)  =  H2H1U1  + H2r2  =  H2Hiy2  + / ? 2 ^ in  +  ^2^2 or {I-H2Hx)y2 = H2Hh+H2h. (3.81) Assume  that  det{I that  the  determinants  are equal  is completely  analogous  to the proof  of Lemma  3.10. Then -  H2H1)  ¥=  0.  Note  that  the  proof  of  the  fact -  H1H2)  =  det{I (/ -  //li/2)-'//l ( /-H2H\) tin  Hn H21 ^22 H2H\ (/ (/ H\H2) -H2Hi)-'H2 H1H2 (3.82) 577 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems The  significance  of  the  assumption  det(I -  H1H2)  #  0  can  be  seen  as  follows. Let  Dizi  =  Niu\y  y\  =  z\  and  D2Z2  =  ^2 3^2  =  ^2Z2  be  representations  of  the systems  ^i  and  52.  As  will  be  shown  the  closed-loop  system  description  in  this case  is  given  by  (D1D2  -  NiN2)Z2  =  A^i^i  +  5 i r2  and  yi  =  D2Z2  ~  ^2 and  y2  = N2Z2-  Now  note  that  /  -  H1H2  =  I  -  D\^NxN2D:^^  =  p\\DxD2 NxN2)D:^\ which  implies  that det  (/  -  H1H2)  7^ 0 if  and  only  if det  {D1D2  -  N1N2)  ¥^ 0 i.e.  if -  H\H2)  =  0  then  the  closed-loop  system  cannot  be  described  by  the  poly det(l nomial  matrix  representations  discussed  in  this  chapter.  Thus  the  assumption  that det  (I  -  H1H2)  7^ 0 is  essential  for  the  closed-loop  system  to be  well  defined. - EXAMPLE  3.6.  Consider Hi{s)  =  2 and H2(s)  =  1/s as in Example  3.3. Clearly  1  -H1H2  =  0  which  implies  that the  closed-loop  system  is not well  defined.  This  agrees with the result of Example 3.3 where it was shown that a PMD of the closed-loop system does not exist. • EXAMPLE  3.7.  Consider  Hi(s)  = ^/i^2  = 7  as  in  Example  3.4.  Here  1  -H1H2  =  0 and the closed-loop system is not well defined  which agrees with the results in Example 3.4. • 5 +1 5 +  2 5 +  2 5 +1 EXAMPLE  3.8.  Consider  Hi(s)  = 7  ^ 2^  =  1  as  in  Example  3.5.  Here  1  -5 +1 H1H2  =  —-rr  ^  ^'  ^^^  therefore  the  closed-loop  system  is  well  defined.  Relation (3.82) in this case assumes the  form 5+  1 'yi'  = 5 5 5  1 5 + lJ a nonproper transfer function that is the transfer function matrix H(s)  derived in Example 3.5. • Now  consider  systems  Si  and  5*2 with  proper  transfer  function  matrices  Hi(s) =  1 2 be their  state-space =  AiXi  + BiUuyi  =  CtXi^-DiUui SindH2(s)3ndlQtXi representations. LEMMA  3.11.  If  det(I  -  D1D2) 7^ 0  then  det(I  -  Hi(s)H2(s))  7^ 0. Furthermore  in this  case /  -  Hi(s)H2(s)  is biproper  [i.e. both /  -  H\{s)H2{s)  and  (/  -  Hi(s)H2(s))~^ are proper rational matrices]. Proof  It is not difficult  to see that I -Hi  (5)7/2(5)  =  I-D1D2  + (strictly proper terms) which  implies  that  det (I  -  D1D2)  is  the  coefficient  of  the  highest  degree  5 term  in 578 Linear Systems the numerator of det{I  — Hi(s)H2(s)). Then if det(I  — D1D2)  7^ 0 we have det(I  -Hi(s)H2(s)) ^  0. Furthermore nm(/ -  Hi(s)H2(s)) = I-  D1D2 (3.83) which impHes that when det{I  -  D1D2) ^  0 then /  -  Hi(s)H2(s) is a biproper rational matrix i.e. I -  H1H2 and (/ -  H\H2y^  are proper. • In  Lemma  3.11 we  have  used  the  fact  that  a  proper  rational  matrix  H{s) has  a proper  inverse  e.g. H~^{s)  proper  if  and only  if  detD  7^ 0  where  D  = lim^^oo H(s). This result is based on Lemma 3.12 and Corollary 3.13. LEMMA 3.12.  Consider H(s) G R(S)P^P  which can be expressed uniquely as H(s)  = H(s) + Q(s) using polynomial  division where H(s) G R(sy^P  is strictly proper and Q(s) G R[sy^P. Then H~^(s) is proper if and only if Q~^(s) exists and is proper. • Proof The proof is left to the reader as an exercise. COROLLARY 3.13.  If H(s) G R(s)P^P is proper then H~\s)  is proper if and only if detD  #  0 where lim_oo//(5)  =  D G RP^'P. Proof D = Q(s) of the above lemma. • Note  that  det(I  -  Hi(s)H2(s))  7^ 0  does  not necessarily  imply  that  det(I -D1D2) 7^ 0. To see this recall that in Examples  3.5 and 3.8 Hi  =  ^  ^ ^ H2  = 1 1   ¥=  0. However in any state-space  realization and  1 -  H1H2  =  1  -Di  =  lD2  — 1 and  1 -  D1D2  =  0. In other  words  when  Hi(s)  and H2(s) are proper  the condition  det(I  -  D1D2)  =  det(I  -  D2D1)  ¥- 0 implies  that  det{l -Hi(s)H2(s))  =  det(I  -  H2(s)Hi(s))  7^ 0  or  that  a  closed-loop  representation  in state-space  form  exists. When  det(I  -  Hi(s)H2(s))  #  0 a polynomial  matrix  rep resentation for the closed-loop  system exists but it is not necessarily in state-space form i.e. det(I  — D1D2) is not necessarily nonzero. s+  1 LEMMA 3.14.  Let//i(5) i/2W be proper with Di  =  \mis-^oH\{s\D2 =  lim^-^ooi/aW and assume that (/ -  Hi{s)H2{s)y^  exists. Then Hn{s)  =  (/ -  Hi{s)H2{s)y'^Hi{s) is proper if and only if det (I -  D1D2)  7^ 0. Proof  If det (I -  D1D2)  7^ 0 then in view of Lemma 3.11 /  -  Hi(s)H2(s) is biproper and Hu(s) is then proper. If Hii(s) is proper then Hu(s)H2(s) and /  + Hii(s)H2(s) are proptv.But(I-Hi(s)H2(s)y^  = I+ (I -  Hi(s)H2(s)y^Hi(s)H2(s)  =  I + Hn(s)H2(sX and therefore (/ -  Hi(s)H2(s)y^  is proper or /  -  Hi(s)H2(s) is biproper. This implies in view of Corollary 3.13 that det(I  -  D1D2)  7^ 0. • Similarly it can be shown that all the rational matrices in (3.82) namely. In U2 (/ -  HiH2y^HiH2] J (I-H2Hiy^H2 Ul  -  HiH2y^Hi (I-H2Hi)H2Hi tin H22\ Hn H21 \h U2. yi h. are proper if and only if det (I  —  D1D2) 7^ 0. This result can also be seen from the following  system  theoretic  argument:  The rational  matrices in (3.82)  exist  and are proper if and only if there exists a state-space representation of the closed-loop sys tem. This will happen if and only if det (/  -  D1D2) 7^ 0 in view of the  assumptions in (3.78). In the following  it is assumed that det {I -  Hi(s)H2(s))  #  0 that is the closed-loop system is well  defined. Controllability  and  observability Controllability  and  observability  of  closed-loop  systems  are  studied  next.  In view of the representation  (3.76) the following  is  evident: 1.  If  GIL is  a geld  of  Pi  Qi  and  G2L is  a  geld  of  P2 Q2 then \GiL 0 0 G2L\ IS  a eld of Pi -Q1R2 -Q2R1 Pi Qi 0 0 22 Thus  all uncontrollable  eigenvalues  of ^1  in  Gil  are  uncontrollable  eigenvalues  of  the  closed-loop  system  from  ri or r2 or . Also all uncontrollable  eigenvalues  of 52 in  G2L  are  uncontrol lable eigenvalues of the closed-loop system from  ri  or r2 or . There may be additional uncontrollable eigenvalues  in the closed-loop  system  and these  are studied below. 2.  If  GiR  is  a gcrd  of  Pi  Ri  and  G2R  is  a gcrd  of  P2 R2  then \GiR 0 0 G2R\ IS  a 579 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems crd of Pi -Q2R1 -Q1R2 P2 Ri 0 0 R2 That is all unobservable eigenvalues of Si  in  GiR  are unobservable  eigenvalues  of the closed-loop  system from  j i  or J2  or yi LJ2 . Also all unobservable  eigenvalues  of ^2 in  G2R  are  unobservable eigenvalues  of  the  closed-loop  system  from  j i  or  y2  or .  There  may  be additional  unobservable  eigenvalues  in  the  closed-loop  system  and  these  are studied below. Controllability  and  observabiHty  can  of  course  be  studied  directly  from the  PMDs  and  (3.76)  as  was  done  above.  However  further  insight  is  gained  if the additional uncontrollable and unobservable eigenvalues are determined from  the PMFDs of ^i  and ^2. For simplicity assume that both ^i  and ^2 are controllable and observable and consider the following  representations: For system 5i: (la) or (lb) Di(q)zi(t)  =  ui(t) yiit)  =  Ni(q)zi{t) Di(q)zi(t) -  Ni(q)ui(t) yi(t)  =  zi(t) where (Di(q)  Ni(q))  are re and 0i{q)  Ni(q))  are Ic. For system 5*2: (2a) or (2b) D2(q)z2(t)  =  U2(t) j2(0  =  N2(q)z2(t) D2(q)z2(t)  =  N2(q)u2(t\ J2(0  =  Ut\ where {D2{q) N2{q)) are re and (D2(q) N2{q)) are Ic. In view of the  connections (3.84) (3.85) (3.86) (3.87) uiit)  =  y2(t)  +  ri(t) (3.88) the closed-loop feedback  system of Fig. 7.4 can now be characterized as follows  [see also (3.76)]: U2(t)  =  yi(t)  +  r2(t) 1.  Using descriptions  (la)  and (2a) Eqs. (3.84) and (3.86) we have Ol Nil /  Ol 0 /J -Ni] £>2j In Vl.  ' £>i Ni y\ yi. Ni 0 \z\ U2. n U2 (3.89) 580 Linear  Systems 2.  Using descriptions  (lb)  and (2b) we have A^il D2J \z\ [h. 'Ni 0 ol A^2j ['•1 l.''2. ' yi y2. I  ol 0 i\ \zi U2. (3.90) 3.  Using descriptions  (lb)  and (2a) we have \n L'"2_ ' NxN2\ D2 \ Ol l\ "A^i 0 \zi U2. -I yi 72. 7  0] 0  7V2J r^i [^2_ (3.91) Also_D2Z2  =  M2 =  >'i  +  r2  =  Dj  WiMi  +  r2  =  £)j Wi()'2  +  '"1)  +  r2  = D~[^N\{N2Z2  +  n)  +  ^2 and yi  =  M2 — r2  =  D2Z2 "" ^2 from  which we obtain (D1D2 -  NiN2)z2  =  [NiDi] D2 N2  Z2  + n (3.92) 4.  Using descriptions  (la)  and (2b) we have Dx _-A^2A^i -11 JO2J \z\ U2. = I 0 0 A^2j V2. ' =  1  0 [y2. Ol l\ \zi [Z2. (3.93) Also^Z)izi  =  ui  =  y2  +  n  =  D2^N2U2  -^ n  =  ^2  ^^2(^1  +  ^2)  +  n  = £^2^N2(NiZi  +  r2) +  n  and 3^2  =  ^1 "" n  =  Dizi  — n  from  which we obtain {I )2£>l  - N2Ni)Zl  =  [ D2 A^2] nl /2J ' 'Ni' Pi  zi  + = 0 / Ol oj .3'2j (3.94) The preceding descriptions of the closed-loop system are of course equivalent.  This can be proved  directly using the definition  of equivalent representations  discussed in Subsection 7.3A. Therefore  these descriptions have the same uncontrollable  and unobservable  modes.  In  the  following  analysis  we  shall  carefully  select  different representations  to  study  different  properties  in  order  to  secure  additional  insight. The reader is encouraged to derive similar results using different  representations. It is stressed  that in the following  both S\  and  5*2 are assumed  to be controllable  and observable. The uncontrollability  and unobservability  discussed below is due to the feedback  interconnection  only.  As  was  previously  shown  there  will  in  general  be additional  uncontrollable  and  unobservable  eigenvalues  in  a closed-loop  system  if S\  and S2 are uncontrollable and/or unobservable. To study controllability consider the representation (3.89) in (1). It is clear  from the  matrices and that the eigenvalues  that  are  uncontrollable from  ri  will be the roots of the determinant  of  a geld  of[—NiD2\  and the  eigen values that are uncontrollable  from  r2 will be the roots of a geld of  [D\  -A^2]- The -N2 D2 closed-loop  system  is  controllable  from ^2 Clearly  all possible  eigenvalues  that are uncontrollable from r\  are eigenvalues of ^2. These are the poles of//2  =^ ^2^2 ^ that  cancel  in the product  H2N1 or as can be  shown  they  are the poles  of H2 that 581 CHAPTER?: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems cancel  in / H2 H\.  Similarly  all possible  eigenvalues  that  are uncontrollable  from  r2 are eigenvalues  ofS\.  These  are the poles  ofH\  =  NiD^^ that cancel  in the  product H1N2  or  as  can  be  shown  they  are  the  poles  of  Hi  that  cancel  in Hi I Ho To  study  observability  consider  the  representation  (3.90)  in  (2).  It  is  clear from  the  matrices Di -Ni and that  the  eigenvalues  that  are  unobserv-able  from  ^yi will  be  the  roots  of  the  determinant  of  a  gcrd  of   and  the  eigen D2 values  that  are unobservable  from  y2  will be the roots of the determinant  of a gcrd  of Di -N2 . The  closed-loop  system  is  observable  from . Clearly  all possible  eigen values  that  are  unobservable  from  yi  are  eigenvalues  of  S2.  These  are  the  poles  of H2  =  D2^N2  that cancel in the product N1H2  or as can be  shown  they  are the  poles of  H2  that  cancel  in Hi  [I H2\-  Similarly  all possible  eigenvalues  that  are  unobserv able  from  y2  are  eigenvalues  of  ^ i. These  are  the  poles  of  Hi  =  D^^Ni that  cancel in  the  product  N2H1  or  as  can  be  shown  they  are  the  poles  of  Hi  that  cancel  in H2[HiIl Summary  The  poles  of  H2  that  cancel  in  H2N1  (or  in / Hi Hi)  are  the  eigen values  that  are  uncontrollable  from  r i;  the  poles  of  H2  that  cancel  in  N1H2  (or  in HiU  H2\)  are  the  eigenvalues  that  are  unobservable  from  yi.  The  poles  of  Hi  that cancel  in  H1N2  (or  in \Hi' H2)  are  the  eigenvalues  that  are  uncontrollable  from  r2\ the  poles  of  Hi  that  cancel  in  N2H1  (or  in  H2[Hi  /])  are  the  eigenvalues  that  are unobservable  from  j 2' The  above  results  may  also  be  expressed  in  the  following  convenient  way which  however  should  be  used  only  when  the  poles  of  Hi  are  distinct  from  the poles  of  H2  to  avoid  confusion.  In  the  product  H2H1  the  poles  of  H2  that  cancel are  the  eigenvalues  that  are  uncontrollable  from  r i;  the  poles  of  Hi  that  cancel  are the  eigenvalues  that  are  unobservable  from  ^2- In  the  product  H1H2  the  poles  of  ^i that  cancel  are  the  eigenvalues  that  are  uncontrollable  from  r2; the  poles  of  H2  that cancel  are the  eigenvalues  that  are  unobservable  from  yi. EXAMPLE  3.9.  Consider  systems  Si  and  S2 connected  in  the  feedback  configuration of Fig. 7.4  and let S\  and ^2 be described by the transfer  functions  Hi(s)  = ^ +  1 s  -  I -  and Hiis)  = ——. For the  closed  loop to be  well  defined  we must have  1 -  H1H2  = s + b 1  -s +  I  ais  + ao s -  I s  + b (1 —  ai)s^  + (b  —  ai  —  ao —  l)s  —  (b  -\-  ao) 7^ 0.  Note  that  for {s -  \){s  +  b) a\  =  lao  =  -landZ?  = values  are not allowed for the parameters  if the closed-loop  system is to be  represented by  a  PMD.  If  state-space  descriptions  are  to  be  used  let  Di  =  \ims-^o=Hi(s)  =  1 and  D2  =  lims^ooH2(s)  =  a\  from  which  we  have  1 -  D1D2  =  1 -  ^1  7^ 0  for  the 1 -1  = 0.  Therefore  these ^-^mdl-HiH2 ^ +  1 l//2 582 Linear  Systems closed-loop  system  to be  characterized  by  a  state-space  description  [and  also in  view of  Lemma  3.14  for  the  transfer  functions  in  (3.82)  to  be  proper]. Let  us  assume  that ai  7^ 1. The uncontrollable  and unobservable  eigenvalues  can be determined  from  a  PMD such as (3.92). Alternatively  in view  of the discussion just preceding  this example  we conclude the following  (i) The eigenvalues that are uncontrollable from  ri  are the poles of H2 that cancel in H2N1  =  ais  +  ao s + b trollable from  r\  (at  - 1)  only when  b  =  \.  If this is the case  -1  is also an  eigenvalue {s +  1) i.e. there is an eigenvalue that is uncon that is unobservable from y\.  (ii) The poles of Hi  that cancel in H\ N2  = are the eigenvalues that are uncontrollable from  r2 i.e. there is an eigenvalue that is un controllable  from  r2 (at  -1-1) only  when  ao/ai  =  - 1. If  this  is the  case  -1-1 is  also  an eigenvalue that is unobservable from  j2- • r (ais  + UQ) ^ +  1 Stability The closed-loop feedback system is internally stable if and only if all its eigenval ues have strictly negative real parts. The closed-loop eigenvalues can be determined from  the  closed-loop  descriptions  derived  above.  If  for  example  (3.76)  is  con-sidered then the closed-loop eigenvalues  are the roots of  det \  Pi [-Q2R1 -Q1R2 Pi Since  any  uncontrollable  or  unobservable  eigenvalues  of  S\  and  ^'2  is  also  an uncontrollable  or  unobservable  eigenvalue  of  the  closed-loop  system  it  is  clear that  for  internal  stability  both  ^i  and  5*2  should  be  stabilizable  and  detectable i.e. any uncontrollable  or unobservable  eigenvalues  of 5*1  and ^2 should have neg ative  real  parts.  This  is  a necessary  condition  for  stability.  We  shall  nov^  concen trate on the descriptions (3.87) to (3.94) in (1) through (4). First recall the identities det A  D C  B =  det(A)det(B -  CA'^D)  =  det(B)det(A -  DB'^C) (3.95) v^here  in  the  first  expression  it  v^as  assumed  that  det {A)  ^  0  and  in  the  second expression  it  was  assumed  that  det(B)  7^ 0.  The  proof  of  this  result  is  imme-diate  from  the  matrix  identities I -CA-^ 0] \A  D [c  B  = l\ D B-CA-^D and [/ [0 -Dfi-'l /  J \A  D [c  B A-DB-^C C 0 B Now consider the polynomial  matrices -N2 D2 -N2 D2  (D1D2  -N1N2)  and  {D2D1 -  N2N1)  from  the closed-loop  descriptions  in  (1)  (2) (3)  and (4). Then det £>! -Ni -N2 £>2 =  det(Di)  det{D2  -  NiD^^N2) -  Dx^N\N2) =  det(Di)det(D2 =  det(Di)  det01^)  det0iD2 =  aidet(DiD2 -  N1N2) - N1N2) (3.96) 583 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems where at  is a nonzero real number. Also det -N2 D2  =  det(D2)  det(Di  -  7V2^2 ^M) =  det(D2)  det(Di  -  D2^N2Ni) =  det(D2)  det 02^)  det(D2Di  -  N2N1) =  a2 det 02^1 -  N2N1I (3.97) where 0:2 is a nonzero real  number. Similarly det -N2 D2 =  didet02Di -  N2Ni\ (3.98) where di  =  det(Di)  det(D^  ^) is a nonzero real number  and det -N2 Di =  d2det(DiD2 -  N1N2I (3.99) where ($2  "=  det 02)  det(D2^)  is a nonzero real number.  These  computations  ver ify  that the equivalent  representations  given by  (1) (2) (3) and  (4) have  identical eigenvalues. The following  theorem presents conditions for the internal stability of the  feed back  system  of Fig. 7.4. These conditions  are useful  in a variety  of  circumstances. Assume that the systems Si  and 52 are controllable and observable and that they are described by (3.84) to (3.87) with transfer  function  matrices given by and Hi  =  NiDi^  =  Di^Ni H2  =  N2D2^  =  D2^N2 (3.100) (3.101) where the {Nt Di) are re and the {Nt Di) are Ic for /  =  12. Let a 1 (s) and a2(s) be the pole (characteristic) polynomials ofHi(s)  and H2(s) respectively. Note that ai(s)  = i  =  1 2 for  some nonzero real numbers  ki ki.  Con kidet(Di(s))  =  kidet0i(s)) sider the feedback  system in Fig. 7.4. THEOREMS.15.  The following statements are equivalent: (i)  The closed-loop feedback system in Fig. 7.4 is internally stable (ii)  The polynomial or -Ni' (a)  det (b)  det -N2 Di. Di Ni bx Ni (c)  det0iD2 (d)  det02Di is Hurwitz; i.e. its roots have strictly negative real parts -NiN2)ov -N2N1) O2. or (iii)  The polynomial ai(s)a2(s)det(I  -  Hi(s)H2(s)) ==  ai(s)a2(s)det(I  -  H2(s)Hi(s)) (3.102) is a Hurwitz polynomial. 584 Linear  Systems iv)  T he  poles  of ill Ml. I -Hi -1 -H2 I ^1 72. {I-H2H1)-' Hi(I-H2Hir H2(I-HiH2r'] J {I-HIH2V \ri U2 are  stable  i.e.  they  have  negative  real  parts. (v)  The  poles  of 'y\ yi. '-H2 .  I I -Hi_ -1  "0 L ^l H2] \ri L^2_ 0 J (I-HiH2r'Hi {I-H2H1)-'H2H1 (I-HiH2r'HiH2] (/  -  H2H1)  H2  J \ri ih are  stable. (3.103) (3.104) Proof.  It  was  shown  previously  in  (3.96)  to  (3.98)  that  all  the  determinants  in  (ii)  are equal  within  some  nonzero  constants  i.e.  they  have  the  same  roots.  These  roots  are  the eigenvalues  of  the  closed-loop  feedback  system  [recall  that  the  representations  (3.89) to  (3.94)  are  all  equivalent].  The  feedback  system  is  internally  stable  if  and  only  if  its eigenvalues  have  negative  real  parts.  This  shows  that  (ii)  is  true  if  and  only  if  (i)  is  true. det(D^\DiD2-and  a2(s)  = NiN2)D2^)  =  det(D^^) k2 det  (D2(s))  where  ki  k2 are  nonzero  real  numbers  it follows  that  ai(s)a2(s) det  (/  -Hi(s)H2(s))  =  ki /c2 det(Dk)  which  is  a  polynomial  the  roots  of  which  are  the  closed-loop  eigenvalues.  Note  also  that  det  (I  -  H1H2)  =  det  (I  -  H2H1).  Since  the  roots  of (3.102)  are  exactly  the  closed-loop  eigenvalues  (iii)  is  true  if  and  only  if  (i)  is  true. T o s h o w ( i i i )  l e t D^  =  D1D2-  NiN2  2indwvitQ  det  (I  -  H1H2)  = det(Dk).  Since  ai(s)  =  kidet(Di(s)) det(D2^) To  show  (iv)  we  write / -Hi H2 I D2 0 Di -Ni 0" 6 1^ -1  r -7^2" Di_ -1 62 [ -Ni \Di 0" -N2 [0  D2_ D2_ (3.105) and  we  notice  that  these  are  coprime  polynomial  matrix  factorizations  (show  this).  The poles  of / -Hi -H2 I are the roots of J^H I Di -Ni -N2 D2 that are precisely  the  closed-loop  eigenvalues.  Note  that the poles  are  also  equal  to the  roots  of det  [ | -N2 Di 0 / /  0 So (iv) is true if and only if (i) is true  since the poles in (3.103)  are exactly  the  eigenvalues of  the  closed-loop  system. [see  (ii) above]  since 0 / /  0 Di -N2 D2 -Ni -N2 Di -Ni D2 -Ni D2 bi N2 -Ni D2 det To  show  (v)  we  note  that of  (iv)  above  implies  that yi' y2_ = "0  n  lui 1^2. -~riT    which  in  view  of  the  proof 'yi  = = / -Hi 0  nl I  ojl 0 /ir /  oj^ I -Hi / -Hi -H2 I -H2 I -H2] I I 0 ro [HI 0 / H2] 0 0' Di. -1  "0 .A^i A^2l 0 J \r\ ih. /] "0 / oj /] "0 _/ OJ /I "0 J  oj D2 -N2 62 -N2 -1 -1 D2 . 0 "0 Nx To  /] 1/  OJ -A^if 62J [  61 L-A^2 ^\Ni 01 [0  7V2J - ^ il ^2] Tn ' ih -1 \b. 0] bi\ [0 \r\ N2\ Vh. 0  J [0  /] [/  oJ 1•-1  "0 Ni Ni] 0 J \r\ ih. (3.106) or 'yi .h. "  A  - / ! /2 585 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems which is an Ic factorization. This of course is the expression for the transfer function that could have been derived directly from the internal description in (3.90). Similarly from (3.89) it can be shown that yi 0 N2j Di -Ni 0 -1 I -N2 D2\ (3.107) which is an re factorization. Clearly the poles of the transfer function matrix in (3.104) are precisely the closed-loop eigenvalues which implies that that relates and (v) is true if and only if (i) is true. • Remarks 1.  Parts  (iv)  and  (v) of Theorem  3.15  can  also be  shown  to be  true by  using  the following brief argument. It was shown that the feedback  system is controllable from and observable  from . Also it can easily be shown that the system is observable  from . Therefore  the poles of the transfer  function  from or to are precisely  the eigenvalues  of the closed-loop  system  which must have negative real parts for internal  stability. 2.  It is  important  to consider  all  four  entries  in  the  transfer  function  (3.104)  be tween and [or  in  (3.103)  between and ]  when  considering  inter nal stability. Note that the eigenvalues that are uncontrollable from  r\  or r2 will not appear in the first or the second column of the transfer  matrix respectively. Similarly the eigenvalues  that are unobservable  from  y\  or j2  will not  appear in  the  first  or  the  second  row  of  the  transfer  matrix  respectively.  Therefore consideration  of  the  poles  of  some  of  the  entries  may  lead  only  to  erroneous results  since possible  uncontrollable  or unobservable  modes  may  be  omitted from  consideration  and these may lead to instabilities. 3.  Write and If and (/  -  HiH2y^  =  D2(DiD2  -  NiN2y^Di ( /- -H2Hi)-^  = DiiD2Di-N Dk  =  D1D2 -  N1N2 Dk  =  D2D1 -  N2NU (3.108) (3.109) (3.110) (3.111) 586 Linear  Systems then  (/  -  7/1//2)"'  =  D2D-'Du (I  -  H2H1) -- DiDl^D2 and D2Dl^Ni N2Dl^Nx NiDl^N2 DiDl^N2 NiD-^D2 [N2D-^Ni NiDl^N2 N2D^^Di\ (3.112) Note that the  first  column  above  can be  written  as D2 N2 D^^N\  and possible  can cellations may  only occur between  D^  and A^i since  the  D2 N2 are re. In view  of  the representation  (3.92)  these  are  precisely  the  eigenvalues  of  the  closed-loop  sys tem that are uncontrollable  from  ri.  Similar results  can be derived  by  considering the  second  column.  These  results  agree  with  the  comments  made  in  Remark  (2) above.  Similarly  consider  the  first  row  NiDl^[D2N2\. Since  the  [^2>  A^2]  are Ic  possible  cancellations  may  occur  only  between  Ni  and  D^  which  in  view  of representation  (3.94)  are precisely  the  eigenvalues  of the closed-loop  system  that are unobservable  from  y.  Analogous  results  can be established by  considering  the second  row  N2Dl^{N\  D\\. Finally  we  note  that N2D DiD ^-1 D2D-'Ni D^D l ^ r ^2 D2D-'Di (3.113) from  which  similar results concerning  uncontrollable  and unobservable  eigenval ues  can  easily  be  derived. 4.  The roots  of each  of the polynomials  in  (ii) of Theorem  3.15  are equal  to the  roots of  the  polynomials  in  (iii)  and  are  equal  to  the  poles  of  the  transfer  functions  in (iv)  and  in  (v). They  are  exactly  the  eigenvalues  of  the  closed-loop  system. 5.  The  open-loop  characteristic  polynomial  of  the  feedback  system  is a\{s)a2{s). The  closed-loop  characteristic  polynomial  is  a  monic  polynomial  aci{s)  with roots  the  closed-loop  eigenvalues  i.e. it is equal  to  any  of the polynomials  in  (ii) within  a multiplication  by  a nonzero  real  number.  Then  relation  (3.102)  implies in view of (4) that the  determinant  of the  return  difference  matrix  [I—Hi (s)H2(s)] is the  ratio  of  the  closed-loop  characteristic  polynomial  over  the  open-loop char acteristic  polynomial  within  a  multiplication by  a  nonzero  real  number ais  + ao EXAMPLE  3.10.  Consider  the  feedback  configuration  of  Fig.  7.4  with  Hi  = s + 1 s-  r the  transfer  functions  of  systems  Si  and  5*2 respectively.  Let  ai  7^ I Ho  = so that  the  loop is well  defined  in terms  of  state-space  representations  (and  all  transfer functions  are proper). (See Example  3.9.) s + b All  polynomials  in  (ii)  of  Theorem  3.15  are  equal  within  a  multiplication  by  a nonzero  real  number  to  the  closed-loop  characteristic  polynomial  given  by  aci(s)  = 9 s  + .    „ . This  polynomial  must  be  a Hurwitz  polynomial  for b —  ai  —  ao —  I s  - b + ao  ^  . .   ^^ .    I  —  ai I  —  ai internal stability. If ai(^)  =  s —  I and a 2(5) =  s + bsiVt the pole polynomials of Hi  and H2 then the polynomial in (iii) is given by ai(s)a2(s)(l -  Hi(s)H2(s))  =  (1 -  ai)s^  + (b — ai  -  ao -  l)s  -  (b -\- ao)  =  (I  — ai)aci(s)  which implies that the return  difference 1  -  Hi(s)H2(s) (s -  m  + b) ai(s)a2(a) (1  -  ai)aci(s)  and  the  transfer  function  matrix  in  (iv) of -.  Note  that  (1  -  H1//2)" (1  -  H2Hir with  a(s)  = (1  -  ai) a{s) 587 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems Theorem  3.15 is given by (s -  l)(s  -\-  b) (s ~  l)(ais  +  ao) a(s) a(s) (s +  1)(^ +  b) (s -  l)(s  +  b) a{s) a{s) The polynomial  a{s)  has a factor  s  +  \  when  b  =  I.  Notice that when b  =  1. If this is the case (b  =  1) then a(-l) 2-2b  =  0 -s  -  I a(s) s+  1 a(s) (s -  l)(ais  +  ao)-a(s) s-  1 a(s) where a(s)  =  (s + l)d(^). Notice that three out of four transfer  functions  do not contain the  pole  at  -1  in  d(s).  Recall  that  when  b  =  1  -1  is  an  eigenvalue  that  is  uncon trollable from  ri  and it cancels in certain transfer  functions  as expected  (see  Example 3.9). Similar results can be derived when ao/ai  =  -I.  This illustrates the necessity  for considering  all the transfer  functions  between  wi W2  and  ri  r2 when  studying  internal stability  of  the  feedback  system.  Similar  results  can  be  derived  when  considering  the transfer  functions  between  j i  y2 and ri  r2 in (v). • COROLLARY 3.16.  The closed-loop feedback  system in Fig. 7.4 is internally  stable if and only if (i)  Djf i[(/  -  H2Hi)-\  H2(I  -  HiH2)-^]  is stable or (ii)  D2^[Hi(I  -  H2Hi)-\ is stable or (I  -  HiH2y^] (iii) (iv) (I-H2H1)-' [Hi(I  -  H2H1)-' H2(I  -  H1H2)-' (I  -  HH2)-' D2 ^ is stable or D^ ^ is stable. Proof.  The  above  expressions  originate  from  rows  and  columns  of  the  transfer  func tion  matrix  between and .  In  view  of  (3.113)  expression  (i)  is  equal  to D\^DiDl^[D2  N2\  =  D^^[D2 A^2] the poles of which are precisely the roots of  detDk the closed-loop eigenvalues since the pair (D2 N2) is Ic. Similar arguments hold for (ii) (iii) and (iv). • It  is  of  interest  to  point  out  that  expression  (i)  of  the  corollary  is  precisely  the transfer  function  between  the  partial  state  z\  and  the  input   in  view  of  (3.103) and  Dizi  =  u\.  Expression  (ii)  corresponds  to the  transfer  function  between  zi  and . Notice  that  \ir\  = = = (i-H2Hir' (I-HiH2)-'Hi //2^2  + \l-H2Hi)-'H2 {I-HH2)-' Hxh from  (3.104).  Since  H2  =  ^2  ^^2  is  an  Ic  factorization  the  transfer  function  from zi\ to  r2 is  stable  if  and  only  if  expression  (iii)  is  stable.  A  similar  argument  holds for  (iv). The  following  result  can  be  proved  in  a  completely  analogous  manner  to  the proof  of  the  above  corollary. 588 Linear  Systems COROLLARY  3.17.  The closed-loop feedback  system in Fig. 7.4 is internally stable if and only if (i)  DfH/  -  H2Hi)-^D2^ (ii)  D^^I -  HiH2T^b^^ is stable or is stable. • Note  that  expression  (i) in  the  corollary  is  stable  if  and  only  if the  transfer  func tion between  z\  and is  stable. To  see this note that  u\  =  (I  -  HjHi) ^ n  from whichzi  =  Df^wi  =  [ Z ) j ~ H / - ^ 2 ^ i ) " ^2 ] ^ 2 n . N o w D 2 ri  =  D2U1 -  D2y2  = D2U1 -  N2U2  =  [D2 -N2\ i.e.£i  - [D-\l-H2Hi)-'D^'W2-N2} Since  the  pair  (62 N2)  is  Ic  this  transfer  function  is  stable  if  and  only  if  expres sion  (i)  is  stable.  Similarly  U2  =  (I  -  H\H2)~^r2^ from  which  £2  =  [^2  H^  ~ HiH2r'D^']Dir2 =  [D^\I - HiH2r'D^'][-NiD]   which  is  stable  if and  only  if  expression  (ii)  is  stable  since  (-A^i  Di) matrices. is  an  Ic  pair  of  polynomial EXAMPLES.11.  Consider again the systems described by Hi (s)  = a\s  + ao  as in Example 3.10. The expressions in Corollary  3.16  are given by s+  1 s-  1 and 7/2 W  = s + b (i) 1 s-  1 1 (ii) s + b a(s) (s -  l)(s  +  b) {s -  \){s  + b) ' (s +  l)(s  + b) a(s) (s- l)(ais  +  ao) a(s) (s- l)(s  +  b) a(s) 1 a(s) [s +  b ais  + ^o] [s+hs- 1] a(s) a(s) 1 (iii) (s +  l)(s  +  b) s + b s-  1 s+  1 1 a(s)' a{s) (s -  l)(ais  +  ap) a{s) (iv) (s -  1)(^ +  b) a(s) 1 1 ais  + ao I  s  + b  \  a(s) 1 Clearly the monic pole polynomial  of each of the transfer  functions  in (i) to (iv) is 9 =  s  + (b -  ai  —  ao -  I) a(s) I  —  ai polynomial.  This  is  so even  when  Z? =  1 or aolai  =  -1  (see Example  3.9)  in  which case there are uncontrollable/unobservable  eigenvalues. Notice that s  + b and a\s  + ao are coprime by the definition  of  H2(s). =  cici(s) the closed-loop  characteristic (b + ao) \  —  a\ 1 —  a\ s -  —   ^  Similarly the expressions in Corollary  3.17  are given by (i) (ii) 1 (s -  l)(s  + b)  1 s —  I a(s) s  + b 1 (s -  l)(s  + b)  1 s + b as  expected. a(s) s-  1 1 a(s) 1 a(s)' 589 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems PART 2 SYNTHESIS OF CONTROL  SYSTEMS 7.4 FEEDBACK CONTROL SYSTEMS In this  section  feedback  systems  are  studied  further  with  an emphasis  on  stabiHz-ing feedback  controllers. In particular  it is shown how  all the stabilizing  feedback controllers can be conveniently parameterized. These parameterizations are very im portant in control  since they  are fundamental  in methodologies  such  as the  optimal /7°°-approach to control design. Our development  of the subject  at hand builds upon the controllability  observability and particularly the internal  stability results intro duced  in  Section  7.3 as well  as on the Diophantine  Equation  results  of  Subsection 7.2E.  First  in  Subsection  7.4A  all  stabilizing  feedback  controllers  are  parameter ized using PMDs. A number of different  parameterizations  are introduced  and dis cussed  at length.  State  feedback  and  state estimation  using  PMDs  are  introduced in Subsection 7.4B and are then used in Subsection 7.4C where all stabilizing  feed back controllers  are parameterized  using proper  and  stable MFDs. Two degrees of freedom  feedback  controllers  offer  additional  capabilities  in control design  and  are discussed in Subsection 7.4D. Control problems are also described in this subsection. A.  Stabilizing  Feedback  Controllers Now consider  systems Si  and ^2 connected  in the feedback  configuration  shown in Fig. 7.4 or equivalently in Fig. 7.5. Given S\  it is shown in this section how to pa rameterize all systems ^2 so that the closed-loop feedback  system is internally stable. Thus if 5*1 called the plant  is a given system to be controlled then ^2 is viewed as the feedback  controller  that is to be designed. Here we provide the parameterizations of all stabilizing feedback  controllers. ^^O S^ yi 3^2 Or FIGURE 7.5A Feedback configuration '^ /-2 +^ -N. ^2   Q^ >H Si yi FIGURE 7.5B Feedback configuration 590 Linear Systems Several parameters will be used to parameterize  all stabilizing  systems S2. The results  are  based  on  PMDs  of  the  systems  S\  and  5*2 and  of  the  closed-loop  sys tem in particular PMFDs. Parameterizations  are introduced using first the polyno mial matrix parameters (1) D^ N^  and D^ Nk and then the stable rational parameter (2) K  =  NkDl^  =  D^^TV^. These parameters are very convenient in characterizing stability  but  cumbersome  when  propemess  of  the  transfer  function  H2  of 5*2  is  to be  guaranteed.  The rational  proper  and  stable parameters  (3)  Qi  and  Q2 are intro duced  next. These  are very  convenient  when properness  of H2 is to be  guaranteed but  cumbersome  when  characterizing  stability  except  in  special  cases  e.g.  when Hi  is  stable.  The  stable  parameters  (4)  Su  and  521  that  are  the  comparison  sen sitivity  matrices  of the feedback  loop will  also be used.  These  can be employed  to conveniently parameterize all stabilizing H2 controllers only in special cases for ex ample when / /f  ^  exists. Parameters  (5) X2 and X2 which are closely related to Q2 will also be introduced  and discussed.  The results based  on X2 in the case of  SISO systems reduce to well-known  classical control results. Finally rational  parameters (6) Li  L2 are introduced  to help in providing  alternative proofs  for  the results  that involve the previously  introduced parameters. These results provide a link with pa rameterizations  of  all  stabilizing  controllers  using  proper  and  stable  factorizations. Such parameterizations  are addressed in Subsection  7.4C. The distinguishing characteristics of our approach of the parameterizations of all stabilizing feedback  systems S2 (of all stabilizing  controllers) is that they are based on internal representations which were developed at length earlier in this book. This approach builds upon previous results and provides significant insight that allows the introduction  of  several  parameters  that  are  useful  in  different  circumstances.  This approach  also  makes  it possible  to easily  select  the  exact  locations  and  number  of the desired stable closed-loop eigenvalues. Utilizing the development of this section a parameterization that uses proper and stable MFDs and involves a proper and stable parameter  K'  is introduced  in Subsection  7.4C. The parameter  K'  is closely  related to the stable rational parameter K used in the approach enumerated above. This type of parameterization  is useful  in certain control design methods  such as optimal //°°-control  design. In  the  following  the  term  "stable  system  5"  is  taken  to  mean  that  the  eigen values  of  the  internal  description  of  system  S  have  strictly  negative  real  parts  (in the continuous-time  case) i.e. the system S is internally  stable. Note that when  the transfer functions in (3.103) and (3.104) of the feedback  system S are proper internal stability  of S implies  BIBO  stability  of the feedback  system  since the poles  of  the various  transfer  functions  are  a  subset  of  the  closed-loop  eigenvalues  (see  Section 7.3 and Chapter 6). Feedback  systems The feedback  system of Fig. 7.5 was studied at length in Subsection 7.3C. Recall that if Pi(q)Zi(t)  =  Qi(q)uiit) ytit)  =  Ri(q)Zi(t) i  =12 are polynomial matrix descriptions of Si and ^'2 then the closed-loop system is given by (3.76) i.e.. 591 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems Pi -QiRi QIR2] P2 J [Z2j = Qx 0 yi 'J2_ = ^1 0 0] U2_ ol R2\ \zi\ [Z2\ (4.2) where the relations Mi(0 =  nCO + nit) mit)  = yiit) +  nit) (4.3) have  been  used.  Based  on  this  description  it  was  shown  that  for  the  closed-loop eigenvalues  to be  stable  (to have  strictly  negative  real parts) it is necessary  for  5i and  ^2 to be  stabilizable  and  detectable. This  is  so because  the uncontrollable  and unobservable  eigenvalues  of both  S\  and  ^2 will  also  be  uncontrollable  and  unob-servable eigenvalues  of the closed-loop  system. Now  assume that S\  and  ^2 are controllable  and  observable  and  let system  S\ be described by or (la) (lb) Dx{q)Zi{t)  =  ui(t)yiit)  =  Niiq)ziit) Di(q)zi(t)  =  Ni(q)ui(t)  yiit)  =  zi(t) (4.4) (4.5) where  the  pair  {D\{q) N\{q))  is  re  and  the  pair  (D\iq)  Ni{q))  is  Ic. Let  Hi{s)  = Ni{s)D^^{s)  =  D'[^(s)Niis)  be the transfer  function  matrix of S'l. Next let  system 52 be described  by or (2a) (2b) D2(q)z2{t)  =  U2(t) yiit)  =  N2iq)z2(t) D2(q)Z2{t)  =  N2(q)u2(t)  y2{t)  =  Mt) (4.6) (4.7) where  the  pair  {D2{q) N2{q))  is  re  and  the  pair  {D2{q) N2(q))  is  Ic. Let  H2is)  = N2(s)D^Hs)  D2 ^(s)N2(s) be the transfer  function  matrix of S2. Recall from  Sub section 7.3C [see (3.89) to (3.94)] that the closed-loop system descriptions are in this case as  follows: 1.  Using descriptions  (la)  and (2a) we have Di A^2l D2\ \zi U2. 7  ol 0 i\ \n U2 \yi [y2. 'Ni 0 0] / ^2} \zi L^2. 2.  Using descriptions  (lb)  and (2b) we obtain Ni] D2J \zi [Z2. -N2 \yi y2. 'Ni 0 7 0 ol A^2j ['"' [''2. -| [zi LZ2 Ol /J (4.8) (4.9) 592 Linear Systems 3.  Using descriptions  (lb)  and (2a) we have (D1D2 -  NiN2)Z2  =  [NiDi] D2 N2  Z2  + 4.  Using descriptions  (la)  and (2b) we obtain {D2D1 -  N2Nl)Zl  =  02  N2] zi  + 0  0 -/  0 (4.10) (4.11) These descriptions of the closed-loop system are all equivalent and the polynomials det -N2 det -N2 -Ni D2 det{DiD2-NiN2) and det(D2Di  -  N2N1) are all equal within multiplication of nonzero scalars. Their roots are the closed-loop eigenvalues  (see Theorem  3.15). Parameterizations  of all stabilizing systems S2 The six parameterizations of all stabilizing systems ^2 previously mentioned are now introduced  and discussed  at length. 1.  Parameters  Dt  N^  and  Dj^ N^.  Consider  now  the  closed-loop  description given in (4) above and let D2D1 -  N2N1  =  Dk (4.12) where  D^  is  some  desired  polynomial  matrix  so that  the roots  of  detD^  are  stable (have  negative  real  parts).  If  Z)i  A^i D^  are  assumed  to  be  given  and  D2  -N2 that satisfy  (4.12) are to be determined then this equation is seen to be a polynomial matrix linear Diophantine equation. Such equations were studied in Subsection 7.2E. In  view  of  Theorem  2.15  of  Chapter  7  Eq.  (4.12)  has  a  solution  for  any  Dj^ (of appropriate dimension) since the pair (Di A^i) is re. All solutions of the Diophantine Equation  (4.12) can be parameterized  as follows  [see (2.54)]: D2  =  DkX  -  N^N -N2  =  DkYi  +  NkDi [D2-N2]  =  [DkNk] Xx (4.13a) (4.13b) (4.14) or where  X\  Y\  satisfy  X\D\  -\-  Y\N\  =  /  the  pair  {NiD\) is  Ic  and  is  such  that —N\D\  + D\N\  =  0  and  N^  is  an  arbitrary  polynomial  matrix  of  appropriate  di-mensions. Note that  {D^Xi  D]J^\\ is a particular solution of  {D2  -A^2] \Dx ^1 =  D. For the  system  D2Z2 =  N2U2 J2  =  Z2  to be well  defined  we must  have detD2  = det (Dj^Xi — NkNi)  7^ 0 i.e. Nk (and D^) must be such that this condition is  satisfied. Similar results can be derived using the closed-loop description given in (3). Let D1D2  -  N1N2  =  Dk (4.15) where D^ is some desired polynomial matrix so that the roots of det D^ are stable. In view of Theorem 2.15 all solutions of the Diophantine Equation (4.15) are given by D2  =  XiDk  -  NiNk -N2  =  YiDk  +  DrNk N2 D2_ Di Ni -Yi\ Xi\ \-Nk [Dk (4.16a) (4.16b) (4.17) or 593 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems where  Xi  Yi  satisfy  DiXi  +  A^i7i  =  /  the  pair  (NiDi) is  re  and  is  such  that —DiN\  +  NiD\  =  0  and Nk  is  an  arbitrary  polynomial  matrix  of  appropriate  di mensions.  Note  that  XiDk YiDk is  a particular  solution  of  [Di  Ni]  D2 -N2 =  Dk.  For the  system  D2Z2 ="  ^2  j2  =  ^^2^2  to  be  well  defined  we  must  have  detD2  = det{XiDk  -  NiNk)  7^ 0  i.e.  Nk  (and  Dk)  must  be  such  that  this  condition  is  sat isfied. The preceding two sets of parameterizations  for ^'2 are of course related. A con venient  way  of  presenting  the  above  results  in  a  unified  way  is  to  use  the  doubly coprime  factorizations  of  H\.  To this  end  assume  that  the  descriptions  (4.4)  and (4.5) for  system Si  satisfy UU-'  = Fi Xi A^i  A  J £»! [A^I - I 'l ^ i. / 0 0 / (4.18) where  U  is  3.  unimodular  matrix  (i.e.  det U  is  a.  nonzero  real  number)  and  the X\  YiXi  Yi  are  appropriate  matrices.  Factorizations  of  Hi  =  NiD^^  =  D^^Ni that  satisfy  (4.18)  are  called  doubly  coprime factorizations  of  Hi.  Such  factoriza tions always exist (see e.g.  [21] [1]). In particular in Theorem 2 of  [1] it is shown that if the relations XiDi  +  FiA^i  -  / DiXi  + Nifi  =  I  and  -NiDi -f- DiNi  =  0 are  satisfied  then  (4.18)  is  satisfied  for  Di  A^i Di  Ni  Xi  Yi  and  [Xi  Yi]  = [Xi  Yi]  -h ^[-A^i Di] where  S  =  XiYi  -  YiXi  i.e.  any  Ic and  re  factorizations of Hi  with associated matrices can be adjusted  so that they are doubly  coprime. If Di  Ni  and Di  Ni  are doubly  coprime factorizations  and  satisfy  (4.18) then the solutions of the Diophantine Equations in (4.14) and (4.17) can be written as and from  which it follows  that and [D2.-N2]  =  [DkNk\U N2 D2 =  U -1 -Nk [DkNk]  = [D2-N2]U-' -Nk Dk = u\ N2 D2 (4.19) (4.20) (4.21) (4.22) 594 Xhe relation  between  D^  N^  and  D^ N^  is  now  clear  namely Linear Systems -DuNk  +  NkDk  =  0. (4.23) THEOREM  4.1.  Assume  that  the  system  Si  is  controllable  and  observable  and  is  de scribed  by  the PMD  (or PMFD)  as  (i) DiZi  =  u\ y\  =  N\Z\  given  in  (4.4) or by  (ii) b\z\  =  N\u\y  y\  =  zi  given  in  (4.5).  Let  the  pair  (DiNi)  and  the  pair  (DiNi)  be doubly coprime factorizations  of the transfer function  matrix//i (5)  =  MDj"^  =  D^^Ni [i.e.  (4.18)  is  satisfied].  Then  all  the  controllable  and  observable  systems  S2 with  the property  that the closed-loop feedback  system eigenvalues  are stable (i.e. have  strictly negative real parts) are described  by (i) D2Z2  =  N2U2 (4.24) where  D2  =  DkXi  -  NkNi  and  N2  =  -{DkYi  +  A^^^i)  with  Xi  Fi A^i 5i  given  in (4.18)  [see  also  (4.19)]  and  the  parameters  D^  and  Nk  are  selected  arbitrarily  under the conditions  that D^^  exists  and is stable and the pair  {Dky Nk)  is Ic and is  such  that det(DkXi -NkNi)y^O. y2  =  Z2 Equivalently  all stabilizing ^2 can be described  by (ii) D2Z2  =  W2 (4.25) where  D2  =  XiDk  -  NiNj  and N2  =  -{Y\Dk  +  i^iA^^)  with Xu  ?i  Nu  Di  given  in (4.18)  [see  also  (4.20)]  and  the  parameters  Dk  and  A^^^ are  selected  arbitrarily  under the conditions  that D^^  exists  and is  stable and the pair  (Dk Nk)  is re  and is  such  that det(XiDk-NiNk)7^0. yi  =  N2Z2> Furthermore  the  closed-loop  eigenvalues  are  precisely  the  roots  of  detbk  or  of detDk.  In addition the transfer  function  matrix of 5*2 is given by -0kX-NkN)-\DkY+Nkb) H2  = -  NiNk)-\ =  -{YDk  +  DiNk)(XDk (4.26) Proof  The  closed-loop  description  in  case  (i) is  given  by  (4.11)  in  method  (4)  above and  in  case  (ii) it is  given  by  (4.10) in method  (3). As  was  shown  in  Subsection  7.2E (4.19)  and  (4.20)  are  parameterizations  of  all  solutions  of  the  Diophantine  Equa tions  b2Di  -  N2N1  =  Dk  [in  (4.10)]  and  D1D2  -  N1N2  =  Dk  [in  (4.11)]  respec tively.  The  fact  that  5^^  (or  D^^)  exists  and  is  stable  guarantees  that  the  closed loop  is  well  defined  and  all  of  its  eigenvalues  which  are  the  poles  of  D^^  (or  of D^i)  will  be  stable.  The  condition  det(bkXi -  NkNi)  ¥^ 0  [or det{XiDk  -  NiNk) ¥^ 0]  guarantees  that  detb2  7^ 0  [or  detD2  ¥^ 0]  and  therefore  the  PMD  for  ^2  in is  Ic  if  and  only  if  the (4.10)  is  well  defined.  Finally  note  that  the  pair  0kNk) pair  02  N2)  is  Ic  as  can  be  seen  from  02  -N2]  =  0kNkW given  in  (4.19) where  U  is  unimodular.  This  then  implies  that  the  description  02  N2> 1}  for  S2  is both  controllable  and  observable.  Similarly  the  pair  (DkNk) is  re  guarantees  that {D21 N2} with D2 and N2 given in (4.20) is also a controllable and observable descrip tion for 5*2. • 2.  Parameter  K.  In place  of  the  polynomial  matrix  parameters  Dk  N^  or Z)^ Nk  it is possible  to use  a  single parameter  a  stable  rational  matrix  K.  This  is  shown next  in  Theorem  4.2. THEOREM  4.2.  Assume  that  the  system  Si  is  controllable  and  observable  and  is de scribed by its transfer  function  matrix Hi  =  NiD^^  =  b^^Ni (4.27) where  the  pairs  (A^i Z)i) 0i  Ni)  are  doubly  coprime  factorizations  satisfying  (4.18). Then  all the  controllable  and  observable  systems  S2 with  the  property  that  the  closed-loop feedback  system eigenvalues  are stable (i.e. they have strictly negative real parts) are described by the transfer  function  matrix 595 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems H2 = -(Xi  -  KNir\Y  + KDi) =  -(Fi  + DK)(Xi -  NiKr\ (4.28) where  the parameter  K is an arbitrary  rational  matrix  that  is stable  and is such  that det{X\  -  KN\) #  0 or det(Xi  — NiK) 7^ 0. Furthermore  the poles  of ^  are precisely the closed-loop  eigenvalues. Proof  This is in fact a corollary to Theorem 4.1. It is called a theorem here since it was historically one of the first results established in this area. The parameter K is called the Youla parameter  (see Section 7.6). In Theorem 4.1 descriptions for H2 were given in (4.26) in terms of the parameters Dk Nk and Dk Nk. Now in view of -DuNk  + N^Dk  = 0 given in (4.23) we have D-'Nk  = NkDl'  =  K (4.29) a  stable  rational  matrix.  Since  the pair  (D^ N^) is Ic and the pair  {Nk Dk) is re they are  coprime  factorizations  for K.  Therefore  H2 in (4.28)  can be written  as the H2  of (4.26) given in the previous theorem from which the controllable and observable internal descriptions for S2 in (4.24) and (4.25) can immediately  be derived. Conversely  (4.28) can immediately  be derived  from  (4.26) using  (4.29). Note that the poles of K are the roots of det Dk or det Dk that are the closed-loop eigenvalues. • EXAMPLE  4.1.  Consider//i  = . Here A^i  = A^i =  5 + 1 and Di  = Di = 5 - 1. These are doubly coprime factorizations  (a trivial case) since (4.18) is satisfied. We have s+  1 s-  1 uu-Xi -Ni s + -{s  +1) Di -Yi Xi -s  + s -\ s -  1  -{-s  +  |) s+l s+  \ In view of (4.26) and (4.28) all stabilizing controllers H2 are then given by H2 {-s+l)dk^{s-\)nk {s + \)dk  -{s+  \)nk _  _{-s+l) + {s-l)K (5 + i) -  (5 + 1)K  ' where K = rik/dk any stable rational  function. EXAMPLE 4.2.  Consider//i(5")  = ' [10] -(s  +  1) 1 =  NiD- 1  _ 1 -1 [1 s+l]  = Di Wi which are coprime polynomial MFDs. Relation (4.18) is given by uu-^  =  Xi -Ni 5i D - Fi Xi 1 s+  1 -s^  + l '2 -(S  + 1) -(s  +  1) s'^ + s +  1 0 1 -1 ~(s+l) 0 0 :1 All stabilizing controllers may then be determined by applying  (4.26) or (4.28). 596 Linear Systems We  shall  now  express  the  transfer  functions  between yi J2. or U2 and m / 2_ terms of the parameters D^ N^  or D/. N^  or  K. Recall that  [see (3.103) and  (3.104)] \y\ Ui U2 = = (I  - HIHJT' (I-H2Hi)-'H2Hi (I-HxH2r'HiH2 (I-H2Hi)-'H2 Hid -  H2Hir' H2(I HiH2r' and (4.30) (4.31) It is not difficult  to see in view of (4.26) and (4.28) that ( / - i ? i / / 2 ) "'  =D2D^^Di  = (XiDk  -  NiNk)Dl^Di  =  (Xi  -  NiK)Du and  (/  -  //z^/i)"^  =  £>i-D^'^2  = DiiXi  -  KNi).  Also (/  -  HxH2)-^Hi  =  D2Dl^Nx  = Dxbl\bkXx-NkNx) (XiDk  -  NxNk)D7'Nx -  H2Hi)-^  =  NiDl^D2  = (Xi  -  NiK)Nx  =  Hiil -  NkNx)  =  NxiXx  -  KNx)  and  (/  -  H2Hx)-^H2  =  Z^ID^'JVJ  = NxDl\DkXi -DxiYx  + KDx)  =  H2(I  -  HxH2)-'  =  N2Dl'Dx  = -DxDl'{DkYx+NkDx) = -{YxDu  +  DxNk)Dl'Dx  = - ( fi  +  DxK)bx.  Note  that  (/  -  HiH2)-^HxH2  = Hx{I  -  H2Hxr^H2  =  Nxb +  A^^^i)  =  -A^i(Fi  +  KDx). To express  (/  -  HxH2)~^HxH2  in  terms  of  D^ N^  we  write  it  as  D2D^^NxH2  = -{XxDk-NxNk)Dl'Nx{YxDk + DxK){Xi  —  NxK)~^  which  is  not  as  simple  as  the  expression  given  in  terms  of bkNk. + DxNu)0txDu-NxNkr^   'N2  =  -NxDl\DkYx =  -{Xx-NxK)Nx{Yx Similarly (/-772^i)-iH2Hi  =  H2{I-HxH2)-^Hx  =  A^2£>^'A^i  =  -(?i£>*+ in  terms  of = -DxiYx  + //2ffi)~'^2^i -DxDl^{DkYx+Nkbx)NxD:^^ - ( Fi  +  DxK)Nx.  To  express  (/  - DxNk)Dl'Nx DijV<;wewriteitasDiD^i7^2/^i  = KDx)NxDiK It is straightforward  to express the transfer  functions  (4.30) and (4.31) in  terms of the parameter ^  in view of A" =  N^D^^  =  D^'iV^. In particular we have = = J 2. U2 (Xi -NiK)Ni - (f  1 +  DxK)Nx -(Xx  -  NxK)NxH2 -{Yx  +  DxK)Dx (I-H2NxD^')-(Xx -NxK)Nx -{Yx {Xx-+  DxK)Dx -  NxK)Dx (4.32) (4.33) These  expressions  were  derived  from  the  previous  expressions  given  that  involve Dj^  and  N^.  The right  factorization  of H2 in  (4.28)  is to be  considered  in these  ex pressions.  Similarly ^ = U2 NxiXx -KNx) -DxiYx  +  KDx)NxD7 -NxiYx  +KDx) -DxiYx  +  KDx) DxiXx-KNx) NxiXx-KNx) -DxiYx  +  KDx) il  -  b^^NxH2)-^ (4.34) (4.35) which  were  derived  from  the  previous  expressions  involving  D^  and  N^.  The  left factorization  of  H2  in  (4.28)  is  to  be  considered  in  entry  (2  2)  of  the to 597 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems transfer  function.  These  expressions  for  the transfer  functions  in terms  of K  can of course be combined in order to use the most convenient  parameterizations. 3.  Parameters  Q\  and  Q2.  Recall  from  Subsection  7.3C  the relations  (3.103) and (3.104) given by = = = and I -H2 I -Hi (I-HiH2r'Hi -1  "0 til (I-H2Hr'H2H H2] 0  J  U2_ (I-HH2r'HH2] (I-H2Hr'H2 In U2_ J -1 -H2 I I -til (I-H2H1)-' Hi{I-H2Hir' H2{I-HiH2r' (I-HH2r' (4.36) (4.37) We shall now express the transfer  function  matrices in terms of some important  pa rameters. To this end  define and note that Qi  =  / / i (/  -  H2Hi)-'  =  (/  -  HH2)-'H Q2  ^  H2Q  -  HxH2)-'  =  (/  -  H2H)-'H2 Hi  =  Qxil  +  //2Q1)-'  =  (/  +  QiH2)-'Qi H2  =  & (/  +  HiQ2)-'  ={I  +  Q2H)-'Q2. (4.38) (4.39) (4.40) (4.41) It  is  not  difficult  to  see  that  QxH2  =  H1Q2  and  Q2H1  =  H2Q1  from  Q1H2  = (I  -  H\H2r^HxH2  =  Hi(I  -  H2Hi)-^H2  =  H1Q2 and  similarly  for  Q2HX.  Also note that (/  -  //1//2)"'  =  /  +  HiQ2  =  /  +  QiH2 (I  -  H2Hx)-^  =  /  +  H2Q1  =  /  +  22^1 (4.42) (4.43) from  which  by  postmultiplying  the  first  relation  by  Hi  and  the  second  by  H2  we obtain  Qi  =  (/  +  HiQ2)Hi  =  Hi(I  ^  Q2H1) and  62  =  (/  +  H2Qi)H2  =  H2(I  + Q1H2). In view of these relations it is now straightforward  to write yi [y2_ =  Q\ H2Q1 Q1H2 (I  +  H2Qi)H2 Ui U2 I  + H2Q1 (/  +  H2Qi)H2 Qi 1 +  Q1H2 (4.44) (4.45) Note that (4.44) and  (4.45) are useful  when H2 is given in which case they  depend only on the parameter g i.  In this case Hi  is given by (4.40). It is not difficult  to see that  Qi  is the transfer  function  matrix between  yi  or U2  and  ri.  Similarly \yi [h. - ^ -Ui ^ U2  — Hi(I  + Q2Hi)  H1Q2 QiHi Q2 I  +  Q2H1 Hi(I^Q2Hi) Q2 I  + H1Q2 (4.46) (4.47) 598 Linear Systems Expressions (4.46) and (4.47) are useful when H\  is given in which case the transfer functions  depend only on the parameter  Q2. In this case H2 is given by (4.41). Q2 is the transfer  function  matrix between  J2 or wi and r2. If  the  given  transfer  function  H2  or  Hi  is  proper  then  it  is  straightforward to  guarantee  properness  of  all  other  transfer  functions  by  appropriately  select ing  the  parameter  Qi  or  Q2.  In  particular  given  that  Hi  is  proper  let  a  proper Q2  be  such  that  (I  -\- HiQ2)~^  exists.  Then  in  view  of  Lemma  3.14  in  Sub section  7.3C  H2  =  Q2{J +  HiQ2)~^  given  in  (4.41)  is  proper  if  and  only  if det{I  +  (lim5^oo//i)(lim5^oo22))  ^  0.  If  this  is  true  then  all  the  transfer  func tions in (4.46) and (4.47) will be proper. This implies that if the given Hi  is  strictly proper then any proper Q2 will guarantee that the above transfer  functions  exist and are  proper.  If  Hi  is  not  strictly  proper  i.e.  (lim^^oo//i  7^ 0)  then  care  should  be taken in the selection of a proper Q2; if in this case it is possible to select Q2 strictly proper  then H2 will be  (strictly)  proper  for  any  g2- Similar results  are valid  when H2 is given and Qi  is  selected. In Theorem 4.3 we assume that Hi  is given and the parameter Q2 is to be chosen to guarantee internal  stability.  Such a situation  arises when Hi  represents  the given plant  and H2 is the  stabilizing  controller to be designed.  It should be noted that  (as was  shown  in  Subsection  7.3C)  if  the  given  system  ^i  is not  controllable  and  ob servable all its uncontrollable  and unobservable  eigenvalues  will appear as uncon trollable  from and unobservable  from or eigenvalues in the closed-loop system.  Therefore  for  stability  it is necessary  to assume that Si  is  stabilizable  and detectable.  Here  working  with  Hi  any  possible  uncontrollable  and  unobservable parts  of Si  are ignored.  Exactly  analogous  results  exist  for  the parameter  Qi  when H2 is given. Consider the feedback  system of Fig. 7.5 and assume that Si  is controllable and observable with transfer  function  matrix  Hi. THEOREM 4.3.  Given//i let H2 = G2(/ + //lG2)" (4.48) where the rational matrix Q2 is such that (/  + HiQ2)~^  exists. Then the eigenvalues of the closed-loop feedback  system will be stable (i.e. they will have negative real parts) if and only if Q2 is selected so that (i)  the poles of Hi(I  + Q2H1) H1Q2 Q2H1 and Q2 are stable or (ii)  the poles of Df^/  +  22^1 G2] are stable or (iii)  the poles of \  2^  1 b^ ^  are stable. Furthermore the eigenvalues of the closed-loop system are precisely the poles in (i) or (ii) or (iii). In addition if Hi is proper then H2 and the transfer matrices in (4.36) and (4.37) are proper if and only if Q2 is proper and det (I + (lim^^oo //^(lim^^oo Q2))  ^  0. Proof The expressions in (i) are precisely the entries of the transfer  matrix between and given in (4.46) the poles of which are the closed-loop eigenvalues (see Theorem 3.15 in Subsection 7.3C). The expressions in (ii) and (iii) come from Corollary 3.16 where it  was  shown that the poles in these expressions  are exactly  the closed-loop eigenvalues. The  statement  about properness  was proved  above just  before  the • theorem. 599 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Remarks 1.  It is not difficult  to see that this theorem characterizes all H2 that lead to a feed back  system  with  stable  eigenvalues. When  Hi  is proper  it  also  characterizes all proper  stabilizing  H2. 2.  The  eigenvalues  of the closed-loop  system  are the poles  of the expressions  in conditions  (i) or (ii) or (iii). These will be poles of  Q2 but also stable poles of H\  depending  on the choice for Q2. 3.  It is easy to select Q2 so that H2 in (4.48) is proper (when H\  is proper). Also it is not difficult  to select Q2 so that the stability conditions are satisfied.  However to conveniently  characterize  all Q2 that  satisfy  the stability  conditions  (i) (ii) or (iii) is not  straightforward  unless  special conditions  apply. This is the  case for example when Hi  is stable as in Corollary  4.4. 4.  For further  discussion of the relation between propemess and stability in a sys tem described by a PMD refer  to Chapter 6 of  [331 and also [9]. COROLLARY  4.4.  Given //i  assume  that  its poles  are  stable. Let  H2 be  given by (4.48)  where  the  rational  matrix  Q2 is  such  that  (/  +  HiQ2)~^ exists.  The  eigen values  of  the  closed-loop  feedback  system  will  be  stable  (i.e.  will  have  negative real  parts)  if  and  only  if  Q2 is  stable.  In  addition  if  Hi  is  proper  then  H2 and  all the transfer  matrices  in  (4.36)  and  (4.37)  are proper  if  and  only  if  Q2  is proper and det{I  + (lim_oo//i)(lim_oo Q2))  ^  0. Proof When the poles of H\  are stable (i.e. have negative real parts) then the condi tions (i) or (ii) or (iii) of Theorem 4.3 are true if and only if the poles of Q2 are stable.  • Corollary  4.4 greatly  simplifies  the conditions  on Q2. However  it is valid  only when Hi  has  stable poles. The result in this corollary  was pointed  out by Zames  in [38]  where  it was  used  to introduce  the  //°°-framework  of  optimal  control  system design. 4.  Parameters  Sn  and 821-  At this point it is of interest to introduce the matri ces and S12  = (I-  HiH2r\ 52i  = (I -  / / 2 ^ i ) '\ (4.49) which are of importance in control system design. They are the companion  sensitiv ity matrices  of  the feedback  loop.  Su  is  also  the transfer  function  between  U2  and r2  while  ^21 is  the  transfer  function  between  ui  and  ri.  Note  that  Q2  =  H2{I  -HiH2r^  =  H2S12 =  S21H2  and  Qi  =  Hi(I  -  i/2//i)"^  =  H1S21 =  SnHi  Also H1Q2  =  S12 -  / 22^1  =  ^21 -  /  and Q1H2 =  S12 -1  H2Q1  =  S21  -  L  Now con sider Theorem 4.3 and note that H2  =  Q2S 12 ^2"/22 (4.50) and  that  the conditions  (ii)  and  (iii)  can  be  written  as D.  ^[821 Q2] and  22 ^12 DI which must be  stable in order to have  stable closed-loop  eigenvalues. This  implies that a necessary condition for stable closed-loop eigenvalues is that all unstable poles of Hi  must appear as zeros of 821  and of 812. In  general  it is not possible  to parameterize  all  stabilizing  controllers  in  terms of only 812 or 821 but this can be accomplished under certain assumptions on Hi.  In particular we have the following  result. 600 Linear Systems COROLLARY  4.5.  Assume that detHx  7^  0. Let H2 = H[\Sn -  I)S^2  =  DdN{\Sn -  I)]Su' or H2 = 52-/(^21  -  I)H['  =  S2i'[(S2i  -  I)N{']Du (4-51) (4.52) where ^12 or ^21 are rational matrices such that S^2 ^^^ ^21^ exist. Then the eigenvalues of the closed-loop feedback  system will be stable if and only if (i)  S12 is such that the poles of H;\Sn-I) are stable D^' ^12 (ii)  ^21 is such that the poles of Z)f^ [521 (52]-/)//f^]  =  [Dl^S2u  D^\S2i  - I)N;^Di] are stable. Proof.  The proof of this result follows  directly from  Theorem 4.3. • It is clear  from  (i) of Corollary  4.5 that  all the unstable  zeros  of ^i  must  cancel with  zeros  of  5*12  —  /  which  is  the  transfer  function  between  yi  and  r2  [it  equals (/  -  H\H2)~^H\H2\.  Also  all the unstable poles of Hi  must  cancel  with  zeros ofSn. From  (ii) a similar  result  follows  for ^'21 -  /  which  is the transfer  function  between j2  and ri  [it equals  (/  -  / / 2 ^ i ) ~ ^ ^ 2 ^ i]  and for 821 [see Exercise  7.23(c)  and (d)]. 5.  Parameters  X2  and  X2'  Alternative  parameters  closely  related  to  Q2 may be  used  in Theorem  4.3. To this  end let X2  =  Di^Q2 and X2  =  Q2Di\ (4.53) COROLLARY  4.6.  Given//i let H2  =  DiX2(I  + NiX2r^  =  [(/ + X2Ni)D^^]-^X2 (4.54) where  the rational  matrix  X2 is such  that  (/  + A^iX2)~^  exists. The eigenvalues  of the closed-loop  feedback  system  will  be  stable  (i.e.  will  have  negative  real  parts)  if and only if X2 is selected  so that the poles of [iI  + X2N)D^\X2] (4.55) are stable. Furthermore these poles are precisely the eigenvalues of the closed-loop sys tem. In addition if Hi  is proper then H2 and all the transfer matrices in (4.36) and (4.37) are proper if and only if D1X2 is proper and det (I  +  (lim^-^oo Hi)(\ims^o  D1X2)) 7^ 0. Proof.  Assume that the poles in (ii) of Theorem 4.3 are stable. Then in view of Q2 = Z)iZ2Dfi[/  +  G2^bG2]  =  Dl^lI  + DiX2NiD;\DiX2] X2Ni)D^\X2]^\so has stable poles. Conversely if (4.55) has stable poles then D]^^[I + Q2H1 Q2]  also has (the same) stable poles where  Q2 was selected to be Q2 =  D1X2. The remainder of the corollary follows  easily in view of Theorem 4.3. =  [{I + Note  that X2 can be seen  to be the transfer  function  between  z\  and r2 since Q2 is  the transfer  function  between  ui  =  Dizi  and r2 and Q2  =  D1X2.  Also  N1X2  = H1Q2  =  (I  -  HiH2Y^H\H2 is  the transfer  function  matrix  between  y\  and  r2. It should  be  pointed  out  that  contrary  to  Corollary  4.4 the  result  in  Corollary  4.6 is valid  for Hi  unstable  as well  [7]. In a completely  analogous  manner  it can easily  be shown  that  the following  result  is  true. COROLLARY  4.7.  Given//i let H2  =  (1 + X2Ni)-^X2bi -  X2[b-\I  + NxX2)r\ (4.56) where  the  rational  matrix  X2  is  such  that  (/  +  X2N\Y^  exists.  The  eigenvalues  of  the closed-loop  feedback  system  will  be  stable  (i.e.  will  have  negative  real  parts)  if  and only if X2 is selected  so that the poles of X2 D^\l  +  NiX2). (4.57) are stable. Furthermore these poles are precisely the eigenvalues of the closed-loop sys tem. In addition if Hi  is proper then H2 and all the transfer matrices in (4.36) and (4.37) are proper if and only if ^2^1  is proper and det (/  + (lim^_>oo X2Di)(\ims^oo Hi))  7^ 0. 601 CHAPTER  7: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems EXAMPLE  4.3.  Consider  Hi  = s-h  1 s-  1 of  Example  4.1. In  view  of  Theorem  4.3  all stabilizing  controllers  H2  are  given  by  i^2  =  G2  1 + s+  1 where 1 1-H &^a is  stable.  If  Q2  =  n/d then —  stable  implies Id that  n {s -  \)n  and d is stable. Also 1 {s -  \)d  +  (5 -  \)n{s  + 1) d  + n{s+  1) ^ -  1 d{s-\) {s- \)d stable implies that d{\)  + n{\)2  =  0 and d is stable. Therefore  for  stability  Q2  =  nid  with d stable and  n  =  {s -  l)n  d{l)  +  n{\)l  =  0. In view of Corollary 4.6 all stabilizing controllers H2 are given by H2  =  (^ -1)[1 + ^2(^+1)]"^^2  where  1  +X2(S+  1)- 1 X2 is stable. If X2  =  n/d  then d is stable. and^(l) + /i(l)2  =  0 for internal stability. Note that ^2  =  n/d  =  D1X2  =  {s-l)X2  = (s -  l)nld. m 1 c2' is"- -^+  ^ c2 s-EXAMPLE  4.4.  Consider  Hi(5) - r [ l 5  +  l]  =  Dr^TVi whichis  an Ic  polynomial  MFD.  In  view  of  Theorem  4.3 all  stabilizing  controllers  H2  are  given by  H2  =  Q2(l  + HiQ2)~\  where  the  poles  of 5f  ^ are  stable.  If  Q2 22 1  +  ^^162  then  Q2D^^  stable implies that ni  =  s^fii  ^2  =  s^h2 and d is stable. Now (1  + un^f^-\ HiQ2)Di - s^ + rii + {s +  l)n2  1 ^  = 1 +  m  +  (^ +  l)/22 -^ stable imposes additional ^^ conditions on fii  and «2. EXAMPLE  4.5.  Consider  Hi  = lary  4.4  applies. All  stabilizing  controllers  H2 are given  by H2  =  ^2(1  +  ^ 1 6 2 )^  = n  ({s  +  \)d  + {s- d Furthermore  if  Q2 is  proper  with  lim^-^oo Q2(^)  ^  - 1 then  H2  will  be  proper.  Note 1 -.  This  system  is  stable  and  therefore  Corol 5 +  r Dllei n(s  +  1) -  where  Q2 {s+l)d  + (s+l)d \)n^~^ is  stable. {s-\)n n/d ) that in view  of Theorem  4.3 the  closed-loop  eigenvalues  will be  the poles  of (5 +  1) +  (5 -  \)n  n 'd\ -[(s+  l)d  + (s- (s+l)^d l)n(s+ {s+\)d l)nl 1 1 s+  1 Relations  among  parameters.  It  is  now  straightforward  to  derive  relations among the parameters  Q2 and  Qi;  X2 and X2; D^  N^  and D^  Nk' K\  and  also 512 and 521- In particular in view of (4.32) to (4.35) we have 602 Linear Systems (22  =  //2(/  -  HxH2r^  =  (/  -  H2Hiy^H2 =  N2D^^Di  =  Dibl^N2 =  -(YiDk  + DNk)Dl^bx =  - (f  1 +  DiK)bi  =  -£>i(Fi  +  KDi). =  ~Dxbl\bkYi +  NuDi) (4.58) Also Qi  =  Hid  -  H2H1)-'  =  (/  -  HH2r'Hi =  NiDl^b2  =  D2Dl^Ni =  Nibl\bkX =  NiiXi  -KNi)  =  (Xi  -  NiK)Ni. -  N„Ni)  =  (XiDk  -  NiNk)D-k^Ni (4.59) The parameters  Q2 are used when the system Si  is assumed to be given. We have K  =  -D-[\Q2  +  Yrb)bi' =  -D\\Q2 +  DY{)b\\ (4.60) Next we note that in view of (4.49) and (4.50) we have 5i2  =  /  +  HxQ2  =  /  +  Nxbl^N2  =  /  -  Nibl\bkYx + A^2^i) =  /  -  A^i(yi  +  Kbx) and 521  =/  +  Q2H1  =  1 + N2D^^Ni  =  I-  (YiDk  +  DxNk)Dl^bi =  /  -  (fi  +  DxK)bx. (4.61) (4.62) From  (4.53)  it now  follows  that  Q2  =  D1X2  =  X2D1 from  which  the  parameters X2 and X2 can be expressed  as X2  =  - ( 7 1+  Kbi) and X2  =  - ( ?i  +  DiK). (4.63) The expressions  of  H2  in terms of the parameters  are now  summarized: H2  =  -0kXi -  NkNi)-\bkYi + Nkbi)  =  - ( f  £ )  +  DiNk)iXiDk -  T^XN^Y' =  -(Xi  -  KNxr\Yx +  Kbx)  =  - ( Fi  +  £>ii^)(^i  - = (/ + Q2Hxr'Q2  = [DfH/ + e2Hi)]-uor'Q2] =  Qiii + HxQi)-^ =  [e25r>][(/ + i?ie2)5ri]-' =  52-/22  =  [D-x'S2xr\D-x'Q2\  =  Q2Sx^  =  [Q2b-x'}[Sx2b~x'T' =  [(/  +  X2Nx)Dx^r'X2  =  MbxHl + NiX2)r\ ^xKT^ (4.64) 6.  Parameters  Lx La and  Lx  L2.  Expressions (4.64) that describe H2  as a ratio of rational  matrices can also be derived  in an alternative way using a theorem  that we establish  next. This result  also provides  a link between  the development  of this subsection  and the descriptions of all stabilizing controllers using proper and  stable factorizations  given in Subsection  7.4C. Consider the feedback  system of Fig.  7.5 and assume that 5i  is controllable and observable. The following  constitutes one of the principal results of this  section. THEOREM 4.8.  Let the transfer  function  matrix of the system Si  be Hi  with Hi  = NiD^^iHi  =  D^^Ni)  being  a coprime polynomial  matrix  factorization.  If  H2  is the transfer function of ^2  then the eigenvalues of the  closed-loop feedback system are sta ble if and only if H2  can be written as //2  =  L^^U (H2  =  L1L2 ^) (4.65) 603 where the L2 L\  (Li L2) are stable rational matrices with detL2  #  0 {detLi  #  0) which satisfy L2D1 -LiNi  =  I 01L2  -NiLi  =  I). (4.66) Furthermore if [L2Li]  =  D-'[D2N2] DZ (4.67) CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems are coprime polynomial  matrix factorizations  then  the closed-loop  description  is  given by DkZ  =  02  N2] Dz  =  [Ni  bi] ' 72. In yi J 2. y\ z + 0 01 .-/  oj Pi. D2 N2_ z + 0 -/I .0  0  J In 1^2. (4.68) Proof  The proof of the parts in parentheses will not be shown since it follows the proof given  below  in  a completely  analogous  manner.  Let  [Li L2] be  stable  with  detL2  9^  0 and  satisfying  (4.65)  and  (4.66)  and  write  an  Ic polynomial  matrix  factorization  as  in (4.67).  Then  D2D1  -  N2N1  =  D^  where  D^^  exists  and  is  stable.  Furthermore  the pair  02  N2)  is  Ic  [since  any  eld  of  the  pair  (D2 A^2) would  be  an  Id  of  Dk]  and  ^2 ^ exists. Therefore the closed-loop system with//2  =  ^ 2 ^ ^!  "^ 62^/^2is well defined. Its internal description is given in (4.68)  [see also (4.11)] and the closed-loop  eigenvalues are the stable roots of  detbk. Now let H2  =  62^  N2 be an Ic polynomial factorization and note that the closed-loop system is given by (4.68) where Dk  =  D2D1 -  N2N.  Assume that D^^  is stable. Define [L2 Li]  =  bl^02  N2] and note the D^  and 02  A^2]  are Ic since the pair 02  N2) is  Ic Then H2  =  ^2 ^^1 where L2 and Li  are stable with L2D1 LiNi  =  /. In  view  of  the  above  theorem  all  stabilizing  H2  are  given  by  (4.65)  v^here  the stable  rational  matrices  L2 Li  (Li L2)  span  all  solutions  of  the  Diophantine  Equa tion  given  in  (4.66).  There  are  different  ways  of  parameterizing  all  stable  solutions L2 Li  (L2 Li)  of  (4.66)  with det  L2  7^ 0  (det  L2  ¥" 0) each  one leading  to a  different parameterization  of  all  stabilizing  H^.  In  fact  in  this  way  one  may  generate  the  pa-rameterizations  developed  above thus providing  alternative  proofs  for  those  results. This  is  shown  in  the  next  corollary. COROLLARY  4.9.  All Stabilizing H2 are given in the  following. (i) ^2  ~  ^2  ^1 {H2  =  L1L2') (4.69) where the L2 Li  (Li L2) are stable with detL2  7^ 0 (detL2  7^ 0) and L2D1 -  L\N\  =  / 0\L2  — N\Li  =  /). The closed-loop eigenvalues  are the poles of  [L2 ^i]  (L VLMJ (ii)  H2  =  -(Xi -  KNir'iYi +  KDO [H2 =  -(Yi  +  DiK)(Xi - NiKy'l (4.70) where i^ is any stable matrix such that J^r(Zi  -  i^A^i)  #  0[det(Xi-NiK) FiXiFi  satisfy  t/t/  -1  _  r  ^i [-Af [7  01 0 /. pi [iV - ^I Xlj Yx\ 61J ¥^  0].TheZi  where f/  is a unimodular 604 Linear  Systems matrix. The closed-loop eigenvalues  are the poles of [Xi  -  KNi  Yi  +  KDi] Xi  -  NiK or equivalently the poles of  K. (iii) H2  =  [D-\I  +  Q2H)r'[D-'Q2] =  (/  +  QiH^r'Qi where  Q2 is such that [D-\I  +  Q2H)D-'Q2] Qib'x' {I  +  HQ2)b- (4.71) (4.72) (4.73) is stable and det{I  +  Q2H1) 7^ 0 (det(I  + H1Q2) ^  0). The closed-loop eigenvalues  are the poles of (4.73). (iv) H2  =  [D^'S2i]-'[D^'Q2] (H2  = [Q2b^'][Si2b^']-') (4.74) where 5*21  (Su)  and  Q2 are such that [D^'S2uD^'Q2] Qib-' W VSnb-\ (4.75) is  stable  with  ^^^5*21 7^ 0  (detSu  ^  0)  and ^'21 closed-loop eigenvalues  are the poles of (4.75). Q2H2  =  HSn-HiQ2 =  /).  The (V)  H2  =  [(I  + X2Ni)D^'r'X2 (H2  =  X2[b-\I  +  NX2)r') (4.76) where X2 (X2) is such that [(I  +  X2Ni)D^\X2] X2 b^\l  +  NiX2)\ (4.77) is stable and det(I  +  ^ 2 ^)  7^ 0 (det(I  + N1X2) 7^  0). The closed-loop eigenvalues  are the poles of (4.77). Proof.  The  proof  is  straightforward  in  view  of  Theorem  4.8. Part  (i)  follows  directly from  Theorem 4.8. To show  (ii) note that  [Xi  -  KNi]Di  -  [-(Fi  +  KDiWi  =  I  for any  K  (compare  with  Theorem  4.2).  To  show  (iii)  note  that  [Z)f ^7  +  Q2Hi)]Di -[D^^Q2]Ni  =  /  for  any  Q2  (compare  with  Theorem  4.3).  To  show  (iv)  note  that [D^^S2i]Di  -  [D^^Q2]Ni  =  /  if  and  only  if  S21 -  Q2H1  =  1. To show  (v) note  that [(/  +  X2Ni)Dl^^]Di  -  [X2\Ni  =  I  for  any X2  (compare  with  Corollaries  4.6  and 4.7). What has not been  shown  yet is that  all stabilizing H2 can be expressed  by say  (4.70) in (ii). This can be accomplished  in a manner  analogous to the proof  of the theorem. In particular  any  stabilizing  H2  =  02^N2  where  the  pair  (D2iV2)  is  Ic  that  gives  rise to  a  closed-loop  description  (4.68)  implies  that  bl^[D2  -N2]  =  [Xi  -  KNi  - ( Fi  + KDi)]  =  [IK] Xi Yi] -Ni  D ij =  [/  K]U  where  U  is  a.  unimodular  matrix.  Therefore from  [/  K]  =  b}^^ 02  —N2W  ^ it follows  that a stable K can be determined  uniquely. Similarly in (iii) the relation D ' ^ ^ i  A^2] =  D^^[I  + Q2H1 Q2] =  D^^[I Q] I Hi determines  uniquely  a stable  Q2  =  Dibj^^N2'  The details  are left  to the reader.  These results were of course also shown in Theorems 4.2 and 4.3 using alternative approaches. Remarks In Theorem 4.8  and Corollary  4.9 H\  may or may not be proper. Also the  sta bilizing  H2 may  or may  not be proper. Thus the above results  characterize  all  sta bilizing H2 both proper  and not proper. It is frequently  desirable to restrict H\  and H2 to be proper  rational  matrices.  The  problem  of  interest  then  is  to determine  all proper  stabilizing  H2 given  a proper  Hi.  Note  that  this  problem  has  already  been addressed previously in this section using the parameter Q2 and it will be studied at length in Subsection  7.4C. We conclude  by  summarizing  the relations  among  the parameters  used  in  this subsection. Note that the relations for all parameters except Li L2 and L2 L\  were derived in (4.58) to (4.64). The relations to L2 Li  can easily be obtained in view of Corollary 4.9. These relations  are summarized  as: 605 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems L2 u and Also Xi-KNi =  D\\l  + Q2Hx)  = D^'S2i  =(I  +  X2Ni)D^' -{Y+KD) =  D-'Q2  =  X2. (4.78) L2 Xi  -  NyK  =  (1 + HiQ2)Di' -  SnDi'  =  D^^I  +  N1X2) and Li  =  -(Y^DK) =  Q2D-'  =X2. (4.79) EXAMPLE 4.6.  Consider//i  =  s+  1  In view of Corollary 4.9 all stabilizing control lers are given by H2 = L2^L\ where the L2 L\ are stable and satisfy L2D1 -L\N\  =  /. Now  in  view  of  (4.78)  and  (4.79) L2  =  (/  + X2Ni)D\^  and  Lx  = X2. All ap propriate  X2 however  were  characterized  in  Example  4.3  to  be  X2  =  hid  with d stable  and  d{l)  +  n{l)2  =  0.  Therefore  all  appropriate  L2 L\  are  given  by  L2 = d  +  h(s  +  I) ; d l)(i i.e./2(1)  =  -^d(l). =  -7 ^1  =  -: where d is stable and n is such that d + n{s + \)  = • d  ~ d s -  I ^1 d (s- 1   i ~. ~^ 1 . I  B.  State Feedback  and  State  Estimation State feedback  and  state estimation  are studied  in this  subsection  using PMFDs of systems.  Our  current  development  which  offers  additional  insight  parallels  that given  in  Chapter  4  where  state-space  descriptions  were  used.  The  study  of  state feedback  and  state  estimation  using  PMFDs  however  is  important  in  its  own right. An additional reason for  discussing  state feedback  and  state observers  at this point  is  to provide  the  necessary  background  needed  to  connect  the  parameteriza-tions  of  all  stabilizing  controllers  using  proper  and  stable  factorizations  with  the internal  descriptions  (PMDs  or PMFDs)  of  systems. This  is  accomplished  in  Sub section  7.4C. State  feedback State feedback  control laws are closely related to the state-space  representations of systems. Recall from  Chapter 4 that  given X =  Ax  +  Bu y  =  Cx  -^ Du (4.80) 606 Linear Systems with  A  G  /^"><^ B  G  /^"><^ C  G  iR^x^  and  D  G  7?^><^ the  hnear  state  feedback control law is defined  by u  =  Fx  +  Gr (4.81) where F  G /^^^^  and G  G i^'^x^  with  G nonsingular. Frequently  G  -  /.  Then  the closed-loop system description is given by i:  =  (A  +  BF)x  +  BGr y  =  {C  + DF)x  +  DGr (4.82) and the closed-loop eigenvalues are the zeros of det [ql  -  (A +  BF)].  In view of the relation ql  -A-BF BG - (C  +  £>7^)  DG ^/ -  ^  5l \ ^ -c [-F  G D\ ^ (4.83) it is not difficult  to  see that  any  geld  of  [ql  -  A B\  will be  an Id of  ^/  -  A -  BF for any F. This implies that for complete eigenvalue assignment ql  — A  and B must be left  coprime i.e. (A B) must be completely  controllable which is a well-known result (see Chapter 4). Also for stability (A 5) must be a stabilizable pair. Note that any geld of  \_ql  -  A 5]  will be an Id of  \ql  -  A-  BF  BG]  for  any F and  G.  Since here G is taken to be nonsingular  [ql  -  A B] and [ql -  A-  BF  BG] have the same geld i.e. the open- and closed-loop systems have precisely the same uncontrollable eigenvalues. When G is singular the closed-loop system may have additional uncon trollable modes (show this). Although F does not affect  the uncontrollable modes of the  system it may  alter its unobservable  modes. For example  in an  SISO  control lable and observable system it is possible to select F so that some of the closed-loop eigenvalues  are at the same location  as the finite zeros and therefore  they  become unobservable. This corresponds to pole/zero cancellations in the closed-loop  transfer function.  The  closed-loop  unobservable  eigenvalues  can  also be  studied  by  means of relation  (4.83) and the gcrd of the pair (ql  -  A-  BF-C -  DF) The  effects  of  state  feedback  control  laws  can  conveniently  be  studied  using polynomial matrix descriptions. In particular  assume that the state-space  represen tation  (4.80) is controllable  and in controller  form  [A^  Be Cc Dc)  (see Chapter  3). An equivalent PMFD is then given by Dc(q)Zc(t)  =  u(t) y(t)  =  Nc(q)Zc(t) (4.84) with Dciq)  G R[q]'^'''^  and A^^(^) G R[q]P'''^ where Be Dc 0 ^PA Dciq) -NAq) Im 0. ql  -  Ac -Cc Be] Del \Sciq) [  0 (4.85) with the pair (B^  ql  -  Ac) being Ic and the pair (Ddq)  Sdq))  being re  [see (3.26)]. The matrix Sdq)  =  blockdiag[(1  q...  ^^'~^)^]  is an  n  X m matrix  with J/  /  = 1...  m the controllability indices of {A^ Be Ce Dc). Note that the above relations can be written as {ql  -  Ac)Se{q)  =  BcDc(q) Ndq)  =  CcSdq)  +  DcDdq) (4.86) which  were  derived  via  the  Structure  Theorem  in  Subsection  3.4D.  Note  that  the states are related by xdt)  =  Sc(q)Zc(t). When the state feedback  control law u(t)  =  FcXeit)  +  Gr{t)  =  FeSe(q)Zc(t)  +  Gr(t)  -  Fe(q)Zc(t)  +  Gr(t) (4.87) 607 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems is  applied  the  closed-loop  system  state-space  representation  is  {Ac  +  BcFc BcG Cc +  DcFc DcG} and the polynomial matrix description is Dp{q)Zc{t)  = Gr(t\ y(t)  =  NF(q)zM (4.88) where Dj.(^)  ^  Ddq)  -  Fdq)  = resentations  are equivalent  since :  Dciq)  -  FcSc(q) and Npiq)  =  Ndql  These rep-Be Dc 0 Ip\ \Dc(q)  -  FcSciq)  G 0_ [ -Nc(q) ql  -  Ac-  BcFc ^c J-^c^ c BcG] DCG\ \Sc(q) [  0 0 (4.89) where the pair (Be ql  — Ac  -  BcFc) is Ic and the pair (Ddq)  -  FcSdq)  Sdq))  is re (see Subsection 7.3 A). Note that (ql  -Ac-  BcFc)Sc(q)  =  Bc[Dc(q) -  FcSc(q)] and NF(q)  =  (Cc + DcFc)Sc(q) + Dc[Dc(q)-FcSc(q)]  =  CcSc(q) +DcDc(q)  =  Nc(q\ i.e. the numerator Nc(q)  is invariant under state feedback.  Note that Nc(q)  contains the zeros of the system (see Subsection  7.3B). Now  assume  that  the  state-space  representation  in  (4.80)  is  controllable  but not  necessarily  in  controller  form  and  let  A  =  Q~^AcQ  B  =  Q~^Bc  C  =  CcQ D  =  Dc  with  Q  a  similarity  transformation  matrix.  Relations  (4.86)  then  assume the  form (ql  -  A)S(q)  =  BDc(q) Nc(q)  =  CS(q)  +  DDc(q\ (4.90) where  S(q)  =  Q-^Sc(q).  Then Dc(q)Zc(t)  =  u(t\ y(t)  =  Nc(q)Zc(t) (4.91) is an equivalent polynomial matrix description  and now x(t)  =  S(q)zc(t).  Note that deg^. S(q)  =  deg^. Sc(q)  =  di i  =  1...  m which are the controllability indices of the system. The linear state feedback  control law is then given by u(t)  =  Fx(t)  +  Gr(t)  =  FS(q)zc(t)  +  Gr(t)  =  Fc(q)Zc(t)  +  Gr(tl (4.92) W e n o w h a v e ( ^ / - A - 5 F ) S ( ^)  -  B[Dc(q)-FS(q)\dindNc(q) D[Dc(q)  -  FS(q)]  =  CS(q)  +  DDc(q\ =  (C + DF)S(q)  + In view of the above it can be seen that linear state feedback can be equivalently defined  for the case of (controllable) polynomial matrix right fractional  descriptions as shown in the following.  Given D(q)z(t)  -  u(t) y(t)  =  N(q)z(tl (4.93) where D(q)  is column reduced define  the linear state-feedback  control law by u(t)  =  F(q)z(t)  +  Gr(t) (4.94) where  deg^. F(q)  <  deg^. D(q)  with  F(q) closed-loop system is then described by /^M^x^  G  E  7?^x^ detG  7^ 0.  The DF(q)z(t)  =  Gr(t\ y(t)  =  NF(q)z(t) (4.95) where  DF(q)  =  D(q)  -  F(q)  NF(q)  =  N(q).  The  F(q)  =  FcSc(q)  can be  chosen to  arbitrarily  assign  the polynomial  entries  of  D(q)  up  to  and  including  the  terms of  degrees  J/  -  1 in  the  /th  column  of  D(q)  i  ^  1...  m.  In  fact  recall  from the development in Chapter 3 (Theorem 4.10—the  Structure Theorem) that D(q)  — =  B^^[diag[q^q-F(q)  =  D(q)-FcSc(q)  =  B'^ldiagiq^q-AmSc(q)]-FcSc(q) 608 Linear Systems (Afn +  BmFc)Sc{q)]. It was shown  in Chapter  4 how to appropriately  select  Fc to arbitrarily  assign all the closed-loop eigenvalues i.e. the roots of det (D(q)  -  F{q)). Now  consider  the closed-loop  state-space  representation  (4.82).  As was dis cussed in Chapter 4 the closed-loop transfer  function  can be written as HFG{^)  =  [(C + DF)[sI  -  (A + BF)]-^B  + D]G =  [C(sl -  A)-^B  + D][F[sI  -  (A + BF)Y^B  + /]G =  H(s)He(s). (4.96) Here H(s) is the open-loop transfer  function  and He(s) represents the transfer  func tion of a system that if connected  in series  with the given  system  will  apparently produce the same overall transfer  function  as the feedback  system. Recall that  this issue  was addressed  at length  in Chapter  4. If the PMD (4.93)  is used  then the closed-loop transfer  function  is given by HFG(S)  =  NF(S)DF\S)G =  N(s)D^\s)G =  [N(s)D-\s)][D(s)D^\s)]G = H(s)He(s). (4.97) It is not difficult  to verify  that  the system  {Dpiq)  Im D{q) 0} is equivalent  to the system {A + BF  B F Im} both of which have the same transfer function He(s)  (show this). Relation  (4.97) also impHes that H(s)  =  N(s)D'\s) = HFG(S)H;\S) =  [N(s)D^\s)G][D(s)D^\s)G]-\ (4.98) Note that both HFG and He are proper and stable (H(s) is proper). Furthermore H~^ is also proper. Thus HFG  and He are proper and stable factors in the MFD H(s)  =  HFG(S)H;\S). (4.99) This  is further  discussed  in the next  subsection  where  it is shown  how all proper and stable right MFDs can be generated by means of state feedback.  The left  proper and stable MFDs can be generated from  observers of the partial  state. Observers of the state are examined  next. State  observers State observers were discussed at length in Chapter 4. Here we wish to present additional material concerning  observers in terms of PMDs. Consider the plant S and the observer  Sob of Fig. 7.6 and let the plant S be de scribed by (4.93) where D(q) G R[qr'''^  and N(q)  G Rlq^"^  As was discussed above when D(q) is column  reduced  the linear  state feedback  control law can be defined by u(t)  = F(q)z(t)  -h r(t) where deg^. F < deg^. Di  =  1...  m. Then the ^ J + u  1 *• o 1 y H  ^ob r ' ' w FIGURE 7.6 Plant and observer 609 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems closed-loop system is represented  by [D{q) -  F{q)Ut)  =  r(t\ y(t)  ==  N(q)z(t). (4.100) When the state is not readily available then a state observer for Fz  may be used. Let the observer Sob be described by Q(q)Zob(t)=^ [K(qlH(q)] u{t) W(0  =  Zob(t) (4.101) where  Q{q)  E  R[qT'''^.  K(q)  G  Riq^'"'^  and H(q)  G  R[qr''P.  Note that in Fig. 7.6 u(t)  == w(t) + r(t). Assume now that the observer polynomial matrices Q K and H satisfy  the relation K(q)D(q)  +  H(q)N(q)  =  Q(q)F(q) (4.102) Recall  that Q~^ where  Q'^  and  Q~^[K H]  are proper  and  stable  (F  G  Rlq^'"'^). proper is needed  for  Q(q)Zob(t)  =  0 to be a "well-formed"  set of differential  equa tions so as to avoid impulsive behavior at ^ =  0. Note that if Q is say row or column proper then  Q~^ is proper. The rational function  Q~^[K H]  is the transfer  function of the observer. Then  w  =  Zob =  Q~^[Ku  +  Hy]  =  Q~^[KD  +  HN]z  =  Fz  i.e. Zob is a candidate for estimating a function  F(q)z(t)  of the partial state z(t).  To show this consider the closed-loop internal  description D(q) -Im -Q(q)F(q)  Q(q) \  z(t) • [Zob(t). Um [0_ r(t) )0] y(t)  =  [N(qlO] _Zol (4.103) derived  by  using  u  =  Dz  =  w  +  r  =  Zob  -^ ^  and Qzob HN)z  =  QFz  and consider the unimodular  transformation Ku  + Hy  =  (KD  + Im [-F(q) z(t) Zob(t). 0 z(t) Zob  -  F(q)z(t) Z(t) e(t) . Then the closed-loop  system description  becomes D(q)-Fiq) 0 -I Q(q)\ Z(t) e(t) r(t) y(t)  =  [N(q\0] zit) e{t)_ (4.104) First  note  that  the  closed-loop  eigenvalues  are  the  roots  of  detiP -  F)detQ and  therefore  the  closed-loop  system  is  stable  if  and  only  if  all  the  roots  of  both detiP  -  F)  and  detQ  have  negative  real  parts.  The  roots  of  det{D  -  F)  are  of course the closed-loop eigenvalues  under  state feedback  when there is no  observer while  the  roots  of  det Q  are  the  observer  eigenvalues  that  are  taken  to  be  stable. Note  that  in  view  of  Q(q)e(t)  =  0  where  Q~^(q)  is  proper  and  stable  the  error ^(0  ="  ZobO)  -  F(q)z(t)  =  w(t)  -  F(q)z(t)  will go to zero as t goes to infinity  and therefore the output w(t) of the observer will asymptotically approach the function of the state F(q)z(t).  This will happen independently of r(t). Note that the roots of det Q are uncontrollable  from  r  as can  easily  be  seen using  e.g.  the  eigenvalue  test  for 610 Linear Systems controllability.  As  expected  these  uncontrollable  eigenvalues  cancel  in  the  closed-loop transfer  function  matrix given by ms)o] D(s)-F{s) 0 -1 --N{s)[D{s)-F{s)]-(4.105) In  other  words  after  the  transients  caused  by  initial  conditions  have  died  out  (see Chapter 4) the system behaves to the outside world as though  an observer were not present. The observer in (4.101) was introduced in Wolovich [36]. For  an  observer  (4.101)  to  exist  KH  and  Q  must  satisfy  the  Diophantine Equation  KD + HN  =  QF  given  in  (4.102)  where  QT^  and  Q~^[K^H]  are  proper and  stable  to  ensure  causality.  Note  that  if  Dp  =  D — F  then  F  =  D — Df  and KD + HN  =  Q{D-DF) or {Q-K)D  + {-H)N  =  QDp  which implies that [Q-\Q-K)][DD-p']  + [-Q-'H][ND-p']=L (4.106) The pair (g  ^{Q — K)^—Q  ^H) is a proper and stable solution of the equation XD  + YN  = /  where  D = DD^^  and N  = ND^^  are  proper  and  stable.  This  equation  is important in the parameterization  of  all  stabilizing  feedback  controllers  when  using the ring of proper and stable rational functions  discussed in the next  subsection. It is possible to implement the observer discussed above in an alternative manner. In particular u = w^r  = Q~^Ku^Q~^Hy  + r impUes that ( /-  Q~^K)u  = QT^Hy  + roxu  =  {I-Q-^K)-\Q-^Hy +  r)ox u =  {Q-K)-^Q{Q-^Hy + r). (4.107) This corresponds to the configuration  in Fig. 7.7. o {Q-K)-^Q Q-'H FIGURE 7.7 Observer-based controller implementation Note  that  the  feedback  path  controller  Q~^H  is  always  stable  while  the  con troller  in  the  feedforward  path  is  biproper  (i.e.  it  along  with  its  inverse  is  proper) but not necessarily  stable. If the external input r is of no interest then take r =  0 in which case we have  u={Q-K)-^Hy. (4.108) This corresponds to the configuration  depicted in Fig. 7.8. c i ^ J i " S y ff^ (f ^  — T\j IX\ -1  U r 7 FIGURE 7.8 Observer-based controller implementation when r --It is of interest  to compare these results  with  the corresponding  state-space  re sults in Chapter 4. In particular consider the state-space plant representation  (4.80) and assume that it is controllable and observable. Now comparing Fig. 7.7 with Fig. 4.7 of Chapter 4 we obtain in view of (4.25) and  (4.24) of Chapter 4 the relations (Q(s)-K(s))-'Q(s) + -  KD)  -h /  Q-\s)H{s)  =  Gy(s)  =  F[sl  -  (A -  KC)]-^K  and BF  -  KDF)r\B Q-\s)K(s)  =  Guis)  =  F[sI-(A-KC)]-\B-KDXA\so(Q(s)-K{s)r^H(s) = (/ -  Gu(s))~^Gy(s)  =  F[sl  -{A-KC-\-BF- (see Fig. 7.8). It is there fore  clear  that  the  two  degrees  of  freedom  controller  in  Fig.  4.7  of  Chapter  4  is  a special case (of order n) of the controller in Fig. 7.7. =  (I  -  Gu(s))-'  =  F[sI-{A-KC =  (I  -  Q-\s)K(s)r' KDF)T^K 611 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems EXAMPLE 4.7.  Consider ^(5*) given in Example 4.2 of Chapter 4. In view of the above it is not difficult  to see that the results developed in Example 4.2 can also be derived if we let 2(5)  =  s^ + d\s + d{)F{s) = (2-ai)s  + (2-ao)  = [^ao-l2-2] =  FS(sl  K(s) ai s(2 -  ai)  -  do(^ao -  1) and  H{s)  =  s{{do  -  2)(^ao  -di)(ao -  2) + (do  -  2)(s -  ai)X which satisfy (4.102). 1) + (di -  2)(2 -  ai)) + ((do • Verify this. Finally  it remains  to  be  shown  that  polynomial  matrices  K  H  and  Q  which satisfy  (4.102)  with  Q~^  and  g " ^ [ ^  / /]  proper  and  stable  exist.  Here  deg^.F  < deg^. D where D is assumed to be column reduced  and the N  D are assumed to be re. The system S is assumed to be controllable and observable. That such K H and Q exist will not be shown here. This is shown in Wolovich  [36] where an algorithm is given that is based on the Eliminant Matrix ofD  and N  (see Subsection 7.2E Theo rem 2.13 and Lemma 2.14) to select an appropriate row reduced Q and to determine K and H. Note that (4.102) can also be solved by using other methodologies such as polynomial matrix interpolation  (see the Appendix). C.  Stabilizing  Feedback  Controllers  Using Proper  and  Stable  MFDs Now  consider  systems  ^i  and  ^2  connected  in  a feedback  configuration  as  shown in Fig. 7.5. Let  system ^i  be controllable  and observable  and let it be described  by its  transfer  function  matrix  Hi.  In  Subsection  7.4A  all  systems  ^2  that  internally stabilize the closed-loop feedback  system were parametrically characterized. In that development  Hi  was  not  necessarily  proper  and  the  stabilizing  H2  as  well  as  the closed-loop  system transfer  function  were not necessarily  proper either. Recall  that a system is said to be internally  stable when all its eigenvalues which  are the roots of its characteristic polynomial have strictly negative real parts. Polynomial  matrix descriptions  that  can  easily  handle  the  case  of  nonproper  transfer  functions  were used to derive the results in Subsection 7.4A and the case of proper Hi  and H2 was handled by restricting the parameters used to characterize all stabilizing controllers. Here  we  concentrate  exclusively  on  the  case  of  proper  Hi  and  parametrically characterize  all proper H2 that internally  stabilize the closed-loop  system. For this proper and stable MFDs of Hi  and H2 are used. These are now  described. Consider  H(s)  G  R(sy^^ to  be  proper  i.e.  \ims-^ooH(s)  <  00 and  write  the MFD as H(s)  =  N'(s)D'(sy (4.109) 612 Linear Systems where the N'(s)  andD'(s)  are proper and stable rational matrices that we denote here as A^'(^)  ^  RH^^'^mdD'is)  G /?H^>^"'; that is they are matrices with elements in/?^oo the set of all proper and stable rational functions  with real coefficients.  For instance if  H(s) s-  1 (s -  2)(s  +  1) then  H(s) s-  1 (s +  2)(s  +  3) (s -  2)(s  +  1) (s +  2)(s  +  3) "  5-  1  1 _(S+  1)2 J Is-2' [s + l_ 1 are examples of proper and stable MFDs. A pair (N\  D')  G RHo is called  right coprime  (re)  in RHoo if there exists a pair (X'  Y')  G RHoo  such that X'D'  +  Y'N'  =  L (4.110) This is a Diophantine  Equation  over the ring of proper and stable rational  functions. It is also called a Bezout  identity. Let H  =  N'D'-^  and  write  (4.110)  as X'  +  Y'H  -  WK  Since the  left-hand side is proper  D'~^  is  also proper  i.e.  in the MFD  given  by  H  =  N'D'~^  where the pair (N'  D')  is re D'  is biproper  (D'  and D'~^  are both proper). Note that X'~^  where X'  satisfies  (4.110) does not necessarily  exist.  If  how ever H is strictly proper [lim^^oo H{s)  =  0] then lim^^oo X'{s)  =  lim5_>oo D'{sy^ a nonzero real matrix and in this case X'~^  exists and is proper i.e. in this case  X' is biproper. is When  the  Diophantine  Equation  (4.110)  is used  to  characterize  all  stabilizing controllers it is often  desirable to have solutions {X'  Y')  where X'  is biproper. This is always possible. Clearly  when H  is  strictly  proper  this is automatically  true  as was shown. When H is not strictly proper however care should be exercised in the selection of the solutions of (4.110). LEMMA 4.10.  luoXH  =  N[D'\^  =  A^2^'2^ be rc factorizations. Then U' (4.111) where  U'U' RHoo Proof Given the two re factorizations let  f/'  =  D\  D2 and note that A'^2 = ^^'2  ^ N[D\^D'2  = N[U'.NowX!^D'2 + Y!^N^  = (X!^D[  + Y!^N[)U'  = / from which f/'-^  = X^D;  +  r^A^j  i.e.  U'-^  E  RHoo. Similarly  X[D[  +  Y[N[  =  I U' E RHoo. implies  that m Remarks 1.  A  matrix  U\  as  given  above  with  U'  and  f/'~^  G  RHoo  is  a unit  in  the  ring RHoo  (refer  to the discussion on rings and modules in Subsection  7.2E). 2.  If//  is also  stable i.e. if  H  E  RHoo  then H  =  HI~^  is an re factorization.  If now  H  =  N'D'~^  in  any  re proper  and  stable  MFD  then  in  view  of  Lemma 4.11 /  -  D'U\  i.e. D'  and D'-^  G  RHoo. EXAMPLE  4.8.  Let  7/ 1 1 (s-2)(s-\-  1)  Us+  1)2/U+  1 =  A^'D'-^HereA^' and D'  are re since X'D'  +  Y'N'  =  5  +  1 /\5  +  1 + (9) s-  1 (S +  1)2 1. In view of 613 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Lemma 4.10 all U' •• s-1 s-\ t/'  where t/'t/'-i  G/?//oo are also re factorizations  of//.  Here  U\s)  = a{s)/b{s)  where a{s) and b{s) are prime Hurwitz polynomials of the same degree n and n can be arbitrarily large but • finite. The above example illustrates that when N' ^D' ^  RHoo  dire re in RHoo  they  may have common factors  that include common poles and zeros; however these possible common poles and zeros have to be stable. Note that any common right divisor G^ of an re pair N^D^ G RHoo  must  satisfy  G^ G^~^  G RHoo.  Contrast this with the case of two re polynomial matrices. Analogous  results  hold  for  Ic  proper  and  stable  MFDs  of  H  = b'~^N'  with b'^N'  ^  RHoo where the Diophantine  Equation b'X'+N'f = 1 (4.112) is satisfied for some X F G RHoo. In this case the result corresponding to Lemma 4.10 becomes  [52^/^1 Ic MFDs. U'[b\N[]  with tJ' tJ'-^  GRHoo whenH - -&-^ Ni=b'-^Nl are As in  the polynomial  case  doubly  coprime  factorizations  in RHoo  of  a  transfer function  matrix  Hi  = N[D'-^  = b'-^N[  where  D[N[  G RHoo  and  b[N[  G RHoo are important in obtaining  parametric  characterizations  of all stabilizing  controllers. Assume therefore  that u'u'-X[  Y[  D\ N[ I 0 0 / (4.113) -n XI where U^ is unimodular in RHoo i.Q.U'^U'~^ G RHoo. Also assume that X[^X[  have been selected  so that det X[ ^  0 and det X[ ^  0. To see how such relations can be derived [compare with the discussion  following (4.18) in Subsection  7.4A] assume that some X'^^Y^  and Y^^X'^ have been found  that satisfy  X^D;  XN[ : /  and b[X'^  -N[n- I. Note then that 0 / D[ D'S'-?; N'  N'Sl  -K -N[  D' I 0 (4.114) whereS'  4  x'X-YX- Fj)  to obtain (4.113). It can be shown that matrices are indeed  unimodular. Let  {X[Y[)  =  {X'X)  and (Z(-?/)  =  {NS'.+X'.^iys'  -Internal  stability Consider now the feedback  system in Fig. 7.5  and let Hi  and H2 be the  transfer function  matrices of Si  and ^2 respectively that are assumed to be controllable  and observable.  Internal  stability  of  a  system  can  be  defined  in  a  variety  of  equivalent ways in terms of the internal description  of the  system. For example in this  chapter polynomial matrix internal descriptions were used and the system was considered as internally  stable when its eigenvalues were stable i.e. they had strictly negative real parts. In Theorem 3.15 in Subsection 7.3C it is shown that the closed-loop  feedback system is internally stable if and only if the  transfer  function  between and 614 Linear  Systems or and n have  stable  poles i.e. if and only  if the poles of / -Hi -H2 I I or -1-1 / -Hi 0 Hi Hi 0 I  respectively  are stable. In  this  subsection  we  shall  regard  the feedback  system  to  be  internally  stable when / -Hi 1-1 -H2 I ^RH^ (4.115) i.e.  when  all the transfer  function  matrices  in  (4.115)  are proper  and stable.  In  this way internal  stability can be checked without necessarily  involving internal  descrip tions of 5i  and ^2. This  approach  to stability has advantages  since it can be  extended to  systems  other  than  linear  time-invariant  systems. THEOREM  4.11.  Leti/i  =  A^;Z)V^  =  D'i^N[  mdH2  =  D'^^N^  =  N^D'^^  be dou bly coprime MFDs in RHoo. Then the closed-loop feedback  system is internally  stable if and only if or if and only if D ^ D;  -N2N[  =  U' D[D2-N{Ni =  U' (4.116) (4.117) where  U' U'~^ G RHa. and U' U''^  G RHo.. Proof  Consider//i  =  N[D'^\H2 have/  =  D'2^N2N[D\^  +  6 2 ^ ^ '^ T^  which implies  that =  D'2 ^^2 ^nd assume that (4.116) is satisfied. We / -Hi -H2 I / -Hi -H2 I and D'2U' -D'^'N^ -N[D'-' D'l u'-^b'2^ U'-^D'2^N': 0 (4.118) which is proper and stable since both factors in the right-hand side are proper and stable. Therefore  if  (4.116)  is  satisfied  the closed-loop  feedback  system  is internally  stable. Similarly it can be shown that if Hi  =  b'\^N[  H2  =  Nl^D'^^  and (4.117) is  satisfied then the closed-loop feedback  system is internally  stable. The  converse  will  now be  established  namely  if  the feedback  system  is  inter nally  stable  then  (4.116)  is satisfied.  The proof  that  (4.117)  is also  true  is  completely analogous.  Let Hi  =  N[D']^^  be re  and H2  =  ^^'2 ^A^2 ^^ ^^ MFDs  in  RHo. and let D ^ D;  -  N^N[  =  U' with  U' some matrix in RH^. Recalling  (3.103) or (4.37) we have I -Hi '-' -H2 I r  (I-H2H1)-' ~ [Hi(I-H2Hir I  + {I'H2Hi)-'H2 Hi(I-HiH2r^H: 2j (4.119) where the identities (/-^1/^2)"^  =  I + (I ~ HiH2)-^HiH2and(I -  HiH2r^HiH2  = Hi{I  -  H2Hi)~^H2  were used. It is not difficult  to see [compare also with (3.113)] that / -Hi -1 -H2 I = 'D[U' -'D'2 N[U'-^D'2 "0  0" 0 /.  + [N[\ D[U"^N^ I  +  N[U'-^N2. U'-'[D'2N^l (4.120) 615 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Assume now that the system is internally  stable i.e. / -H2 I e  RHoo.  Then since  D[ N[  are  re  and  D2 N2  are  Ic  (7  Ms  in  RHo. To  see  this  premultiply U'~^[D2N2l  which  is  in  RHa. by  [D^-A^^]  G RH^  and  postmultiply by  I -N: e  RHo..  These  operations leave  the  matrix  in  RHoo. Therefore RH^ COROLLARY 4.12.  Let//i  = N{D'l^  =  D'f^TVJ  be doubly coprime MFDs in/?//oo i.e. (4.113) is satisfied. Then the closed-loop feedback system is internally stable if and only if H2 has an Ic MFD in RHa H2 = D'2^N'2 such that or if and only if H2 has an re MFD in RHoo H2 =  A/^2^'2 ^' ^^^^ that D'2D[  -N'2N[  = / D[D'2-N[N2  = I. (4.121) (4.122) Proof. The proof is straightforward in view of Theorem 4.11 and Lemma 4.10. • Parameterizations of all stabilizing  controllers Several  parameterizations  of  all  proper  stabilizing  controllers  are  now con-sidered. Parameter  K' THEOREM 4.13.  Let//i  = N[D'\^  =  D'f ^A^j be doubly coprime MFDs in/?//oo that satisfy (4.113). Then all H2 that internally stabilize the closed-loop feedback system are given by H2 = -(X[  -  K'N[)~\Y[  + K'D[) =  - ( ?; + D[K')(X[ -  N[K')-\ (4.123) where K' G RHoo is such that {X[  -  K'N[)~^  [or (Xi -  N[K')~^] exists and is proper. Proof. It can be shown that all solutions of 62^1 ~ ^2A^i  =  I are given by [D'2  -N^]  U.K'] x[  rr -N[  D[\ (4.124) where K' E RHoo.  The proof of this result is similar to the proof of the corresponding result for the polynomial matrix Diophantine Equation in Subsection 7.2E and is omitted (see also Subsection 7.4A). Similarly all solutions of D[D2 -  N[N2 =  /  are given by (4.125) where K' E RHoo. The result then follows directly from Corollary 4.12. The above theorem is a generalization of the Youla parameterization of Theorem 4.2 over the ring of proper and stable rational functions. Generalizations of the Youla parameterization over rings other than the polynomial ring were introduced in Desoer et al. [11]; for a detailed treatment see also Vidyasagar [34]. It is interesting to note that in view^ of (4.113) H2 in (4.123) can be written as follows. Assume that X^^  and Xf^  exist. Then 616 Linear  Systems H2  =  -(Y[  +  X'^\l -  Y[N[)K')(Xi -  N[K')-^ =  -[Y[X\\X[ -  N[K')  +  X\^K'](Xi -  N[KT^ =  -Y[X\' -  X'i'K'iXi -  N{K'r' =  H20 +  H2a (4.126) i.e.  any  stabilizing  controller  H2  can  be  viewed  as  the  sum  of  an  initial  stabiliz ing  controller  H20  =  -Y[X'^^ and an additional  controller  H2a that  depends  on  ^ '. When  K'  =  0 then  H2a is  zero. In  view  of  (4.120)  the  poles  of / -Hi 1-1 -H2\ I   which  are  the  closed-loop eigenvalues  are the poles of / H2 I D[ -1 U'~\D2  N2I  Similarly  since I  + N^U = I  0 0  0 + U'-\N[D[l (4.127) where Hi  =  D ' f ' ^i  i^ ^^ ^"'l ^2  =  A^2^'2  '  is re and both  are MFDs  in RHo. with U'  and  U'  f/'"'  G  /?//„  it  follows  that  the  eigenvalues  of  the D[D'^  -  N[NL closed-loop  feedback  system  are the poles of forward  to prove  the following  result. N'2 D' U'~^[N[  D[].  It is now  straight-LEMMA4.15.  Given//i  = N[D\^  =  5 'f  ^ ^ j  doubly  coprime MFDs in 7?//oo if ^2 is given by (4.123) the closed-loop eigenvalues  are the poles of [/ K'] x:  Y[ -N[  D[\[0 0 -I [X[ -  Y[]  -K'[N[D[] (4.128) or of -K' I [N[D[] [N[D[]-x: K'[N[D[l (4.129) Proof.  Note  that D'2D[-N^N[  =  (X[-K'N[)D[+(Y[+K'D[)Ni I  =  U' which in view of the above discussion directly  implies the lemma. =  X[D[+Y[Ni  = • Therefore  in  view  of  Lemma  4.15  when  the parameterization  for H2  of  The m   of orem  4.13 is  used  the closed-loop  eigenvalues  are in  general  the poles  of [-N[  D[   andof/i:'. EXAMPLE  4.9.  Let Hi  = 1 fs-l\-i 1 i-l-  lVs+  1 NID'J' = I r'  1 s + a/ D\  N[ with a  >  0 which are doubly coprime factorizations. Note that X'l  Y[ -N[  D[  N'l -Y[ x: s-\-3 s +  2 1 s -\- a ^ + 5-1 ^ + 2 ^-  1 s -\-  a-^ s-  1 s+  1 1 s+  1 (s + 5)(s + a) (s +  1)(^ + 2) (s +  3)(^ +  a) (s +  l)(s  + 2) s + a 1  0 0  1 If all stabilizing  H2 are parametrically  characterized  by means of (4.123) then in view of Lemma 4.15 the closed-loop eigenvalues  are in general the poles of that are at -1 the poles of X[  Y[ -N[  D[ that are at -2  and -a  and the poles of K'.  Also H2 in this 617 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems case is given by H2  = where K'  G  RHoo. .  +  5 ^ ^  . -l s_±2 s + 3 s + 2 K'\ 1 s + a s + al  _ 1 1^' (s +  5)(s  +  1) (5 +  1)(^ +  2)  V^  +  1 {s +  3)(5 +  a) {s +  1)(5 +  2)  V5  + ITK Note  that  it  is  possible  to  select  -m 1 ^ iJ so  that  the  poles  of  X[  and  Y[  are those of  -  A^{ and D[.  In the above example this is the case when  a  =  2. This  is  also the  case  when  these  quantities  are  expressed  in  terms  of  a  state-space  realization  of Hi  as  is  shown  later  in  this  subsection. It  is  always  possible  of  course  to  select  K'  so  as  to  minimize  the  number  of the  closed-loop  eigenvalues.  This  corresponds  to  minimizing  the  McMillan  degree (number  of  poles)  of//2-  This  follows  from  the  fact  that  any  proper  stabilizing  con troller  can  be  expressed  as  (4.123)  for  appropriate  K'.  If  for  example  H\  can  be stabilized  by  means  of  a  real  static  H2  then  a  ^'  E  RHo  exists  for  this  to  happen. In  this  case  the  number  of  closed-loop  eigenvalues  is  equal  to  the  number  of  poles of  Hi.  It  is  not  easy  however  to  find  such  K'  unless  for  example  X[  =  I  and  Y[  is real  in  which  case  ^'  =  0 a n d H2  =  ^ j-In  view  of  Lemma  4.15  it  is  recommended  that  the  number  of  poles  in  ^1 and  in  [-A^{ D['\  be  taken  to  be  the  minimum  possible  which  is  the  number  of poles  in Hi.  Also  the  number  of poles  in  [X[  Y[]  should  be  taken  to be  low.  This  is accomplished  in  a  systematic  way  in  the  following  using  state-space  descriptions. If  the  desired  stabilizing  H2  is  known  then  the  appropriate  K'  that  will  modify yield  the  desired  H2  can  be  calculated  from  (4.122)  as the initial  //20  ^  X''[^Y[io K'  = -{Y[+X[H2){D[-N[H2r\ (4.130) EXAMPLE  4.10.  In the  above example 7/2  = stabilizing H2. Then for a  =  1 we have -(/? +  1) Z?  >  0 characterizes  all  static K'  = s+5 s + 2 5 +3 s + 2 ( ^ + 1) -bs-3/7 +  2 8 + 2 s + b s+  1 5 -1 5+  1 1 b+l (s +  l)(bs  +  3/7-2) ' (s +  2)(s  +  b) which will yield the desired H2  = at  -b  as can easily be  verified. -{b  -\-  1). The closed-loop eigenvalue is in this case Parameters  Q2 X'2 Parameters  other  than  K'  can  also  be  used  to  parametrically  characterize  all stabilizing  H2.  These  parameters  were  introduced  in  Subsection  7.4A  and  in  the following  these  results  are  modified  to  accommodate  the  MFDs  in RHoo. THEOREM  4.16.  Let  Hi  =  N[D\  =  D':[^N[  be  doubly  coprime  MFDs  in  RHoo  that satisfy  (4.113). Then all H2 that internally  stabihze the closed-loop system are given by (i) 7/2  =  (/  +  Q2Hi)-'Q2  =  [D'-\I  +  Q2Hi)r'[D'-'Q2-\ =  [{I +  X'2N'i)D'-']-'X'^ (4.131) 618 Linear  Systems where Q2 is such that U\^\l  + 22^1  G2] ^  ^^00 and (/ + QiHiY^  exists and is proper; or where Z2 is such that  [(/  + X'^N'^~^U\^  X'^  E  RHo.  and (/  +  X'jN'^~^  exists and is proper. Or by (ii) //2  =  Qiil^H^QiY' =  [Q2D'-'W  +  HQ2)D'-'r X'2[D'-\l  +  N[X'2)r' (4.132) where Q2 is such that where X2 is such that Qi 1  + H1Q2 X' VD'-\I  + N[X'2) D'l^  E  RHoo and (/  + Hi ^2)"^  exists and is proper; or E  RHo and {I  + N[X'2)~^ exists and is proper. Proof  First notice the similarity of the results in this theorem and in Theorem 4.3  and Corollary  4.6  in  Subsection  7.4A.  To verify  (i)  directly  note  that  if  H2  =  ^'2  ^^2'  ^^ solutions of b2D[  -  A^2^|  =  /  are given by [b'2 N!2\ =  [(/  +  X!2N[)D'i\  X^]  E  RH^ (4.133) where X2  E  RHoo is a parameter that we set equal to N2- Then in view of Corollary 4.12 the result in (i) that involves X2 follows  directly. Notice that (/  + X2N[)D']^^ is biproper and therefore H2 in (4.131) is proper. Now if Q2  =  D[X2 then the results involving Q2 also follow. Note that Q2 E  RHo. Part (ii) can be verified  in an analogous manner.  Here Qi  =  X'2D[. • In  view  of  Lemma  4.15  and  the  discussion  preceding  it  the  next  result  follows readily. LEMMA  4.17.  Let  Hi  =  N[D'\^  =  D'l^N[  be  doubly  coprime.  If  H2  is  given  by (4.131) then the closed-loop eigenvalues  are the poles of [D'i\l  +  Q2Hi\D'i'Q2] U  + QiHu  Qi] [(I  + X^N[)D'i\X^l (4.134) If ^2  is given by (4.132) then the closed-loop eigenvalues  are the poles of Qib'^' [Nib[]  = Qi 1 + H1Q2 [Hi I] X'2 D'-\I-VN[X'2). [N[b[l (4.135) Proof  The proof  of this result is  straightforward  in view  of Lemma  4.15  and the dis cussion preceding it. • An  interesting  case  is  when  Hi  is  stable  as  the  following  corollary  shows. COROLLARY  4.18.  Let  ^1  E  RHo.. Then  all H2 that  internally  stabilize  the  closed-loop feedback  system are given by H2  =  (I  -  K'HiY^K'  =  K'(I  -  HiK')-\ (4.136) where K'  E  RH^  such that (/  -  K'Hi)  ^ [or (/  -  H^K')  ^] exists and is proper. Further more the closed-loop eigenvalues  are the poles of \[IK'] 0 I -Hi (4.137) 619 CHAPTER 7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Proof Note that in this case (4.113) can be written as / -H 01 l\ \I [HX 0' /. I  0' 0 /. for the doubly coprime MFD/fi  =  Hil'^  = /-^//i. Then (4.123) of Theorem 4.13 reduces to expression (4.136) for H2. The closed-loop eigenvalues are then given by the poles of (4.137) in view of Lemma 4.15. • Compare  this  corollary  with  Theorem  4.16 where H2 is expressed  in terms of Q2. In this case it is clear that K'  =  -Q2.  Note that K'  E  RH00 suffices  to guarantee stability. MFDs and internal  representations Consider^  =  N'D'~^  =  D'"^A^' a doubly coprime factorization  in i^ii/oo i.e. (4.113) is satisfied.  It is possible to express all proper and stable matrices in (4.113) in terms of the matrices  of a state-space  realization  of the transfer  function  matrix H{s).  In particular we have the following  result. LEMMA 4.19. Let {A 5 CyD}  be a stabilizable  and detectable  realization  of H{s) i.e. H{s)  =  C{sl -  A)-^B + D which is also denoted by H(s) = and with C  D (A B) stabilizable and (A C) detectable. Let F be a state feedback gain matrix such that all the eigenvalues of A -f- BF have negative real parts and let K be an observer gain matrix such that all the eigenvalues of A -  A'C have negative real parts. Define U' X' y -N'  b' and U' = D' N' -r X' A-KC B-  KD  K -F -C -D A + BF B  K F C + DF / D (4.138) (4.139) / - I/ Then (4.113) holds and// =  N'D'-'^  = /)'"iA^'are coprime factorizations  of/^. Proof. Relation  (4.113) can be shown to be true by direct computation  and is left to the reader to verify.  Clearly  U\ U' G RHoo.  That A^' D'  and D' N'  are coprime is a direct  consequence  of (4.113). That N'D' computation and is left to the reader. =  D'  ^N' = H can be shown by direct In view of Lemma 4.19 U' and U' E  RHoo in (4.113) can be expressed as U'  = Y' -F -C [si  -  (A -  KC)Y'[B -  KD K] +  -D /  Ol I\ andt/' = D' N' -r X' F C  ^DF - Ir [sI-(A-^BF)r'[BK] + (4.140) (4.141) 620 Linear Systems These  formulas  can  be  used  as  follows.  A  stabilizable  and  detectable  realization {A B C D] of H{s)  is first determined  and  appropriate  F  and  K  are found  so that A + BF  and A-KC  have eigenvalues with negative real parts. Then U^ and t/'"^  are calculated from (4.140) and (4.141). Note that appropriate state feedback gain matri ces F and observer gain matrices K can be determined using the methods  discussed in  Chapter  4.  The  matrices  F  and  K  may  be  determined  for  example  by  solving appropriate  optimal  linear quadratic  control  and  filtering  problems.  All proper  sta bilizing controllers//2  =  A^2^'2^  ^  D'2 ^^2 ^f theplant/i/i  are then  characterized as in Theorem 4.13. It can now be shown in view of Lemma 4.19 that all stabilizing controllers  are described by k  =  {A + BF  -  K(C  +  DF))x  -V  Ky  + {B -  KD)ri u  =  Fx  + rir2  =  y-{C  + DF)x  -  Dri  n  =  K'{q)r2 (4.142) which can be rewritten  as k  =  Ax  + Bu  + K(y  -  (Cx  +  Du)) u  =  Fx  + K'{q){y  -  {Cx  +  Du)\ (4.143) Thus every  stabilizing  controller is a combination  of an asymptotic  (full-state/full-order) estimator or observer and a stabilizing state feedback  plus K'(q)r2  with r2 = y -  (Cx  +  Du)  the output "error" (see Fig. 7.9). Let be coprime factorizations  in RHoo where N'  D' N'  D'  G RHoo. Also let H  =  N'D'-^  =  D'~^N' H  =  ND-^  =  D-^N (4.144) (4.145) be polynomial matrix coprime factorizations. The relation of proper and stable MFDs of H(s)  to internal PMDs of the system is established in the next result. FIGURE 7.9 A state-space representation of all stabilizing controllers THEOREM 4.20.  (i) The pair  {N'D')  e RHoo  defines  an re factorization  of H{s)  as in (4.144) if  and only if there exists a rational matrix n  with n  n~^  stable and DYl biproper such that Furthermore if only n  is stable and DYl is proper then (N^D^) is a right  factorization but it is not necessarily coprime. n. (4.146) (ii) Similarly the pair  (N^&)  G RHoo  defines  an Ic factorization  of H(s)  in RHoo 621 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems as in (4.144) if and only if there exists a rational matrix fl  with fl fl~^  stable and YID biproper such that (4.147) Furthermore if only n  is stable and YID is proper then  {N\&)  is a left  factorization but it is not necessarily coprime. [N'D']  =U[ND]. Proof The proof of this result can be found in Antsaklis  [4] and will not be repeated • here. An interesting  implication  of Theorem  4.20 is the following:  let H  =  ND~^  be a right  PMFD  and  let D be  column  proper;  Dz  =  uy  = Nz  is  di controllable  PMD. Following the development in Subsection 7.4B define the linear state feedback  con trol law by (4.94) as w =  F{q)z  +  Gr det G 7^ 0 where deg^.F{q)  < dQg^.D{q).  The closed-loop  system  is  then  Dpz  =  Gr^y = Nz  given  in  (4.95)  where  Dp  = D — F. Now in view of Theorem 4.20 the relation D^'G (4.148) defines  re proper  and  stable factorizations  of H  when  {DJ^N} is detectable.  If the pair  (N^D)  is  re  then  Yl =  D^^G.  In  fact  it  is  shown  in  Antsaklis  [4]  that  all  re factorizations  in  RHoo  may  be  obtained  by  means  of  (4.148)  i.e.  by  using  linear state  feedback  on  controllable  and  detectable  realizations  of  H.  It  is  interesting  to note that if  {A5CD}  is an equivalent  state-space representation  to  {DJ^N}  with (F G) the corresponding  state feedback  gain matrices. F C + DF [sI-{A-BF)]-^BG G.  This  expression  is  the  same  as  (4.141)  when  G =  1.  Simi lar  results  can  be  derived  for  the  Ic  factorizations  {N^&)  that  are  related  to  state observers. As  an  application  of  (4.148)  consider  Theorem  4.8  and  Corollary  4.12  where it is  shown that if the given plant Hi  =  NiD^^  =  N[D'^^  where Ni^Di  are re poly nomial  matrices  and A^( D\  are proper  and  stable matrices  then H2 is  a  stabilizing controller  if  and  only  if  it  can  be  written  as  H2 =  L^^Li  where  L2D1 — LiNi  = /  (Theorem 4.8)  or  if  and  only  if  H2  can  be  written  as  H2  =  ^2"^^/^'  where A^(A^( =  /  (Corollary 4.12). Assuming that Di  is column proper then in view D^D[ / of  (4.148)  the  last  relation  can  be  written  as  [(D^^G)5^]Di  -  [{Dp^G)N^]Ni and therefore the relation between L2Li  and D21N2 ^^ gi^^i^ by G-^DF \L2\ \LI\ (4.149) 622 Linear Systems ^ ^ -1 ^.. TV =  A^^ ^^  2)(5 + 1)  where N  =  s -  I  md  D  = \    then A^' = NU D'  = DU  and X'D'  +  Y'N'  =  ^-^  ^-^  + EXAMPLE  4.11.  Let H(s)  =  1-^^^. . .. (s -  2)(^ + 1). (i)  If n  =   s-  1 (s + 1> ®  If  n  =   o(^ + 3) (^ + 2) Notice that in both (i) and (ii) 11 H"^ are stable and DU is biproper as required by • (s -  1)(^ + 2)  ^ (^ +  1)2(^ + 3) j^  '  ^^  ^  ^j  • ^ ^ ^ ^^  +    then  X'D'  +  FW  = (5 +  1)2(5' +  3) Theorem 4.20. (5 +  1)(^ +  3) (5 +  1)(^ +  2) ; t; L The  X-approach Given the transfer  function  H(s)  instead of obtaining proper and  stable  factor izations  to  characterize  all  proper  stabilizing  controllers  via  a  Diophantine  Equa tion  over  the  ring  of  proper  and  stable  rational  functions  the  transformation  A  = l/(s  -\-  a)a>  0  may  be  used.  Then  one  works  with  a Diophantine  Equation  that involves  polynomial  matrices  in  A.  This  transformation  maps  the  stable  region in  the  s-plane  into  a  "stable"  region  in  the  A-plane  and  the  point  ^  =  oo to  the point  A  =  0  (see  Pernebo  [29]  for  a  detailed  discussion).  This  approach  corre sponds  to  working  with  proper  and  stable  factorizations  of  HN'  and  D'  with  all the  poles  of  A^' and  D'  at  -a  and  requires  only  polynomial  matrix  manipulations [here  H  =  (l/(^  +  a)")/  in  Theorem  4.20].  Recall  that  a  rational  R(s)  is  proper (there  are  no  poles  at s  =  ^)  if  and  only  if  R[(l  -  Xa)/X] has  no  pole  at  A == 0. Therefore  for  proper  stabilizing  controllers  solutions  of  appropriate  polynomial Diophantine  Equations  are  sought  where  the  denominator  Z)2(A) of  the  stabilizing controller  H2  has  no  A factors  in  det  D2W i.e.  ^2^'^)  has  no  poles  at  A == 0. Note that in this case all the poles of the solutions of the corresponding proper  and stable Diophantine Equation will also be at  -a  and stabilizing controllers  obtained by  this  method  tend  to  assign  multiple  closed-loop  eigenvalues  at  -a.  As  an  il lustration  consider  H(s)  =  (s  -  l)/[(s  -  2){s  + 1 )]  (see  the  example  above)  and let A - l/(^  +  1). Then H(X)  =  (I  -  2A)A/(1 -  3A). The polynomial  Diophantine Equation in A is solved to obtain ZD +  M  =  (-6A  +  1)(1 -  3A) + 9(1 -  2A)A  =  1. The  corresponding  (proper  and  stable)  Diophantine  Equation  is  obtained  if  we  let A =  l/(^  +  1) in X A  Y N.  Then the Z' D'  Y' N'  of case (i) of the above example are derived. If the controller u  =  Cy  + r with C{s)  =  -9(s  + l)/(s  -  5) is used all three closed-loop eigenvalues  will be at  - 1. [C(s)  =  C(A)  =  -X'^Y"^  with A = l/(s  +  1).] D.  Two Degrees  of Freedom  Controllers Consider the two degrees of freedom  controller Sc  in the feedback  configuration  of Fig.  7.10.  Here  SH represents  the  system  to  be  controlled  and  is  described  by  its transfer  function  matrix H(s)  so that y(s)  =  H(s)u(s) (4.150) 623 CHAPTER  7: Polynomial Matrix Descriptions and  Matrix Fractional Descriptions of  Systems The two degrees  of freedom  controller 5c  is described  by  its transfer  function  matrix C(s)  in u(s)  =  C(s) m r(s). [Cy(s\  Cr(s)] (4.151) Since  the  controller  Sc  generates  the  input  u  to  SH  by  processing  independently  y the  output  of  SH  and  r  the  external  input  it  is  called  a  two  degrees  of  freedom controller. In  the  following  we  shall  assume  that  //  is  a proper  transfer  function  and  shall determine proper controller transfer  functions  C that internally  stabilize the  feedback system  in Fig.  7.10.  The  restriction  that H  and  C  are proper  may  easily  be  removed if  so  desired.  Note  that  in  the  development  in  Subsection  7.4C  we  assumed  proper transfer  functions  in  the  feedback  loop  while  the  development  in  Subsection  7.4A applies  to  nonproper  transfer  functions  as  well. r Sc u SH y * FIGURE  7.10 Two degrees of freedom  controller  Sc Internal  stability THEOREM  4.21.  Given is the proper  transfer  function  H  of  SH and the proper  trans fer  function  C of  Sc  in  (4.151)  where  det(I  -  CyH)  T^ 0.  The  closed-loop  system  in Fig. 7.10 is internally  stable if and only if (i)  u  =  Cyy  internally  stabilizes the system  y  =  Hit (ii)  Cr is such that the rational  matrix M  =  {I -CyHY^Cr (4.152) (w  =  Mr)  satisfies  D  ^M  =  X a stable rational matrix where Cy satisfies  (i) and H ND~^  is a right coprime polynomial matrix  factorization. Proof  Consider controllable  and observable PMDs for SH given by and for Sc  given by Dz  =  u y  =  Nz DcZc =  [NyNr] U  =  Zc. (4.153) (4.154) where the N D  are re and the Dc [Ny Nr] are Ic polynomial  matrices. The  closed-loop system is then described by {bcD  -  NyN)z  =  Nrr y  =  Nz (4.155) and  is  internally  stable  if  the  roots  of  det Do  where  Do  =  DcD  -  NyN  have  strictly negative real parts. (Necessity)  Assume  that  the  closed-loop  system  is  internally  stable  i.e.  D~^  is stable.  Since  Cy  =  D~^Ny  is  not  necessarily  an  Ic  polynomial  factorization  write [DcNy]  =  GdDcyNcy]'^^^reGLis2igcldofthQpaiY0cNy).ThenDcyD-NcyN = Gl^Do  =  bk  where  D^  is  a polynomial  matrix  with  D^^  stable; note also that  G^^  is stable. Hence  u  =  Cyy  =  D'^^Ncyy  internally  stabiUzes  y  =  Hu  =  ND'^u i.e. part (i)  of  the  theorem  is  true.  To  show  that  (ii)  is  true  we  write  M  =  (I  -  CyH)~^Cr  = 624 Linear Systems Dbl^Dcy0-^Nr)  = Db^^Gl^Nr  = DX where X  = D'^Nr  is a stable rational ma-^^^- ^^^^ shows that (ii) is also necessary. (Sufficiency) Let C satisfy  (i) and (ii) of the theorem. If C  = D~^[Ny Nf] is an Ic polynomial MFD and GL is a geld of the pair (5c Ny) then [Dc Ny] = GiVDcy Ncy] is true for some Ic matrices Dcy  and Ncy(Cy = D^^Ncy)- Because (i) is satisfied DcyD  -NcyN  =  D;t where D^Ms stable. Premultiplying by GL we obtain Dc/)-A^^A/^ =  GLDJ. Now if G^^ is stable then 5~^ where Do = DcD -  NyN  = GiDk will be stable since 6^1 is stable. To show this write D'^M  = D-\l -  CyUy^Cr  = D^^Dcy0~^Nr)  = b^^Gl^Nr  and note that this is stable in view of (ii). Observe now that the GL Nr are Ic; if they were not then C  = b~^[Ny Nr] would not be a coprime factorization. In this case no unstable cancellations take place in D^^G^^Nr 0^^  is stable) and therefore if D~^M is stable then {GLDkY^ = b~^  is stable or the closed-loop system is internally stable. • Remarks (1)  It is  straightforward  to  show  the  same results using  proper  and  stable  factor izations of H given by H  =  N'D'-\ where the pair (N'  D')  G RHoo  and (A^' D')  is re and of C  =  &;'[N;N^I (4.156) (4.157) where the pair 0'^  [N!^  N;.])  G  RHOO and (D^ [A^^ TV/]) is Ic. The proof is com pletely  analogous and is left to the reader. The only change in the theorem will be in part (ii) which will now read: (ii)  Cr  is  such  that  the  rational  matrix  M  =  (I  -  CyH)~^Cr  satisfies is an re MFD in D'-^M  =  X'  G RH^  where Cy satisfies  (i) and H  =  N'D'-^ RHoo. (2)  Theorem 4.21 separates the role of Cy the feedback  part of C from  the role of Cr in achieving internal stability. Clearly if only feedback action is considered then only part (i) of the theorem is of interest; and if open-loop control is desired then  Cy  =  0  and  (i)  implies  that  for  internal  stability  H  must  be  stable  and Cr  =  M must satisfy  part (ii). In (ii) the parameter M  =  DX appears naturally and in (i) the way is open to use any desired feedback  parameterizations. In view of Theorem 4.21 it is straightforward  to parametrically  character ize all internally stabilizing controllers C  In the theorem it is clearly stated [Part (i)] that  Cy must be a stabilizing  controller. Therefore  any parametric  charac terization of the ones developed in the previous subsections can be used for  Cy. Also Cr is expressed in terms of D~^M  =  X  (or D'  M  ^  X'). THEOREM 4.22.  Given that }) =  //w is proper with//  =  ND~^  = D"^A^ doubly co-prime polynomial MFDs all internally stabilizing proper controllers C in w  =  CK  are given by: (i) C  = (1 + QHr^[QM]  =  [(/ + LN)D-^]-^[LXl (4.158) where Q =  DL and M  =  DX are proper with L X and D'^I  + QH)  =  (/  + LN)D-^ stable so that (/  + QH)~^ exists and is proper; or (ii) C  =  (Xi  -  KN)-\-{X2  + Kb)  XI (4.159) where K and X  are stable  so that  {X\  — KN\)  ^ exists and C is proper. Also X\  and X2 625 are determined  from  UU '  Xi -N X2\ D\ ID [N -X2 ^ 1. I  0" / 0 with U unimodular. lfH=  N'D'-^  = D'-^N'  are doubly  coprime  MFDs  in RHoo  then  all stabilizing proper C are given by (iii) C = {X[-K'N')-\-{X!^ + K'D')X% (4.160) CHAPTER 7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems where  K'X'  e  RH^o  so  that  {X[ -K'N')'^ exists  and is  proper.  Also  U'U'-^  = X'] & D' N' \  ^1 -N' (iv) -xn K 7 0 0" / with  U'U'-^  eRK c = (/+e//)-i[eM]:  [(/  +  L W ) Z ) ' - l ] - l [ L ^ r] (4.161) where  Q = D'L'.M  = D'X'  e  RH^o  with L^X'  and D'-^{I  + QH) RHoo  so that  {I + QH)~^  or {I + LlN')-^  exists and is proper. {I +  L'N')D'- e Proof  In view of part (i) in Theorem  4.21  Cy of C =  [Cy  Q]  can be expressed in terms of any parameterization of all stabilizing feedback  controllers developed in  Subsections 7.4A and 7.4C. Cr can then be expressed as Cr = {I — CyH)M  with Cy  given from  above and M = DX with X  any stable rational matrix. If  Cy =  {I + QH)-^Q  with  D-^[I + QHQ]  stable  (see Theorem  4.3) or  Cy = [{I + LN)D-^]-^L  with  [{I + LN)D-\L] stable  (see Corollary  4.6) part  (i) of the theorem  follows.  Notice  that  Cr = (I-  CyH)M  =  (/ + QH)-^M  =  [{I +  LN)D-^]-^X since Q = DL and M = DX. Similarly if Cy  = -{Xi-  KN)'^  {X2 + KD)  with K stable (see Theorem 4.2) then C =  {Xi -  KNy^D'^M  = {Xi -  KN)-^X  and part (ii) of the theorem  follows. For part  (iii) express  Cy in terms of K'  G RHoo  as in Theorem  4.13 and use X' G RHoo  in part  (ii) of Theorem  4.21 as in remarks  following  (4.157).  Part  (iv) follows from  Theorem 4.16 of Subsection 7.4C. • Response  maps It is straightforward  to express  the maps  between  signals  of interest  of Fig. 7.10 in  terms  of the  parameters  in Theorem  4.22.  For instance  u = C - [C-yC^ CyHu+Crr from  which  we have u = {I — CyH)~^Crr  = Mr.  (In the following  we  will use the symbols  u^y^  r etc. instead of w j r etc. for convenience.)  If expressions  (iv) of  Theorem  4.22  are used  then and y = Hu=N'D'-^D'X'r u = D'X'r = N'X'r (4.162) in  view  of {I — CyH)~^  =  D'{I+  L'N')D'~^.  Similar  results  can be derived  using the other  parameterizations  in Theorem  4.22.  To determine  expressions  for other  maps of  interest  in control  systems  consider  Fig.  7.11  where  du and dy are assumed  to be  disturbances  at the input  and output  of the  plant  H  respectively  and 77  denotes measurement  noise.  Then  u  =  [C3;Cr] \y + dy +  vi\ du from  which  we  have {I-CyH)-^[Crr + Cydy+Cy^+du] diXidy = Hu  = H{l-CyU)-^[Crr + Cydy CyVi+du]. Then in view  of (4.161)  in Theorem  4.22 we obtain u = D'X'r  + D'Lldy  +  D'L'I]  +D\I  +  L'N')D'-^du =  Mr + Qdy +  Qri+Sidu (4.163) 626 Linear Systems o^ FIGURE7.il Two degrees of freedom control configuration and y  =  N'X'r  +  N'L'dy  +  N'L'r]  +  A^'(/ +  L'N')D'~^du =  Tr  + (So -  I)dy  +  HQT] +  HSidu. (4.164) Notice that  Q  =  (I  -  CyHy^Cy  =  D'L'  is the transfer  function  between  u and dy or 7]. Also Si  =  (I  -  CyHY^  =  D'(I  +  L'N')D / -I I  + QH (4.165) is the transfer  function  between u and du. The matrix St is called the input  compari son sensitivity  matrix.  Notice also that yo  =  y -^ dy  =  Tr-\-  Sody + HQrf  +  HSidu\ i.e. So  =  {I-  HCy)  -1  _  I^HQ (4.166) is the transfer function between yo and dy. The matrix So is called the output  compar ison sensitivity  matrix.  The  sensitivity  matrices  St and So are important  quantities in control design. Now HQ  =  So-  N'L'  =  I (4.167) .yL±) HCyil-HCy)-^ L±^yyL ±±^yj since//g  =  H(I-CyH)-^Cy =  -J  + So. ^y where So and HQ  are the transfer functions  from  yo to dy and T; respectively. Equa tion  (4.167)  states  that  disturbance  attenuation  (or  sensitivity  reduction)  and  noise attenuation  cannot  occur over the same frequency  range  (show  this). This is a  fun damental limitation of the feedback  loop and occurs also in two degrees of  freedom control  systems. Similarly we note that =  -I  + il-HCyT^ We now summarize  some of the relations discussed  above: Si  -QH  =  I. (4.168) -CyHy^Cr  DX =  DL T  =  H(I  -  CyHY'Cr  =  HM  =  NX M  =  (I Q  =  (I-CyHy^Cy So  =  {I-HCyY^ =  I  +  HQ Si  =  (I-  CyHy^  =  I  +  QH {y  =  Tr) (u  =  Mr) (u  =  Qdy) {yo  =  Sody) (u  =  Sidu). The input-output maps attainable from  r using an internally  stable two degrees of  freedom  configuration  can  be  characterized  directly.  In  particular  consider  the two maps described  by (4.169) i.e. the command/output map T and the command/input map M. Let H  =  ND  ^ be an re polynomial MFD. THEOREM 4.23.  The Stable rational function matrices T and M are realizable with in ternal stability by means of a two degrees of freedom control configuration  [that satisfies (4.169)] if and only if there exists stable X so that X (4.170) Proof (Necessity) Assume that T and M in (4.169) are realizable with internal stability Then in view of Theorem 4.20 X  = Z)"^M is stable. Also 3; = Hu  = (ND'^)(Mr)  = NXr. 627 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems (Sufficiency) Let (4.170) be satisfied. If X is stable then T and M are stable. Also note that T  =  HM.  We now show that in this case a controller configuration  exists to implement these maps (see Fig. 7.12). Note that  u  = Mr  +  Cy(fr  -\-  y)  =  [Cy M  + CyT]  from which we obtain r-y] \_r\ u  = (I + CyH)-\M  + Cyf)r. (4.171) HM  this relation implies that u = Now if M  =  M and T  =  T then in view of T (I  + CyH)-\l  +  CyH)Mr  = Mr  and y  =  Hu HMr  =  Tr.  Furthermore  Cy is a stabilizing feedback  controller and the system is internally  stable since t  and M  are stable. • m.\  A  1 1  M^M r L o-C^  ;o u 1 1 y FIGURE 7.12 Feedback realization of (T M) Note that other internally stable controller configurations to attain these maps are possible. (The realization of both response maps T and M instead of only T as in the case of the Model Matching  Problem  makes the convenient  formation  in  Theorem 4.24 possible. The realization  of both  T and M  is sometimes referred  to as the Total Synthesis  Problem;  see [7] [8] and the references  therein.) The  results  of  Theorem  4.23  can  be  expressed  in  terms  of  H  =  N'D'~^  re MFDs in RHoo. In particular we have the following  result. THEOREM4.24.  TM  EL  RHOO are realizable with internal stability by means of a two degrees of freedom control configuration  [that satisfies (4.169)] if and only if there exists 628 Linear  Systems X'  E  RH^  so that \X'. (4.172) Proof.  The proof is completely analogous to the proof of Theorem 4.23 and is omitted. It  is  now  clear  that  given  any  desirable  response  maps r  such  that N' D' X\  where  X'  G  RHo^ the  pair  (T  M)  can  be  realized  with  internal  sta bility  by  using  for  instance  a  controller  (4.161)  C  =  [(/  + L'N')D'-T^[L'X% where  [(/  +  L'N')D'~^  L']  G  RH^^  and  X'  is  given  above  as  can  easily  be  veri fied.  It  is  clear  that  there  are  many  C  that  realize  such  T  and  M  and  they  are  all parameterized  via  the  parameter  L'  E  RHoo that  for  internal  stability  must  satisfy the condition  (/  + L'N')D'~^  E  RHoo. Other parameterizations  such  as K'  can also  be used.  In  other  words  the  maps  T  M  can  be  realized  by  a  variety  of  configurations each  with  different  feedback  properties. Remark In  a  two  degrees  of  freedom  feedback  control  configuration  all  admissible  re sponses  from  r  under  condition  of internal  stability  are  characterized  in  terms  of  the parameters  X  (or  M)  while  all  response  maps  from  disturbance  and  noise  inputs that  describe  feedback  properties  of  the  system  can  be  characterized  in  terms  of  pa rameters  such  as  ^  or  2  or  L.  This  is  the  fundamental  property  of  two  degrees  of freedom  control  systems:  it  is  possible  to  attain  the  response  maps  from  r  indepen dently  from  feedback  properties  such  as  response  to  disturbances  and  sensitivity  to plant  parameter  variations. EXAMPLE  4.12.  We  consider  H(s)  =  (s -  m  + 2) proper  and stable transfer  functions  T(s)  that can be realized by means  of some  control and  wish  to  characterize  all (s -  2)2 configuration  with  internal  stability.  Let  H(s)  = 1  (s -  2f {s + 2)2 s +  2\ N'D"^  be  an re  MFD  in  RH s + 2 1 Then  in  view  of  Theorem  4.24  all  such  T  must  satisfy  N' 'T  = T  ^  X'  G  RHoo.  Therefore  any proper  T with  a zero at  +1  can be realized  via a two degrees of freedom  feedback  controller with internal  stability. Now  if  a single  degree  of freedom  controller  must be used  the class  of  realizable T(s)  under internal stability is restricted. In particular if the unity feedback  configuration {/; Gff  1} in  Fig.  7.15  is  used  then  all  proper  and  stable  T  that  are  realizable  under internal  stability  are  again  given  by  T N'X' [s  -  I' s + 2 X\  where  X'  =  L'  E  RH^ [see (4.184)] and in addition  (/  +  X W ) ^' 1 +r  s-  1  ^  • ^^   \5  +  2 (s +  2)2 (s -  2)2 X'  =  Hx/dx is proper and stable and should also satisfy  (s+2)dx  + (s for  some polynomial  p(s). E  RHoo i.e. ^  (s-2fp(s) This illustrates the restrictions imposed by the unity feedback controller as opposed to a two  degrees  of freedom  controller.  Notice that these  additional  restrictions  are im posed because the given plant has unstable poles. • It is  not  difficult  to prove  the  following  result. THEOREM4.25.  TMSG RHOO are realizable with internal stability by a two degrees of freedom  control configuration  that satisfies  (4.169) and (4.167)  [S  =  S^  see Fig. 7.11 and (4.163) (4.164)] if and only if there exist X'  L  G RH^  so that r ri M _S _ = [N' D' .0 0] 0 A^'J \X''  + ro] 0 ./_ (4.173) where  (/  +  L'N')D'-^  G  RHoo  Similarly  T M Q there exist X'L'  G RHo so that RHoo  are realizable  if  and  only  if 629 CHAPTER 7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems T' M Q\ = [N' D' .0 0 0 D' (4.174) where  (/  +  L'N')D'-^  G  RH^o. Proof  The proof  is straightforward  in view  of Theorem  4.24. Note that S ox Q are se lected  in  such  a  manner  that  the  feedback  loop  has  desirable  feedback  characteristics that are expressed  in terms of these maps. • Controller  implementations The  controller  C  =  [Cy Cr] may  be  implemented  for  example  as  a  system Sc as  shown  in  Fig.  7.10  and  described  by  (4.154)  or  as  shown  in  Fig.  7.12  with C  =  [Cy M  + CyT]  where  Cy stabilizes H  and  T M  are desired  stable maps  that relate  r  to j  and  r  to u i.e.  y  =  Tr  and  u  =  Mr  There  are  also  alternative  ways of  implementing  a  stabilizing  controller  C.  In  the  following  the  common  control configuration  of Fig. 7.13 is briefly  discussed. It will be denoted by {R; Gff  Gf^}. The {R\ Gff  Gfb}  Configuration Note that  since U  =  [Cy Cr] -  [GffGfb  GffRIl (4.175) {R; Gff  Gfb)  is  a  two  degrees  of  freedom  control  configuration  that  is  as  general as the ones  discussed as the ones discussed before. To see this let C  =  [Cy Cr\  =  D'c ^i^r  ^rl  be an Ic MFD in 7^7^00 and let R  =  N;.  Gff  =  D Gft  =  N;. (4.176) Note that R  and  G/^  are always  stable; also G^j  exists  and is stable. Assume now that C was chosen so that D'D'  -  N'N'  =  U' (4.177) R  PC J  + • >! r 1 I 1 1 1 1 1 1 L u  H y Gff Gfb FIGURE 7.13 Iwo degrees or Ireedom controller {R; Off  Gfb} 630 Linear Systems where  U'  U' in (4.176) is internally  stable as is shown in the  following. E  RHa. Then  the  system in Fig. 7.13  with  R Gff  and  Gft  given G  RHoo.  It  can  be  shown  that  (4.177)  implies  (i).  Note  that internally  stabilizes  H  and  (ii)  X'  =  D'-^M  =  D'-\l In view of Theorem 4.21 the feedback  system is stable if and only if  (i)  Cy  = -GffGfb  =  D'c'^Ny GffGfbHy^GffR any  possible  cancellation  in  the product  GffGft>  between  D'c~^  and  Ny  will  in volve  only  stable  poles;  this  can  easily  be  shown  using  (4.177).  The  cancelled stable  poles  will  be  uncontrollable  or  unobservable  eigenvalues  in  the  closed-loop  system.  In  addition  X'  =  D'-\l -N!^N'y^D'c]£>'c^N;.  =  U'^N'r  E  RHoo.  Therefore the  control  configuration {R;Gff  Gfb}  =  {Nl\D'-^N'y}  with  (4.177)  satisfied  is  internally  stable.  In  view of this result and Theorem 4.22 Gff  Gft  and R may  also be selected as -  GffGfbHy^GffR =  D'-^[D'(D'^D' R  =  X\  Gff  =  [(/  +  L'N')D'-^] 1-1-1 Ub L' (4.178) WiihX'  L{I  + L'N')D'-^  E  J?//oo and J^r(/  + L W)  ^  0 w h e r er  =  D'-^Mand L'  =  D'-^Q  Now if X'  and L'  satisfy  (4.173)  or (4.174) of Theorem 4.25 desired command  responses  T M  and disturbance responses  S ox Q are achieved  under  in ternal stability. Note that Gff  Gfb  and R may also be selected  as R  =  X'  Gff  =  (X[  -  K'N'y -1  Gfb  =  -(X^  +  K'N') (4.179) where X'K'  G  RHoo. We  shall  now  briefly  discuss  some  special  cases  of  the  {R; Gff  Gfb}  control configuration  which are quite common in practice. Note that the configurations  be low are simpler; however they restrict the choices of attainable response maps  and so the flexibility offered  to the control designer is reduced when using these  config urations. /.  The {/;  Gff  Gfb}  controller.  In this case u  =  [Cy Cr] [GffGfb  Gff]  that is. In view of (4.161) given in Theorem 4.22 this implies  that Cv  CrG r^fb-L'  =  X'G fb> (4.180) (4.181) or that the choice for the parameters L' and X'  is not completely independent as in the {R; Gff  Gfb} case. The L' and X'  must of course satisfy  L' X'  and (/ + L'N')D'-^  E RHa. In  addition  in this case L'  and X'  must be  such that  a proper  solution  Gfb  of (4.181) exists and no unstable poles cancel in X'Gfb.  Note that these poles will can cel in the product  Gff  Gfb  and will lead to an unstable  system.  Since L'  and X'  are o FIGURE 7.14 The {/; Off  Gfb} controller both  stable we will require  that  (4.181)  have  a solution  Gfb  ^  RHoo.  This  implies that  if  for  example  X'~^  exists  then  X'  and  L'  must  be  such  that  X'~^L'  G  RHoo i.e. X'  and  L'  have  the  same unstable  zeros  and  L'  is  "more proper"  than X'.  This provides  some guidelines  about the conditions that X'  and L'  must satisfy.  Also Gff  =  [(I  -hL'N')D'-T X'. (4.182) It  should  be  noted  that  the  state  feedback  law  implemented  by  a dynamic  ob server  can be represented  as an {/; Gff  Gfb}  controller  (see  Subsection  7.4B Fig. 7.8). 631 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems 2.  The {/; Gff  1} controller  A  special case of (i) is the unity feedback  control configuration.  Here  u  =  [Cy Cr] ; I.e. (4.183) ^ J L Gff '  u H y FIGURE 7.15 The {/; G// /} controller which in view of (4.161) implies  that X'  =  L'. (4.184) In  this  case  the  responses  between  y  or  u  and  r  (characterized  by  X')  cannot  be designed independently  of feedback  properties such as sensitivity  (characterized  by L').  This  is  a  single  degree  of  freedom  controller  and  is  used  primarily  to  attain feedback  control  specifications. 3  The {R\GffI} controller.  Here  w -  [C^; C] [Gff  GffR] ; I.e. In view of (4.161) given in Theorem 4.22 this implies  that Cr  — CyR. X'  =  L'R. (4.185) (4.186) The L'  and X'  must satisfy  L' X'  (I  +  L'N')D'-^  e  RHoo. In addition they must be such that (4.186) has a solution R  G RHoo. Note that R stable is necessary for internal R  ^ Gff u H y FIGURE 7.16 The {R; Off  1} controller 632 Linear Systems stability. The reader  should refer  to the discussion  in (1) above for the  implications of such assumptions on X'  and L'.  Also Off  =  [{I + L'N')D'-^Y^L'. (4.187) 4  The {R\ I  Gft} controller.  In this case U =  [Cy Cr]  =  [GfbR] (4.188) 1 ^ H y r R tr^ •?  % FIGURE 7.17 The {R; / Gft} controller For internal  stability R  must be  stable. In  view  of  (4.161)  given  in  Theorem  4.22 this implies the requirement  [(/ + VN')D'~^]~'^X'  G RHoo in addition to L' X'  (I  + L'N')D~^  E  RHoo which imposes  significant  additional restrictions  on L'. Here [GfbR]  =  [(I + L'N')D'-T'[L\X'l (4.189) 5.  The  {11 Gfh}  controller.  This  is  a  special  case  of  (4)  a  single  degree  of freedom  case where R  =  I.  Here R  =  I  implies  that or that X'  and L'  must satisfy  additionally the relation X'  =  (/  +  L'N')D'-\ D'X'  -  L'N'  =  I (4.190) (4.191) a (skew) Diophantine Equation. This is in addition to the condition that L' X'  (I  + L'N')D-^  G  RHoo. 1  U H y ~\ J i G* FIGURE 7.18 The {/; / G/b} controller Control  problems In control problems design specifications  typically include requirements for in ternal  stability  or  pole  placement  low  sensitivity  to  parameter  variations  distur bance attenuation and noise reduction. Also requirements such as model matching diagonal decoupling  static decoupling regulation  and tracking  are included  in the specifications. 633 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems Internal  stability  has  of  course  been  a central  theme  throughout  the book  and in  this  section  all  stabilizing  controllers  were  parameterized.  Pole  placement  was studied in Chapter 4 using  state feedback  and output feedback  via the  Diophantine Equation was addressed earlier in Chapter 7. Sensitivity and disturbance/noise reduc tion are treated by appropriately  selecting the feedback  controller Cy. Methodologies to  accomplish  these  control  goals  frequently  in  an  optimal  way  are  developed  in many control books. It should be noted that many important design approaches  such as the Hoo-optimal  control  design  method  are based  on the parameterizations  of  all feedback  stabilizing controllers discussed earlier. In particular an appropriate or opti mal controller is selected by restricting the parameters used so that additional control goals are accomplished  optimally while guaranteeing internal  stability in the loop. Our development of the theory of two degrees of freedom controllers can be used directly to study model matching and decoupling and a brief outline of this approach is now given. Note that this does not by any means constitute a complete  treatment of these important  control problems but rather  an illustration  of the  methodologies introduced in this  section. In the model matching  problem the transfer  function  of the plant H{s){y  =  Hu) and  a  desired  transfer  function  T{s){y  =  Tr)  are  given  and  a  transfer  function M{s)(u  =  Mr)  is sought so that T{s)=H{s)M{s). (4.192) Typically H{s)  is proper and the proper and stable T{s)  is to be obtained from  H{s) using  a  controller  under  the  condition  of  internal  stability.  Therefore  M{s)  can  in general  not be implemented  as an open-loop  controller  but rather  as a two  degrees of freedom  controller. In view of Theorem 4.24 (or Theorem 4.23) if H  = N'D'~^  is an re MFD in RHoo then the pair  {TM)  can be realized with internal  stability if and only  if  there  exists X^ G RHoo  so  that X\  Note  that  an M  that  satisfies (4.192)  must first be  selected  (there may  be  an infinite  number  of  solutions  M).  In the case when det H{s)  7^ 0  T can be realized with internal  stability by means  of a two degrees of freedom  control configuration  if and only if N^~^T  =X'  ^  RHoo  (see Example 4.12). In this case M  =  D'X'.  Now if the model matching is to be achieved by  a more  restricted  control  configuration  then  additional  conditions  are  imposed on T  for this to happen  which are expressed  in terms of X'  (see for instance Exam ple 4.12 for the case of the unity feedback  configuration  and Exercises 7.23 7.26). In the problem of diagonal  decoupling  T{s)  in (4.192) is not completely  speci fied but is required to be diagonal proper and  stable. In this problem the first input affects  only the first output  the  second input affects  only the  second  output  and  so forth.  \f  H{s)~^  exists  then  diagonal  decoupling  under  internal  stability  via  a two degrees of freedom  control configuration  is possible if and only if •ni_ di N'-^T  =N'-^ X'  G RHo. (4.193) Clyn J where  H  =  N'D'-^ l...m. It  is  clear  that  if  H{s)  has  only  stable  zeros is  an  re  MFD  in  RH^.  and  T{s)  =  diag  [ni{s)/di{s)]i  = then  no  additional 634 Linear Systems restrictions  are imposed  on  T{s).  Relation  (4.193)  implies  restrictions  on the  zeros ^^ ^i^^) when H{s)  has unstable zeros. It is  straightforward  to show that if  diagonal  decoupling  is to be  accomplished by means  of more restricted  control configurations  then  additional restrictions  will be imposed on T{s)  via X'.  (See Exercise 7.25  and Exercise 4.17 4.20 of Chapter 4 for the case of diagonal decoupling via linear state  feedback.) The problem of diagonal decoupling has a long and interesting history and a very rich  literature.  The  original  solution  of  the problem  involved  linear  state  feedback and state-space descriptions  and is due to Falb and Wolovich  [12]. For an  approach involving PMDs  and the transfer  function  matrix  see Williams  and Antsaklis  [35]. Other types of decoupling such as block dynamic and static are also treated there. A problem closely related to diagonal decoupling  is that of the inverse  of  H{s). In  this  case  T{s)  =  I.  There  is  also  a very  rich  literature  on this  problem  and  the interested  reader  is encouraged  to find out more  about it. A  starting point  could  be Williams  and Antsaklis  [35]. (See also Exercises 4.17 4.18 in Chapter  4.) In  the  problem  of  static  decoupling  T(s)  E  RHa  is  square  and  also  satis fies  r(0)  =  A  a  real  nonsingular  diagonal  matrix.  An  example  of  such  T(s) r ^2  +1 [s(s +  2) s(s^  + 2)  ] s^  ^3s+ I d{s) where  d{s)  is  a  Hurwitz  polynomial. Note  that  if  T(0)  =  A  then  a  step  change  in  the  first  input  r  will  affect  only the  first  output  in  y  at  steady-state  and  so  forth.  Here  y  =  Tr  =  T(l/s)  and lims^osT(l/s)  =  T(0)  =  A which is diagonal and nonsingular. For this to happen with internal  stability  when H(s)  is nonsingular  (see Theorem 4.24) we must  have N'  T  =  X'  G  RHoo  from  which  can  be  seen  that  static  decoupling  is possible  if and only if H{s) does not have zeros at 5* =  0. If this is the case and if in addition  H{s) is  stable  static  decoupling  can  be  achieved  with just  a precompensation  by  a real gain matrix  G where  G  =  H-\Qi)K.  In this case  T{s)  =  H{s)G  =  H{s)H~\0)A from  which  T(0)  =  A. 7.5 SUMMARY In this chapter  alternatives  to state-space  descriptions  were introduced  and used  to further  study  the  behavior  of  linear  time-invariant  systems  and  to  study  in  depth structural properties of feedback  control systems. In Part 1 the properties of systems described by Polynomial Matrix Descriptions (PMDs)  were explored  in Section  7.3  and background  on polynomial  matrices  was provided  in  Section  7.2. The Diophantine  Equation  which  plays  an important  role in feedback  systems was studied  at length in Subsection  7.2E. An in-depth study of the theory of parameterizations of all stabilizing controllers with emphasis on PMDs was undertaken in Part 2 Subsection 7.4A and the param eterizations of all proper stabilizing controllers in terms of proper and stable Matrix Fraction Descriptions  (MFDs) were derived in Subsection 7.4C. State feedback  and state estimation  using PMDs were studied in Subsection  7.4B. Finally control sys tems  with  two  degrees  of  freedom  controllers  were  explored  in  Subsection  7.4D with  an  emphasis  on  stability  parameterizations  of  all  stabilizing  controllers  and attainable response maps. 635 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of Systems 7.6 NOTES Two  books  that  are  original  sources  on  the  use  of  polynomial  matrix  descriptions in systems and control are Rosenbrock  [30] and Wolovich  [36]. In the former  what is now called Rosenbrock's Matrix is employed and relations to state-space descrip tions are emphasized. In the latter what are now called Polynomial Matrix Fractional Descriptions  (PMFDs)  are  emphasized  and  the  relation  to  state  space  is  accom plished primarily  by using  controller  forms  and the Structure  Theorem  which  was presented in Chapter 3. Good general  sources for the polynomial matrix  description approach  also include the books by Vardulakis  [33] Kailath  [21] and Chen [10]. Basic  references  for  the  material  on polynomial  matrices  discussed  in  Section 7.2  are  the  books  by  MacDuffee  [25]  and  Gantmacher  [14]. These  books  also  in clude  material  on  the  Diophantine  Equation  discussed  in  Subsection  7.2E.  Addi tional sources for the properties of polynomial matrices that we found useful  include Wolovich  [36] Vardulakis  [33] and Kailath [21]. A key  concept  in our  development  of polynomial  descriptions  for  the  study  of systems  is  the  notion  of  equivalence  of  representations  discussed  in  Subsection 7.3A  since  it  establishes  not  only  relations  between  polynomial  descriptions  but also between  polynomial  and  state-space  representations.  Original  sources  for  this include  Rosenbrock  [30]  and  Fuhrmann  [13]. See  also  Pernebo  [28]  and  the  com ments by Rosenbrock in [31 ] and [32] noting that the definition of equivalence given by Wolovich [36] was shown by Pernebo in [28] to be the same as strict system equiv alence. Additional material on this topic can be found in Kailath [21] Wolovich [36] and Vardulakis  [33]. A good source for the study of feedback  systems using  PMDs and MFDs is the book by Callier and Desoer [9]. The development of the properties of interconnected systems addressed in Sub section 7.3C which include controllability observability and stability of systems in parallel in series and in feedback  configurations  is based primarily on the approach taken in Antsaklis  and  Sain  [8] Antsaklis  [2] and  [3] and Gonzalez  and  Antsaklis [18]. Parameterizations  of  all  stabilizing  controllers  are  of  course  very  important  in control theory today. Historically their development  appears to have evolved in the following  manner  (see  also  the  historical  remarks  on  the  Diophantine  Equation  in Subsection 7.2E): Youla et al. [37] introduced the K parameterization (as in Theorem 4.2) in  1976 and used it in the Wiener-Hopf  design of optimal controllers. This work is considered to be the seminal contribution in this area. The proofs of the results on the parameterizations in Youla et al. [37] involve transfer functions  and their charac teristic polynomials. Neither the Diophantine Equation nor PMDs of the system are used  (explicitly). It should be recalled  that in the middle  1970s most of the  control results in the literature concerning MIMO systems involved state-space descriptions and  a few  employed  transfer  function  matrices. The  PMD  descriptions  of  systems presented  in  the  books  by  Rosenbrock  [30]  and  Wolovich  [36]  were  only  begin ning  to make  some impact.  A version  of the linear  Diophantine  Equation  namely AX  ^-YB  =  C polynomial in z~^ was used in control design by Kucera in work re ported in 1974 and  1975. In that work parameterizations of all stabilizing controllers were  implicit  in  the  sense  that  the  stabilizing  controllers  were  expressed  in  terms of the general  solution of the Diophantine Equation which in turn can be  described parametrically. Explicit parameterizations were reported in Kucera [23]. In Antsaklis 636 Linear Systems [1] the doubly coprime MFDs were used with the polynomial Diophantine Equation working over the ring of polynomials to derive the results by Youla in an alternative way. In Desoer et al. [11] parameterizations K'  of all stabilizing controllers using co-prime MFDs  in rings other than polynomial rings  (including  the ring of proper  and stable rational functions)  were derived. It should also be noted that proper and stable MFDs  had  apparently  been  used  earlier  by  Vidyasagar.  In  Zames  [38] a parame terization  Q of all  stabilizing  controllers but only for  stable plants was  introduced and  used  in  //oo-optimal  control  design.  (Similar  parameterizations  were  also  used elsewhere but apparently not to characterize all stabilizing controllers; for example they were used in the design of the closed-loop transfer  function  in control  systems and  in  sensitivity  studies  in  the  50s  and  60s  and  also  in  the  "internal  model  con trol"  studies  in  chemical  process  control  in  the  80s.)  A  parameterization  X  of  all stabilizing  controllers  (where  X  is  closely  related  to  the  attainable  response  in  an error feedback  control  system) valid for unstable plants  as well was introduced  in Antsaklis  and  Sain  [7]. Parameterizations  involving  proper  and  stable MFDs  were further  developed  in the  80s in connection  with  optimal  control  design  methodolo gies  such  as ii/oo-optimal  control  and  connections  to  state-space  approaches  were derived.  Two degrees  of freedom  controllers  were  also  studied  and  the  limitations of the different  control configurations  became better understood. By now MFDs and PMDs have become important system representations  and their study is essential if optimal control design methodologies  are to be well  understood. In Subsection 7.4A the discussion of parameters N^  D^  and K  (Theorems  4.1 and 4.2) follows Antsaklis  [1]. The material for the parameter X2 (Corollary 4.6) fol lows Antsaklis  and  Sain  [7] where X2 was introduced  in connection  with the  error feedback control configuration.  Q2 was used in Zames [38] for stable systems (Corol lary 4.4); however. Theorem 4.3 is valid for unstable systems as well. The discussion of the parameters  Sn  and  Li L2 follows  Antsaklis  and  Sain  [8] and Antsaklis  [3]. Subsection 7.4B is based on Wolovich  [36] and Antsaklis  [2] and [3]. The results on proper and  stable MFDs in Subsection  7.4C  and their use in pa rameterizing  all proper  stabilizing  controllers  are due to Desoer  et al.  [11]. The de velopment  in  Subsection  7.4C  was based  on Vidyasagar  [34] Antsaklis  [3] Green and Limebeer [20] and Maciejowski  [26]. The development of the relations between MFDs in RHoo and PMDs of a system follows  AntsakHs  [4] and Nett et al. [27]; see also Khargonekar  and Sontag  [22]. The A-approach was developed in Pernebo [29]. The material on two degrees of freedom  controllers in Subsection 7.4D is based on Antsaklis [3] Antsaklis and Gonzalez [6] and Gonzalez and Antsaklis [16] [17] [18]  [19]; a good  source  for  this  topic  is  also  Vidyasagar  [34]. Note that  the  main stability  theorem  (Theorem  4.21)  first  appeared  in Antsaklis  [3] and Antsaklis  and Gonzalez  [6]. For  additional  material  on  model  matching  and  decoupling  consult Chen  [10] Kailath  [21] Falb and Wolovich  [12] Williams  and AntsakUs  [35] and the extensive list of references  therein. 7.7 REFERENCES 1.  P. J. Antsaklis "Some Relations Satisfied by Prime Polynomial Matrices and Their Role in Linear Multivariable System Theory" IEEE Trans  on Autom. Control  Vol. AC-24 No. 4 pp. 611-616 August 1979. 2.  P. J. Antsaklis Notes  on: Polynomial  Matrix  Representation  of Linear  Control  Systems 637 3. 4. Pub. No. 80/17 Dept. of Elec. Engr. Imperial College London  1980. P. J. Antsaklis  Lecture Notes  of the graduate  course. Feedback  Systems  University  of Notre Dame Spring  1985. P.  J.  Antsaklis  "Proper  Stable  Transfer  Matrix  Factorizations  and  Internal  System Descriptions" IEEE  Trans  on Autom.  Control  Vol. AC-31  No. 7  pp. 634-638  July 1986. 5.  P. J. Antsaklis "On the Order of the Compensator  and the Closed-Loop Eigenvalues in the Fractional Approach to Design" Int.  J. Control  Vol. 49 No. 3 pp. 929-936  1989. 6.  P.  J.  Antsaklis  and  O.  R.  Gonzalez  "Stability  Parameterizations  and  Stable  Hidden Modes in Two Degrees of Freedom Control Design" Proc.  of the 25th Annual  Allerton Conference  on  Communication  Control  and  Computing  pp. 546-555 Monticello IL Sept. 30-Oct.  2 1987. 7.  P. J.  Antsaklis  and  M.  K.  Sain  "Unity  Feedback  Compensation  of  Unstable  Plants" Proc.  of  the  20th  IEEE  Conf.  on  Decision  and  Control  pp.  305-308  San  Diego December 1981. CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems 10 11 12. 13. 14. 15. 16. 9. 8.  P. J. Antsaklis  and M. K.  Sain  "Feedback  Controller Parameterizations:  Finite  Hidden Modes and Causality" in Multivariable  Control: New  Concepts  and Tools S. G. Tzafes-tas ed. D. Reidel Pub. Dordrecht  Holland  1984. F. M. Callier and C. A. Desoer Multivariable  Feedback  Systems  Springer-Verlag  New York  1982. C. T. Chen Linear  System  Theory  and Design  Holt Rinehart  and Winston New York 1984. C.  A.  Desoer  R.  W.  Liu  J.  Murray  and  R.  Saeks  "Feedback  System  Design:  The Fractional  Approach  to  Analysis  and  Synthesis"  IEEE  Trans  on  Autom.  Control Vol. AC-25 pp. 399-412 June  1980. P.  L.  Falb  and  W.  A.  Wolovich  "Decoupling  in  the  Design  of  Multivariable  Control Systems" IEEE  Trans on Autom.  Control  Vol. AC-12 pp. 651-659  1967. P. A. Fuhrmann "On Strict System Equivalence and Similarity" Int. J. Control Vol. 25 pp. 5-10  1977. F. R. Gantmacher  The Theory  of Matrices  Vol.  1 and 2 Chelsea New York  1959. Z.  Gao  and  P. J.  Antsakhs  "On  Stable  Solutions  of  the  One  and  Two  Sided  Model Matching  Problems"  IEEE  Trans  on Autom.  Control  Vol.  34  No.  9  pp.  978-982 September  1989. O.  R.  Gonzalez  and  P.  J.  Antsaklis  "Implementations  of  Two  Degrees  of  Freedom Controllers" Proc.  of the 1989 American  Control  Conference  pp. 269-273 Pittsburgh PA June 21-23  1989. O. R.  Gonzalez  and P. J. Antsaklis  "Sensitivity  Considerations  in the  Control  of  Gen-erahzed  Plants" IEEE  Trans  on Autom.  Control  Vol. 34 No.  8 pp. 885-888  August 1989. O. R.  Gonzalez  and P. J. Antsaklis  "Hidden  Modes  of  Two Degrees  of  Freedom  Sys tems in Control Design" IEEE  Trans on Autom.  Control  Vol. 35 No. 4 pp. 502-506 April  1990. O.  R.  Gonzalez  and  P. J.  Antsaklis  "Internal  Models  in  Regulation  Stabilization  and Tracking" Int.  J. of Control  Vol. 53 No. 2 pp. 411-430 1991. M.  Green  and  D.  J.  N.  Limebeer  Linear  Robust  Control  Prentice  Hall  Englewood Cliffs  NJ  1995. 17 18 19 20. 21.  T. Kailath Linear  Systems  Prentice-Hall Englewood  Cliffs  NJ  1980. 22.  P. Khargonekar  and  E.  D.  Sontag  "On  the  Relation  Between  Stable  Matrix  Fraction Factorizations  and Regulable Realizations of Linear Systems Over Rings" IEEE  Trans on Autom.  Control  Vol. AC-27 pp. 627-638 June  1982. 23.  V  Kucera Discrete  Linear  Control Wiley New York  1979. 638 Lh^^Systems 24.  V. Kucera  "Diophantine  Equations  in  Control—A  Survey"  Automatica  Vol. 29 pp. 1361-1375 1993. 25.  C. C. MacDuffee  The Theory  of Matrices  Chelsea New York 1946. 26.  J.  M.  Maciejowski  Multivariable  Feedback  Design  Addison-Wesley  Reading  MA 1989. 27.  C. N. Nett  C. A. Jacobson  and M. J. Balas  "A Connection  Between  State-Space and Doubly Coprime Fractional Representations" IEEE  Trans on Autom.  Control Vol. AC-29  pp. 831-832 September 1984. 28.  L. Pernebo "Notes on Strict System Equivalence" Int. J. of Control Vol. 25 pp. 21-38 1977. 29.  L. Pernebo "An Algebraic Theory for the Design of Controllers for Linear Multivariable Systems" IEEE  Trans on Autom.  Control  Vol.  AC-26 pp. 171-194 February 1981. 30.  H. H. Rosenbrock  State-Space  and Multivariable  Theory Nelson London 1970. 31.  H. H. Rosenbrock  "A Comment  on Three Papers" Int. J. of Control  Vol. 25 pp. 1-3 1977. 32.  H. H. Rosenbrock  "The Transformation  of Strict  System Equivalence" Int. J.  Control Vol. 25 pp. 11-19 1977. 33.  A. L G. Vardulakis Linear  Multivariable  Control  Wiley New York 1991. 34.  M. Vidyasagar  Control  System  Synthesis.  A Factorization  Approach  MIT Press Cam bridge MA  1985. 35.  T. Williams  and P. J.  Antsaklis  "Decoupling"  The  Control  Handbook  Chap.  50 pp. 745-804 CRC Press and IEEE Press Boca Raton FL 1996. 36.  W. A. Wolovich Linear  Multivariable  Systems  Springer-Verlag New York 1974. 37.  D. C. Youla H. A. Jabr and J. J. Bongiorno Jr. "Modern Wiener-Hopf Design of Optimal Controllers—Part  II:  The Multivariable Case" IEEE  Trans on Autom.  Control Vol. AC-21  pp. 319-338 June 1976. 38.  G. Zames "Feedback and Optimal Sensitivity: Model Reference  Transformations  Mul tiplicative Seminorms and Approximate Inverses" IEEE  Trans on Autom.  Control Vol. 26  pp. 301-320 1981. 7.8 EXERCISES 7.1.  Given  a polynomial  matrix  P{s) G R[sY^^  with  rankP(s)  =  mm(p  m) write a com puter program to reduce P(s) to a row proper (row reduced) matrix via row elementary operations and to derive the corresponding unimodular matrix  UL(S).  Use your computer algorithm to verify  the results of Example 2.10. Hint:  Apply the algorithm to [P(s) Ip] so that in  UL(S)[P(S) Ip]  =  [P(s)  UL(S)]  P(S) is in row proper  form  and  UL(S) is the appropriate unimodular  matrix. 7.2.  Given  a polynomial  matrix  P(s)  G RlsV^"^ with p  >  m write a computer program to reduce P(s) to column Hermite form and to derive the corresponding unimodular matrix UL(S).  Use your computer algorithm to verify  the results of Example 2.13. Hint:  Apply the algorithm to [P(s) Ip] so that in  UL(S)[P{S) Ip]  =  [P(s)  UL(S)1  P(S) is in Hermite form  and  UL(S)  is the appropriate unimodular  matrix. 7.3.  Consider  A  G R^^^  and let  IAI//'"'!^   be the r  X r  minor  formed  by selecting  r rows I \{ki...kr} -^ ^ denoted by {ii... ir}  and r columns of A denoted by {^i... Z:^} where r <  min(m n). Let B G  R'^^P  and consider the minors of the product AB. These can be determined using \An\iH'-M ^  ^ I .  |{'l.-.^r}  |^|{/i.../r} 639 CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems formula. where  {/i... Ir}  denotes  all possible  sets  of  r integers  among  the  n columns  of  A and  n rows  of 5  with  r < mm{mnp).  This  formula  for  the  minors  of  the  product of matrices is known as the Binet-Cauchy (a)  \iA^R^^^B^R^^^ with m<n  determine  an expression for  detAB. (b)  Show that if A and B are both square then det AB  = det A  det B = det  BA. (c)  Suppose that Pi {s)  and Piis)  polynomial  matrices  of the  same dimensions  are related  by  P\{s)  =  U{s)P2{s)V{s)  where  U{s)  and  V{s)  are  also  polynomial matrices.  Show that the  (monic)  gcd of  all j  x j  minors of P2('^) i-e. the  deter-minantal divisor Dj{s)  of P2{s) divides the gcd of all j  x j  minors of Pi (5)  i.e. the corresponding  determinantal  divisor of Pi {s). (d)  \iU{s)  and y  (5)  (in (c)) are unimodular  show that all the determinantal  divisors Dj{s)  and  therefore  all  the  invariant  factors  ej{s)  of  P\{s)  and  Piis)  are  the same.  That  is  the  invariant  factors  of  a  matrix  are  not  affected  by  row  and column  elementary  operations.  This  also  shows  that  if  Pi (5)  =  U(s)P2(s)V(s) with  U{s)  and  V{s)  unimodular  then  Pi{s)  and  P2{s) have  precisely  the  same Smith  form.  It  is  straightforward  to  also  show  the  opposite  i.e.  if  Pi (s)  and P2{s) have the same Smith form  then there exist unimodular  matrices  U{s)  and V{s)  such that Pi (s)  =  U{s)P2{s)V{s)  (see Subsection  7.2C). 7.4. Given  a polynomial  matrix  P{s)  e  R[S]P^^  with  p  > m  write  a computer  program to  determine  a  gcrd  G^{s)  of  all  the  rows  of  P{s)  and  to  derive  the  corresponding unimodular matrix U{s).  Use your computer program to determine a gcrd and a geld of  the  matrices  Pi{s)  and  P2{s)  of  Example  2.15.  Hint:  Determine  a  unimodular matrix  U{s)  such  that  U{s)P{s) .  Apply  your  algorithm  to  [P(s)  Ip] so 0 that  U{s)[P{s)Ip G%{s) 0 U{s) 7.5.  Consider the polynomial matrices P{s) -s^  + s - . 2 -1 R{s)-s -s-l (a)  Are they re? If they  are not find a greatest common right divisor (gcrd). (b)  Are they Ic? If they are not find a geld. 7.6.  (a)  Show that two square and nonsingular polynomial matrices the determinants of which  are coprime  polynomials  are both re  and Ic. Hint:  Assume  they  are  not say re and then use the determinants of their gcrd to arrive at a contradiction (b)  Show  that  the  opposite  is not  true  i.e.  two re  (Ic) polynomial  matrices  do  not necessarily  have determinants that are coprime  polynomials. 7.7.  (a)  Show  that  if  (the nonsingular)  G^^ (s)  and  G^^ (s)  are both  gcrds  of  Pi  and P2 then there exists a unimodular  matrix  UR{S)  such that G^  (s)  =  UR{S)G^ (S). (b)  Similarly  show  that  if  the  nonsingular  G^  (s)  and  G|^ (s)  are  both  gelds  of Pi  and  P2  then  there  exists  a  unimodular  matrix  UL{S) such  that  G£  (5)  = GI{S)UL{S). 7.8.  Show that v  defined  in (2.43) is indeed the observability  index of the  system. 7.9.  Let P{s)  =  P„5"  + P„_i5"-i  +  • • • +Po  be a matrix polynomial with Pi  e  P"><" and let A  G P"><". Show  that (a)  P{s)  =  Qr{s){sl-A)  +Rr  withRr  = P„A" + P„_iA"-i  +  • (b)  P{s)  =  {sI-A)Qi{s)+Ri withRi  =A"P„+A"-ip„_i  + • •+Po •+Po. Hint:  Qr{s)  = PnS^'^ +  (P„A +  P„_i)5"-^ +  • • • +  {PnA^'^  +  • • • + P i ). 640 Linear  Systems 7.10. Let P(s) be a polynomial matrix of full  column rank and let y(s) be a given  polynomial vector. Show that the equation  P(s)x(s)  = y(s) will have a polynomial  solution x(s) for any y(s) if and only if the columns of P(s)  are Ic or equivalently if and only if P(A) has full  column rank for any complex number A. 7.11.  Let P(s)  =  PdS^ + Pd-is"^'^  + • • • + Po ^  R[s]P'''^ and let Pe(s)  =  block  diag (I^... Imy P(s)) with d blocks  on the diagonal. It can be shown  that by means  of elementary row and column operations Pe(s) can be transformed  to a (linear) matrix pencil  sE -  A. In  particular Pe(s)  =  U(s)(sE  -  A)V(s) where E and A are real matrices  given by E  = block  diag(Pd  Im •  •> Im) A  =  • -Pd-i / 0 •" ••• -Pi 0 -Po 0 ••• / 0 and  U(s) and V(s) are unimodular  matrices given by I si s^-'l y(s) U(s)  = si I 0 Bi(s) -I Bd-i(s) with  Bi+i(s)  =  sBi(s)  +  Prf-o+i) /  -  1... J  -  2 and Bi(s)  =  sPd +  Pd-i-  Note that  P(s)  and sE  -  A  have  the  same  nonunity  invariant  polynomials.  Let  P(s)  = s^  -\- s  —s 1 and obtain the equivalent matrix pencil sE -  A. 7.12. Let {DR I NR} and {DL NL I] be minimal realizations of H(s)  where H(s)  •-^  NRDI'  = DI^NL X NL with Fl DL\ {NRPR) -Y X and /  where  U is unimodular  i.e. they  are doubly co-prime factorizations  of H(s).  Show that these realizations are equivalent representations. Hint: re I  0 0 related  by \DR [NR (PLNL) and Ic NL  0 0 / Y  0 X I DR -NR I 0 DL -I NL 0 DL I NL 0 DR -NR / 0 NR  0 / 0 -X / Y 0 and also 7.13.  Consider  P{q)z{t)  =  Q{q)u{t)  and y{t)  =  R(q)z(t)  +  W(q)u(t)  where P{q)  = R(q)  = q  -0 \q  -q [-q-2 \2q^  + q + 2 -q-2 Q(q)  = W(q)  = q -  1 -2q  + 2 1 3q 1  3q + 4 1 -3q Iq 0 with q  =  d/dt. (a)  Is this system representation  controllable? Is it observable? (b)  Find the transfer  function  matrix//(>y)(y(5')  =  H(s)u(s)). (c)  Determine  an equivalent  state-space  representation  x  =  Ax  + Bu  y  =  Cx  +  Du 641 and repeat (a) and (b) for this  representation. 7.14. Use system theoretic arguments to show that two polynomials d{s)  =  s"^  + dn-is""'^  + \-dis  -\-do and n(s)  =  n„_i5'"~^ +  nn-2S^~^ + \- nis  -\- no art coprime if and only if Cc rank =  n where Ar  = 0 0 1 0 0 1 0 -do 0 -di 0 -d2 CcAT' 0 0 1 -dn-\ a n dQ  =  [no n\... nn-\\. CHAPTER?: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems 7.15. Consider the transfer function  H{s) s s+  1 Determine a minimal real ization in (a)  Polynomial Matrix Fractional Description  (PMFD)  form (b)  State-Space Description  (SSD)  form. 7.16. In the following  assume that a PMD [P Q R W} realizes an invertible mXm transfer function  matrix  H. (a)  Show that the system matrix for H~^  is given by P  Q -R  W 0 Im 0 0 Hint:  Note thaiSdz^ -w^  - / ]^  =  [00 -u'^f  when Pz  =  Quy  =  Rz  +  Wu. (b)  Show that the following  system matrix can also be used to characterize  / / ~\ R P -W Q SM  — 0 0 (c)  Let  detW  ^  0.  Show  that  H'^  =  RP'^Q  -H W  where  W  =  W'K  Q  =  QW'K R  =  -W-^R  mdP  =  P  +  QW-^R. 7.17. Consider the system Dz  =  u y  =  Nz  where D  = 0 0 andA^  =  [s^ -ls+ 1]. (a)  Determine an equivalent  state-space  representation. (b)  Is  the  system  controllable?  Is  it  observable?  Determine  all  uncontrollable  and/or unobservable eigenvalues if any. (c)  Determine the invariant  and the transmission  zeros of the  system. 642 Linear Systems 7.18. Consider the series connection depicted in Fig. 7.2 and the PMFDs for ^i and ^2 given in (3.63) (3.64) and (3.66) (3.67). (a)  Show that the system S is controllable if and only if (i)  (A^i D2) is Ic or (ii)  (5iZ)2M)islc or (iii)  (N2NiD2)is\c (b)  Determine similar conditions [as in (a)] for S to be observable. 7.19. Consider the parallel connection depicted in Fig. 7.1 and the PMFDs for ^i and S2 given in (3.52) (353) and (3.55) (3.66) Derive conditions for controllability and observabil ity of S analogous to the ones derived for the series connection in Exercise 7.18. 7.20. Consider the double integrator//i  = l/s'^. (a)  Characterize all stabilizing controllers H2 for Hi using all the methods developed in Subsections 7.4A and 7.4C. (b)  Characterize all proper stabilizing controllers H2 for Hi of order 1. 7.21. Consider the double integrator//i  =  l/s^. (a)  Derive a minimal state-space realization for Hi and use Lemma 4.19 to derive dou bly coprime factorizations in RHoo. (b)  Use the polynomial Diophantine Equations (4.102) and (4.106) to derive factoriza tions in RHoo. 7.22. Consider// s^ + I s+1 (a)  Derive a minimal state-space realization {A B C D} and use Lemma 4.19 and The orem 4.13 to parameterize all stabilizing controllers H2. (b)  Derive a stabilizing controller H2 of order 3 by appropriately  selecting K'.  What are the closed-loop eigenvalues in this case? Comment on your results. Hint: A minimal state-space realization was derived in Example 3.2. The eigenval ues of A + BF and A -  KC  are stable but otherwise arbitrary. Note that some of these eigenvalues become closed-loop eigenvalues. 7.23. Consider the unity feedback (error feedback) control system depicted in Fig. 7.19 where H and C are the transfer function matrices of the plant and controller respectively. r +  ^^ e a FIGURE 7.19 Assume that (/  + HC)  ^  exists (a)  Verify the relations y = {1 + HCy^HCr  + {I + HCT^d  ^  Tr-\-Sd u  = (1 + CHY^Cr  -  (/  + CHY^Cd  = Mr  -  Md. Compare  these  with  relations  (4.163)  to  (4.168)  for  the  two  degrees  of  freedom controller  u  =  Cyy  +  G r.  Note  here  that  u  =  -Cy  +  Cr  and  therefore  Cy  = -C  and  Cr  =  C.  Hence for  the  error  feedback  system  of Fig. 7.19 the  relations following  (4.168) assume the  forms M  =  (/  +  CHY^C  =  DX  =  -Q  = T  =  H(I  + CHY^C  =  (/  +  HCY^HC  =  HM  =  NX So  =^  (1 + HCY^  =  I  + HQ  =  I-  HM  =- Si  =  (/  +  CHY^  =  I  + QH  =  I  -  MH. -DL I-T 643 CHAPTER  7: Polynomial Matrix Descriptions and Matrix Fractional Descriptions of  Systems (b) (i)  Let  H  =  ND~^  be  an  re  polynomial  factorization.  Show  that  all  stabilizing controllers are given by C  =  [(/ -  XN)D-^T^X where [{I-XN)D~^  X] is stable and (I-XNY^ 4.22 to the error feedback  case exists. Hint:  Apply Theorem (ii)  If  H  is  proper  and  H  =  N'D' is  an re  MFD  in  RH^  show  that  all  proper stabilizing controllers are given by C =  [(/ - X'N')D' \'X\ where  [(/  -  X'N')D' Hint:  Apply Theorem 4.22 to the error feedback  case. \  X']  G  RH^  and  (/  -  X'N'Y^  exists  and  it is  proper. v -l (c)  Assume  that  H  is  proper  and  H~^  exists  i.e.  H  is  square  and  nonsingular.  Let H  =  ND~^  be  an  re  polynomial  MFD.  If  T  is  the  closed-loop  transfer  function between y  and r show that the system will be internally  stable if and only if [N-\I-T)HN~^T] is stable. Assume that T  ^  I  for the loop to be well defined. Note that if T is proper then C  =  H-^T(I-TY^ is proper if and only if H~^T  is proper and I  -  T  is biproper. (d)  Assume that in  (c) H  and  T are  SISO transfer  functions.  Let H  =  n/d.  Show  that the closed-loop  system will be stable if and only if (1 -  T)d  -1  _  Sd-and Tn-are stable i.e. if and only if the sensitivity matrix has as zeros all the unstable poles of the plant and the closed-loop transfer  function  has as zeros all the unstable zeros of the plant. (e)  Given  His)  = Remark:  Note that this is a result known in the classical control literature  (refer to J. R. Ragazzini and G. F. Franklin Sampled  Data  Control Systems  McGraw-Hill 1958). It is derived here by specializing the more general MIMO case results to the SISO case. 5 -1 ^rrz  —  characterize  all  scalar  proper  transfer  functions  T that can be realized  via the error feedback  configuration  shown in Fig. 7.19 under internal  stability. For comparison  purposes  characterize  all  T that  can be  realized via a two degrees of freedom  controller and comment on your results. In both cases comment  on  the  location  and  number  of  the  closed-loop  eigenvalues.  Hint:  Use Theorems 4.22 and 4.24. 644 Linear  Systems 7.24. Consider/^W (s -  l)(s  +  2) (s  -  2)2 and  characterize  all proper  and  stable  transfer  func-tions  T(s)  that  can  be realized  via  linear  state  feedback  under  internal  stability.  Hint: From Subsection  7.4B note that T NDp^G  and consider Theorem 4.23. 7.25.  Consider//  = 1 s+  1 1 5  +  3 1 L^H-  1 5  +  l-J (a)  Derive an re MFD in  RHo.  H  =  N'D'~ (b)  Let  T r ^l ^1 0 0 1 ni ^2-i and characterize  all diagonal  T that  can be realized  under  in ternal stability via a two degrees of freedom  control  configuration (c)  Repeat  (b) for a unity feedback  configuration  {/; G//  /} (see Fig. 7.15). 7.26. In the model matching problem the transfer function  matrices H  E  RP^^{S)  of the plant and  T  G  RP'^'^is) of the model  must be found  so that  T  =  HM  [see (4.192)]. M  is to be  realized  via  a  feedback  control  configuration  under  internal  stability.  Here  we  are interested  in the model  matching  problem  via  linear  state feedback.  For this let H  = ND~^  an re polynomial factorization  with D column reduced. Then Dz  =  u y  =  Nz  is a minimal realization of H.  Let the state-feedback  control law be defined  by  u  =  Fz  + Gr  where  F  G  Rlsr"""^  G  G  /?^><^ with  detG  y^O and deg^. F  <  deg^.  D.  To  allow additional  flexibility  let r  =  KvK  ^  R"^""^. Note that (see Subsection 7.4C)  HFGK  = NDp^GK  =  (ND-^)(DDp^GK)  =  {ND-^){D{G-^DF)-^K] =  HM. In view of the above solve the model matching problem via linear state  feedback determine F G and K  and comment on your results  when (a)  H (b)  H (c)  H  = {s +  \){s  +  2) 2^2 -  3^ +  2  ' T 5+  1 s + 2 ^ 0 s 1 s s + 2 s+  1 1 L5+  1 -^ + 2 s S  +  3-] s + 2 0 T  =  h s+  1 5 +  4 -2 .(5 +  2)(5 +  4)J //mr.'  The  model  matching  problem  via  linear  state  feedback  is  not  difficult  to  solve  when p  =  mmdrankH =  m in view of (G-^D^)"^/^  =  D'^M  =  D'^H-^T  =  N'^T. APPENDIX Numerical  Considerations A.l INTRODUCTION To compute the rank of the controllabiHty matrix [B AB...  A^~^B\  or the eigen values of A or the zeros of the system {A B C D] typically requires use of a digital computer. When this is the case one must deal with selection of an algorithm and interpret numerical results. In doing so two issues arise that play important roles in numerical computations using a computer namely the numerical stability or insta bility of the computational method used and how well or ill conditioned iht problem is numerically. An example of a problem that can be ill conditioned is the problem of calculat ing the roots of a polynomial given its coefficients.  This is so because for certain polynomials small variations in the values of the coefficients  introduced say via round-off  errors can lead to great changes in the roots of the polynomial. That is to say the roots of a polynomial can be very sensitive to changes in its  coefficients. Note that ill conditioning is a property of the problem to be solved and depends on neither the floating-point system used in the computer nor on the particular algorithm being implemented. A computational method is numerically stable if it yields a solution that is near the true solution of a problem with slightly changed data. An example of a numer ically unstable method to compute the roots of ax^ + Ibx  +  c  =  0 is the formula {-b  ±  JQP-  — ac))la that for certain parameters a b c may give erroneous results in finite arithmetic. This instability is due to subtraction of two approximately equal large numbers in the numerator when b^ »  ac. Note that the roots may be calcu lated in a numerically stable way using the mathematically equivalent but numeri cally very different  expression c/(-b^ J(b^  -  ac)). 645 646 APPENDIX: Numerical Considerations We would of course always like to use numerically stable methods and we would prefer to have well-conditioned problems. In the following section we briefly  discuss the problem of solving a set of algebraic equations given by Ax  =  b. We will show that a measure of how ill conditioned  a given problem is is the size of the  condition number  (to be  defined)  of the matrix A.  There  are many  algorithms  to  numerically solve Ax  =  b and we will briefly  discuss numerically  stable ones. Conditioning  of  a problem  and  numerical  stability  of  a method  are key  issues in  the  area  of  numerical  analysis.  Our  aim  in  this  appendix  is  to  make  the  reader aware  that  depending  on  the  problem  the  numerical  considerations  in  the  calcu lation of a solution may be nontrivial. These issues  are discussed  at length in many textbooks on numerical analysis. Examples of good books in this area include Golub and Van Loan [6] and Stewart [9] where matrix computations are emphasized. Also see Petkov  et  al.  [8]  and  Patel  et  al.  [7] for  computational  methods  with  emphasis on  system  and  control  problems.  For  background  on  the  theory  of  algorithms  on optimization  algorithms  and  their  numerical  properties  see Bazaran  et  al.  [2]  and Bertsekas and Tsitsiklis [3]. In Section 2 we present methods for solving linear algebraic equations. Singular values  and  singular-value  decompositions  are discussed  in  Section  3. In  Section  4 an  approach  for  solving  polynomial  matrix  and  rational  matrix  equations  based  on polynomial matrix interpolation  is presented. A.2 SOLVING  LINEAR  ALGEBRAIC  EQUATIONS Consider the set of linear algebraic equations given by Ax  =  b (2.1) where  A  E  R^^^  Its  solution  is important  in  many  engineering  problems.  It is of interest  to know  the  effects  of  small  variations  of A  and  b to the  solution x  of  this system  of  equations.  Note  that  such  variations  may  be  introduced  for  example  by rounding errors when calculating  a solution or by noisy  data. Condition  number Let A  E  R^^^  be nonsingular. If A is known exactly and b has some uncertainty AZ? associated  with  it  then  A{x  +  Ax)  =  b  + Afo. It  can  then  be  shown  that  the variation in the solution x is bounded by llAxll cond (A) \m\ (2.2) where | notes the condition  number  of A  where cond (A) denotes any vector norm (and consistent matrix norm) and cond (A) de-^  ||A||||A-i||. Note that cond{A) (Tn  .(A) .(AY (2.3) where  crjnax(^)  and  o-mm(A)  are  the  maximum  and  minimum  singular  values  of A  respectively  (see  next  section).  From  the  property  of  matrix  norms  ||AA~^||  < 647 APPENDIX: Numerical Considerations ||A||||A~^||  it  follows  that  cond(A)  >  1. This  also  follows  from  the  expression  in volving  singular  values. If  cond{A)  is  small then A  is  said  to be well  conditioned with  respect  to  the  problem  of  solving  Hnear  equations.  If  cond{A)  is  large  then A is  /// conditioned  with respect  to the problem  of  solving  linear  equations. In  this case the relative uncertainty  in the  solution  (||Ax||/||x||)  can be many  times  the rela tive uncertainty  in &(||AZ?||/||Z7||). This is of course undesirable. Similar results can be derived  when variations  in both b and A  are considered  i.e. when b and A  become Z?  -h AZ?  and A  +  AA. Note that the conditioning  of A and of the given  problem  is independent  of the algorithm used to determine a solution. The condition number of A provides a measure of the distance of A to the set of singular  (reduced  rank)  matrices. In particular  if  ||AA|| is the norm  of the  smallest perturbation AA such that A + AA is singular and is denoted by d{A)  then (iA/||A||  = l/cond  (A). Thus a large condition number indicates a short distance to a singularity and  it  should  not  be  surprising  that  this  implies  great  sensitivity  of  the  numerical solution xof  Ax  =  bio  variations in the problem  data. The condition number of A plays a similar role in the case when A is not square. It can be determined in terms of the singular values of A defined  in the next  section. Computational  methods The  system  of  equations  Ax  =  b is  easily  solved  if A has  some  special  form (e.g. if it is diagonal or triangular). Using the method of Gaussian  elimination  any nonsingular matrix A can be reduced to an upper triangular matrix  U. These  opera tions can be represented by premultiplication of A by a sequence of lower triangular matrices. It can then be shown that A can be represented  as A  =  m (2.4) where L is a lower triangular  matrix with all diagonal elements equal to  1 and  U is an upper  triangular  matrix. The  solution  of  Ax  =  b is then reduced  to the  solution of  two  systems  of  equations  with  triangular  matrices  Ly  =  b  and  Ux  =  y.  This method of solving Ax  =  bis  based on the decomposition  (2.4) of A which is called the LU decomposition  of A. If A is a symmetric positive definite  matrix then it may be represented  as A  =  U^U (2.5) where U is an upper triangular matrix. This is known as the Cholesky  decomposition of  a positive  definite  matrix.  It can  be  obtained  using  a variant  of  Gaussian  ehmi-nation. Note that this method requires half of the operations necessary  for  Gaussian elimination on an arbitrary nonsingular matrix A since A is  symmetric. Now  consider  the  system  of  equations  Ax  =  b  where  A  G  j^^nxn^ ^^^ j ^^ rank A  =  n(^  m).  Then A  =  Q =  [Qh  Qi\ =  QiR. (2.6) where  Q is an orthogonal matrix  (Q^  =  Q~^)  and R  G R^^^  is an upper  triangular matrix  of full  rank n. Expression  (2.6) is called  the  QR decomposition  of A. When rank A  =  r the QR  decomposition  of A is expressed  as AP  =  Q Ri  R2 0 0 (2.7) 648 APPENDIX: Numerical Considerations where  Q is  orthogonal  Ri  G  R^^^  is  nonsingular  and  upper  triangular  and  P  is  a permutation  matrix  that represents  the moving  of the columns  during  the  reduction (in  Q^AP). Q R decomposition can be used to determine solutions of Ax  =  b. In particular consider A  G R^^^  with rank A  =  n(<  m) and assume that a solution exists. First determine the 2/^ decomposition ofA  given in (2.6). Then 2^Ax  =  Q^box\  0  X  = Q^b  (since  Q^  =  Q'^)  or Rx  =  c. Solve this  system  of equations where R  is tri angular  and  c  =  [In 0]Q^b.  In the general  case  when  rank (A)  =  r  <  mm(n  m) determine  the  QR  decomposition  of A (2.7) and  assume that  a solution  exists. The r DI solutions  are given  by  x  =  P\ arbitrary. \R^\c-R2y)] y c  =  [In OJG^Z? where  y  G  R"^''  is A related problem is the linear  least  squares problem  where a solution x  of the system of equations Ax  =  bis  to be found that minimizes  \\b -  Ax\\2. This is a more general  problem  than  simply  solving  Ax  =  b  since  solving  it provides  the  "best" solution  in  the  above  sense  even  when  an  exact  solution  does  not  exist.  The  least squares problem is discussed further  in the next  section. A.3 SINGULAR  VALUES  AND  SINGULAR-VALUE  DECOMPOSITION The  singular  values  of  a  matrix  and  the  Singular  Value  Decomposition  Theorem play a significant role in a number of problems of interest in the area of systems and control from the computation of solutions of linear systems of equations to compu tations of the norm of transfer  matrices at specified  frequencies  to model reduction and  so forth.  In  the following  we provide  a brief  description  of  some basic  results and introduce  some terminology. Consider  A  E  %^^^  and  let  A*  =  A^  the  complex  conjugate  transpose  of A. A  G ^"><^ is  said  to be Hermitian  if  A*  =  A. If  A  G  /^^x^  then  A*  =  A^  and if A  =  A^ thenA is ^jmm^mc. A G ^"^'^ is wn/^ryif  A*  =  A~^ In this case A*A  = AA"" =  4.  If A G /^'^xnhenA*  =  A^andif  A^  =  A'^ i.e. if A^A  =  AA^  =  In then A is orthogonal  (refer  to Section  6.2). Singular  values Let  A  G  ^^^^'^  and  consider  AA''  G  ^ ^ x ^.  Let  A/ /  =  1...  m  denote  the eigenvalues  of AA''  and note that these  are  all real  and nonnegative  numbers. As sume  that  Ai  >  A2 ^  ---A^  >  •••  >  A;„. Note  that  if  r  === rank A  ===  rankiAA'') then  Ai  >  A2 ^  • • • ^  A^ >  0 and  A^+i  =  • • •  =  A^  =  0.  The  singular  values  at of A  are  the  positive  square  roots  of  A/ /  =  1... min(m n).  In  fact  the  nonzero singular values of A are A/2 CTi  =  (KY /  =  1 . >  r. (3.1) where  r  =  rank A  while the remaining  (min(m n)  -  r) of the  singular  values  are zero. Note that  cri  >  0*2 ^  • • • ^  cr^ >  0  and  cr^+i  =  (j^+2  =  "  ^  o-mm{mn)  = 0. The  singular  values could have  also been found  as the square roots of the eigen values of A* A G %''^'' (instead of AA* G ^"^x^). To see this consider the  following result. LEMMA 3.1.  Letm n. Then AA*| =  A'^-"|A/„ -  A*A| (3.2) i.e.  all eigenvalues  of A*A are eigenvalues  of AA* which  also has  m -  n additional eigenvalues  at zero. Thus AA* E ^'w><^ and A*A E ^"^^  have precisely  the same r nonzero eigenvalues  (r  =  rank A); their remaining  eigenvalues  (m -  r) for AA* and (n —  r) for A*A are all at zero. Therefore either AA* or A*A can be used to determine the r nonzero singular values of A. All remaining singular values are zero. Proof of the lemma.  The proof is based on Schur's formula for determinants. In partic ular we have 649 APPENDIX: Numerical Considerations \X^'^J ^''^ = \J^  A\ A  I |Ai/^/^||Ai/2/-A*A-i/^/^A| \X'^'aX-'^W\\In-A*A\ = =  A(^-«)/2 . |A/„ -  A*A| (3.3) where Schur's formula was applied to the (11) block of the matrix. If it is applied to the (2 2) block then Equating (3.3) and (3.4) we obtain |A/^ -  AA*| = A^-"|A/„ D(A)  =  A^«-^V2 . |A/^ -  AA*|. (3.4) A*A| which is (3.2).  • EXAMPLE  3.1 |o  0  0 E  R^''\  Here rank A  =  r  =  1 A^AA*) -  Ar =  {50} and Ai  =  5 A2 =  0. Also A/(A*A)  = =  A. "4 2 .0 2 1 0 0" 0 0_ and  Ai  =  5  A2 =  0  and  A3 =  0. The only nonzero singular value is ori  =  J~\i  =  + V5. The remaining singular values are zero. • There is an important relation between the singular values of A and its induced Hilbert or 2-norm also called the spectral norm ||A||2  =  \\A\\s In particular l|A||2(-)  =  sup  \\Ax\\2  =  max{(AKA*A))i/2}  =  a(A) M2 = l (3.5) where  &(A)  denotes  the largest  singular  value  of A. Using  the inequalities  that  are axiomatically true for induced norms (see Subsection  I.IOB) it is possible to estab lish relations between  singular  values of various matrices that are useful  in  MIMO control  design.  The  significance  of  the  singular  values  of  a  gain  matrix  A(ja))  is discussed  later in this  section. There is an interesting relation between the eigenvalues and the singular values of  a (square)  matrix. Let  A/ /  =  1...  n denote  the eigenvalues  of A  E  R^^^^  let A(A)  =  min/ |A/| and let A(A)  =  max/ |A/|. Then a(A)  <  A(A) <  A(A) <  &(A). (3.6) Note that the ratio a(A)/a(A) i.e. the ratio of the largest and smallest  singular values of A is called the condition  number  of A  and is denoted by cond{A).  This is a very useful measure of how well conditioned a system of linear algebraic equations 650 APPENDIX: Numerical Considerations Ax  =  bis  (refer  to the discussion in previous  section). The singular values provide ^ reliable  way  of  determining  how  far  a  square  matrix  is  from  being  singular  or a  nonsquare  matrix  is  from  losing  rank.  This  is  accomplished  by  examining  how close to zero a(A)  is. In contrast the eigenvalues  of a square matrix  are not a good indicator of how far  the matrix is from  being  singular and a typical example in the literature to illustrate this point is an n  X n lower triangular matrix A  with  -  I's  on the diagonal  and  H-l's everywhere  else. In this case a(A)  behaves  as  1/2"  and  the matrix is nearly  singular for large n while all of its eigenvalues are at  - 1.  In fact  it can be shown that adding  1/2""^ to every element in the first column of A results in an exactly  singular matrix (try it for  n  =  2). Singular-value  decomposition Let A  E  ^^^^  with  rank A  =  r  <  min(m n). Let A*  =  A^ the complex  con jugate transpose of A. THEOREM 3.1.  There exist unitary matrices U G ^'"^'^ and V E ^"^"^ such that A  =  UXV\ (3.7) where  2  = ^r • : ^rX{n-r) with  Sr  =  diag(ai  (72.. .crr)  E  R^^^ se-LO(m-r)Xr • ^(m-r)X(n-r)i lected so that o-i  >  0-2 ^  • • •  >  o"^ > 0. Proof. For the proof see for example Golub and Van Loan [6] and Patel et al. [7]. • Let  U  =  [Uu  U2] with  Ui  E  ^'"^^  U2 E  ^^^(^-') and  V  =  [Vi  V2]  with Vi  E  ^'^^^^ V2 E  ^"XC"-'-). Then A  =  f/SV*  =  UiXrVl Since  U and  V are unitary we have u*u = -y*-v*v = [UlU2]  =ImU\U =Ir [Vi  V2]  =  /„. VlVx  =  /. and (3.8) (3.9) (3.10) Note  that  the  columns  of  Ui  and  Vi  determine  orthonormal  bases  for  2/l(A)  and 9l(A*) respectively. Now AA"" =  (UiXrVDiVarUl) IjT* =  UiXjUl (3.11) from  which we have AA*[/i  =  Ui^jUlUi  =  UiXl If  Ui i  =  1...  r is the ith column of  C/i i.e.  Ui  =  [u\ U2...  Ur] then AA*Ui  =  ajui i  =  l...r. (3.12) (3.13) This shows thattheo"? are thernonzeroeigenvaluesofAA*i.e.CT/  /  =  1...  rare the nonzero singular values of A. Furthermore  M/ /  =  1...  r are the eigenvectors of AA*  corresponding to cr?. They are the left singular  vectors  of A.  Note that the ut are orthonormal vectors (in view of  UIU\  =  /r). Similarly 651 APPENDIX: Numerical Considerations A'^A  =  (Vi^rUlXUiXrVl) =  ViXjVl (3.14) from  which we obtain A*AVi  =  ViS^Vt^i  = Vill (3.15) If v„ /  =  1... r is the /th column of Vi  i.e.  V\  =  [vi V2. Vr] then A*Avi  =  crjvi i  =  12... r. (3.16) The vectors v/ are the eigenvectors of A*A corresponding to the eigenvalues a*?. They are the right singular  vectors  of A. Note that the vt are orthonormal vectors (in view 0fVlVi=Ir) The  singular  values  are unique while the singular  vectors  are not.  To see this consider Vi  =  VidiagieJ^O and f/i  = Uidiagie'^^^ Their columns are also singular vectors of A (show this). Note also that A  =  t/iSrVj  implies  that A  = ^diUiv]. / =i (3.17) The significance of the singular values of a gain matrix A{jo))  is now briefly dis cussed. This is useful  in the control theory of MIMO systems. Consider the relation between  signals y and v given by j  =  Av. Then max  I*  =  max  MHb  =  ^ ( ^) IHl2^o||v||2 IMb^o ||v||2 or . m ax  \\y\\2  =  m ax  ||Av||2  =  &(A) IM|2 = 1 IH|2 = 1 (3.18) Thus cr(A) yields the maximum  amplification  in energy terms (2-norm) when  the transformation  A  operates on a signal v. Similarly Therefore min  Ibib  =  min  ||Av||2  =  cr(A). IH|2 = 1 ||v||2 = l QL(A) llAvlb IMb (T(A) (3.19) (3.20) where  ||v||2  T^ 0.  Thus  the  gain  (energy  amplification)  is  bounded  from  above and below by d-(A) and gi(A) respectively. The exact value depends on the direction of V. 652 APPENDIX: Numerical Considerations To  determine  the  particular  directions  of  vectors  v for  which  these  (max  and min) gains are achieved consider (3.17) and write Av  =  ^ aiUiv]v. (3.21) Notice  that  |v*v| <  ||v/||||v||  =  ||v|| since  ||v/||  =  1 with  equality  holding  only  when V =  avia  E  ^.  Therefore  to  maximize  consider  v  along  the  singular  value directions  vt  and  let  v  =  avi  with  |a|  =  1  so  that  ||v||  =  1.  Then  in  view  of v]vj  =  0 /  #  7 and v]vj  =  I  i  =  j we have that y  =  Av  =  aAvi  =  aaiUi  and \y\i  =  \\Av\\2  =  cTi since  ||w/||2  =  1. Thus the  maximum  possible  gain  is cri  i.e. max  ||j||2  =  max  ||Av||2  =  ai(= hh = 1 occurs  when  v is  along  the right  singular  vector  vi. Then  Av  -  Av\  — a\U\  =  y in view  of (3.17) i.e. the projection  is along the left  singular  vector  u\  also of the same  singular  value  a\.  Similarly  for  the  minimum  gain  we  have  ar  =  ^(A)  = min  ||j||2  =  min  ||Av||2 in which case Av  =  Avr  =  (TfUr  =  y. IMb = 1 (T(A))  as was shown above. This maximum gain llvlb = 1 Iklb = 1 Additional interesting properties  include gi(A)  =  9l(f/i)  =  span{ui...  w^} J{(A)  =  91(^2)  =  span{vr+u • •.  v j (3.22) (3.23) where  U  =  [MI  ..  Ur Ur+h '"Um\  =  [Ui  U2]  and  V  =  [vi... Vr v^+1.. Least squares  problem Consider  now  the  least  squares  problem  where  a  solution  x  to  the  system  of linear  equations  Ax  =  b  is  to  be  determined  that  minimizes  \\b -  Ax||2.  Write min  \\b -  Ax\\l  =  mm(b  -  Axf{b -  2b^Ax  +  b'^b). Then -  Ax)  =  mm(x^A^Ax XX X Vjcix'^A'^Ax  -  Ib^Ax  +  b^b)  =  IPJAx  ~  IPJb  =  0 impUes that the x that mini mizes  \\b —  Ax\\2 is a solution of A^AJC  = A^b. (3.24) in  view  of  (3.14)  and Rewrite  this  as  ViX^.Vjx  =  (UiXrViVb  =  Vi%Ulb (3.8).  Now  X  =  ViX~^Ufb is  a  solution.  To  see  this  substitute  and  note  that VjVi  =  Ir. In view  of the fact  that X(A'^A)  =  J{(A)  =  ^(¥2)  =  span{vr+h  • • • v„} the complete  solution is given by x^  - ViX;^Ujb-^V2W for  some w  E  R"^~^. Since  Vi^'^Ujb is orthogonal to  V2W for  all w xo =  ViX^Ulb is the optimal solution that minimizes  ||fc -  Ax||2 (prove this). The Moore-Penrose  pseudoinverse  of A  E  R^^^  can be shown to be A*  =  ViS.-'f/ 1  • (3.25) (3.26) (3.27) It was seen that x  =  A ^Z? is the solution to the least squares problem. Furthermore it  can  be  shown  that  this  pseudoinverse  minimizes  \\AA^  -  Imllf  where  \\A\\F  de notes  the  Frobenius  norm  of A  that  is  equal  to  the  square  root  of  trace[AA^]  — =  Xr=i^^(^)-  It  is  of  interest  to  note  that  the  Moore-Penrose YJ^^ihiAA^) pseudoinverse  of A  is  defined  as the  unique  matrix  that  satisfies  the  conditions  (i) AA+A  =  A (ii) A+AA+  =  A+ (iii) (AA+f  =  AA+  and (iv) (A+Af  =  A+A. 653 APPENDIX: Numerical Considerations Note that if rank A  =  m  ^  n then it can be shown that A+  =  A'^(AA^y^; is in fact  the  right  inverse  of A  since A(A^(AA^y^) n^  m then A+  =  (A'^Ay^A'^ the left inverse  of A  since ((A'^Ay^A'^)A  = In-this =  Im- Similarly if  rank A  = Singular values and singular-value decomposition  are discussed in a number of references.  See for  example  Golub  and  Van Loan  [6] Patel  et al.  [7] Petkov  et al. [8]andDeCarlo[4]. A.4 SOLVING  POLYNOMIAL  AND  RATIONAL  MATRIX  EQUATIONS USING  INTERPOLATION  METHODS Many  system  and control problems  can be formulated  in terms  of matrix  equations where polynomial  or rational  solutions  with  specific  properties  are of interest.  It is known that equations involving just polynomials can be solved by either equating co efficients  of equal power of the indeterminate s or equivalently by using the values obtained  when  appropriate values for  s are substituted  in the given polynomials. In the latter case one uses results from the classical theory of polynomial  interpolation. Similarly  one may  solve polynomial  matrix  equations  using  the theory  of polyno mial matrix interpolation. This approach has significant  advantages. Full details can be found  in AntsakHs and Gao  [1] (see also Gao and AntsakHs [51). First  some required  results  from  the theory  of polynomial  and  rational  matrix interpolation  are briefly  summarized.  These are then used to determine  solutions of polynomial and rational matrix  equations. Polynomial matrix  interpolation Consider first the polynomial case. The following  is a fundamental  result of the theory of polynomial interpolation: given / distinct complex scalars Sj j  =  1...  / and  /  corresponding  complex  values  bj  there  exists  a  unique  polynomial  q(s)  of degree /  -  1 for  which q(sj)  =  bj j  =  1...  /. (4.1) Thus an nth-degree polynomial  ^(.s*) can be uniquely represented by the /  =  n 4- 1 interpolation  (points or doublets or) pairs (sj  bj)  j  =  1...  /. The polynomial matrix interpolation theory deals with interpolation in the matrix case. In the following we cite a basic result upon which the solution to the polynomial matrix interpolation problem rests. Let S(s)  =  block  diag([1  s... s^']^)  where thedt  i  =  1...  m are nonneg-ative integers. Let aj  ^  0 and bj  denote m X 1 and p  X  I  complex vectors respec tively and let Sj be complex  scalars. THEOREM 4.1.  Given interpolation triplets (sj aj bj) j  =  1...  I (i.e. interpolation points) and nonnegative integers dt with /  =  Sjl ^di + m such that the (Xjl^dt  +  m)xl 654 APPENDIX: Numerical Considerations matrix (4.2) has  full  rank  there  exists  a unique  p  X m polynomial  matrix  Q(s) with  /th-column degree equal to J/ /  =  1... m for which Si  ^  [S(si)au...S(si)ai] Proof. Since the column degrees of Q(s) are J/ Q(s) can be written as Q(sj)aj  = bj j  =  h...J. Q(s) = QS(s) (4.3) (4.4) where the /? X (X/11 di + m) matrix 2 contains the coefficients of the polynomial entries. Substituting into (4.3) Q must satisfy (4.5) where Bi  =  [b\ ...bi\.  Since Si is nonsingular  g  and therefore  Q{s) are uniquely determined. • QSi = Bu It should be noted that when/?  =  m  =^  l a n d Ji  =  l-l  =  n the above theorem reduces to the Polynomial Interpolation Theorem. In that case for Uj  =  1 Si reduces to a Vandermonde Matrix that is nonsingular if and only if Sj j  =  1... / are distinct (show this). EXAMPLE 4.1.  Let Q(s) b e a l x2  =  /?Xm  polynomial matrix and let the follow ing  /  =  3 interpolation  points  {(Sj aj bj) j  =  1 2 3} be  specified:  {(-1 [10]^ 0) (0 [-1 1]^ 0) (1 [0 1]^ 1)}. In view of Theorem 4.1 Q(s) is uniquely specified when Ji  and (^2 are chosen so that /  =  3 =  Xdi + m  = {d\+d2) + 2Qxd\+d2  =  1 assuming that ^3 has full rank. Clearly there is more than one choice for d\ and ^2- The resulting Q{s) depends on the particular choice for the column degrees df. (i)  LetJi  =  lmdd2  = 0.Then S(s)  =  block diag([hsf  I) and (4.5) becomes: QS3  =  Q[S(si)au  S(s2)a2 S(s3)a3] = --Q =  [0 0 1]  =  53 '  1 -1 0 -1 0 1 0 0 1 from which we obtain g  =  [1 1 1] and Q{s) = QS(s) =  [^ +  1  1]. (ii)  Let di  = 0 and J2  =  1- Then S(s)  = blockdiag(\  [hsf)  and (4.5) yields Q  =  [0 0 1] from which we have Q(s) =  [0 s] which is clearly different  from (i).  • Rational matrix  interpolation Similar to the polynomial matrix case the problem here is to represent a p  X m rational  matrix  H(s)  by  interpolation  triplets  or  points  (sj aj  bj)  j  =  1...  / which  satisfies Hisj)aj  =  bj j  =  \...l (4.6) where the Sj are complex scalars and the aj  ¥=  0 and bj  are complex m X 1  and p X  1 vectors respectively. It can be shown that the rational matrix interpolation problem reduces to a special case of polynomial  matrix interpolation.  To see this we write H{s)  =  D~^{s)N{s) where D{s) and N{s)  are pX  p  and pX  m polynomial matrices respectively. Then (4.6) can be written as N(sj)aj  =  D{sj)bj  or as VN{sj)  -D{sj)] bj =  Q{sj)cj  =  0 ;• = l...l (4.7) i.e. the rational  matrix  interpolation  problem  for  a p  X m rational  matrix H(s)  can be viewed as a polynomial interpolation problem for SL pX  (p  -\- m) polynomial ma trix  Q{s)  =  [N{s\  -D{s)\  with  interpolation  points  {sj Cj 0)  =  (sj  [aj b^jf  0) j  =  1...  /. There is also the additional constraint that D~^(s)  exists. 655 APPENDIX: Numerical Considerations Solution of matrix  equations In this segment polynomial matrix equations of the form M(s)L(s)  =  Q(s)  are considered.  The  main  result  is Theorem  4.2 which  essentially  states  that  all  solu tions M(s)  of degree r can be derived by solving Eq. (4.16). In this way all solutions of  degree  r of  the  polynomial  equation  if  they  exist  are parameterized.  The  Dio-phantine Equation  is an important  special case and is examined  at length. It is also shown that Theorem 4.2 can be applied to solve rational matrix equations of the form M(s)L(s)  -  Q(s) Consider the equation M{s)L(s)  =  Q(sl (4.8) where L(s)  and  Q(s)  are given  t  X  m and  kx  m polynomial  matrices  respectively. We wish to determine thQ  kX  t polynomial matrix  solutions M(s)  when they  exist. First consider the left-hand  side of Eq. (4.8). Let M(s)  =  Mo +  • • • +  Mrs' (4.9) and let di  =  deg^^ [L{s)\ i  =  1...  m denote the column degrees of L{s) If Q{s)  ^  M(s)L(s\ (4.10) then deg^i [Q(s)]  =  di + r for i  =  1...  m. According to the Polynomial Matrix In terpolation Theorem Theorem 4.1 the matrix Q(s) can be uniquely  specified  using ^T=\(^i  +  r)  -h m  =  ^f=\di  +  m{r  +  1) interpolation  points. Therefore  consider / interpolation points {sj Uj bj)  j  =  1...  / where Z -  2 r . i J/  +  m ( r+  1). (4.11) Let Sr(s)  =  block  diag ([1 ^...  Z'+H^) and assume that the &Jlidi  -h m(r +1)) X / matrix Sri  = [Sr(si)ai...Sr(si)ai] (4.12) has full rank i.e. the assumptions in Theorem 4.1 are satisfied. Note that for distinct Sj Sri will have full  column rank for  almost  any  set of nonzero  aj.  Now in view of Theorem 4.1 the matrix  Q(s) that  satisfies Q(sj)aj  =  bj ;•  =  h...J (4.13) is  uniquely  specified  given  these  / interpolation  points  (sj aj  bj).  To  solve  (4.8) these interpolation points must be appropriately  chosen so that the equation Q(s)  (=  M(s)L(s))  =  Q(s) is  satisfied. We write (4.8) as MLris)  =  Q(s) (4.14) where  M  =  [MQ . . .  M^] and  Uis)  =  [L'^(s)...  s''L'^(s)f  are k  X  t(r  +  1) and f(r +  1) X m matrices respectively. Let s  =  sj  and postmultiply by  aj  j  =  1...  /. 656 APPENDIX: Numerical Considerations Note that Sj and Uj j  =  1...  / must be such that Sri has full  rank.  Define bj  =  Q{sj)aj j  =  1  . . .  / and combine the above equations to obtain MLri  =  Bu (4.15) (4.16) where L^/  == [L^(^i)ai... L^(5'/)a/] and5/  =  [^i.. matrices respectively. bi\2irtt{r+ l)XlandkXl THEOREM 4.2.  Given L{s) and Q{s) in (4.8) let dt select r to satisfy degci [L{sy\ i  =  1... m and Then a solution M(^) of (4.16) exists. degci [Q(s)]  <  di + r. (4.17) M[/ si...  s^lY  of degree r exists if and only if a solution M 1 It is not difficult  to show that solving (4.16) is equivalent to solving (4.18) (4.19) (4.20) [/  si I. ^i^j^j =  bp A j  =  h. ./ where The M(s)  that satisfy  (4.18) are obtained by  solving bj  =  Q(sj)aj Cj  =  L(sj)aj j  =  1  . . .  /. MSri  =  Bu where Sri  "=  [Sr(si)ci... . . .  5'''/]^ has dimensions  t(r  -\-  I)  X  t and Bi  =  [b\ Solving (4.20) is an alternative to solving (4.16). Sr(si)ci]  has dimensions  t(r  +  1) X / and  Sr(s)  bi\  has dimensions  kx Constraints on solutions.  When there are more unknowns than equations [t{r +1) and /  =  Sjl i<^/ + ^{r +1) respectively] in (4.16) this freedom can be exploited so that M{s) satisfies additional constraints. In particular k  = t(r+\)-l  additional linear constraints expressed in terms of the coefficients  of M{s) (in M) can in general be satisfied. The equations describing the constraints can be used to augment Eqs. (4.16). In this case the equations to be solved become (4.21) where MC  =  D represents the k linear constraints imposed on the coefficients  M and C and D are matrices (real or complex) with k columns each. • MiLruC]  =  [BiDl The Diophantine  Equation An important case of (4.8) is the Diophantine  Equation X(s)D(s)  +  Y(s)N(s)  =  Q(sl (4.22) where the polynomial  matrices  D(s) N{s)  and  Q(s)  are given  and X(s)  Y(s)  are to be determined. Note that if M(s)  =  [X(s\  Y(s)l L{s)  = D(s) N(s) (4.23) then  it  is  immediately  clear  that  the  Diophantine  Equation  is  a  polynomial  equa tion  of  the  form  (4.8)  and  all  previous  results  apply.  Theorem  4.2  guarantees  that all  solutions  of  (4.22)  of  degree  r  are  determined  by  solving  (4.16).  In  systems and control theory the Diophantine Equation that is used involves  a matrix  L{s)  = [D^{s) N^{s)Y  which  has  rather  specific  properties. These  are exploited  to solve the Diophantine Equation and to derive conditions for existence of solutions of (4.22) of degree r. It can be shown that the following  result is true. 657 APPENDIX: Numerical Considerations THEOREM 4.3.  Let r satisfy deg.i [Q{s)\  <  di + r i  =  1... m  and r • 1 (4.24) where v is the observability index of the system {D / N 0}. Then the Diophantine Equa tion (4.22) has solutions of degree r that can be determined by solving (4.16) [or (4.20)]. EXAMPLE 4.2.  Let D(s) =  s-2 0 0 s+  1 ;v(^) =  s-\ 1 0 1 and Qis) 1  0 0  1 We have di  = di  =  1 degd Q(s) =  0 /  =  1 2 and /  =  2 + 2(r + 1). For r  =  I  Sj  =  -2  - 1  0 1 2 3 and 0 1  ' 1 3  ' 0 -1 ' -1 3  ' -1 1  ' 1 -1 A solution is given by M(s)  =  [X(s) Y(s)] -S 0 5 +1 - i . +i Solving rational matrix  equations Now let us consider the rational matrix  equation M(s)L(s)  =  Q{s\ (4.25) where L{s) and Q{s) are given t  X  m and k  X  m rational matrices respectively. The polynomial  matrix  interpolation  theory  developed  above  can  be  used  to  solve  this equation  and  determine  the rational  matrix  solutions M(s)  of dimension  kx t.  Let M(s)  =  D~^(s)N(s)  be a polynomial fraction  form of M(s)  that is to be  determined. Then (4.25) can be written as [A^(^)  -D(s)] L(s) =  0. Note that one could equivalently  solve [A^(^)  -D(s)] Lp(s) =  0 (4.26) (4.27) where [Lp(s)^  Qp(s)^]^  =  [L(s)^  Q(sW(t>(s)  is a polynomial matrix with (l)(s) the least common  denominator  of all entries  of L(s)  and  Q(s).  In general  (l)(s)  may  be any matrix  denominator  in a right fractional  representation  of  [L(s)^  QisW-  The problem to be solved is now of the form  (4.8) a polynomial matrix equation  where L(s)  =  [Lp(sf  Qp(sfV and  Q(s)  =  0.  Therefore  all  solutions  [A^(^) -D(s)]  of degree r can be determined by solving (4.16) or (4.20). Let s  =  Sj and postmultiply (4.27) by aj  j  =  1...  / with aj  and / chosen properly.  Define 658 APPENDIX: Numerical Considerations \Lp(s)] [QP(S)\ 1  . . .  /. (4.28) The problem now is to determine a polynomial matrix  [N(s)  -D(s)]  that  satisfies [N(sj\  -D(sj)]cj  =  0 J  =  1...  /. (4.29) Note that restrictions on the solutions can easily be imposed to guarantee that  D~^(s) exists and/or that M(s)  =  D~^(s)N(s)  is proper. Additional constraints can be added so that the solution satisfies  additional specifications  [see (4.21)]. Pole  placement Output feedback.  All proper  output  controllers  of  degree  r (of  order  mr)  that assign  all  the  closed-loop  eigenvalues  to  arbitrary  locations  are  characterized  in  a convenient way using interpolation  results. Given N(s)D~^(s)  =  H(s)  which is assumed to be proper we are interested in solutions [X(s)  Y{s)] of dimensions m X (p + m) of the Diophantine Equation where only the roots of  \Q(s)\ are specified.  Furthermore X~^(s)Y(s)  =  C(s)  should  exist and be proper since it represents the controller. Here the equation to be solved is (X(sj)D(sj)  +  Y(sj)N(sj))aj  = 0 y  =  1...  / (4.30) orMLri  =  0(1  =  l.Jl^di  + mr).ThusthQ%^^^di  + mrrootsof\X(s)D(s)  +  Y(s)N(s)\ are to be assigned the values Sj j  =  1... /. Note the difference between the problem studied earlier where  Q(s) is known and the problem  studied here where only  the roots  of  \Q(s)\  (or  of  |G('^)I  within  multiplication  by  some  nonzero  real  scalar)  are given. In the present case the vectors aj  can be viewed as design parameters and can be selected almost arbitrarily to satisfy  requirements in addition to pole  assignment. Note  that  this  design  approach  is rather  well  known  in  the  state  feedback  case  as is  discussed  later  in  this  section  (see  also  Chapter  4). The  following  result  can  be shown. THEOREM 4.4.  Let r >  v -  1. Then (X(s) Y(s)) exists such that all the n + mr zeros of \X{s)D{s) + Y{s)N{s)\ are arbitrarily assigned and X~^{s)Y{s) is proper. • EXAMPLE  4.3.  Let  D{s)  = Is-2 L  0 0 s+l\ and  A^(^)  =  s-  1 1 with  n  = deg \D(s)\ =  2. In this case there are deg \X(s)D(s)  +  F(5')A^(5)|  =  n + mr  =  2 + 2r closed-loop poles to be assigned. Note that r > ^ ' -l  = l - l = 0. (i)  For r  =  0 and {(sp ajX j  =  1 2} =  {(-1 [1 0]^) (-2  [01]^)} a solution of MLri =  Ois M = For this case M  = M(s)  =  [X(sX Y(s)] and C(s)  =  X-\s)Y(s)  =  ^ is a static output controller. (ii)  For  r  =  1  and  {(sjaj)J  =  1...4}  =  {(-1 [10]^) (-2  [0 If  (-3 -10]^) (-4  [0-1]^)}  a  solution  of  MLri  =  0  is  given  by  [X(s\Y(s)]  = s-1 . Note that C{s) = X{s)  ^ Y{s) exists and is proper. -1 5 +4 12  5 + 1] -6 5 +4 -3  0 1  2 which State  feedback.  Let  A B  and  F  he  n  X  n  n  X  m  and  m  X  n  real  matri ces  respectively.  Note  that  1^/ -  (A  +  BF)\  =  \sl  -  A\  • |/„  -  {si  -  A)~^BF\  = 659 APPENDIX: Numerical Considerations 1^/ -  A\  ' \lm —  F(sl  -  Ay^B].  Now  if  the  desired  closed-loop  eigenvalues  Sj  are different  from  the  eigenvalues  of A  then  F  will  assign  all  n  desired  closed-loop eigenvalues Sj if and only if F[(sjl  -  Ay^Baj]  =  Uj j  =  l...n. (4.31) The m X 1 vectors aj  are selected so that (sjl  — A)~^Baj  j  =  1.  .n  are linearly independent vectors. Alternatively one could approach the problem as follows  (see also Subsection 4.2B of Chapter 4). Let M(s)  and D(s)  be re polynomial matrices of dimensions  nX  m and mX  m respectively  such that (si  -  A)~^B  =  M(s)D~^(s). An internal representation equivalent to x  =  Ax  -^ Bu  in polynomial matrix form is Dz  =  u with  X  =  Mz  (see Subsection  7.3A  of Chapter 7). The eigenvalue  assign ment problem now is to assign all the roots of  \D(s) -  FM(s)l  or to determine F so that FM(sj)aj  =  D{sj)aj J  =  h (4.32) Note  that  this  formulation  does  not  require  that  Sj  be  different  from  the  eigen values  of A  as  in  (4.31). The  m X  1 vectors  aj  are  selected  so that  M(sj)aj  j  = 1...  ^  are  independent.  Note  that  M(sj)  has  the  same  column  rank  as  S(sj)  = sj'~^]^)  where  dt  are  the  controllability  indices  of  (A 5). block  diag([l  Sj... Therefore  it is possible to select aj  so that M(sj)aj  j  =  I..  .n  are  independent even  when  the  Sj  are repeated.  In  general  there  is  great  flexibility  in  selecting  the nonzero vectors aj.  For example  when the Sj  are distinct  which is a very  common case the aj  can be selected  almost  arbitrarily. For all the appropriate choices of  aj [M(sj)aj  j  =  1..  .n  linearly independent] the n eigenvalues  of the  closed-loop system  will be  at the desired  locations  Sj j  =  \.. .n.  Different  aj  correspond  to different  F which results in general in different  closed-loop  behavior. The  exact  relation  of  the  eigenvectors  to  the  aj  can  be  determined  by  [sjl {A + BF)\M{sj)aj  =  (sjI-A)M(sj)aj-BFM(sj)aj Therefore  M(sj)aj  =  Vj are the closed-loop eigenvectors corresponding to  Sj. =  BD(sj)aj-BD(sj)aj -=  0. One  may  select  aj  in  (4.32)  to  impose  constraints  on  the  gains  fij  in  F.  For example  one  may  select  aj  so that  a column  of F  is  zero  (take  the  corresponding row  of  all  aj  to be  nonzero)  or  so that  an  element  of  F  is  zero. Alternatively  one may  select aj  so that additional design goals are attained. Note that this approach for eigenvalue/eigenvector assignment by state feedback has  also  been  discussed  in  Subsection  4.2B  of  Chapter  4.  For  further  details  see Antsaklis and Gao [1]. A.5 REFERENCES 1.  P. J. Antsaklis and Z. Gao "Polynomial and Rational Matrix Interpolation: Theory and Control Applications" Int. J. of Control Vol. 58 No. 2 pp. 349-404 August 1993. 2.  M. S. Bazaraa  H. D. Sherali and C. M. Shetty Nonlinear Programming  Theory  and Algorithms 2d edition Wiley New York 1993. 3.  D. P. Bertsekas and J. N. Tsitsiklis Parallel and Distributed Computation—Numerical Methods  Prentice-Hall Englewood Cliffs NJ 1989. 4.  R. A. DeCarlo Linear Systems. A State Variable Approach with Numerical Implementa tion  Prentice-Hall Englewood CHffs NJ 1989. 660 APPENDIX: Numerical Considerations 5.  Z. Gao and P. J. Antsaklis "New Methods for Control System Design Using Matrix Inter polation" Proc. of the IEEE Conference on Decision and Control Orlando FL December 1994. 6.  G. H. Golub and C. F. Van Loan Matrix Computations  The Johns Hopkins University Press Baltimore 1983. 7.  R. V. Patel A. J. Laub and P. M. VanDooren eds.. Numerical Linear Algebra Techniques for Systems and Control  IEEE Press Piscataway NJ 1993. 8.  P. Hr. Petkov N. D. Christov and M. M. Konstantinov Computational Methods for Linear Control Systems Prentice-Hall International Series 1991. 9.  G. W. Stewart Introduction to Matrix Computations  Academic Press New York 1973. INDEX Balanced representations  430 Basis 99  100  170 439 551 552 See  also  Vector  space Bessel's inequality  440 Bezout identity 553 612 See also  Diophantine  equation BIBO  stable 481 490 505 See  also  Bounded-input/bounded-output stable See  also  Linear  transformation Bilinear functional  435 signature 435 symmetric 435 436 Binet-Cauchy  formula  639 Biproper 552 577 See  also  Rational  function Block diagonal  130 See  also  Matrix Blocking property  307 See  also  Zero Bolzano-Weierstrass  property  18 44 Bounded 491 493 See also  Solutions  of  differential Bounded-input/bounded-output  stability 481482483484490505 See also  Stability Brunovsky  canonical form  288 See  also  Canonical  form Building earthquake protection  208 B-W property  18 See  also  Bolzano-Weierstrass  property See also  Solutions of algebraic  equations Bijective  101 102 Algebraic  multiplicity  124 444 See  also  Eigenvalue Algebraic Riccati equation  343 349 358 363 Asymptotically  stable  159  189 449 491 494 497 504 563 See also  Equilibrium Asymptotically  stable in the large 450 491 equations Abel's formula  140 Ackermann's  formula  334 353 Adjoint  equation  197 Adjoint  of a matrix  115 442 Aircraft  209 320 381 Algebra  102 associative  103 commutative  103 Algebraic  equation  115 See also  Riccati  equation Algebraically  closed  123 See  also  Field Ascoli-Arzela lemma  22 Asymptotic behavior  156  186 See  also  Mode of  system Asymptotic  state estimator  351 See also  State  observers 494 499 See  also  Equilibrium;  Globally asymptotically  stable At rest  7177 See  also  System Attractive 449 491 See  also  Equilibrium Automobile  suspension  system 207 380 Autonomous  11 62 See also  Linear ordinary  differential equation;  System Axioms: of a field 37 of a metric 438 of a norm 438 of a vector space 38 Canonical form  116 127 535 Brunovsky  288 Jordan  130  135 136 Canonical Structure Theorem  270 See  also  Kalman's  Decomposition Theorem 661 662 INDEX Cartesian product  8 Cauchy  criterion  20 Cauchy  sequence  18 Cauchy-Peano  existence theorem  25 Causal 67 70 71 77 See  also  System Cayley-Hamilton  Theorem  124  154 Characteristic  equation  123 Characteristic  exponent  163 Characteristic polynomial  123 299 563 See also  Matrix; Transfer  function Characteristic  value  122 See  also  Eigenvalue Characteristic  vector  122 See  also  Eigenvector Chemical reaction process  381 Cholesky  decomposition  647 Closed loop control  327 Cofactor  113 See also  Matrix Column rank  119 See also  Matrix Column reduced  526 Column vector  107 See  also  Matrix Command input  326 See also  Polynomial  matrices See  also  Input; Reference  input Common divisor  535 left  536 right  536 See also  Divisor Commutative  algebra  103 See also  Algebra Compact  set 9  18 Companion  form  131 153 468 See also  Matrix Complete instability 473 501 See also  Equilibrium  unstable Complete  space  18 Completeness  property  288 Complex vector space 38 Composite  system  203 See  also  System Condition number  646 647 649 See  also  Matrix Conformal  matrices  109 See  also  Matrix Congruent matrices 435 436 See  also  Matrix Conservative  system  81 See also  System Constructibility  219 247 251 252  260 continuous-time  system 248 252 discrete-time  systeni 219 257 Gramian 251256  262 See also  Observability Continuation  of solutions 26 27 See  also  Solutions of  differential 3137 equations Continuous  dependence of solutions 37 See also  Solutions of  differential equations Continuous  function: over an interval 9 at a point 9 44 uniformly  9 Continuous  time systems 4 See also  Systems Control problems  632 ControllabiHty  214 215 228 235 242 264 265 274 561 continuous-time  system 227 235 discrete-time  system 215 241 eigenvalue/eigenvector  (PBH) test  272 Gramian  233 240 245 index  283 matrix  217 from  the origin 215 217 228 236 See  also  Reachability totheorigin  215 218 228 output  312 subspace 229 236 242 Controllable 228 235 561 companion form  279 differentially  315 eigenvalues  265 instantaneously  315 mode 265 pair  229 single-input  373 state 242 subspace 229 236 242 uniformly  315 Controller: digital  182 feedback  589 implementations  629 with two degrees of  freedom 368  622 Controller companion form  203 Controller form  279 280 285 291 multi-input  283 single-input  280 Convergence of a function  18 pointwise  18  1944 uniform  18  1920 Convergence  of a sequence  18 44 Convergence  of a series 21 pointwise 21 uniform  21 Converse theorem  513 Converter  (A/D D/A)  183 Convolution: integral 78 sum 70 Coordinate representation  100 See  also  Vector Coprime 535 593 See  also  Polynomial  matrices; Polynomials Critical 460 462 See also  Eigenvalue Cyclic  132 See  also  Matrix D/A converter  183 Dead-beat: control 201 observer  359 Decoupling: diagonal 377 378 633 static 634 Decoupling  indices  377 See  also  Indices Degree McMillan  397 polynomial vector  526 Detectable  351 Determinant  112 121 586 See also  Linear transformation;  Matrix; Return  difference  matrix Determinantal  divisor 299 534 Diagonal decoupling  377 378 633 Difference  equations 62 63 See also  Solutions of difference  equations Differential  equations  ordinary: classification  11 first-order  10 linear 47 linear homogeneous  13 138 linear homogeneous  with  constant coefficients  148 linear nonhomogeneous  138 145 nth order  12 systems of first order  37 See also  Solutions of  differential equations Digital controller  182 Digital filter 65 Digital to analog converter  183 Dimension  99 See also  Vector  space Diophantine equation 540 612 656 all solutions 550 Bezout identity 553 612 historical remarks 552 Dirac delta distribution 72 74  154 Direct form  65 Direct link matrix  169 Direct  sum  126 Discrete-time impulse 68  178 Discrete-time  Kalman  filter  363 See  also  Kalman  filter Discrete-time  system 3 5 60 66  174 215 241 242 257 348 358 362 388 392 401489 See also  System Distance function  438 Divisor: common  535 left  right  536 greatest common 535 536 Domain of attraction 449 491 See also  Equilibrium Double integrator  205 Doubly coprime  593 See  also  Coprime Dual system 222 402 Economic model 205 382 Eigenvalue  121 122 298 301 305 564 algebraic multiplicity  124 444 controllable  265 critical 460 462 geometric multiplicity  122 repeated  129 spectrum  122 direct method  328 eigenvector  assignment  337 using controller form  332 Eigenvector  121 122 663 INDEX Elementary  operations: column  526 row 526 Elementary  unimodular matrices  526 Eliminant  matrix 542 547 e-approximate solution  23 Equicontinuous 22 46 Equilibrium  158 189 445 446 490 asymptotically  stable  158 189 449 491 494 497 504 563 asymptotically  stable in the large 450 491 494  499 attractive 449 491 completely  unstable 473 501 domain of attraction 449 491 exponentially  stable 449 477 480 512 exponentially  stable in the large 450 globally asymptotically  stable 450 491 459471 494 499 isolated  446 qualitative characterization  447 stable  159  189 448 452 455 459 491492 trivial solution  138 446 uniformly  asymptotically  stable 449 uniformly  asymptotically  stable in the large uniformly  stable 448 453 454 455 459 450459451 461470512 unstable  159  189 450 459 462 472 479 481 491 495 497 499 504 513 See also  Stability Equivalence: of internal representations  170  180 554 relation  535 transformation  181 Equivalent  116 119 172  181 535 554 Estimator  361 See  also  State  estimator Euclidean: norm 42 ring 551 space 437 441 Euler: method 25 65 85 polygon  25 Exponentially  stable 449 477 480 512 See  also  Equilibrium Exponentially  stable in the large 450 459 471 See  also  Equilibrium Extended  system matrix  557 External input  326 External  system description 5 65 See  also  System Feedback  326 589 configuration  203 573 gain matrix  327 integral  379 output  363 379 658 state 321 323 324 326 327 Feedback control  systems 58 589 590 See also  Feedback;  Output  feedback Feedback  stabilizing controller  589 paramererizations  592 Eigenvalue or pole assignment  328 330 658 364 658 664 INDEX parameterizations proper and stable (MFD) 611615 two degrees of freedom  622 Fermat's Last Theorem  553 Field  37 algebraically  closed  123 complex numbers 38 rational functions  37 real numbers 38 Filtering theory  357 Floquet multipliers  163 Force field inverse square law 52 Fractional description  521 See also  System  representations Frequency  response 204 463 465 Frobenius norm  653 Function 8 bijective  101 continuous 9 Hamiltonian  81 indefinite 437 469 negative definite  semidefinite  437 491 one-to-one or injective  101 onto or surjective  101 piecewise continuous 9 58 positive definite  semidefinite  437 469 491 Fundamental  matrix  139  140 Fundamental  sequence  18 Fundamental  set of solutions  140 Fundamental  theorem of linear equations  101 Gap and position criterion  465 See  also  Stability Gaussian  elimination  647 Generalized  distance function  468 See  also  Lyapunov  function Generalized  energy function  468 See  also  Lyapunov  function Generalized  function  74 See  also  Dirac delta  distribution Geometric multiplicity  122 See also  Eigenvalue Globally  asymptotically  stable 491 494 See  also  Asymptotically  stable; Equilibrium Gram determinant  234 matrix  234 Gramian constructibility  251 256 262 controllability 233 240 246 observability  249 253 259 reachability 230 236 245 Gram-Schmidt process 440 Graphical  criteria  462 See  also  Stability Gronwall inequality  29 Hamiltonian: dynamical  system  81 function  81 matrix  344 349 Hankel matrix  399 Hard disk read/write head 212 382 Harmonic oscillator  198 H-B property  1844 Heine-Borel property  18 44 Hermite form  532 Hermitian  matrix  648 Highest degree coefficient  matrix  526 Homogeneous  differential  equations  138 See  also  Differential  equations Homogeneous  solution 58 64  145 See  also  Solutions of  differential equations Hurwitz matrix  460 Hurwitz polynomial  462 Hybrid  system  3  183 Impulse response matrix 71 77 78 79 165 Idempotent matrix  126 Identity  matrix  110 Identity transformation  103 Ill-conditioned  647 Impulse response: continuous-time 75 77 discrete-time  70 time-invariant  78 time-varying  79 166  177 Index: bilinear functional  437 nilpotent operator  134 set  17 Indices: controllability  283 decoupling  377 Kronecker  288 observability  295 Induced  norm  43 Infinite  series convergence  21 Infinite  series method  150 See also  Matrix  exponential Initial conditions 4 5 Initialtime 4 5  6162 Initial value problem  10  11 12 62 examples  13 existence of solutions 21 Inner product  437 Inners 498 Input: command  or reference  326 comparison  sensitivity matrix  626 decoupling  zeros 302 564 external  326 function  observability  302 normal representations  430 output decoupling  zeros 302 564 output  stability  451481 vector 4 Input-output  description  5 65 165  174 See  also  System  representations Input-output  stabiUty 481 505 See also  Stability Instability  159  189 450 459 472 479 481 491495497499504513 See  also  Equilibrium  unstable Integral equation  11 Integral feedback  379 Integral representation  72 75 76 Integration forward  rectangular rule 85 Interconnected  systems  568 feedback  573 parallel  568 series 570 665 INDEX Interlacing  464 See also  Stability Internal description  5 See also  System  representations Internal qualitative properties 490 See also  Internal  stability Internal  stability 448 451 481 490 623  624 Internally balanced realization 424 430 See also  System  representations Interpolation  of polynomial  rational matrix  653 Invariance principle  493 See  also  Asymptotically  stable Invariant factors polynomials 298 534 Invariant property  of (A B)  288 Invariant  subspace  260 Invariant zeros 302 306 563 564 565 See also  Zero Inverse system 377 634 Inverse z-transformation  178 Inverted pendulum  89 380 See  also  Pendulum Jacobian matrix 48 447 Jordan canonical form  130 135 136 Kalman  filter: continuous-time  352 discrete-time  363 Kalman-Bucy  filter  357 Kalman's Decomposition  Theorem 269270 Kronecker indices 288 Lagrange's  equation  83 Lagrangian  83 Lamda approach  622 Laplace transform  75  154 La Salle's Theorem  493 principle Latent value  122 See  also  Eigenvalue Least squares 648 652 Least-order realization  394 Left  coprime  536 See  also  Polynomial  matrices Leonhard-Mikhailov  stability criterion  463 Level curve 471 See  also  Lyapunov  function Lienard equation  16 479 Limit cycle 88 Linear algebraic  equation: fundamental  theorem  101 homogeneous nonhomogeneous  115 646 Linear manifold  97 Linear matrix pencil 288 640 Linear operator  40 nilpotent  134 See  also  Linear  transformation Linear ordinary  difference  equation: autonomous  12 homogenous 62 63 174 homogenous  w/constant coefficients  175 matrix  495 nonhomogenous 62 63 periodic 62 63 Linear ordinary  differential  equation autonomous  12 homogeneous  12 13 62 63 138 174 homogeneous  w/constant coefficients  148 164  175 matrix  140 495 nonhomogeneous  12 54 62 63 145 periodic  12 13 62 63 161 systems: continuation  54 continuity  with respect to parameters  54 existence  54 uniqueness  54 See  also  Solutions of  differential equations Linear  space 37 See  also  Vector  space Linear subspace  97126 direct sum  126 Linear system 3 60 66 94 509 Linear transformation  40  100 bijective  101 102 determinant  121 identity  103 injective  102 invariant  127 nilpotent  134 nullity  101 orthogonal  441 primary  decomposition  theorem 133134 principle of superposition  66 projection  126 445 range space  100 reduced  127 representation  by a matrix  104 spectral theorem 445 spectrum  122 surjective  101 102 Linearization  5 48 477 504 574 Linearized  equation 50 51 Linearly  dependent  98 234 524 Linearly  independent  97 98 99 234 524 See  also  Vector See  also  Vector Lipschitz condition 30 45 Lipscitz constant 30 45 Lipschitz continuous  30 LQG  (linear quadratic Gaussian)  problem 352  359 LQR  (linear quadratic regulator)  problem: continuous-time  342 discrete-time  348 LU decomposition  647 Luenberger  observer  351 Lyapunov function  468 491 construction  of  474 500 level curve 471 Lyapunov  matrix equation 468 469 499 Lyapunov  stability 445 492 linear systems 452 495 See  also  Stability Lyapunov's Direct Method  468 Lyapunov's  Second Method  468 See also  Asymptotically  stable; Invariance examples  52 666 INDEX Magnetic ball suspension 91 92  174-2 Mapping 8 See  also  Function Markov parameter  204 387 399 Matrix  106 block diagonal  128 characteristic polynomial  123 cofactor  113 column  107 column rank  119 companion  461 companion form  131 153 468 condition number 646 647 649 conformal  109 congruent  436 controllability  217 cyclic  132 determinant  111 113 diagonal  120 elementary  unimodular  526 eliminant  542 546 exponential 57  149  150 fundamental  139  140 Gram  234 Hamiltonian  344 349 Hankel  399 Hermite form  532 Hermitian  648 highest degree coefficient  526 Hurwitz  460 idempotent  126 identity  110 ill-conditioned  647 impulse response 77  166 indefinite  443 inverse  110 Jacobian 48 447 Jordan  130  135 136 left  inverse  653 linear pencil 288 640 logarithm  161 lower triangular  131 LU decomposition  647 minimal polynomial  132  133 299 minor  113469 modal  128 Moore-Penrose inverse  652 negative definite  semidefinite  443 469 nilpotent  134 nonsingular  101 110 norm 43 null  109 observability  219 253 258 orthogonal 441 648 permutation  of  112 positive definite  semidefinite  443 principal minor  113 469 proper rational  391 properties  107 QR decomposition  647 rank  101 107 306 437 524 525 right inverse 653 Rosenbrock  301 554 564 self-adjoint  443 singular  110 square  107 state transition 56 63 143 Smith form  531 Sylvester  541 symmetric 469 648 symmetric part  436 system  301 554 564 system extended  557 Toeplitz  259 transfer  function  78  168 178 transpose 8  107 unimodular  526 unitary  648 upper triangular  131 532 well conditioned  647 Matrix difference  equation  496 Matrix differential  equation  140 Matrix fractional  description 517 521 See  also  System  representations Maximal element  27 Zom's  lemma  26 McMillan  degree  397 Metric 438 Metric  space 439 Microphone  84 MIMO  system multi input-multi  output  66 71 Minimal basis 552 Minimal polynomial  132  133 299 Mode of system  156  157 186  187 See  also  System Model matching problem  374 633 644 Module free  552 Monic polynomial  132 298 Moore-Penrose pseudo inverse  652 Motor  servomotor  14 206 380 Natural basis  100 See  also  Basis; Vector  space Negative: definite 437 469 491 indefinite  469 semidefinite  437 469 491 See  also  Function;  Matrix Nilpotent  operator  134 See  also  Linear  transformation Nonhomogeneous  differential  equations 138  145 See  also  differential  equations Nonlinear  systems 3 451 477 Norm: Euclidean  42 Frobenius  653 induced  43 linear space  41438 Manhattan  42 matrix 43 taxicab 42 Numerical  solutions of  algebraic equations  646 See  also  Solutions of algebraic  equations Numerical  solutions of differential  equations 25  85 86 See  also  Solutions of differential  equations Numerical  stability  645 Observability  169 214 219 247 249 250 253 258 263 268 274 560 continuous-time  system 248 252 discrete-time  system 219 257 Pole assignment problem 330 658 See also Eigenvalue or pole assignment Pole polynomial 299 Poles at infinity 319 Poles of a transfer function 298 299 301 488 508 563 Poles of the system 298 301 564 See also Eigenvalue 667 eigenvalue/eigenvector (PBH) test 272 Gramian 249 253 259 index 295 matrix 219 253 258 subspace see Unobservable Observable 562 eigenvalue 268 mode 268 Observer Luenberger 351 See also State observer Observer companion form 203 Observer form 292 293 296 297 multi-output 294 single-input 293 Open covering 18 Open loop control 327 Operator linear 40100 See also Linear transformation Optimal: control problem 342 348 estimation problem 352 357 359 362 Optimality principle 371 Orbit 492 Order of a realization 394 Orthogonal 439 basis 440 complement 441 linear transformation 441 matrix 441648 projection 445 Orthonormal basis 439 Orthonormal set of vectors 439 Output: decoupling zeros 302 564 equation 5 58 61 function controllability 313 normal representations 430 reachability controllability 312 vector 4 Output feedback: dynamic 363 589 642 658 observer based 363 static 379 Parallelogram law 438 Parseval's identity 440 Partial state 519 Partial sum 21 Partially ordered set 27 Peano-Baker series 56 143147 Pendulum: inverted 89 380 simple 16 52 82 93 two-link 83 Periodic solution 492 See also Solutions of differential  equations Periodic system 11 62 161 Phase: plane 198 portrait 88 variable 198 Picard iteration 31 Piecewise continuous 58 derivative 23 function 9 Plant 182 Polynomial: Hurwitz 462 monic 132 298 stable 462 Polynomial matrices 524 column row reduced 526 common divisors 535 coprime left right 536 612 division theorem 545 doubly coprime 593 equations 540 653 equivalent 535 Hermite form 532 proper or reduced row column 527 rank 524 regular 528 Smith form 298 531 533 unimodular 526 Polynomial matrix description 517 519 521553 See also System representations Polynomial matrix interpolation 653 Polynomial of linear transformation  104 Polynomial vector degree 526 linear independence 524 definite 437 443 469 491 indefinite 469 semidefinite 437 443 469 470 491 See also Function; Matrix Positive innerwise 498 Positive orbit 492 Positively invariant 492 Prediction estimator 360 Proper rational matrix 391 Proper transfer function  170 Proper value 102 See also Eigenvalue Proper vector 122 See also Eigenvector Pythagorean Theorem 439 QR decomposition 647 Quadratic form 436 Quantization 183 Radially unbounded 491 See also Lyapunov function Range space 100 Rank 107 524 of a functional 437 normal 306 525 test 274 Rational function: biproper 552 577 proper and stable 552 zeroing or blocking property 307 Positive: 668 INDEX Rational  matrix: equations  653 right inverse  313 Smith-McMillan  form  299 563 Rayleigh's  dissipation function  83 Reachability  214 215 226 235 242244 continuous-time  system 227 235 discrete-time  system 215 241 Gramian 230 236 245 matrix see  Controllability output  312 subspace 228 235 242 See  also  Controllability  from  the origin Reachable 217 228 235 pairs  228 state 242 subspace 217 228 235 242 See  also  Controllable; Controllability  from the origin Real numbers 8 Real sequence  17 Real vector space 38 Realization  algorithms 402 block companion form  418 controller/observer  form  404 matrix A  diagonal 417 singular value decomposition  423 Realization  of systems 383 385 388 565 existence and minimality 390 394 565 impulse response  386 least order irreducible minimal  order 394401565 order of  394 pulse response  388 transfer  function  202 388 389 Reconstructible  255 See  also  Constructibility Reduced  order model  424 Reference  input  326 Relation  535 Response 4 59 61 maps  625 total  165 166  174  176 zero-input  146  165 177 zero-state  146  165 166  177 Resultant  542 Return difference  matrix  586 Riccati equation  343 349 358 363 continuous-time  case 342 343 358 discrete-time case 349 363 Ring 552 Euclidean  551 RLC circuit  14 16 206 276 Robust control  327 Rosenbrock  system matrix 301 554 564 Rosenbrock  system equivalence  555 Routh-Hurwitz  stability criterion  466 Row  107 Hermite form  532 proper see Polynomial  matrices rank  119 vector  107 Row reduced  526 See also  Polynomial  matrix Sampled  data system 65 182 183 319 Sampling period rate  185 Scalar  38 Schur-Cohn  stability criterion  498 Schwarz inequality 42  49438 Semi-group property  175 Sensitivity  matrix 599 626 Separation principle property 325 365 Sequence  17 functions  18 vectors 44 Series 203 Shift  operator 68 92 Signal digital  183 Similarity  transformation  116  120  151 Singular  101 value  648 value decomposition 423 430 648650 vector left  right  651 See  also  Linear transformation;  Matrix Skew  adjoint  443 Skew  symmetric 435 436 Smith form  298 531 533 See  also  Polynomial  matrices Smith-McMillan  form  298 299 563 See also  Rational  matrix Solutions bounded 491 493 Solutions of algebraic equations  101 115 646 Solutions of difference  equations: 655  657 particular  64 total 64 Solutions of differential  equations  10 12 bounded  452 continuable  27 continuation  26 27 28 31 37 45 54 continuous dependence 33 37 45 continuous dependence  on initial  conditions continuous dependence on parameters 4554 4754 e-approximate  23 Euler's  method 25 85 existence 25 37 45 54 homogeneous 58 64  145 noncontinuable  27 particular  58 64  145 Peano-Baker  series 56  143 147 periodic  492 predictor-corrector  method  86 Runge-Kutta  85 successive approximations 31 47143 Space: total 58 uniqueness 29 30 32 45 See  also  Variation of constants  formula of linear transformations  102 of ^-tuples 38 of real-valued  continuous functions  39 span  97 Spectral theorem  445 Spectrum  122 Spring  16 88 320 Spring mass system  13 82 83 206  320 Stability 445 492 560 algebraic criteria 461 462 asymptotic  159  189 449 491 494 497 504 563 669 INDEX asymptotic in the large 450 491 attractive equilibrium  449 bounded-input^ounded-output  (BIBO) 481 494 499 490  505 causal  67707177 domain of attraction 449 491 exponential 449 477 480 512 exponential  in the large 450 459 471 external 481490  506 gap and position  criterion  464 global asymptotic 491 492 494 499 graphical geometric criteria  462 input-output  452 481 490 505 interlacing  464 internal 452 481 490 613 623 Leonhard-Mikhailov  criterion  463 linear systems continuous  159 linear systems discrete  189 Lyapunov 445 448 590 Routh-Hurwitz  criterion  466 Schur-Cohn  498 uniform  448 uniform  asymptotic 449 uniform  asymptotic  in the large 450 459  461 See  also  Equilibrium StabiUzable  330 Stable  159  189 448 452 455 459 491 492 See also  Stability Standard  form: Kalman's  canonical  269 uncontrollable  system 264 265 unobservable  system 267 268 State 56 58 60 228 235 controllable  242 partial  519 phase variable  198 variables 4 56 58 61 vector 4 56 58 State constructibihty  251 260 See also  Constructibihty State controllability  229 235 242 See also  Controllability State estimator  351359 See  also  State  observer State equation 5 58 61 165 linear solution 55  145 State feedback  321 322 326 605 input-output relations  345 See also  Feedback State observability 250 258 See  also  Observability State observer  321 322 350 359 608 current  360 full-order  350 358 identity  350 359 optimal 357 362 partial  state 354 362 608 reduced-order  355 362 State reachability  215 235 242 243 See  also  Reachability State space 56 58 State space description  5 continuous-time  58 discrete-time  60 See  also  System  representations State transition matrix 56 63 143 State unconstructible  250 255 State unobservable  248 253 258 Static: decoupling  634 output feedback  379 Structure  theorem: controllable version  291 observable  version  297 Subsequence  18 Successive approximations  31 47 See  also  Solutions of  differential equations Superposition  principle 60 66 Sylvester: matrix  541 rank inequality  205 396 resultant  542 theorem  437 Symmetric 435 436 648 See also  Bilinear  functional; Matrix System: at rest  7177 autonomous  12 62 classification  3 composite  203 conservative  81 continuous time 4 description external 5 65 discrete-time 5 60 68  174 distributed parameter  3 dual 222 402 finite dimensional  3 4 5 Fuhrmann  equivalence  554 Hamiltonian  81 hybrid  3  183 infinite  dimensional  3 input  66 inverse 377 634 linear 60 66 94 509 linear time-invariant  59 61 linear time-varying  59 61 lumped parameter  3 matrix  301564 with memory  67 memoryless  66 mode  156  186  187 multi input/multi  output  66 71 nonanticipative  67 nonlinear  3 451477 output  66 periodic  12 62  161 realization  383 response 64 Rosenbrock  equivalence  555 sampled data 65  183 single input/single  output  66 strict equivalence  555 time-invariant  5 242 time-varying 5 68 zeros 301 565 System  interconnections: feedback  203 573 parallel 203 568 series or tandem 203 570 System  representations: balanced  430 differential/difference  operator  517 equivalence of  170  180 554 external  65 670 fractional  left  right 517 521 input normal  430 input-output  66 integral 72 75 76 internal 5 619 linear continuous-time  76 linear discrete-time  68 matrix fractional  521 612 619 output normal  430 polynomial matrix 517 519 521 553 standard form  uncontrollable  unobservable 265267  268 state space 5 zero-input  equivalence  170  171 173 181 zero-state equivalent  170  171 173 181 Time reversibility  145 175 Toeplitz matrix  259 Trajectory  88 198 Transfer  function: McMillan  degree 397 pole polynomial 299 563 proper  170 530 strictly proper  170 530 Transfer  function  matrix 78  168  177  178 Transmission  zero 303 564 565 See  also  Zero Triangle inequality  41 Trivial  solution  138446 See also  Equilibrium Truncation  operator  92 Two degrees  of freedom  controller  622 Uncertainties  327 Unconstructible  250 260 subspace 250 255 260 See  also  Constructibility Uncontrollable 264 274 eigenvalues modes 265 See also  Controllability Uniformly  asymptotically  stable 449 Uniformly  asymptotically  stable in the large 450 459 461 Uniformly  BIBO-stable  (bounded-input/bounded-output  stable) 482 483  4t84 Uniformly  bounded  22 Uniformly  continuous 9 Uniformly  controllable  315 Uniformly  convergent  19 Uniformly  stable 448 453 454 455 459 461 470511512 Unimodular matrix  526 Unit impulse 74 Unit pulse or unit sample 68  178 Unit pulse or unit impulse response  70 Unit step: function  154 sequence 69 Unity  feedback  631 642 Unobservable  264 274 eigenvalues modes 268 subspace 220 253 258 See  also  Observability Unstable: complete instability 273 501 See also  Equilibrium  unstable van der Pol equation  16 88 Variation of constants formula  57  146  197 Vector  38 coordinate representation  100 linearly dependent  97 98 234 524 linearly  independent  98 234 524 normalized  439 null 38 unit 439 Vector space 37 38 97 basis 99  100  170 439 552 complex  38 dimension  99 finite dimensional  99 437 inner product  437 normed 42 438 439 null  100 444 real 38 Weierstrass M-test 21 See also  Infinite  series Without memory  66 Youla parameter  595 615 635 Zero 298 563 blocking property  307 decoupling  input/output  302 564 direction  306 at infinity  320 invariant  302 303 564 565 polynomial  302 303 564 565 system 301564  565 of transfer  functions  301 302 303 564  565 transmission  303 564 565 Zero-input equivalence  170  171 174  181 See  also  System  representations Zero-input response  146  165 176 See  also  Response Zero-order hold  183 See  also  Sampled  data  system Zero-state equivalence  170  171 173 181 See  also  System  representations Zero-state response  146  165 166  177 See  also  Response Zom's  lemma  26 z-transform  177 
